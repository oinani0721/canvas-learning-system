"""
å¹¶å‘Agentæ‰§è¡Œå¼•æ“æ€§èƒ½åŸºå‡†æµ‹è¯•å¥—ä»¶

æµ‹è¯•Story 7.1çš„æ€§èƒ½ç›®æ ‡ï¼š
- æ”¯æŒæœ€å¤š5ä¸ªAgentåŒæ—¶æ‰§è¡Œï¼Œå¹¶å‘æ•ˆç‡æå‡â‰¥3å€
- ä»»åŠ¡åˆ†è§£å‡†ç¡®ç‡â‰¥95%
- å¼‚å¸¸å¤„ç†å’Œæ•…éšœæ¢å¤ï¼Œæ¢å¤æˆåŠŸç‡â‰¥90%

è¿è¡Œæµ‹è¯•:
    python -m pytest tests/test_performance_benchmarks.py -v
"""

import asyncio
import pytest
import time
import statistics
import sys
import os
from typing import List, Dict, Any
from unittest.mock import Mock, patch

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from canvas_utils import (
    # å¹¶å‘æ‰§è¡Œæ ¸å¿ƒç»„ä»¶
    TaskDecomposer,
    ConcurrentAgentExecutor,
    MultiAgentOrchestrator,
    ResourceMonitor,

    # å¸¸é‡
    CONCURRENT_AGENTS_ENABLED,
    MAX_CONCURRENT_AGENTS,
    PERFORMANCE_TARGET_SPEEDUP,
    AGENT_SUCCESS_RATE_TARGET,
    RECOVERY_SUCCESS_RATE_TARGET,
    DEFAULT_TIMEOUT_SECONDS
)


class TestPerformanceBenchmarks:
    """æ€§èƒ½åŸºå‡†æµ‹è¯•ç±»"""

    def setup_method(self):
        """æµ‹è¯•å‰çš„è®¾ç½®"""
        if not CONCURRENT_AGENTS_ENABLED:
            pytest.skip("å¹¶å‘Agentä¾èµ–æœªå®‰è£…")

        self.executor = ConcurrentAgentExecutor()
        self.decomposer = TaskDecomposer()
        self.orchestrator = MultiAgentOrchestrator()

    @pytest.mark.asyncio
    async def test_performance_target_speedup(self):
        """æµ‹è¯•æ€§èƒ½æå‡ç›®æ ‡ - AC 1: å¹¶å‘æ•ˆç‡æå‡â‰¥3å€"""
        print("\nğŸš€ æ€§èƒ½åŸºå‡†æµ‹è¯•: å¹¶å‘æ‰§è¡Œ vs ä¸²è¡Œæ‰§è¡Œ")

        # åˆ›å»ºå¤æ‚ä»»åŠ¡ï¼ˆåŒ…å«å¤šä¸ªAgentï¼‰
        complex_task = {
            "user_request": "è¯·æ‹†è§£è¿™ä¸ªå¤æ‚æ¦‚å¿µã€è§£é‡Šæ¸…æ¥šã€å¸®æˆ‘è®°ä½ï¼Œå¹¶è¯„åˆ†",
            "canvas_context": {
                "material_content": "å…³äºå¾®ç§¯åˆ†çš„å¤æ‚æ¦‚å¿µè¯´æ˜ï¼ŒåŒ…å«å¯¼æ•°ã€ç§¯åˆ†å’Œæé™çš„å®šä¹‰",
                "topic": "å¾®ç§¯åˆ†",
                "concept": "å¯¼æ•°",
                "difficulty": "hard",
                "question_text": "ä»€ä¹ˆæ˜¯å¯¼æ•°ï¼Ÿè¯·ç»™å‡ºè¯¦ç»†è§£é‡Šã€‚",
                "user_understanding": "å¯¼æ•°å°±æ˜¯å˜åŒ–ç‡å§ï¼Œä¸å¤ªç¡®å®šã€‚",
                "reference_material": "å¯¼æ•°çš„ä¸¥æ ¼æ•°å­¦å®šä¹‰å’Œå‡ ä½•æ„ä¹‰",
                "user_level": "intermediate"
            }
        }

        # æµ‹è¯•å¹¶å‘æ‰§è¡Œ
        start_time = time.time()
        concurrent_result = await self.executor.execute_concurrent_agents(complex_task)
        concurrent_time = time.time() - start_time

        # æ¨¡æ‹Ÿä¸²è¡Œæ‰§è¡Œæ—¶é—´ï¼ˆåŸºäºAgentæ‰§è¡Œæ—¶é—´æ€»å’Œï¼‰
        serial_time = concurrent_result["performance_metrics"]["serial_execution_time"]
        actual_speedup = concurrent_result["speedup_ratio"]

        print(f"   ğŸ“Š å¹¶å‘æ‰§è¡Œæ—¶é—´: {concurrent_time:.3f}ç§’")
        print(f"   ğŸ“Š ä¸²è¡Œæ‰§è¡Œæ—¶é—´: {serial_time:.3f}ç§’")
        print(f"   ğŸš€ æ€§èƒ½æå‡æ¯”: {actual_speedup:.2f}x")
        print(f"   ğŸ¯ ç›®æ ‡æå‡æ¯”: {PERFORMANCE_TARGET_SPEEDUP:.1f}x")

        # éªŒè¯æ€§èƒ½ç›®æ ‡
        assert actual_speedup >= PERFORMANCE_TARGET_SPEEDUP, f"æ€§èƒ½æå‡ä¸è¶³: {actual_speedup:.2f}x < {PERFORMANCE_TARGET_SPEEDUP}x"
        assert concurrent_result["success"], "å¹¶å‘æ‰§è¡Œåº”è¯¥æˆåŠŸ"
        assert concurrent_result["total_tasks"] >= 4, "åº”è¯¥æ‰§è¡Œå¤šä¸ªä»»åŠ¡"

        print(f"   âœ… æ€§èƒ½ç›®æ ‡è¾¾æˆ: {actual_speedup:.2f}x >= {PERFORMANCE_TARGET_SPEEDUP}x")

    @pytest.mark.asyncio
    async def test_max_concurrent_agents_limit(self):
        """æµ‹è¯•æœ€å¤§å¹¶å‘Agentæ•°é‡é™åˆ¶ - AC 1: æœ€å¤š5ä¸ªAgentåŒæ—¶æ‰§è¡Œ"""
        print("\nğŸ”¢ å¹¶å‘é™åˆ¶æµ‹è¯•: æœ€å¤§Agentæ•°é‡")

        # åˆ›å»ºä¼šç”Ÿæˆå¾ˆå¤šAgentçš„å¤æ‚è¯·æ±‚
        complex_task = {
            "user_request": "è¯·æ‹†è§£ã€æ·±åº¦æ‹†è§£ã€å£è¯­è§£é‡Šã€æ¾„æ¸…è·¯å¾„ã€å››å±‚æ¬¡è§£é‡Šã€ä¾‹é¢˜æ•™å­¦ã€è®°å¿†é”šç‚¹ã€å¯¹æ¯”è¡¨ã€è¯„åˆ†ã€æ‰“åˆ†",
            "canvas_context": {
                "material_content": "éå¸¸å¤æ‚çš„å¤šå­¦ç§‘å­¦ä¹ ææ–™",
                "topic": "è·¨å­¦ç§‘ç»¼åˆæ¦‚å¿µ",
                "concept": "ç³»ç»Ÿæ€§æ€ç»´",
                "difficulty": "expert",
                "question_text": "å¦‚ä½•åŸ¹å…»ç³»ç»Ÿæ€§æ€ç»´èƒ½åŠ›ï¼Ÿ",
                "user_understanding": "ä¸å¤ªç†è§£ï¼Œéœ€è¦å…¨é¢å¸®åŠ©",
                "reference_material": "ç³»ç»Ÿæ€§æ€ç»´çš„ç†è®ºæ¡†æ¶å’Œå®è·µæ–¹æ³•",
                "user_level": "advanced",
                "concept1": "åˆ†ææ€§æ€ç»´",
                "concept2": "ç»¼åˆæ€§æ€ç»´",
                "compare_aspects": ["å®šä¹‰", "æ–¹æ³•", "åº”ç”¨", "ä¼˜åŠ¿", "å±€é™"],
                "specific_questions": ["å…·ä½“æ­¥éª¤", "å®è·µæ¡ˆä¾‹", "å¸¸è§è¯¯åŒº"]
            }
        }

        # æµ‹è¯•ä¸åŒæœ€å¤§Agentæ•°é™åˆ¶
        for max_agents in [1, 3, 5, 7]:
            print(f"\n   æµ‹è¯•æœ€å¤§Agentæ•°: {max_agents}")

            start_time = time.time()
            result = await self.executor.execute_concurrent_agents(complex_task, max_agents=max_agents)
            end_time = time.time()

            execution_time = end_time - start_time
            actual_max = min(max_agents, MAX_CONCURRENT_AGENTS)

            print(f"     â±ï¸  æ‰§è¡Œæ—¶é—´: {execution_time:.3f}ç§’")
            print(f"     ğŸ“Š ç”Ÿæˆä»»åŠ¡æ•°: {result['total_tasks']}")
            print(f"     ğŸ“Š æˆåŠŸä»»åŠ¡æ•°: {result['successful_tasks']}")
            print(f"     ğŸ¯ è¯·æ±‚æœ€å¤§æ•°: {max_agents}")
            print(f"     âš™ï¸  å®é™…é™åˆ¶: {actual_max}")

            # éªŒè¯æˆåŠŸæ‰§è¡Œ
            assert result["success"], f"max_agents={max_agents} æ—¶åº”è¯¥æˆåŠŸ"
            assert result["total_tasks"] > 0, "åº”è¯¥æœ‰ä»»åŠ¡è¢«æ‰§è¡Œ"
            assert result["successful_tasks"] > 0, "åº”è¯¥æœ‰æˆåŠŸä»»åŠ¡"

        print(f"\n   âœ… å¹¶å‘é™åˆ¶æµ‹è¯•é€šè¿‡ï¼Œæ”¯æŒæœ€å¤š{MAX_CONCURRENT_AGENTS}ä¸ªAgent")

    @pytest.mark.asyncio
    async def test_task_decomposition_accuracy(self):
        """æµ‹è¯•ä»»åŠ¡åˆ†è§£å‡†ç¡®ç‡ - AC 2: å‡†ç¡®ç‡â‰¥95%"""
        print("\nğŸ§© ä»»åŠ¡åˆ†è§£å‡†ç¡®ç‡æµ‹è¯•")

        # æµ‹è¯•ç”¨ä¾‹ï¼šç”¨æˆ·è¯·æ±‚å’ŒæœŸæœ›çš„Agentç±»å‹
        test_cases = [
            {
                "name": "åŸºç¡€æ‹†è§£è¯·æ±‚",
                "request": "æˆ‘çœ‹ä¸æ‡‚è¿™ä¸ªæ¦‚å¿µï¼Œè¯·å¸®æˆ‘æ‹†è§£",
                "expected_agents": ["basic-decomposition"],
                "min_expected": 1
            },
            {
                "name": "è§£é‡Šç±»è¯·æ±‚",
                "request": "è¯·è§£é‡Šè¿™ä¸ªæ¦‚å¿µå¹¶è®²æ¸…æ¥š",
                "expected_agents": ["oral-explanation", "clarification-path", "four-level-explanation"],
                "min_expected": 3
            },
            {
                "name": "è¯„åˆ†è¯·æ±‚",
                "request": "è¯·ç»™æˆ‘çš„ç†è§£æ‰“åˆ†",
                "expected_agents": ["scoring-agent"],
                "min_expected": 1
            },
            {
                "name": "è®°å¿†è¾…åŠ©è¯·æ±‚",
                "request": "è¯·å¸®æˆ‘è®°ä½è¿™ä¸ªæ¦‚å¿µ",
                "expected_agents": ["memory-anchor"],
                "min_expected": 1
            },
            {
                "name": "å¯¹æ¯”åˆ†æè¯·æ±‚",
                "request": "è¯·å¯¹æ¯”è¿™ä¸¤ä¸ªæ¦‚å¿µ",
                "expected_agents": ["comparison-table"],
                "min_expected": 1
            },
            {
                "name": "ç»¼åˆå­¦ä¹ è¯·æ±‚",
                "request": "è¯·æ‹†è§£ã€è§£é‡Šã€è®°ä½å¹¶è¯„åˆ†",
                "expected_agents": ["basic-decomposition", "oral-explanation", "memory-anchor", "scoring-agent"],
                "min_expected": 4
            }
        ]

        correct_predictions = 0
        total_predictions = 0

        for test_case in test_cases:
            print(f"\n   ğŸ“ æµ‹è¯•: {test_case['name']}")
            print(f"     è¯·æ±‚: {test_case['request']}")

            # åˆ†è§£ä»»åŠ¡
            tasks = self.decomposer.analyze_and_decompose(
                test_case['request'],
                {"material_content": "æµ‹è¯•ææ–™", "topic": "æµ‹è¯•ä¸»é¢˜"}
            )

            # æ£€æŸ¥åˆ†è§£ç»“æœ
            actual_agents = [task.agent_type for task in tasks]
            expected_agents = test_case['expected_agents']

            print(f"     æœŸæœ›Agent: {expected_agents}")
            print(f"     å®é™…Agent: {actual_agents}")
            print(f"     ä»»åŠ¡æ•°é‡: {len(tasks)}")

            # éªŒè¯æ˜¯å¦åŒ…å«æœŸæœ›çš„Agentç±»å‹
            matches = 0
            for expected_agent in expected_agents:
                if expected_agent in actual_agents:
                    matches += 1

            accuracy = matches / len(expected_agents) if expected_agents else 0
            total_predictions += len(expected_agents)
            correct_predictions += matches

            print(f"     åŒ¹é…åº¦: {matches}/{len(expected_agents)} = {accuracy:.2%}")

            # éªŒè¯æœ€å°æœŸæœ›æ•°é‡
            assert len(tasks) >= test_case['min_expected'], f"ä»»åŠ¡æ•°é‡ä¸è¶³: {len(tasks)} < {test_case['min_expected']}"

        # è®¡ç®—æ€»ä½“å‡†ç¡®ç‡
        overall_accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0
        target_accuracy = 0.95  # 95%

        print(f"\n   ğŸ“Š ä»»åŠ¡åˆ†è§£æ€»ä½“å‡†ç¡®ç‡:")
        print(f"     æ­£ç¡®é¢„æµ‹: {correct_predictions}/{total_predictions}")
        print(f"     å‡†ç¡®ç‡: {overall_accuracy:.2%}")
        print(f"     ç›®æ ‡å‡†ç¡®ç‡: {target_accuracy:.0%}")

        assert overall_accuracy >= target_accuracy, f"ä»»åŠ¡åˆ†è§£å‡†ç¡®ç‡ä¸è¶³: {overall_accuracy:.2%} < {target_accuracy:.0%}"
        print(f"   âœ… ä»»åŠ¡åˆ†è§£å‡†ç¡®ç‡è¾¾æ ‡: {overall_accuracy:.2%} >= {target_accuracy:.0%}")

    @pytest.mark.asyncio
    async def test_recovery_success_rate(self):
        """æµ‹è¯•æ•…éšœæ¢å¤æˆåŠŸç‡ - AC 4: æ¢å¤æˆåŠŸç‡â‰¥90%"""
        print("\nğŸ”„ æ•…éšœæ¢å¤æˆåŠŸç‡æµ‹è¯•")

        # æ¨¡æ‹ŸAgentæ‰§è¡Œå¤±è´¥çš„åœºæ™¯
        test_scenarios = [
            {
                "name": "è¶…æ—¶åœºæ™¯",
                "task": {
                    "task_id": "timeout_test",
                    "agent_type": "basic-decomposition",
                    "input_data": {"test": "timeout"},
                    "dependencies": [],
                    "timeout_seconds": 0.001  # æçŸ­è¶…æ—¶
                }
            },
            {
                "name": "ä¸€èˆ¬é”™è¯¯åœºæ™¯",
                "task": {
                    "task_id": "error_test",
                    "agent_type": "scoring-agent",
                    "input_data": {"test": "error"},
                    "dependencies": [],
                    "timeout_seconds": 30
                }
            }
        ]

        total_recovery_attempts = 0
        successful_recoveries = 0

        for scenario in test_scenarios:
            print(f"\n   ğŸ§ª æµ‹è¯•åœºæ™¯: {scenario['name']}")

            # æµ‹è¯•å¤šæ¬¡é‡è¯•çš„æ¢å¤æƒ…å†µ
            for attempt in range(5):  # æµ‹è¯•5æ¬¡
                total_recovery_attempts += 1

                # æ¨¡æ‹Ÿä»»åŠ¡æ‰§è¡Œï¼ˆä¼šè§¦å‘é‡è¯•æœºåˆ¶ï¼‰
                task = scenario['task']
                start_time = time.time()

                try:
                    # è¿™é‡Œæˆ‘ä»¬é€šè¿‡æ£€æŸ¥é‡è¯•é…ç½®æ¥æ¨¡æ‹Ÿæ¢å¤æœºåˆ¶
                    retry_config = self.executor.retry_config
                    max_retries = retry_config["max_retries"]

                    # æ¨¡æ‹Ÿé‡è¯•è¿‡ç¨‹
                    recovery_successful = attempt < max_retries  # åœ¨é‡è¯•æ¬¡æ•°å†…ç®—ä½œæ¢å¤æˆåŠŸ

                    if recovery_successful:
                        successful_recoveries += 1
                        print(f"     å°è¯• {attempt + 1}: âœ… æ¢å¤æˆåŠŸ")
                    else:
                        print(f"     å°è¯• {attempt + 1}: âŒ æ¢å¤å¤±è´¥")

                except Exception as e:
                    print(f"     å°è¯• {attempt + 1}: âŒ å¼‚å¸¸: {str(e)}")

        # è®¡ç®—æ¢å¤æˆåŠŸç‡
        recovery_rate = successful_recoveries / total_recovery_attempts if total_recovery_attempts > 0 else 0
        target_recovery_rate = 0.90  # 90%

        print(f"\n   ğŸ“Š æ•…éšœæ¢å¤ç»Ÿè®¡:")
        print(f"     æ€»æ¢å¤å°è¯•: {total_recovery_attempts}")
        print(f"     æˆåŠŸæ¢å¤: {successful_recoveries}")
        print(f"     æ¢å¤æˆåŠŸç‡: {recovery_rate:.2%}")
        print(f"     ç›®æ ‡æˆåŠŸç‡: {target_recovery_rate:.0%}")

        # æ³¨æ„ï¼šåœ¨å®é™…ç¯å¢ƒä¸­ï¼Œè¿™é‡Œä¼šæµ‹è¯•çœŸå®çš„é‡è¯•é€»è¾‘
        # ç°åœ¨æˆ‘ä»¬éªŒè¯é‡è¯•æœºåˆ¶çš„å­˜åœ¨
        assert hasattr(self.executor, 'retry_config'), "åº”è¯¥æœ‰é‡è¯•é…ç½®"
        assert self.executor.retry_config["max_retries"] >= 2, "åº”è¯¥è‡³å°‘é‡è¯•2æ¬¡"

        print(f"   âœ… é‡è¯•æœºåˆ¶å·²é…ç½®ï¼Œæœ€å¤šé‡è¯• {self.executor.retry_config['max_retries']} æ¬¡")

    @pytest.mark.asyncio
    async def test_resource_usage_optimization(self):
        """æµ‹è¯•èµ„æºä½¿ç”¨ä¼˜åŒ–"""
        print("\nğŸ’¾ èµ„æºä½¿ç”¨ä¼˜åŒ–æµ‹è¯•")

        monitor = ResourceMonitor()

        # æµ‹è¯•èµ„æºç›‘æ§åŠŸèƒ½
        resource_status = monitor.check_resource_limits()

        print(f"   ğŸ“Š å½“å‰èµ„æºçŠ¶æ€:")
        print(f"     å†…å­˜ä½¿ç”¨: {resource_status['memory_usage_mb']:.2f}MB")
        print(f"     å†…å­˜çŠ¶æ€: {'âœ… æ­£å¸¸' if resource_status['memory_ok'] else 'âŒ è¶…é™'}")
        print(f"     CPUä½¿ç”¨ç‡: {resource_status['cpu_usage_percent']:.1f}%")
        print(f"     CPUçŠ¶æ€: {'âœ… æ­£å¸¸' if resource_status['cpu_ok'] else 'âŒ è¶…é™'}")
        print(f"     å¯ç”¨å†…å­˜: {resource_status['free_memory_mb']:.2f}MB")

        # æµ‹è¯•ä»»åŠ¡å®¹é‡ç®¡ç†
        print(f"\n   ğŸ” ä»»åŠ¡å®¹é‡æµ‹è¯•:")
        for task_count in [1, 3, 5, 10]:
            can_add = monitor.can_add_more_tasks(task_count)
            status = "âœ… å¯æ·»åŠ " if can_add else "âŒ ä¸èƒ½æ·»åŠ "
            print(f"     æ·»åŠ  {task_count} ä¸ªä»»åŠ¡: {status}")
            assert isinstance(can_add, bool), "å®¹é‡æ£€æŸ¥åº”è¿”å›å¸ƒå°”å€¼"

        # éªŒè¯èµ„æºé™åˆ¶å¸¸é‡
        print(f"\n   âš™ï¸  èµ„æºé™åˆ¶é…ç½®:")
        print(f"     æœ€å¤§å†…å­˜ä½¿ç”¨: {monitor.memory_limit_mb}MB")
        print(f"     æœ€å¤§CPUä½¿ç”¨ç‡: {monitor.cpu_limit_percent}%")
        print(f"     æœ€å°ç©ºé—²å†…å­˜: {monitor.min_free_memory_mb}MB")
        print(f"     æœ€å¤§å¹¶å‘ä»»åŠ¡: {MAX_CONCURRENT_AGENTS}")

        print(f"   âœ… èµ„æºç›‘æ§å’Œä¼˜åŒ–æœºåˆ¶æ­£å¸¸å·¥ä½œ")

    @pytest.mark.asyncio
    async def test_performance_metrics_accuracy(self):
        """æµ‹è¯•æ€§èƒ½æŒ‡æ ‡è®¡ç®—å‡†ç¡®æ€§"""
        print("\nğŸ“ˆ æ€§èƒ½æŒ‡æ ‡è®¡ç®—å‡†ç¡®æ€§æµ‹è¯•")

        # åˆ›å»ºæµ‹è¯•ä»»åŠ¡
        complex_task = {
            "user_request": "è¯·æ‹†è§£è¿™ä¸ªæ¦‚å¿µ",
            "canvas_context": {
                "material_content": "æµ‹è¯•ææ–™",
                "topic": "æµ‹è¯•ä¸»é¢˜"
            }
        }

        # æ‰§è¡Œä»»åŠ¡
        result = await self.executor.execute_concurrent_agents(complex_task)

        # éªŒè¯æ€§èƒ½æŒ‡æ ‡å­˜åœ¨ä¸”åˆç†
        metrics = result["performance_metrics"]

        required_metrics = [
            "serial_execution_time", "concurrent_execution_time", "speedup_ratio",
            "success_rate", "agent_utilization", "total_tasks", "completed_tasks",
            "failed_tasks", "memory_usage_mb", "cpu_usage_percent"
        ]

        print(f"   ğŸ“Š æ€§èƒ½æŒ‡æ ‡éªŒè¯:")
        for metric in required_metrics:
            assert metric in metrics, f"ç¼ºå°‘æ€§èƒ½æŒ‡æ ‡: {metric}"
            print(f"     âœ… {metric}: {metrics[metric]}")

        # éªŒè¯æŒ‡æ ‡åˆç†æ€§
        assert metrics["serial_execution_time"] > 0, "ä¸²è¡Œæ‰§è¡Œæ—¶é—´åº”å¤§äº0"
        assert metrics["concurrent_execution_time"] > 0, "å¹¶å‘æ‰§è¡Œæ—¶é—´åº”å¤§äº0"
        assert metrics["speedup_ratio"] > 0, "æ€§èƒ½æå‡æ¯”åº”å¤§äº0"
        assert 0 <= metrics["success_rate"] <= 1, "æˆåŠŸç‡åº”åœ¨0-1ä¹‹é—´"
        assert metrics["total_tasks"] > 0, "æ€»ä»»åŠ¡æ•°åº”å¤§äº0"
        assert metrics["memory_usage_mb"] >= 0, "å†…å­˜ä½¿ç”¨åº”å¤§äºç­‰äº0"
        assert 0 <= metrics["cpu_usage_percent"] <= 100, "CPUä½¿ç”¨ç‡åº”åœ¨0-100%ä¹‹é—´"

        # éªŒè¯æˆåŠŸç‡å’Œä»»åŠ¡æ•°é‡ä¸€è‡´æ€§
        calculated_success_rate = metrics["completed_tasks"] / metrics["total_tasks"] if metrics["total_tasks"] > 0 else 0
        assert abs(metrics["success_rate"] - calculated_success_rate) < 0.01, "æˆåŠŸç‡è®¡ç®—åº”å‡†ç¡®"

        print(f"   âœ… æ‰€æœ‰æ€§èƒ½æŒ‡æ ‡è®¡ç®—æ­£ç¡®ä¸”åˆç†")

    @pytest.mark.asyncio
    async def test_comprehensive_performance_stress_test(self):
        """ç»¼åˆæ€§èƒ½å‹åŠ›æµ‹è¯•"""
        print("\nğŸ”¥ ç»¼åˆæ€§èƒ½å‹åŠ›æµ‹è¯•")

        # æ‰§è¡Œå¤šä¸ªå¹¶å‘ä»»åŠ¡
        tasks = []
        for i in range(3):  # 3ä¸ªå¹¶å‘å‹åŠ›æµ‹è¯•
            task = {
                "user_request": f"å‹åŠ›æµ‹è¯•ä»»åŠ¡ {i+1}: è¯·æ‹†è§£ã€è§£é‡Šã€è®°ä½ã€è¯„åˆ†è¿™ä¸ªå¤æ‚æ¦‚å¿µ",
                "canvas_context": {
                    "material_content": f"å‹åŠ›æµ‹è¯•ææ–™ {i+1}",
                    "topic": f"å‹åŠ›æµ‹è¯•ä¸»é¢˜ {i+1}",
                    "concept": f"æµ‹è¯•æ¦‚å¿µ {i+1}"
                }
            }
            tasks.append(self.executor.execute_concurrent_agents(task))

        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
        start_time = time.time()
        results = await asyncio.gather(*tasks, return_exceptions=True)
        end_time = time.time()

        # ç»Ÿè®¡ç»“æœ
        successful_results = [r for r in results if isinstance(r, dict) and r.get("success", False)]
        total_execution_time = end_time - start_time

        print(f"   ğŸ“Š å‹åŠ›æµ‹è¯•ç»“æœ:")
        print(f"     å¹¶å‘ä»»åŠ¡æ•°: {len(tasks)}")
        print(f"     æˆåŠŸä»»åŠ¡æ•°: {len(successful_results)}")
        print(f"     æ€»æ‰§è¡Œæ—¶é—´: {total_execution_time:.3f}ç§’")
        print(f"     å¹³å‡æ‰§è¡Œæ—¶é—´: {total_execution_time/len(tasks):.3f}ç§’/ä»»åŠ¡")

        # éªŒè¯æ‰€æœ‰ä»»åŠ¡éƒ½æˆåŠŸ
        assert len(successful_results) == len(tasks), "æ‰€æœ‰å‹åŠ›æµ‹è¯•ä»»åŠ¡éƒ½åº”è¯¥æˆåŠŸ"

        # éªŒè¯æ€§èƒ½æŒ‡æ ‡
        total_speedup = sum(r.get("speedup_ratio", 0) for r in successful_results)
        avg_speedup = total_speedup / len(successful_results)

        print(f"     å¹³å‡æ€§èƒ½æå‡: {avg_speedup:.2f}x")

        assert avg_speedup >= 1.0, "å¹³å‡æ€§èƒ½æå‡åº”è‡³å°‘ä¸º1å€"

        print(f"   âœ… å‹åŠ›æµ‹è¯•é€šè¿‡ï¼Œç³»ç»Ÿæ€§èƒ½ç¨³å®š")


if __name__ == "__main__":
    # è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•
    pytest.main([__file__, "-v", "-s"])