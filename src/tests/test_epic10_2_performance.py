"""
Story 10.2.5 (AC2): Epic 10.2 æ€§èƒ½åŸºå‡†æµ‹è¯•

Performance Benchmarking Tests for Epic 10.2: Async Parallel Execution Engine

æµ‹è¯•ç›®æ ‡:
- éªŒè¯10èŠ‚ç‚¹å¤„ç†æ—¶é—´ â‰¤ 15ç§’
- éªŒè¯20èŠ‚ç‚¹å¤„ç†æ—¶é—´ â‰¤ 30ç§’
- éªŒè¯50èŠ‚ç‚¹å¤„ç†æ—¶é—´ â‰¤ 60ç§’
- è®°å½•è¯¦ç»†æ€§èƒ½æ•°æ®ç”¨äºåˆ†æ

Author: Dev Agent (James)
Date: 2025-11-04
Version: v1.0
"""

import asyncio
import time
from pathlib import Path
from typing import Dict

import pytest

# ============================================================================
# Performance Benchmarks
# ============================================================================

@pytest.mark.asyncio
@pytest.mark.benchmark
async def test_performance_benchmarks():
    """
    AC2: æ€§èƒ½åŸºå‡†æµ‹è¯•

    æµ‹è¯•10/20/50èŠ‚ç‚¹çš„å¤„ç†æ€§èƒ½ï¼Œè®°å½•è¯¦ç»†çš„æ€§èƒ½æŒ‡æ ‡

    æ€§èƒ½ç›®æ ‡:
    - 10èŠ‚ç‚¹ â‰¤ 15ç§’
    - 20èŠ‚ç‚¹ â‰¤ 30ç§’
    - 50èŠ‚ç‚¹ â‰¤ 60ç§’
    """
    print("\n" + "="*70)
    print("AC2: æ€§èƒ½åŸºå‡†æµ‹è¯•")
    print("="*70)

    # å¯¼å…¥ä¾èµ–
    try:
        from command_handlers.intelligent_parallel_handler import IntelligentParallelCommandHandler
    except ImportError as e:
        pytest.skip(f"Required modules not available: {e}")

    # æµ‹è¯•æ•°æ®è·¯å¾„
    test_cases = [
        ("test_data/canvas_10_nodes.canvas", 10, 15),
        ("test_data/canvas_20_nodes.canvas", 20, 30),
        ("test_data/canvas_50_nodes.canvas", 50, 60),
    ]

    results = {}

    for canvas_path, node_count, time_limit in test_cases:
        # éªŒè¯æ–‡ä»¶å­˜åœ¨
        if not Path(canvas_path).exists():
            print(f"âš ï¸  è·³è¿‡: {canvas_path} ä¸å­˜åœ¨")
            continue

        print(f"\n{'='*70}")
        print(f"æµ‹è¯•: {node_count}èŠ‚ç‚¹")
        print(f"æ–‡ä»¶: {canvas_path}")
        print(f"æ€§èƒ½ç›®æ ‡: â‰¤{time_limit}ç§’")
        print(f"{'='*70}")

        # åˆ›å»ºHandler
        handler = IntelligentParallelCommandHandler()

        # æ‰§è¡Œæµ‹è¯•
        start_time = time.time()

        result = await handler.execute_async(
            canvas_path,
            options={
                "auto": True,
                "max": 12,
                "grouping": "intelligent",
                "verbose": False
            }
        )

        elapsed_time = time.time() - start_time

        # è®°å½•ç»“æœ
        results[f"{node_count}_nodes"] = {
            "elapsed_time": elapsed_time,
            "time_limit": time_limit,
            "passed": elapsed_time <= time_limit,
            "result": result
        }

        # è¾“å‡ºç»“æœ
        status = "âœ… PASS" if elapsed_time <= time_limit else "âŒ FAIL"
        print(f"\n{status} {node_count}èŠ‚ç‚¹æµ‹è¯•")
        print(f"â±ï¸  æ‰§è¡Œæ—¶é—´: {elapsed_time:.2f}s")
        print(f"ğŸ¯ æ€§èƒ½ç›®æ ‡: {time_limit}s")
        print(f"ğŸ“Š æ€§èƒ½æ¯”ç‡: {elapsed_time/time_limit*100:.1f}%")

        # éªŒè¯æ€§èƒ½ç›®æ ‡
        assert elapsed_time <= time_limit, f"{node_count}èŠ‚ç‚¹è¶…æ—¶: {elapsed_time:.2f}s > {time_limit}s"

    # ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
    print("\n" + "="*70)
    print("ğŸ“Š æ€§èƒ½åŸºå‡†æµ‹è¯•æ€»ç»“")
    print("="*70)

    for key, data in results.items():
        node_count = key.replace("_nodes", "")
        print(f"\n{node_count}èŠ‚ç‚¹:")
        print(f"  â±ï¸  æ‰§è¡Œæ—¶é—´: {data['elapsed_time']:.2f}s")
        print(f"  ğŸ¯ æ€§èƒ½ç›®æ ‡: {data['time_limit']}s")
        print(f"  âœ… æµ‹è¯•ç»“æœ: {'é€šè¿‡' if data['passed'] else 'å¤±è´¥'}")

    print("\n" + "="*70)
    print("ğŸ‰ æ‰€æœ‰æ€§èƒ½åŸºå‡†æµ‹è¯•é€šè¿‡ï¼")
    print("="*70)

    return results


# ============================================================================
# Canvas Read/Write Performance Tests
# ============================================================================

@pytest.mark.benchmark
def test_canvas_read_performance():
    """
    æµ‹è¯•Canvasæ–‡ä»¶è¯»å–æ€§èƒ½

    ç›®æ ‡: å•æ¬¡è¯»å– < 0.1ç§’
    """
    print("\n" + "="*70)
    print("Canvasæ–‡ä»¶è¯»å–æ€§èƒ½æµ‹è¯•")
    print("="*70)

    try:
        from canvas_utils import CanvasJSONOperator
    except ImportError as e:
        pytest.skip(f"canvas_utils not available: {e}")

    canvas_path = "test_data/canvas_20_nodes.canvas"

    if not Path(canvas_path).exists():
        pytest.skip(f"Test file not found: {canvas_path}")

    # æ‰§è¡Œ100æ¬¡è¯»å–
    iterations = 100
    start_time = time.time()

    for i in range(iterations):
        canvas_data = CanvasJSONOperator.read_canvas(canvas_path)

    elapsed_time = time.time() - start_time
    avg_time = elapsed_time / iterations

    print(f"ğŸ“Š æ‰§è¡Œæ¬¡æ•°: {iterations}")
    print(f"â±ï¸  æ€»æ—¶é—´: {elapsed_time:.3f}s")
    print(f"â±ï¸  å¹³å‡æ—¶é—´: {avg_time*1000:.2f}ms")
    print("ğŸ¯ æ€§èƒ½ç›®æ ‡: <100ms")

    assert avg_time < 0.1, f"è¯»å–æ€§èƒ½ä¸è¾¾æ ‡: {avg_time*1000:.2f}ms > 100ms"

    print("âœ… Canvasè¯»å–æ€§èƒ½æµ‹è¯•é€šè¿‡")


@pytest.mark.benchmark
def test_canvas_write_performance():
    """
    æµ‹è¯•Canvasæ–‡ä»¶å†™å…¥æ€§èƒ½

    ç›®æ ‡: å•æ¬¡å†™å…¥ < 0.5ç§’
    """
    print("\n" + "="*70)
    print("Canvasæ–‡ä»¶å†™å…¥æ€§èƒ½æµ‹è¯•")
    print("="*70)

    try:
        from canvas_utils import CanvasJSONOperator
    except ImportError as e:
        pytest.skip(f"canvas_utils not available: {e}")

    canvas_path = "test_data/canvas_20_nodes.canvas"

    if not Path(canvas_path).exists():
        pytest.skip(f"Test file not found: {canvas_path}")

    # è¯»å–Canvasæ•°æ®
    canvas_data = CanvasJSONOperator.read_canvas(canvas_path)

    # æ‰§è¡Œ10æ¬¡å†™å…¥
    iterations = 10
    start_time = time.time()

    for i in range(iterations):
        CanvasJSONOperator.write_canvas(canvas_path, canvas_data)

    elapsed_time = time.time() - start_time
    avg_time = elapsed_time / iterations

    print(f"ğŸ“Š æ‰§è¡Œæ¬¡æ•°: {iterations}")
    print(f"â±ï¸  æ€»æ—¶é—´: {elapsed_time:.3f}s")
    print(f"â±ï¸  å¹³å‡æ—¶é—´: {avg_time*1000:.2f}ms")
    print("ğŸ¯ æ€§èƒ½ç›®æ ‡: <500ms")

    assert avg_time < 0.5, f"å†™å…¥æ€§èƒ½ä¸è¾¾æ ‡: {avg_time*1000:.2f}ms > 500ms"

    print("âœ… Canvaså†™å…¥æ€§èƒ½æµ‹è¯•é€šè¿‡")


# ============================================================================
# Async Engine Performance Tests
# ============================================================================

@pytest.mark.asyncio
@pytest.mark.benchmark
async def test_async_engine_overhead():
    """
    æµ‹è¯•AsyncExecutionEngineçš„å¼€é”€

    éªŒè¯å¼‚æ­¥æ‰§è¡Œå¼•æ“æœ¬èº«çš„æ€§èƒ½å¼€é”€æ˜¯å¦åˆç†
    """
    print("\n" + "="*70)
    print("AsyncExecutionEngineå¼€é”€æµ‹è¯•")
    print("="*70)

    try:
        from command_handlers.async_execution_engine import AsyncExecutionEngine, AsyncTask
    except ImportError as e:
        pytest.skip(f"AsyncExecutionEngine not available: {e}")

    # åˆ›å»ºexecutorå‡½æ•° (å¤„ç†ä»»åŠ¡)
    async def simple_executor(task: AsyncTask) -> Dict:
        """ç®€å•çš„æµ‹è¯•executor"""
        await asyncio.sleep(0.01)  # æ¨¡æ‹Ÿ10msä»»åŠ¡
        return {
            "task_id": task.task_id,
            "success": True,
            "data": f"Task {task.task_id} completed"
        }

    # æµ‹è¯•åœºæ™¯: 10ä¸ªå¹¶å‘ä»»åŠ¡
    task_count = 10
    engine = AsyncExecutionEngine(max_concurrency=12)

    # åˆ›å»ºä»»åŠ¡
    tasks = []
    for i in range(task_count):
        task = AsyncTask(
            task_id=f"task-{i}",
            agent_name="test-agent",
            node_data={"index": i, "content": f"Test task {i}"},
            priority=1
        )
        tasks.append(task)

    # æ‰§è¡Œä»»åŠ¡
    print(f"ğŸš€ æ‰§è¡Œ{task_count}ä¸ªå¹¶å‘ä»»åŠ¡...")
    start_time = time.time()

    result = await engine.execute_parallel(tasks, simple_executor)

    elapsed_time = time.time() - start_time

    # ç†è®ºæœ€å°æ—¶é—´ (å¦‚æœå®Œç¾å¹¶è¡Œ) = 10ms
    # å®é™…æ—¶é—´åº”è¯¥æ¥è¿‘10msï¼Œå¼€é”€åº”è¯¥å¾ˆå°
    theoretical_min = 0.01  # 10ms
    overhead = elapsed_time - theoretical_min

    print(f"ğŸ“Š ä»»åŠ¡æ•°é‡: {task_count}")
    print(f"â±ï¸  æ‰§è¡Œæ—¶é—´: {elapsed_time*1000:.2f}ms")
    print(f"ğŸ¯ ç†è®ºæœ€å°: {theoretical_min*1000:.2f}ms")
    print(f"ğŸ“‰ å¼•æ“å¼€é”€: {overhead*1000:.2f}ms")
    print(f"âœ… æˆåŠŸä»»åŠ¡: {result['success']}/{task_count}")

    # éªŒè¯å¼€é”€åˆç† (åº”è¯¥å°äº50ms)
    assert overhead < 0.05, f"AsyncEngineå¼€é”€è¿‡å¤§: {overhead*1000:.2f}ms > 50ms"

    print("âœ… AsyncExecutionEngineå¼€é”€æµ‹è¯•é€šè¿‡")


# ============================================================================
# Test Execution Summary
# ============================================================================

if __name__ == "__main__":
    """æœ¬åœ°æµ‹è¯•æ‰§è¡Œå…¥å£"""
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                   Epic 10.2 Performance Benchmarks                       â•‘
â•‘                   Story 10.2.5 (AC2): æ€§èƒ½åŸºå‡†æµ‹è¯•                       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

æµ‹è¯•è¦†ç›–:
- âœ… AC2: 10/20/50èŠ‚ç‚¹æ€§èƒ½åŸºå‡†æµ‹è¯•
- âœ… Canvasè¯»å–æ€§èƒ½æµ‹è¯•
- âœ… Canvaså†™å…¥æ€§èƒ½æµ‹è¯•
- âœ… AsyncEngineå¼€é”€æµ‹è¯•

è¿è¡Œæ–¹å¼:
  pytest tests/test_epic10_2_performance.py -v
  pytest tests/test_epic10_2_performance.py -v -m "benchmark"

""")
    pytest.main([__file__, "-v", "--tb=short"])
