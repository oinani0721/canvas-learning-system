"""
Canvasæ€§èƒ½åŸºå‡†ç®¡ç†å’Œå›å½’æ£€æµ‹ç³»ç»Ÿ

è¯¥æ¨¡å—æä¾›æ€§èƒ½åŸºå‡†çš„å»ºç«‹ã€ç®¡ç†ã€æ¯”è¾ƒå’Œå›å½’æ£€æµ‹åŠŸèƒ½ï¼Œ
ç¡®ä¿Canvasç³»ç»Ÿæ€§èƒ½çš„æŒç»­ç›‘æ§å’Œä¼˜åŒ–ã€‚

Author: Canvas Learning System Team
Version: 1.0
Created: 2025-10-22
"""

import json
import os
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, field
import uuid

from test_canvas_performance import (
    PerformanceTestResult,
    StressTestResult,
    TestEnvironment
)


@dataclass
class PerformanceBaseline:
    """æ€§èƒ½åŸºå‡†æ•°æ®æ¨¡å‹"""
    baseline_id: str
    created_at: str
    test_environment: TestEnvironment
    baseline_metrics: Dict[str, Any]
    version: str = "1.0"
    description: str = ""


@dataclass
class RegressionTestResult:
    """å›å½’æµ‹è¯•ç»“æœ"""
    test_timestamp: str
    baseline_id: str
    current_results: List[PerformanceTestResult]
    regression_detected: bool
    performance_changes: Dict[str, Any]
    recommendations: List[str]
    overall_score: float  # 0-100åˆ†ï¼Œè¶Šé«˜è¶Šå¥½


class PerformanceBaselineManager:
    """æ€§èƒ½åŸºå‡†ç®¡ç†å™¨"""

    def __init__(self, baseline_file: str = "tests/performance_baseline.json"):
        """
        åˆå§‹åŒ–åŸºå‡†ç®¡ç†å™¨

        Args:
            baseline_file: åŸºå‡†æ•°æ®æ–‡ä»¶è·¯å¾„
        """
        self.baseline_file = Path(baseline_file)
        self.baseline_file.parent.mkdir(parents=True, exist_ok=True)
        self.baselines = self._load_baselines()

        # æ€§èƒ½å›å½’æ£€æµ‹é˜ˆå€¼
        self.regression_thresholds = {
            "processing_time_degradation": 20.0,  # å¤„ç†æ—¶é—´å¢åŠ 20%è§†ä¸ºå›å½’
            "memory_usage_increase": 30.0,        # å†…å­˜ä½¿ç”¨å¢åŠ 30%è§†ä¸ºå›å½’
            "layout_quality_decrease": 15.0,     # å¸ƒå±€è´¨é‡ä¸‹é™15%è§†ä¸ºå›å½’
            "overlap_count_increase": 25.0,      # é‡å æ•°é‡å¢åŠ 25%è§†ä¸ºå›å½’
            "success_rate_decrease": 5.0         # æˆåŠŸç‡ä¸‹é™5%è§†ä¸ºå›å½’
        }

        # æ€§èƒ½æ”¹è¿›å¥–åŠ±é˜ˆå€¼
        self.improvement_thresholds = {
            "processing_time_improvement": 15.0,   # å¤„ç†æ—¶é—´å‡å°‘15%è§†ä¸ºæ”¹è¿›
            "memory_usage_reduction": 20.0,       # å†…å­˜ä½¿ç”¨å‡å°‘20%è§†ä¸ºæ”¹è¿›
            "layout_quality_increase": 10.0,      # å¸ƒå±€è´¨é‡æå‡10%è§†ä¸ºæ”¹è¿›
            "overlap_count_reduction": 30.0,      # é‡å æ•°é‡å‡å°‘30%è§†ä¸ºæ”¹è¿›
            "success_rate_increase": 2.0          # æˆåŠŸç‡æå‡2%è§†ä¸ºæ”¹è¿›
        }

    def _load_baselines(self) -> Dict[str, PerformanceBaseline]:
        """åŠ è½½å·²æœ‰çš„åŸºå‡†æ•°æ®"""
        if not self.baseline_file.exists():
            return {}

        try:
            with open(self.baseline_file, 'r', encoding='utf-8') as f:
                data = json.load(f)

            baselines = {}
            for baseline_id, baseline_data in data.items():
                baseline = PerformanceBaseline(
                    baseline_id=baseline_data["baseline_id"],
                    created_at=baseline_data["created_at"],
                    test_environment=TestEnvironment(**baseline_data["test_environment"]),
                    baseline_metrics=baseline_data["baseline_metrics"],
                    version=baseline_data.get("version", "1.0"),
                    description=baseline_data.get("description", "")
                )
                baselines[baseline_id] = baseline

            return baselines

        except Exception as e:
            print(f"è­¦å‘Š: åŠ è½½åŸºå‡†æ•°æ®å¤±è´¥ - {e}")
            return {}

    def _save_baselines(self) -> None:
        """ä¿å­˜åŸºå‡†æ•°æ®åˆ°æ–‡ä»¶"""
        try:
            data = {}
            for baseline_id, baseline in self.baselines.items():
                data[baseline_id] = {
                    "baseline_id": baseline.baseline_id,
                    "created_at": baseline.created_at,
                    "test_environment": {
                        "python_version": baseline.test_environment.python_version,
                        "platform": baseline.test_environment.platform,
                        "cpu_count": baseline.test_environment.cpu_count,
                        "memory_gb": baseline.test_environment.memory_gb,
                        "canvas_utils_version": baseline.test_environment.canvas_utils_version,
                        "test_machine_id": baseline.test_environment.test_machine_id
                    },
                    "baseline_metrics": baseline.baseline_metrics,
                    "version": baseline.version,
                    "description": baseline.description
                }

            with open(self.baseline_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)

        except Exception as e:
            print(f"é”™è¯¯: ä¿å­˜åŸºå‡†æ•°æ®å¤±è´¥ - {e}")

    def establish_baseline(self,
                          test_results: List[PerformanceTestResult],
                          test_environment: TestEnvironment,
                          description: str = "") -> str:
        """
        å»ºç«‹æ€§èƒ½åŸºå‡†

        Args:
            test_results: æ€§èƒ½æµ‹è¯•ç»“æœåˆ—è¡¨
            test_environment: æµ‹è¯•ç¯å¢ƒä¿¡æ¯
            description: åŸºå‡†æè¿°

        Returns:
            str: åŸºå‡†ID
        """
        # è¿‡æ»¤æˆåŠŸçš„æµ‹è¯•ç»“æœ
        successful_results = [r for r in test_results if r.success]

        if not successful_results:
            raise ValueError("æ²¡æœ‰æˆåŠŸçš„æµ‹è¯•ç»“æœï¼Œæ— æ³•å»ºç«‹åŸºå‡†")

        # è®¡ç®—åŸºå‡†æŒ‡æ ‡
        baseline_metrics = self._calculate_baseline_metrics(successful_results)

        # åˆ›å»ºåŸºå‡†
        baseline_id = f"baseline_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{uuid.uuid4().hex[:8]}"
        baseline = PerformanceBaseline(
            baseline_id=baseline_id,
            created_at=datetime.now().isoformat(),
            test_environment=test_environment,
            baseline_metrics=baseline_metrics,
            description=description
        )

        # ä¿å­˜åŸºå‡†
        self.baselines[baseline_id] = baseline
        self._save_baselines()

        print(f"æ€§èƒ½åŸºå‡†å·²å»ºç«‹: {baseline_id}")
        return baseline_id

    def compare_with_baseline(self,
                             current_results: List[PerformanceTestResult],
                             baseline_id: Optional[str] = None) -> RegressionTestResult:
        """
        ä¸åŸºå‡†è¿›è¡Œæ¯”è¾ƒï¼Œæ£€æµ‹æ€§èƒ½å›å½’

        Args:
            current_results: å½“å‰æµ‹è¯•ç»“æœ
            baseline_id: åŸºå‡†IDï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨æœ€æ–°åŸºå‡†

        Returns:
            RegressionTestResult: å›å½’æµ‹è¯•ç»“æœ
        """
        if not self.baselines:
            raise ValueError("æ²¡æœ‰å¯ç”¨çš„åŸºå‡†æ•°æ®")

        # é€‰æ‹©åŸºå‡†
        if baseline_id is None:
            # é€‰æ‹©æœ€æ–°çš„åŸºå‡†
            baseline_id = max(self.baselines.keys(),
                             key=lambda k: self.baselines[k].created_at)

        baseline = self.baselines[baseline_id]

        # åˆ†ææ€§èƒ½å˜åŒ–
        performance_changes = self._analyze_performance_changes(
            current_results, baseline.baseline_metrics
        )

        # æ£€æµ‹å›å½’
        regression_detected = self._detect_regression(performance_changes)

        # ç”Ÿæˆå»ºè®®
        recommendations = self._generate_recommendations(
            performance_changes, regression_detected
        )

        # è®¡ç®—æ€»ä½“è¯„åˆ†
        overall_score = self._calculate_overall_score(performance_changes)

        return RegressionTestResult(
            test_timestamp=datetime.now().isoformat(),
            baseline_id=baseline_id,
            current_results=current_results,
            regression_detected=regression_detected,
            performance_changes=performance_changes,
            recommendations=recommendations,
            overall_score=overall_score
        )

    def update_baseline(self,
                       new_results: List[PerformanceTestResult],
                       reason: str,
                       baseline_id: Optional[str] = None) -> str:
        """
        æ›´æ–°æ€§èƒ½åŸºå‡†

        Args:
            new_results: æ–°çš„æµ‹è¯•ç»“æœ
            reason: æ›´æ–°åŸå› 
            baseline_id: è¦æ›´æ–°çš„åŸºå‡†IDï¼Œå¦‚æœä¸ºNoneåˆ™åˆ›å»ºæ–°åŸºå‡†

        Returns:
            str: åŸºå‡†ID
        """
        if baseline_id is None:
            # åˆ›å»ºæ–°åŸºå‡†
            from test_canvas_performance import CanvasPerformanceTester
            tester = CanvasPerformanceTester()
            return self.establish_baseline(
                new_results,
                tester.test_environment,
                f"åŸºå‡†æ›´æ–°: {reason}"
            )

        if baseline_id not in self.baselines:
            raise ValueError(f"åŸºå‡†IDä¸å­˜åœ¨: {baseline_id}")

        # æ›´æ–°ç°æœ‰åŸºå‡†çš„æŒ‡æ ‡
        baseline = self.baselines[baseline_id]
        successful_results = [r for r in new_results if r.success]

        if successful_results:
            baseline.baseline_metrics = self._calculate_baseline_metrics(successful_results)
            baseline.description = f"{baseline.description} | æ›´æ–°: {reason}"
            self._save_baselines()

        return baseline_id

    def get_baseline_metrics(self, baseline_id: Optional[str] = None) -> Dict[str, Any]:
        """
        è·å–å½“å‰åŸºå‡†æŒ‡æ ‡

        Args:
            baseline_id: åŸºå‡†IDï¼Œå¦‚æœä¸ºNoneåˆ™è¿”å›æœ€æ–°åŸºå‡†

        Returns:
            Dict[str, Any]: åŸºå‡†æŒ‡æ ‡
        """
        if not self.baselines:
            return {}

        if baseline_id is None:
            baseline_id = max(self.baselines.keys(),
                             key=lambda k: self.baselines[k].created_at)

        return self.baselines[baseline_id].baseline_metrics

    def list_baselines(self) -> List[Dict[str, Any]]:
        """åˆ—å‡ºæ‰€æœ‰åŸºå‡†"""
        return [
            {
                "baseline_id": baseline.baseline_id,
                "created_at": baseline.created_at,
                "version": baseline.version,
                "description": baseline.description,
                "test_environment": {
                    "python_version": baseline.test_environment.python_version,
                    "platform": baseline.test_environment.platform,
                    "cpu_count": baseline.test_environment.cpu_count,
                    "memory_gb": baseline.test_environment.memory_gb
                }
            }
            for baseline in sorted(
                self.baselines.values(),
                key=lambda b: b.created_at,
                reverse=True
            )
        ]

    def _calculate_baseline_metrics(self, results: List[PerformanceTestResult]) -> Dict[str, Any]:
        """è®¡ç®—åŸºå‡†æŒ‡æ ‡"""
        if not results:
            return {}

        # æŒ‰èŠ‚ç‚¹æ•°é‡åˆ†ç»„
        node_groups = {}
        for result in results:
            node_count = result.node_count
            if node_count not in node_groups:
                node_groups[node_count] = []
            node_groups[node_count].append(result)

        # è®¡ç®—å„èŠ‚ç‚¹æ•°ç»„çš„åŸºå‡†æŒ‡æ ‡
        baseline_metrics = {
            "created_at": datetime.now().isoformat(),
            "total_tests": len(results),
            "success_rate": len(results) / len(results) * 100,
            "node_groups": {},
            "overall_stats": {}
        }

        processing_times = []
        memory_usages = []
        quality_scores = []

        for node_count, group_results in node_groups.items():
            group_times = [r.processing_time_ms for r in group_results]
            group_memories = [r.memory_usage_mb for r in group_results]
            group_qualities = [r.layout_quality_score for r in group_results]

            baseline_metrics["node_groups"][str(node_count)] = {
                "count": len(group_results),
                "processing_time": {
                    "avg_ms": sum(group_times) / len(group_times),
                    "min_ms": min(group_times),
                    "max_ms": max(group_times),
                    "p95_ms": sorted(group_times)[int(len(group_times) * 0.95)]
                },
                "memory_usage": {
                    "avg_mb": sum(group_memories) / len(group_memories),
                    "min_mb": min(group_memories),
                    "max_mb": max(group_memories)
                },
                "layout_quality": {
                    "avg_score": sum(group_qualities) / len(group_qualities),
                    "min_score": min(group_qualities),
                    "max_score": max(group_qualities)
                }
            }

            processing_times.extend(group_times)
            memory_usages.extend(group_memories)
            quality_scores.extend(group_qualities)

        # è®¡ç®—æ€»ä½“ç»Ÿè®¡
        baseline_metrics["overall_stats"] = {
            "processing_time": {
                "avg_ms": sum(processing_times) / len(processing_times),
                "min_ms": min(processing_times),
                "max_ms": max(processing_times),
                "p95_ms": sorted(processing_times)[int(len(processing_times) * 0.95)]
            },
            "memory_usage": {
                "avg_mb": sum(memory_usages) / len(memory_usages),
                "min_mb": min(memory_usages),
                "max_mb": max(memory_usages)
            },
            "layout_quality": {
                "avg_score": sum(quality_scores) / len(quality_scores),
                "min_score": min(quality_scores),
                "max_score": max(quality_scores)
            }
        }

        return baseline_metrics

    def _analyze_performance_changes(self,
                                    current_results: List[PerformanceTestResult],
                                    baseline_metrics: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†ææ€§èƒ½å˜åŒ–"""
        successful_current = [r for r in current_results if r.success]

        if not successful_current:
            return {"error": "æ²¡æœ‰æˆåŠŸçš„å½“å‰æµ‹è¯•ç»“æœ"}

        # æŒ‰èŠ‚ç‚¹æ•°é‡åˆ†ç»„å½“å‰ç»“æœ
        current_node_groups = {}
        for result in successful_current:
            node_count = result.node_count
            if node_count not in current_node_groups:
                current_node_groups[node_count] = []
            current_node_groups[node_count].append(result)

        changes = {
            "overall_comparison": {},
            "node_group_comparisons": {},
            "performance_trends": {}
        }

        # æ€»ä½“æ¯”è¾ƒ
        current_times = [r.processing_time_ms for r in successful_current]
        current_memories = [r.memory_usage_mb for r in successful_current]
        current_qualities = [r.layout_quality_score for r in successful_current]

        baseline_stats = baseline_metrics.get("overall_stats", {})

        changes["overall_comparison"] = {
            "processing_time_change_pct": self._calculate_percentage_change(
                sum(current_times) / len(current_times),
                baseline_stats.get("processing_time", {}).get("avg_ms")
            ),
            "memory_usage_change_pct": self._calculate_percentage_change(
                sum(current_memories) / len(current_memories),
                baseline_stats.get("memory_usage", {}).get("avg_mb")
            ),
            "layout_quality_change_pct": self._calculate_percentage_change(
                sum(current_qualities) / len(current_qualities),
                baseline_stats.get("layout_quality", {}).get("avg_score")
            )
        }

        # èŠ‚ç‚¹ç»„æ¯”è¾ƒ
        baseline_node_groups = baseline_metrics.get("node_groups", {})
        for node_count, current_group in current_node_groups.items():
            node_count_str = str(node_count)
            if node_count_str in baseline_node_groups:
                baseline_group = baseline_node_groups[node_count_str]

                current_times = [r.processing_time_ms for r in current_group]
                current_memories = [r.memory_usage_mb for r in current_group]
                current_qualities = [r.layout_quality_score for r in current_group]

                changes["node_group_comparisons"][node_count_str] = {
                    "processing_time_change_pct": self._calculate_percentage_change(
                        sum(current_times) / len(current_times),
                        baseline_group.get("processing_time", {}).get("avg_ms")
                    ),
                    "memory_usage_change_pct": self._calculate_percentage_change(
                        sum(current_memories) / len(current_memories),
                        baseline_group.get("memory_usage", {}).get("avg_mb")
                    ),
                    "layout_quality_change_pct": self._calculate_percentage_change(
                        sum(current_qualities) / len(current_qualities),
                        baseline_group.get("layout_quality", {}).get("avg_score")
                    )
                }

        return changes

    def _calculate_percentage_change(self, current: float, baseline: Optional[float]) -> float:
        """è®¡ç®—ç™¾åˆ†æ¯”å˜åŒ–"""
        if baseline is None or baseline == 0:
            return 0.0
        return ((current - baseline) / baseline) * 100

    def _detect_regression(self, performance_changes: Dict[str, Any]) -> bool:
        """æ£€æµ‹æ€§èƒ½å›å½’"""
        overall = performance_changes.get("overall_comparison", {})

        # æ£€æŸ¥å„é¡¹æŒ‡æ ‡æ˜¯å¦è¶…è¿‡å›å½’é˜ˆå€¼
        regressions = []

        processing_change = overall.get("processing_time_change_pct", 0)
        if processing_change > self.regression_thresholds["processing_time_degradation"]:
            regressions.append(f"å¤„ç†æ—¶é—´å¢åŠ {processing_change:.1f}%")

        memory_change = overall.get("memory_usage_change_pct", 0)
        if memory_change > self.regression_thresholds["memory_usage_increase"]:
            regressions.append(f"å†…å­˜ä½¿ç”¨å¢åŠ {memory_change:.1f}%")

        quality_change = overall.get("layout_quality_change_pct", 0)
        if quality_change < -self.regression_thresholds["layout_quality_decrease"]:
            regressions.append(f"å¸ƒå±€è´¨é‡ä¸‹é™{abs(quality_change):.1f}%")

        return len(regressions) > 0

    def _generate_recommendations(self,
                                 performance_changes: Dict[str, Any],
                                 regression_detected: bool) -> List[str]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []
        overall = performance_changes.get("overall_comparison", {})

        processing_change = overall.get("processing_time_change_pct", 0)
        memory_change = overall.get("memory_usage_change_pct", 0)
        quality_change = overall.get("layout_quality_change_pct", 0)

        if regression_detected:
            recommendations.append("âš ï¸ æ£€æµ‹åˆ°æ€§èƒ½å›å½’ï¼Œå»ºè®®æ£€æŸ¥æœ€è¿‘çš„ä»£ç å˜æ›´")
        else:
            recommendations.append("âœ… æœªæ£€æµ‹åˆ°æ˜æ˜¾çš„æ€§èƒ½å›å½’")

        if processing_change > self.improvement_thresholds["processing_time_improvement"]:
            recommendations.append("ğŸš€ å¤„ç†æ—¶é—´æ˜¾è‘—æ”¹å–„ï¼Œç»§ç»­ä¿æŒ")
        elif processing_change > self.regression_thresholds["processing_time_degradation"]:
            recommendations.append("ğŸŒ å¤„ç†æ—¶é—´æ˜¾è‘—å¢åŠ ï¼Œå»ºè®®ä¼˜åŒ–ç®—æ³•æˆ–æ£€æŸ¥ç“¶é¢ˆ")

        if memory_change > self.regression_thresholds["memory_usage_increase"]:
            recommendations.append("ğŸ’¾ å†…å­˜ä½¿ç”¨å¢åŠ ï¼Œå»ºè®®æ£€æŸ¥å†…å­˜æ³„æ¼æˆ–ä¼˜åŒ–æ•°æ®ç»“æ„")
        elif memory_change < -self.improvement_thresholds["memory_usage_reduction"]:
            recommendations.append("ğŸ¯ å†…å­˜ä½¿ç”¨æ˜¾è‘—ä¼˜åŒ–ï¼Œè¡¨ç°è‰¯å¥½")

        if quality_change < -self.regression_thresholds["layout_quality_decrease"]:
            recommendations.append("ğŸ“ å¸ƒå±€è´¨é‡ä¸‹é™ï¼Œå»ºè®®æ£€æŸ¥å¸ƒå±€ç®—æ³•å‚æ•°")
        elif quality_change > self.improvement_thresholds["layout_quality_increase"]:
            recommendations.append("âœ¨ å¸ƒå±€è´¨é‡æå‡ï¼Œä¼˜åŒ–æ•ˆæœæ˜æ˜¾")

        return recommendations

    def _calculate_overall_score(self, performance_changes: Dict[str, Any]) -> float:
        """è®¡ç®—æ€»ä½“æ€§èƒ½è¯„åˆ†ï¼ˆ0-100ï¼‰"""
        overall = performance_changes.get("overall_comparison", {})

        processing_change = overall.get("processing_time_change_pct", 0)
        memory_change = overall.get("memory_usage_change_pct", 0)
        quality_change = overall.get("layout_quality_change_pct", 0)

        # åŸºç¡€åˆ†æ•°
        score = 50.0

        # å¤„ç†æ—¶é—´è¯„åˆ†ï¼ˆ40%æƒé‡ï¼‰
        if processing_change < -self.improvement_thresholds["processing_time_improvement"]:
            score += 20  # æ˜¾è‘—æ”¹è¿›
        elif processing_change > self.regression_thresholds["processing_time_degradation"]:
            score -= 25  # æ˜¾è‘—å›å½’
        else:
            score += processing_change * 0.5  # çº¿æ€§è°ƒæ•´

        # å†…å­˜ä½¿ç”¨è¯„åˆ†ï¼ˆ30%æƒé‡ï¼‰
        if memory_change < -self.improvement_thresholds["memory_usage_reduction"]:
            score += 15  # æ˜¾è‘—æ”¹è¿›
        elif memory_change > self.regression_thresholds["memory_usage_increase"]:
            score -= 20  # æ˜¾è‘—å›å½’
        else:
            score += memory_change * 0.3  # çº¿æ€§è°ƒæ•´

        # å¸ƒå±€è´¨é‡è¯„åˆ†ï¼ˆ30%æƒé‡ï¼‰
        if quality_change > self.improvement_thresholds["layout_quality_increase"]:
            score += 15  # æ˜¾è‘—æ”¹è¿›
        elif quality_change < -self.regression_thresholds["layout_quality_decrease"]:
            score -= 15  # æ˜¾è‘—å›å½’
        else:
            score += quality_change * 0.3  # çº¿æ€§è°ƒæ•´

        return max(0.0, min(100.0, score))


# å¯¼å‡ºä¸»è¦ç±»
__all__ = [
    'PerformanceBaselineManager',
    'PerformanceBaseline',
    'RegressionTestResult'
]