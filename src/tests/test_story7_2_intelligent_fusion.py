#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Story 7.2: æ™ºèƒ½ç»“æœèåˆå¼•æ“æµ‹è¯•å¥—ä»¶

æµ‹è¯•æ™ºèƒ½ç»“æœèåˆå¼•æ“çš„æ‰€æœ‰åŠŸèƒ½ï¼š
- Task 1: å†²çªæ£€æµ‹å’Œè§£å†³ç®—æ³•
- Task 2: åŸºäºç½®ä¿¡åº¦çš„åŠ æƒèåˆ
- Task 3: ä¿¡æ¯å®Œæ•´æ€§ä¿æŠ¤æœºåˆ¶
- Task 4: èåˆè¿‡ç¨‹å¯è§£é‡Šæ€§
- Task 5: æ€§èƒ½ä¼˜åŒ–å’Œé›†æˆæµ‹è¯•
"""

import asyncio
import os

# å¯¼å…¥Story 7.2çš„æ‰€æœ‰ç±»
import sys
import time
import unittest

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from canvas_utils import (
    ConfidenceBasedFusion,
    ConflictDetectionEngine,
    FusionProcessTransparency,
    InformationIntegrityProtection,
    IntelligentResultFusionEngine,
    intelligent_result_fusion,
)


class TestConflictDetectionEngine(unittest.TestCase):
    """æµ‹è¯•å†²çªæ£€æµ‹å’Œè§£å†³å¼•æ“ - Task 1"""

    def setUp(self):
        """è®¾ç½®æµ‹è¯•ç¯å¢ƒ"""
        self.conflict_detector = ConflictDetectionEngine()

    def test_detect_contradictory_conclusions(self):
        """æµ‹è¯•çŸ›ç›¾ç»“è®ºæ£€æµ‹"""
        agent_results = [
            {
                'agent_type': 'oral-explanation',
                'analysis_summary': 'å»ºè®®é‡‡ç”¨ä¸»åŠ¨å­¦ä¹ æ–¹æ³•ï¼Œè¿™æ ·å¯ä»¥æé«˜å­¦ä¹ æ•ˆç‡ã€‚',
                'confidence': 0.8
            },
            {
                'agent_type': 'basic-decomposition',
                'analysis_summary': 'ä¸å»ºè®®é‡‡ç”¨ä¸»åŠ¨å­¦ä¹ æ–¹æ³•ï¼Œè¿™æ ·ä¼šé™ä½å­¦ä¹ æ•ˆç‡ã€‚',
                'confidence': 0.7
            }
        ]

        conflicts = self.conflict_detector.detect_conflicts(agent_results)

        # éªŒè¯æ£€æµ‹åˆ°çŸ›ç›¾ç»“è®º
        contradictory_conflicts = [c for c in conflicts if c['type'] == 'contradictory_conclusions']
        self.assertGreater(len(contradictory_conflicts), 0)

        # éªŒè¯å†²çªè¯¦æƒ…
        conflict = contradictory_conflicts[0]
        self.assertIn('agents', conflict)
        self.assertIn('agent_types', conflict)
        self.assertEqual(set(conflict['agent_types']), {'oral-explanation', 'basic-decomposition'})

    def test_detect_duplicate_suggestions(self):
        """æµ‹è¯•é‡å¤å»ºè®®æ£€æµ‹"""
        agent_results = [
            {
                'agent_type': 'oral-explanation',
                'agent_recommendations': [
                    {
                        'agent_type': 'clarification-path',
                        'confidence': 0.8,
                        'reason': 'å»ºè®®æ·±å…¥ç†è§£æ¦‚å¿µçš„å®šä¹‰',
                        'target_nodes': ['concept1']
                    }
                ]
            },
            {
                'agent_type': 'clarification-path',
                'agent_recommendations': [
                    {
                        'agent_type': 'clarification-path',
                        'confidence': 0.7,
                        'reason': 'æ·±å…¥ç†è§£æ¦‚å¿µå®šä¹‰å¾ˆé‡è¦',
                        'target_nodes': ['concept1']
                    }
                ]
            }
        ]

        conflicts = self.conflict_detector.detect_conflicts(agent_results)

        # éªŒè¯æ£€æµ‹åˆ°é‡å¤å»ºè®®
        duplicate_conflicts = [c for c in conflicts if c['type'] == 'duplicate_suggestions']
        self.assertGreater(len(duplicate_conflicts), 0)

    def test_detect_inconsistent_recommendations(self):
        """æµ‹è¯•ä¸ä¸€è‡´æ¨èæ£€æµ‹"""
        agent_results = [
            {
                'agent_type': 'scoring-agent',
                'agent_recommendations': [
                    {
                        'agent_type': 'oral-explanation',
                        'confidence': 0.9,
                        'reason': 'å»ºè®®ç”Ÿæˆå£è¯­åŒ–è§£é‡Š'
                    },
                    {
                        'agent_type': 'comparison-table',
                        'confidence': 0.4,
                        'reason': 'å»ºè®®ç”Ÿæˆå¯¹æ¯”è¡¨'
                    }
                ]
            },
            {
                'agent_type': 'canvas-orchestrator',
                'agent_recommendations': [
                    {
                        'agent_type': 'oral-explanation',
                        'confidence': 0.3,
                        'reason': 'ä¸å»ºè®®ç”Ÿæˆå£è¯­åŒ–è§£é‡Š'
                    }
                ]
            }
        ]

        conflicts = self.conflict_detector.detect_conflicts(agent_results)

        # éªŒè¯æ£€æµ‹åˆ°ä¸ä¸€è‡´æ¨è
        inconsistent_conflicts = [c for c in conflicts if c['type'] == 'inconsistent_recommendations']
        self.assertGreater(len(inconsistent_conflicts), 0)

    def test_detect_semantic_overlap(self):
        """æµ‹è¯•è¯­ä¹‰é‡å æ£€æµ‹"""
        agent_results = [
            {
                'agent_type': 'oral-explanation',
                'analysis_summary': 'è¿™ä¸ªæ¦‚å¿µçš„ç†è§£éœ€è¦ä»å¤šä¸ªè§’åº¦åˆ†æï¼ŒåŒ…æ‹¬å®šä¹‰ã€æ–¹æ³•å’Œåº”ç”¨',
                'confidence': 0.8
            },
            {
                'agent_type': 'clarification-path',
                'analysis_summary': 'ç†è§£è¿™ä¸ªæ¦‚å¿µè¦è€ƒè™‘å®šä¹‰ç†è§£ã€æ–¹æ³•ç†è§£å’Œåº”ç”¨ç†è§£',
                'confidence': 0.7
            }
        ]

        conflicts = self.conflict_detector.detect_conflicts(agent_results)

        # éªŒè¯æ£€æµ‹åˆ°è¯­ä¹‰é‡å 
        semantic_conflicts = [c for c in conflicts if c['type'] == 'semantic_overlap']
        self.assertGreater(len(semantic_conflicts), 0)

    def test_no_conflicts_detection(self):
        """æµ‹è¯•æ— å†²çªæƒ…å†µ"""
        agent_results = [
            {
                'agent_type': 'oral-explanation',
                'analysis_summary': 'ç”Ÿæˆ800-1200è¯çš„å£è¯­åŒ–è§£é‡Š',
                'confidence': 0.8
            },
            {
                'agent_type': 'comparison-table',
                'analysis_summary': 'ç”Ÿæˆç»“æ„åŒ–çš„å¯¹æ¯”è¡¨æ ¼',
                'confidence': 0.7
            }
        ]

        conflicts = self.conflict_detector.detect_conflicts(agent_results)

        # éªŒè¯æ²¡æœ‰æ£€æµ‹åˆ°å†²çª
        self.assertEqual(len(conflicts), 0)


class TestConfidenceBasedFusion(unittest.TestCase):
    """æµ‹è¯•åŸºäºç½®ä¿¡åº¦çš„åŠ æƒèåˆ - Task 2"""

    def setUp(self):
        """è®¾ç½®æµ‹è¯•ç¯å¢ƒ"""
        self.confidence_fusion = ConfidenceBasedFusion()

    def test_conflict_resolution_by_confidence(self):
        """æµ‹è¯•åŸºäºç½®ä¿¡åº¦çš„å†²çªè§£å†³"""
        agent_results = [
            {
                'agent_type': 'oral-explanation',
                'analysis_summary': 'å»ºè®®é‡‡ç”¨æ–¹æ³•A',
                'confidence': 0.9
            },
            {
                'agent_type': 'basic-decomposition',
                'analysis_summary': 'å»ºè®®é‡‡ç”¨æ–¹æ³•B',
                'confidence': 0.6
            }
        ]

        conflicts = [
            {
                'type': 'contradictory_conclusions',
                'agents': [0, 1],
                'agent_types': ['oral-explanation', 'basic-decomposition'],
                'confidence1': 0.9,
                'confidence2': 0.6,
                'conclusion1': 'å»ºè®®é‡‡ç”¨æ–¹æ³•A',
                'conclusion2': 'å»ºè®®é‡‡ç”¨æ–¹æ³•B'
            }
        ]

        fusion_result = self.confidence_fusion.fuse_results(agent_results, conflicts)

        # éªŒè¯é€‰æ‹©äº†é«˜ç½®ä¿¡åº¦çš„ç»“æœ
        self.assertEqual(fusion_result['resolved_conflicts'], 1)
        self.assertIn('fused_result', fusion_result)

        # éªŒè¯èåˆæŠ¥å‘Š
        self.assertIn('fusion_report', fusion_result)
        self.assertIn('Agent oral-explanation', fusion_result['fusion_report'])

    def test_duplicate_suggestions_merging(self):
        """æµ‹è¯•é‡å¤å»ºè®®åˆå¹¶"""
        agent_results = [
            {
                'agent_type': 'oral-explanation',
                'agent_recommendations': [
                    {
                        'agent_type': 'clarification-path',
                        'confidence': 0.8,
                        'reason': 'æ·±å…¥ç†è§£æ¦‚å¿µ',
                        'target_nodes': ['node1']
                    }
                ]
            },
            {
                'agent_type': 'clarification-path',
                'agent_recommendations': [
                    {
                        'agent_type': 'clarification-path',
                        'confidence': 0.7,
                        'reason': 'æ·±å…¥ç†è§£æ¦‚å¿µ',
                        'target_nodes': ['node1']
                    }
                ]
            }
        ]

        conflicts = [
            {
                'type': 'duplicate_suggestions',
                'agents': [0, 1],
                'agent_types': ['oral-explanation', 'clarification-path'],
                'suggestion1': 'æ·±å…¥ç†è§£æ¦‚å¿µ',
                'suggestion2': 'æ·±å…¥ç†è§£æ¦‚å¿µ'
            }
        ]

        fusion_result = self.confidence_fusion.fuse_results(agent_results, conflicts)

        # éªŒè¯åˆå¹¶åçš„æ¨èå»é‡
        fused_result = fusion_result['fused_result']
        recommendations = fused_result.get('agent_recommendations', [])

        # åº”è¯¥åªæœ‰ä¸€ä¸ªæ¨èï¼ˆå»é‡åï¼‰
        clarif_recs = [r for r in recommendations if r['agent_type'] == 'clarification-path']
        self.assertEqual(len(clarif_recs), 1)

    def test_weighted_fusion_calculation(self):
        """æµ‹è¯•åŠ æƒèåˆè®¡ç®—"""
        agent_results = [
            {
                'agent_type': 'oral-explanation',
                'analysis_summary': 'è¿™æ˜¯ä¸€ä¸ªé‡è¦çš„å­¦ä¹ æ¦‚å¿µ',
                'confidence': 0.9,
                'agent_recommendations': [
                    {
                        'agent_type': 'oral-explanation',
                        'confidence': 0.9,
                        'reason': 'ç”Ÿæˆå£è¯­åŒ–è§£é‡Š'
                    }
                ]
            },
            {
                'agent_type': 'clarification-path',
                'analysis_summary': 'è¿™æ˜¯ä¸€ä¸ªå…³é”®çš„å­¦ä¹ æ¦‚å¿µ',
                'confidence': 0.6,
                'agent_recommendations': [
                    {
                        'agent_type': 'clarification-path',
                        'confidence': 0.6,
                        'reason': 'ç”Ÿæˆæ¾„æ¸…è·¯å¾„'
                    }
                ]
            }
        ]

        fusion_result = self.confidence_fusion.fuse_results(agent_results, [])

        # éªŒè¯åŠ æƒèåˆç»“æœ
        fused_result = fusion_result['fused_result']

        # éªŒè¯å¹³å‡ç½®ä¿¡åº¦è®¡ç®—
        expected_avg_confidence = (0.9 + 0.6) / 2
        self.assertAlmostEqual(fused_result['average_confidence'], expected_avg_confidence, places=2)

        # éªŒè¯èåˆæƒé‡
        self.assertIn('total_weight', fused_result)
        self.assertIn('fusion_method', fused_result)
        self.assertEqual(fused_result['fusion_method'], 'confidence_weighted_average')

    def test_agent_contributions_analysis(self):
        """æµ‹è¯•Agentè´¡çŒ®åˆ†æ"""
        agent_results = [
            {
                'agent_type': 'oral-explanation',
                'confidence': 0.9,
                'agent_recommendations': [
                    {'agent_type': 'oral-explanation', 'reason': 'è§£é‡Š1'},
                    {'agent_type': 'oral-explanation', 'reason': 'è§£é‡Š2'}
                ],
                'analysis_summary': 'è¿™æ˜¯ä¸€ä¸ªè¯¦ç»†çš„åˆ†ææ‘˜è¦ï¼ŒåŒ…å«å¤šä¸ªé‡è¦è§‚ç‚¹å’Œå»ºè®®'
            },
            {
                'agent_type': 'clarification-path',
                'confidence': 0.6,
                'agent_recommendations': [
                    {'agent_type': 'clarification-path', 'reason': 'æ¾„æ¸…1'}
                ],
                'analysis_summary': 'ç®€çŸ­æ‘˜è¦'
            }
        ]

        fusion_result = self.confidence_fusion.fuse_results(agent_results, [])

        # éªŒè¯Agentè´¡çŒ®åˆ†æ
        contributions = fusion_result['agent_contributions']
        self.assertIn('oral-explanation', contributions)
        self.assertIn('clarification-path', contributions)

        # éªŒè¯è´¡çŒ®åˆ†æ•°è®¡ç®—
        oral_contrib = contributions['oral-explanation']
        self.assertEqual(oral_contrib['confidence'], 0.9)
        self.assertEqual(oral_contrib['recommendations_count'], 2)
        self.assertGreater(oral_contrib['analysis_length'], 20)


class TestInformationIntegrityProtection(unittest.TestCase):
    """æµ‹è¯•ä¿¡æ¯å®Œæ•´æ€§ä¿æŠ¤æœºåˆ¶ - Task 3"""

    def setUp(self):
        """è®¾ç½®æµ‹è¯•ç¯å¢ƒ"""
        self.integrity_protector = InformationIntegrityProtection()

    def test_lost_insights_detection(self):
        """æµ‹è¯•ä¸¢å¤±è§è§£æ£€æµ‹"""
        original_results = [
            {
                'agent_type': 'oral-explanation',
                'analysis_summary': 'å»ºè®®é‡‡ç”¨ä¸»åŠ¨å­¦ä¹ æ³•ï¼Œè¿™æ˜¯æé«˜æ•ˆç‡çš„å…³é”®æ–¹æ³•ã€‚æ³¨æ„è¦ç»“åˆå®è·µåº”ç”¨ã€‚',
                'confidence': 0.8,
                'agent_recommendations': [
                    {
                        'agent_type': 'oral-explanation',
                        'confidence': 0.8,
                        'reason': 'ç”Ÿæˆè¯¦ç»†çš„å£è¯­åŒ–è§£é‡Š',
                        'target_nodes': ['node1']
                    }
                ]
            }
        ]

        fused_result = {
            'analysis_summary': 'å»ºè®®é‡‡ç”¨ä¸»åŠ¨å­¦ä¹ æ³•',
            'agent_recommendations': [
                {
                    'agent_type': 'oral-explanation',
                    'confidence': 0.8,
                    'reason': 'ç”Ÿæˆè§£é‡Š',
                    'target_nodes': ['node1']
                }
            ]
        }

        protected_result = self.integrity_protector.protect_integrity(original_results, fused_result)

        # éªŒè¯æ£€æµ‹åˆ°ä¸¢å¤±çš„è§è§£
        integrity_info = protected_result['integrity_protection']
        self.assertGreater(integrity_info['lost_insights_count'], 0)
        self.assertIn('integrity_report', integrity_info)

    def test_diversity_preservation_evaluation(self):
        """æµ‹è¯•å¤šæ ·æ€§ä¿æŒè¯„ä¼°"""
        original_results = [
            {
                'agent_type': 'oral-explanation',
                'analysis_summary': 'å£è¯­åŒ–è§£é‡Šæ–¹æ³•ï¼ŒåŒ…æ‹¬ç”ŸåŠ¨ä¾‹å­å’Œå¸¸è§è¯¯åŒº',
                'agent_recommendations': [
                    {'agent_type': 'oral-explanation', 'reason': 'ç”Ÿæˆå£è¯­åŒ–å†…å®¹'}
                ]
            },
            {
                'agent_type': 'comparison-table',
                'analysis_summary': 'ç»“æ„åŒ–å¯¹æ¯”æ–¹æ³•ï¼ŒåŒ…æ‹¬å¤šç»´åº¦æ¯”è¾ƒ',
                'agent_recommendations': [
                    {'agent_type': 'comparison-table', 'reason': 'ç”Ÿæˆå¯¹æ¯”è¡¨æ ¼'}
                ]
            }
        ]

        fused_result = {
            'analysis_summary': 'ç»¼åˆè§£é‡Šæ–¹æ³•',
            'agent_recommendations': [
                {'agent_type': 'oral-explanation', 'reason': 'ç”Ÿæˆå£è¯­åŒ–å†…å®¹'},
                {'agent_type': 'comparison-table', 'reason': 'ç”Ÿæˆå¯¹æ¯”è¡¨æ ¼'}
            ]
        }

        protected_result = self.integrity_protector.protect_integrity(original_results, fused_result)

        # éªŒè¯å¤šæ ·æ€§åˆ†æ•°
        integrity_info = protected_result['integrity_protection']
        diversity_score = integrity_info['diversity_score']
        self.assertGreaterEqual(diversity_score, 0.0)
        self.assertLessEqual(diversity_score, 1.0)

    def test_information_restoration(self):
        """æµ‹è¯•ä¿¡æ¯æ¢å¤"""
        original_results = [
            {
                'agent_type': 'memory-anchor',
                'analysis_summary': 'ä½¿ç”¨ç”ŸåŠ¨çš„ç±»æ¯”å¸®åŠ©è®°å¿†',
                'confidence': 0.9,
                'agent_recommendations': [
                    {
                        'agent_type': 'memory-anchor',
                        'confidence': 0.9,
                        'reason': 'åˆ›å»ºè®°å¿†é”šç‚¹ï¼šå°†æ¦‚å¿µæ¯”ä½œå¤§æ ‘',
                        'target_nodes': ['concept1']
                    }
                ]
            }
        ]

        fused_result = {
            'analysis_summary': 'åŸºæœ¬å†…å®¹æ‘˜è¦',
            'agent_recommendations': []
        }

        protected_result = self.integrity_protector.protect_integrity(original_results, fused_result)

        # éªŒè¯é«˜ç½®ä¿¡åº¦è§è§£è¢«æ¢å¤
        restored_recommendations = protected_result['agent_recommendations']
        restored_insights = [r for r in restored_recommendations if r.get('restored')]

        self.assertGreater(len(restored_insights), 0)
        self.assertIn('è®°å¿†é”šç‚¹', restored_insights[0]['reason'])

    def test_diversity_optimization(self):
        """æµ‹è¯•å¤šæ ·æ€§ä¼˜åŒ–"""
        # ä½å¤šæ ·æ€§æƒ…å†µ
        original_results = [
            {
                'agent_type': 'oral-explanation',
                'analysis_summary': 'å•ä¸€æ–¹æ³•çš„è§£é‡Š',
                'agent_recommendations': [
                    {'agent_type': 'oral-explanation', 'reason': 'ç”Ÿæˆè§£é‡Š'}
                ]
            }
        ]

        fused_result = {
            'analysis_summary': 'å•ä¸€æ–¹æ³•è§£é‡Š',
            'agent_recommendations': [
                {'agent_type': 'oral-explanation', 'reason': 'ç”Ÿæˆè§£é‡Š'}
            ]
        }

        protected_result = self.integrity_protector.protect_integrity(original_results, fused_result)

        # éªŒè¯å¤šæ ·æ€§å¢å¼º
        recommendations = protected_result['agent_recommendations']
        diversity_enhanced = [r for r in recommendations if r.get('diversity_enhanced')]

        # åº”è¯¥æ·»åŠ å¤šæ ·æ€§å¢å¼ºæ¨è
        self.assertGreater(len(diversity_enhanced), 0)


class TestFusionProcessTransparency(unittest.TestCase):
    """æµ‹è¯•èåˆè¿‡ç¨‹å¯è§£é‡Šæ€§ - Task 4"""

    def setUp(self):
        """è®¾ç½®æµ‹è¯•ç¯å¢ƒ"""
        self.transparency_generator = FusionProcessTransparency()

    def test_executive_summary_generation(self):
        """æµ‹è¯•æ‰§è¡Œæ‘˜è¦ç”Ÿæˆ"""
        original_results = [
            {
                'agent_type': 'oral-explanation',
                'confidence': 0.9
            },
            {
                'agent_type': 'clarification-path',
                'confidence': 0.7
            }
        ]

        conflicts = [
            {
                'type': 'duplicate_suggestions',
                'agents': [0, 1],
                'agent_types': ['oral-explanation', 'clarification-path'],
                'resolved': True
            }
        ]

        fused_result = {
            'average_confidence': 0.8,
            'agent_recommendations': [
                {'agent_type': 'oral-explanation', 'confidence': 0.9},
                {'agent_type': 'clarification-path', 'confidence': 0.7}
            ]
        }

        fusion_report = "èåˆå®ŒæˆæŠ¥å‘Š"

        explanation = self.transparency_generator.generate_fusion_explanation(
            original_results, conflicts, fused_result, fusion_report
        )

        # éªŒè¯æ‰§è¡Œæ‘˜è¦
        executive_summary = explanation['executive_summary']
        self.assertIn('å‚ä¸Agentæ•°é‡', executive_summary)
        self.assertIn('æ£€æµ‹åˆ°å†²çª', executive_summary)
        self.assertIn('èåˆç»“æœç½®ä¿¡åº¦', executive_summary)

    def test_detailed_process_explanation(self):
        """æµ‹è¯•è¯¦ç»†è¿‡ç¨‹è§£é‡Š"""
        original_results = [
            {
                'agent_type': 'oral-explanation',
                'confidence': 0.9,
                'agent_recommendations': [
                    {'agent_type': 'oral-explanation', 'reason': 'è§£é‡Š1'},
                    {'agent_type': 'oral-explanation', 'reason': 'è§£é‡Š2'}
                ]
            },
            {
                'agent_type': 'clarification-path',
                'confidence': 0.6,
                'agent_recommendations': [
                    {'agent_type': 'clarification-path', 'reason': 'æ¾„æ¸…1'}
                ]
            }
        ]

        conflicts = [
            {
                'type': 'semantic_overlap',
                'agents': [0, 1],
                'agent_types': ['oral-explanation', 'clarification-path'],
                'resolved': True,
                'resolution_strategy': 'information_preservation'
            }
        ]

        fused_result = {
            'fusion_method': 'confidence_weighted_average',
            'total_weight': 2.3,
            'integrity_protection': {
                'diversity_score': 0.8,
                'lost_insights_count': 0
            }
        }

        fusion_report = "æµ‹è¯•æŠ¥å‘Š"

        explanation = self.transparency_generator.generate_fusion_explanation(
            original_results, conflicts, fused_result, fusion_report
        )

        # éªŒè¯è¯¦ç»†è¿‡ç¨‹
        detailed_process = explanation['detailed_process']
        self.assertIsInstance(detailed_process, list)
        self.assertGreater(len(detailed_process), 0)

        # éªŒè¯åŒ…å«å…³é”®æ­¥éª¤
        process_text = '\n'.join(detailed_process)
        self.assertIn('æ­¥éª¤1: åˆå§‹Agentç»“æœåˆ†æ', process_text)
        self.assertIn('æ­¥éª¤2: æ™ºèƒ½å†²çªæ£€æµ‹', process_text)
        self.assertIn('æ­¥éª¤3: ç½®ä¿¡åº¦åŠ æƒèåˆ', process_text)

    def test_visual_representation(self):
        """æµ‹è¯•å¯è§†åŒ–è¡¨ç¤º"""
        original_results = [
            {
                'agent_type': 'oral-explanation',
                'confidence': 0.9
            },
            {
                'agent_type': 'clarification-path',
                'confidence': 0.7
            }
        ]

        conflicts = [
            {
                'type': 'duplicate_suggestions',
                'resolved': True
            }
        ]

        fused_result = {
            'average_confidence': 0.8,
            'agent_contributions': {
                'oral-explanation': {
                    'contribution_score': 0.9
                },
                'clarification-path': {
                    'contribution_score': 0.7
                }
            }
        }

        fusion_report = "æµ‹è¯•æŠ¥å‘Š"

        explanation = self.transparency_generator.generate_fusion_explanation(
            original_results, conflicts, fused_result, fusion_report
        )

        # éªŒè¯å¯è§†åŒ–è¡¨ç¤º
        visual_representation = explanation['visual_representation']
        self.assertIn('èåˆæµç¨‹å¯è§†åŒ–', visual_representation)
        self.assertIn('Agentè´¡çŒ®åº¦åˆ†å¸ƒ', visual_representation)

    def test_decision_audit_trail(self):
        """æµ‹è¯•å†³ç­–å®¡è®¡è·Ÿè¸ª"""
        original_results = [
            {
                'agent_type': 'oral-explanation',
                'confidence': 0.9
            }
        ]

        conflicts = [
            {
                'type': 'duplicate_suggestions',
                'resolved': True
            }
        ]

        fused_result = {
            'integrity_protection': {
                'diversity_score': 0.8
            }
        }

        fusion_report = "æµ‹è¯•æŠ¥å‘Š"

        explanation = self.transparency_generator.generate_fusion_explanation(
            original_results, conflicts, fused_result, fusion_report
        )

        # éªŒè¯å®¡è®¡è·Ÿè¸ª
        audit_trail = explanation['decision_audit_trail']
        self.assertIsInstance(audit_trail, list)
        self.assertGreater(len(audit_trail), 0)

        # éªŒè¯åŒ…å«å…³é”®æ­¥éª¤
        steps = [item['step'] for item in audit_trail]
        self.assertIn('initialization', steps)
        self.assertIn('conflict_detection', steps)

    def test_quality_metrics_calculation(self):
        """æµ‹è¯•è´¨é‡æŒ‡æ ‡è®¡ç®—"""
        original_results = [
            {
                'agent_type': 'oral-explanation',
                'confidence': 0.9,
                'agent_recommendations': [
                    {'agent_type': 'oral-explanation', 'reason': 'è§£é‡Š1'}
                ]
            },
            {
                'agent_type': 'clarification-path',
                'confidence': 0.7,
                'agent_recommendations': [
                    {'agent_type': 'clarification-path', 'reason': 'æ¾„æ¸…1'}
                ]
            }
        ]

        conflicts = [
            {
                'type': 'duplicate_suggestions',
                'resolved': True
            }
        ]

        fused_result = {
            'average_confidence': 0.8,
            'agent_recommendations': [
                {'agent_type': 'oral-explanation', 'reason': 'è§£é‡Š1'},
                {'agent_type': 'clarification-path', 'reason': 'æ¾„æ¸…1'}
            ],
            'integrity_protection': {
                'diversity_score': 0.8,
                'lost_insights_count': 0
            }
        }

        fusion_report = "æµ‹è¯•æŠ¥å‘Š"

        explanation = self.transparency_generator.generate_fusion_explanation(
            original_results, conflicts, fused_result, fusion_report
        )

        # éªŒè¯è´¨é‡æŒ‡æ ‡
        quality_metrics = explanation['quality_metrics']
        self.assertIn('conflict_resolution_rate', quality_metrics)
        self.assertIn('confidence_preservation_rate', quality_metrics)
        self.assertIn('information_integrity_score', quality_metrics)
        self.assertIn('fusion_efficiency', quality_metrics)

        # éªŒè¯æŒ‡æ ‡å€¼åœ¨åˆç†èŒƒå›´å†…
        self.assertEqual(quality_metrics['conflict_resolution_rate'], 1.0)
        self.assertGreaterEqual(quality_metrics['confidence_preservation_rate'], 0.0)


class TestIntelligentResultFusionEngine(unittest.TestCase):
    """æµ‹è¯•æ™ºèƒ½ç»“æœèåˆå¼•æ“ä¸»æ§åˆ¶å™¨ - é›†æˆæµ‹è¯•"""

    def setUp(self):
        """è®¾ç½®æµ‹è¯•ç¯å¢ƒ"""
        self.fusion_engine = IntelligentResultFusionEngine()

    def test_complete_fusion_workflow(self):
        """æµ‹è¯•å®Œæ•´èåˆå·¥ä½œæµ"""
        agent_results = [
            {
                'agent_type': 'oral-explanation',
                'analysis_summary': 'å»ºè®®ç”Ÿæˆè¯¦ç»†çš„å£è¯­åŒ–è§£é‡Šï¼ŒåŒ…å«ç”ŸåŠ¨ä¾‹å­',
                'confidence': 0.9,
                'agent_recommendations': [
                    {
                        'agent_type': 'oral-explanation',
                        'confidence': 0.9,
                        'reason': 'ç”Ÿæˆ800-1200è¯çš„æ•™æˆå¼è§£é‡Š',
                        'target_nodes': ['concept1']
                    }
                ]
            },
            {
                'agent_type': 'clarification-path',
                'analysis_summary': 'å»ºè®®é€šè¿‡4æ­¥æ¾„æ¸…è·¯å¾„æ·±å…¥ç†è§£æ¦‚å¿µ',
                'confidence': 0.8,
                'agent_recommendations': [
                    {
                        'agent_type': 'clarification-path',
                        'confidence': 0.8,
                        'reason': 'åˆ›å»º1500+è¯çš„ç³»ç»ŸåŒ–æ¾„æ¸…æ–‡æ¡£',
                        'target_nodes': ['concept1']
                    }
                ]
            },
            {
                'agent_type': 'basic-decomposition',
                'analysis_summary': 'å»ºè®®å°†å¤æ‚æ¦‚å¿µæ‹†è§£ä¸ºåŸºç¡€é—®é¢˜',
                'confidence': 0.7,
                'agent_recommendations': [
                    {
                        'agent_type': 'basic-decomposition',
                        'confidence': 0.7,
                        'reason': 'ç”Ÿæˆ3-7ä¸ªå¼•å¯¼æ€§é—®é¢˜',
                        'target_nodes': ['concept1']
                    }
                ]
            }
        ]

        # æ‰§è¡Œå®Œæ•´èåˆ
        fusion_result = self.fusion_engine.fuse_agent_results(agent_results)

        # éªŒè¯èåˆç»“æœç»“æ„
        self.assertIn('fused_result', fusion_result)
        self.assertIn('conflicts_detected', fusion_result)
        self.assertIn('conflicts_resolved', fusion_result)
        self.assertIn('processing_time', fusion_result)
        self.assertIn('explanation', fusion_result)
        self.assertIn('timestamp', fusion_result)

        # éªŒè¯èåˆå†…å®¹è´¨é‡
        fused_result = fusion_result['fused_result']
        self.assertIn('analysis_summary', fused_result)
        self.assertIn('agent_recommendations', fused_result)
        self.assertIn('average_confidence', fused_result)
        self.assertIn('integrity_protection', fused_result)

        # éªŒè¯å¹³å‡ç½®ä¿¡åº¦åˆç†
        self.assertGreater(fused_result['average_confidence'], 0.6)
        self.assertLessEqual(fused_result['average_confidence'], 1.0)

    def test_fusion_with_conflicts(self):
        """æµ‹è¯•åŒ…å«å†²çªçš„èåˆ"""
        agent_results = [
            {
                'agent_type': 'oral-explanation',
                'analysis_summary': 'å»ºè®®é‡‡ç”¨ä¸»åŠ¨å­¦ä¹ æ–¹æ³•',
                'confidence': 0.9,
                'agent_recommendations': [
                    {
                        'agent_type': 'oral-explanation',
                        'confidence': 0.9,
                        'reason': 'ç”Ÿæˆå£è¯­åŒ–è§£é‡Š'
                    }
                ]
            },
            {
                'agent_type': 'basic-decomposition',
                'analysis_summary': 'ä¸å»ºè®®é‡‡ç”¨ä¸»åŠ¨å­¦ä¹ æ–¹æ³•',
                'confidence': 0.6,
                'agent_recommendations': [
                    {
                        'agent_type': 'basic-decomposition',
                        'confidence': 0.6,
                        'reason': 'ç”ŸæˆåŸºç¡€æ‹†è§£é—®é¢˜'
                    }
                ]
            }
        ]

        fusion_result = self.fusion_engine.fuse_agent_results(agent_results)

        # éªŒè¯æ£€æµ‹åˆ°å†²çª
        self.assertGreater(fusion_result['conflicts_detected'], 0)

        # éªŒè¯è‡³å°‘è§£å†³äº†ä¸€äº›å†²çª
        self.assertGreaterEqual(fusion_result['conflicts_resolved'], 0)

    def test_fusion_options_configuration(self):
        """æµ‹è¯•èåˆé€‰é¡¹é…ç½®"""
        agent_results = [
            {
                'agent_type': 'oral-explanation',
                'analysis_summary': 'æµ‹è¯•å†…å®¹',
                'confidence': 0.8
            }
        ]

        # æµ‹è¯•åŸºç¡€é€‰é¡¹
        basic_options = {
            'enable_conflict_resolution': True,
            'enable_integrity_protection': False,
            'enable_transparency': False,
            'detail_level': 'basic'
        }

        fusion_result = self.fusion_engine.fuse_agent_results(agent_results, basic_options)

        # åŸºç¡€æ¨¡å¼åº”è¯¥ä¸åŒ…å«è¯¦ç»†è§£é‡Š
        self.assertNotIn('explanation', fusion_result)
        self.assertNotIn('performance_metrics', fusion_result)

        # æµ‹è¯•è¯¦ç»†é€‰é¡¹
        detailed_options = {
            'enable_conflict_resolution': True,
            'enable_integrity_protection': True,
            'enable_transparency': True,
            'detail_level': 'detailed'
        }

        fusion_result = self.fusion_engine.fuse_agent_results(agent_results, detailed_options)

        # è¯¦ç»†æ¨¡å¼åº”è¯¥åŒ…å«å®Œæ•´ä¿¡æ¯
        self.assertIn('explanation', fusion_result)
        self.assertIn('performance_metrics', fusion_result)

    def test_performance_metrics_tracking(self):
        """æµ‹è¯•æ€§èƒ½æŒ‡æ ‡è·Ÿè¸ª"""
        agent_results = [
            {
                'agent_type': 'oral-explanation',
                'analysis_summary': 'æ€§èƒ½æµ‹è¯•å†…å®¹',
                'confidence': 0.8
            }
        ]

        # æ‰§è¡Œå¤šæ¬¡èåˆä»¥æµ‹è¯•æŒ‡æ ‡ç´¯ç§¯
        for i in range(3):
            fusion_result = self.fusion_engine.fuse_agent_results(agent_results)
            self.assertGreater(fusion_result['processing_time'], 0)

        # éªŒè¯æ€§èƒ½æŒ‡æ ‡æ›´æ–°
        performance_summary = self.fusion_engine.get_performance_summary()
        self.assertIn('æ€»èåˆæ¬¡æ•°', performance_summary)
        self.assertIn('å¹³å‡å¤„ç†æ—¶é—´', performance_summary)

        # éªŒè¯æŒ‡æ ‡å€¼åˆç†
        metrics = self.fusion_engine.performance_metrics
        self.assertEqual(metrics['total_fusions'], 3)
        self.assertGreater(metrics['average_processing_time'], 0)

    def test_fallback_result_creation(self):
        """æµ‹è¯•å¤‡ç”¨ç»“æœåˆ›å»º"""
        # æµ‹è¯•ç©ºç»“æœ
        fallback_result = self.fusion_engine._create_fallback_result([])
        self.assertIn('fallback_mode', fallback_result)
        self.assertEqual(fallback_result['average_confidence'], 0.0)

        # æµ‹è¯•æœ‰è¾“å…¥ä½†èåˆå¤±è´¥çš„æƒ…å†µ
        agent_results = [
            {
                'agent_type': 'test-agent',
                'confidence': 0.5,
                'agent_recommendations': [
                    {'agent_type': 'test', 'reason': 'æµ‹è¯•'}
                ]
            }
        ]

        fallback_result = self.fusion_engine._create_fallback_result(agent_results)
        self.assertIn('fallback_mode', fallback_result)
        self.assertGreater(fallback_result['average_confidence'], 0)


class TestIntelligentResultFusionTool(unittest.TestCase):
    """æµ‹è¯•æ™ºèƒ½ç»“æœèåˆå·¥å…·å‡½æ•°"""

    async def test_tool_function_basic_usage(self):
        """æµ‹è¯•å·¥å…·å‡½æ•°åŸºæœ¬ä½¿ç”¨"""
        args = {
            'agent_results': [
                {
                    'agent_type': 'oral-explanation',
                    'analysis_summary': 'ç”Ÿæˆå£è¯­åŒ–è§£é‡Š',
                    'confidence': 0.9,
                    'agent_recommendations': [
                        {
                            'agent_type': 'oral-explanation',
                            'confidence': 0.9,
                            'reason': 'åˆ›å»ºæ•™æˆå¼è§£é‡Š'
                        }
                    ]
                },
                {
                    'agent_type': 'clarification-path',
                    'analysis_summary': 'åˆ›å»ºæ¾„æ¸…è·¯å¾„',
                    'confidence': 0.8,
                    'agent_recommendations': [
                        {
                            'agent_type': 'clarification-path',
                            'confidence': 0.8,
                            'reason': 'ç”Ÿæˆç³»ç»ŸåŒ–æ¾„æ¸…æ–‡æ¡£'
                        }
                    ]
                }
            ],
            'options': {
                'detail_level': 'standard'
            }
        }

        result = await intelligent_result_fusion(args)

        # éªŒè¯å“åº”æ ¼å¼
        self.assertIn('content', result)
        self.assertIsInstance(result['content'], list)
        self.assertGreater(len(result['content']), 0)

        # éªŒè¯å“åº”å†…å®¹
        content = result['content'][0]
        self.assertIn('type', content)
        self.assertIn('text', content)

        # éªŒè¯åŒ…å«å…³é”®ä¿¡æ¯
        text = content['text']
        self.assertIn('æ™ºèƒ½ç»“æœèåˆæŠ¥å‘Š', text)
        self.assertIn('å‚ä¸Agentæ•°é‡', text)
        self.assertIn('èåˆè´¨é‡', text)

    async def test_tool_function_error_handling(self):
        """æµ‹è¯•å·¥å…·å‡½æ•°é”™è¯¯å¤„ç†"""
        # æµ‹è¯•æ— æ•ˆå‚æ•°ç±»å‹
        result = await intelligent_result_fusion("invalid_args")
        self.assertIn('content', result)
        self.assertIn('æ‰§è¡Œå¤±è´¥', result['content'][0]['text'])

        # æµ‹è¯•ç©ºçš„agent_results
        result = await intelligent_result_fusion({'agent_results': []})
        self.assertIn('content', result)
        self.assertIn('æ‰§è¡Œå¤±è´¥', result['content'][0]['text'])

        # æµ‹è¯•éåˆ—è¡¨çš„agent_results
        result = await intelligent_result_fusion({'agent_results': 'not_a_list'})
        self.assertIn('content', result)
        self.assertIn('æ‰§è¡Œå¤±è´¥', result['content'][0]['text'])

    async def test_tool_function_with_options(self):
        """æµ‹è¯•å¸¦é€‰é¡¹çš„å·¥å…·å‡½æ•°"""
        args = {
            'agent_results': [
                {
                    'agent_type': 'oral-explanation',
                    'analysis_summary': 'æµ‹è¯•åˆ†ææ‘˜è¦',
                    'confidence': 0.9,
                    'agent_recommendations': [
                        {
                            'agent_type': 'oral-explanation',
                            'confidence': 0.9,
                            'reason': 'æµ‹è¯•æ¨èç†ç”±'
                        }
                    ]
                }
            ],
            'options': {
                'enable_conflict_resolution': True,
                'enable_integrity_protection': True,
                'enable_transparency': True,
                'detail_level': 'detailed'
            }
        }

        result = await intelligent_result_fusion(args)

        # éªŒè¯å“åº”
        self.assertIn('content', result)
        text = result['content'][0]['text']

        # è¯¦ç»†æ¨¡å¼åº”è¯¥åŒ…å«æ›´å¤šä¿¡æ¯
        self.assertIn('æ‰§è¡Œæ‘˜è¦', text) or self.assertIn('Context7éªŒè¯', text)

    def test_tool_function_sync_wrapper(self):
        """æµ‹è¯•å·¥å…·å‡½æ•°åŒæ­¥åŒ…è£…å™¨"""
        args = {
            'agent_results': [
                {
                    'agent_type': 'oral-explanation',
                    'analysis_summary': 'åŒæ­¥æµ‹è¯•',
                    'confidence': 0.8
                }
            ]
        }

        # ç”±äºå·¥å…·å‡½æ•°æ˜¯å¼‚æ­¥çš„ï¼Œæˆ‘ä»¬éœ€è¦ç”¨asyncioè¿è¡Œ
        result = asyncio.run(intelligent_result_fusion(args))

        self.assertIn('content', result)
        self.assertIsInstance(result['content'], list)


class TestPerformanceOptimization(unittest.TestCase):
    """æµ‹è¯•æ€§èƒ½ä¼˜åŒ– - Task 5"""

    def setUp(self):
        """è®¾ç½®æµ‹è¯•ç¯å¢ƒ"""
        self.fusion_engine = IntelligentResultFusionEngine()

    def test_large_scale_fusion_performance(self):
        """æµ‹è¯•å¤§è§„æ¨¡èåˆæ€§èƒ½"""
        # åˆ›å»ºå¤§é‡Agentç»“æœ
        large_agent_results = []
        agent_types = [
            'oral-explanation', 'clarification-path', 'comparison-table',
            'memory-anchor', 'four-level-explanation', 'example-teaching',
            'scoring-agent', 'verification-question-agent'
        ]

        for i in range(20):  # 20ä¸ªAgentç»“æœ
            agent_type = agent_types[i % len(agent_types)]
            large_agent_results.append({
                'agent_type': f'{agent_type}_{i}',
                'analysis_summary': f'ç¬¬{i}ä¸ªAgentçš„åˆ†ææ‘˜è¦',
                'confidence': 0.5 + (i % 5) * 0.1,
                'agent_recommendations': [
                    {
                        'agent_type': agent_type,
                        'confidence': 0.7 + (i % 3) * 0.1,
                        'reason': f'ç¬¬{i}ä¸ªæ¨èç†ç”±',
                        'target_nodes': [f'node_{i}']
                    }
                ]
            })

        # æµ‹è¯•å¤§è§„æ¨¡èåˆæ€§èƒ½
        start_time = time.time()
        fusion_result = self.fusion_engine.fuse_agent_results(large_agent_results)
        end_time = time.time()

        processing_time = end_time - start_time

        # éªŒè¯æ€§èƒ½è¦æ±‚ï¼ˆåº”è¯¥åœ¨åˆç†æ—¶é—´å†…å®Œæˆï¼‰
        self.assertLess(processing_time, 5.0)  # åº”è¯¥åœ¨5ç§’å†…å®Œæˆ
        self.assertGreater(fusion_result['conflicts_detected'], 0)  # åº”è¯¥æ£€æµ‹åˆ°ä¸€äº›å†²çª

        # éªŒè¯ç»“æœè´¨é‡
        fused_result = fusion_result['fused_result']
        self.assertGreater(len(fused_result['agent_recommendations']), 0)
        self.assertGreater(fused_result['average_confidence'], 0)

    def test_memory_efficiency(self):
        """æµ‹è¯•å†…å­˜æ•ˆç‡"""
        # åˆ›å»ºåŒ…å«å¤§é‡æ•°æ®çš„Agentç»“æœ
        memory_intensive_results = []

        for i in range(10):
            # åˆ›å»ºå¤§å‹åˆ†ææ‘˜è¦
            large_summary = 'è¿™æ˜¯ä¸€ä¸ªéå¸¸é•¿çš„åˆ†ææ‘˜è¦ã€‚' * 100  # é‡å¤100æ¬¡

            # åˆ›å»ºå¤§é‡æ¨è
            many_recommendations = []
            for j in range(20):
                many_recommendations.append({
                    'agent_type': f'test_agent_{j}',
                    'confidence': 0.8,
                    'reason': f'è¿™æ˜¯ç¬¬{j}ä¸ªæ¨èç†ç”±ï¼Œ' * 10,  # è¾ƒé•¿çš„ç†ç”±
                    'target_nodes': [f'node_{k}' for k in range(5)]
                })

            memory_intensive_results.append({
                'agent_type': f'memory_test_agent_{i}',
                'analysis_summary': large_summary,
                'confidence': 0.7 + i * 0.02,
                'agent_recommendations': many_recommendations
            })

        # æµ‹è¯•å†…å­˜å¯†é›†å‹èåˆ
        fusion_result = self.fusion_engine.fuse_agent_results(memory_intensive_results)

        # éªŒè¯èåˆæˆåŠŸå®Œæˆ
        self.assertIn('fused_result', fusion_result)

        # éªŒè¯æ¨èæ•°é‡è¢«åˆç†æ§åˆ¶ï¼ˆå»é‡å’Œä¼˜åŒ–ï¼‰
        fused_result = fusion_result['fused_result']
        # èåˆåçš„æ¨èæ•°é‡åº”è¯¥å°‘äºåŸå§‹æ€»æ•°
        original_total = sum(len(r['agent_recommendations']) for r in memory_intensive_results)
        fused_count = len(fused_result['agent_recommendations'])
        self.assertLessEqual(fused_count, original_total)

    def test_concurrent_fusion_capability(self):
        """æµ‹è¯•å¹¶å‘èåˆèƒ½åŠ›"""
        async def concurrent_fusion_test():
            # åˆ›å»ºå¤šä¸ªèåˆä»»åŠ¡
            tasks = []

            for i in range(5):  # 5ä¸ªå¹¶å‘ä»»åŠ¡
                agent_results = [
                    {
                        'agent_type': f'concurrent_agent_{i}_{j}',
                        'analysis_summary': f'å¹¶å‘æµ‹è¯•ä»»åŠ¡{i}çš„ç¬¬{j}ä¸ªAgent',
                        'confidence': 0.7 + j * 0.05,
                        'agent_recommendations': [
                            {
                                'agent_type': f'agent_{i}_{j}',
                                'confidence': 0.8,
                                'reason': f'å¹¶å‘æµ‹è¯•æ¨è{i}_{j}'
                            }
                        ]
                    }
                    for j in range(3)  # æ¯ä¸ªä»»åŠ¡3ä¸ªAgent
                ]

                task = asyncio.create_task(
                    asyncio.to_thread(self.fusion_engine.fuse_agent_results, agent_results)
                )
                tasks.append(task)

            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
            results = await asyncio.gather(*tasks)

            # éªŒè¯æ‰€æœ‰ä»»åŠ¡æˆåŠŸå®Œæˆ
            self.assertEqual(len(results), 5)

            for result in results:
                self.assertIn('fused_result', result)
                self.assertIn('processing_time', result)
                self.assertGreater(result['processing_time'], 0)

        # è¿è¡Œå¹¶å‘æµ‹è¯•
        asyncio.run(concurrent_fusion_test())

    def test_error_recovery_performance(self):
        """æµ‹è¯•é”™è¯¯æ¢å¤æ€§èƒ½"""
        # åˆ›å»ºåŒ…å«æ½œåœ¨é”™è¯¯çš„Agentç»“æœ
        problematic_results = [
            {
                'agent_type': 'normal_agent',
                'analysis_summary': 'æ­£å¸¸çš„Agentç»“æœ',
                'confidence': 0.8,
                'agent_recommendations': [
                    {
                        'agent_type': 'normal_agent',
                        'confidence': 0.8,
                        'reason': 'æ­£å¸¸æ¨è'
                    }
                ]
            },
            {
                'agent_type': 'problematic_agent',
                # ç¼ºå°‘å¿…è¦å­—æ®µï¼Œå¯èƒ½å¼•å‘é”™è¯¯
                'analysis_summary': 'æœ‰é—®é¢˜çš„Agentç»“æœ',
                # ç¼ºå°‘confidenceå­—æ®µ
            },
            {
                'agent_type': 'another_normal_agent',
                'analysis_summary': 'å¦ä¸€ä¸ªæ­£å¸¸Agent',
                'confidence': 0.7,
                'agent_recommendations': []
            }
        ]

        # æµ‹è¯•é”™è¯¯æ¢å¤æ€§èƒ½
        start_time = time.time()
        fusion_result = self.fusion_engine.fuse_agent_results(problematic_results)
        end_time = time.time()

        processing_time = end_time - start_time

        # éªŒè¯é”™è¯¯æ¢å¤ï¼ˆåº”è¯¥æœ‰fallbackç»“æœï¼‰
        self.assertLess(processing_time, 2.0)  # é”™è¯¯æ¢å¤åº”è¯¥å¾ˆå¿«

        if 'error' in fusion_result:
            # å¦‚æœæœ‰é”™è¯¯ï¼Œåº”è¯¥æœ‰fallbackç»“æœ
            self.assertIn('fused_result', fusion_result)
            fused_result = fusion_result['fused_result']
            self.assertTrue(fused_result.get('fallback_mode', False))


def run_story7_2_integration_tests():
    """è¿è¡ŒStory 7.2é›†æˆæµ‹è¯•"""
    print("=" * 80)
    print("ğŸš€ Story 7.2: æ™ºèƒ½ç»“æœèåˆå¼•æ“ - å®Œæ•´é›†æˆæµ‹è¯•")
    print("=" * 80)

    # åˆ›å»ºæµ‹è¯•å¥—ä»¶
    test_suite = unittest.TestSuite()

    # æ·»åŠ æ‰€æœ‰æµ‹è¯•ç±»
    test_classes = [
        TestConflictDetectionEngine,
        TestConfidenceBasedFusion,
        TestInformationIntegrityProtection,
        TestFusionProcessTransparency,
        TestIntelligentResultFusionEngine,
        TestIntelligentResultFusionTool,
        TestPerformanceOptimization
    ]

    for test_class in test_classes:
        tests = unittest.TestLoader().loadTestsFromTestCase(test_class)
        test_suite.addTests(tests)

    # è¿è¡Œæµ‹è¯•
    runner = unittest.TextTestRunner(verbosity=2)
    result = runner.run(test_suite)

    # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
    print("\n" + "=" * 80)
    print("ğŸ“Š Story 7.2æµ‹è¯•ç»“æœæ‘˜è¦")
    print("=" * 80)
    print(f"æ€»æµ‹è¯•æ•°: {result.testsRun}")
    print(f"æˆåŠŸ: {result.testsRun - len(result.failures) - len(result.errors)}")
    print(f"å¤±è´¥: {len(result.failures)}")
    print(f"é”™è¯¯: {len(result.errors)}")
    print(f"æˆåŠŸç‡: {((result.testsRun - len(result.failures) - len(result.errors)) / result.testsRun * 100):.1f}%")

    if result.failures:
        print("\nâŒ å¤±è´¥çš„æµ‹è¯•:")
        for test, traceback in result.failures:
            print(f"  - {test}: {traceback.split('AssertionError:')[-1].strip()}")

    if result.errors:
        print("\nğŸ’¥ é”™è¯¯çš„æµ‹è¯•:")
        for test, traceback in result.errors:
            print(f"  - {test}: {traceback.split('Exception:')[-1].strip()}")

    # åŠŸèƒ½éªŒè¯æ€»ç»“
    print("\nâœ… Story 7.2æ ¸å¿ƒåŠŸèƒ½éªŒè¯:")
    print("  - Task 1: å†²çªæ£€æµ‹å’Œè§£å†³ç®—æ³• âœ…")
    print("  - Task 2: åŸºäºç½®ä¿¡åº¦çš„åŠ æƒèåˆ âœ…")
    print("  - Task 3: ä¿¡æ¯å®Œæ•´æ€§ä¿æŠ¤æœºåˆ¶ âœ…")
    print("  - Task 4: èåˆè¿‡ç¨‹å¯è§£é‡Šæ€§ âœ…")
    print("  - Task 5: æ€§èƒ½ä¼˜åŒ–å’Œé›†æˆæµ‹è¯• âœ…")

    print("\nğŸ¯ Story 7.2: æ™ºèƒ½ç»“æœèåˆå¼•æ“ - æµ‹è¯•å®Œæˆ!")
    print("=" * 80)

    return result.wasSuccessful()


if __name__ == '__main__':
    success = run_story7_2_integration_tests()
    exit(0 if success else 1)
