"""
BMad Orchestrator èŠ‚ç‚¹å®ç°

å®ç°9ä¸ªæ ¸å¿ƒèŠ‚ç‚¹:
1. sm_node: SM Agent Story åˆ›å»º
2. po_node: PO Agent Story éªŒè¯ (SoT è‡ªåŠ¨è§£å†³)
3. analysis_node: ä¾èµ–åˆ†æå’Œæ¨¡å¼é€‰æ‹©
4. dev_node: Dev Agent å¼€å‘å®ç° (å¹¶è¡Œ Send)
5. qa_node: QA Agent è´¨é‡å®¡æŸ¥ (å¹¶è¡Œ Send)
6. merge_node: Worktree åˆå¹¶åè°ƒ
7. commit_node: Git æäº¤
8. fix_node: CONCERNS ä¿®å¤å¾ªç¯
9. halt_node: å¤±è´¥å¤„ç†

âœ… Verified from LangGraph Skill:
- Nodes are async functions: async def node(state: State) -> dict
- Return dict with state updates
- Use Send for parallel execution

Author: Canvas Learning System Team
Version: 1.0.0
Created: 2025-11-30
"""

import asyncio
import functools
import json
import logging
import sys
from datetime import datetime
from pathlib import Path
from typing import Any, Callable, Dict, List, Optional

from langgraph.types import Send

# âœ… Verified from Project Code (src/bmad_orchestrator/commit_gate.py)
# Commit Gate v2 - é›¶å¹»è§‰å¼ºåˆ¶éªŒè¯æœºåˆ¶
from .commit_gate import CommitGate, CommitGateError
from .session_spawner import (
    BmadSessionSpawner,
    DevResult,
    POResult,
    QAResult,
    SMResult,
)
from .state import (
    BlockerInfo,
    BmadOrchestratorState,
    DevOutcome,
    QAOutcome,
    SessionInfo,
    SoTResolution,
    StoryDraft,
)

# ============================================================================
# PostProcessHook å¯¼å…¥ (ç”¨äº Story æ–‡ä»¶æ›´æ–°å’Œ QA Gate ç”Ÿæˆ)
# ============================================================================
# âœ… Reference: scripts/daemon/linear_develop_daemon.py:41
# PostProcessHook åœ¨ QA é˜¶æ®µå®Œæˆåè°ƒç”¨ï¼Œæ›´æ–° Story æ–‡æ¡£

_POST_PROCESS_HOOK_IMPORTED = False
PostProcessHook = None

def _ensure_post_process_hook():
    """å»¶è¿Ÿå¯¼å…¥ PostProcessHookï¼Œé¿å…å¾ªç¯ä¾èµ–"""
    global _POST_PROCESS_HOOK_IMPORTED, PostProcessHook
    if not _POST_PROCESS_HOOK_IMPORTED:
        # æ·»åŠ  scripts/daemon åˆ° path
        scripts_daemon_path = Path(__file__).parent.parent.parent / "scripts" / "daemon"
        if str(scripts_daemon_path) not in sys.path:
            sys.path.insert(0, str(scripts_daemon_path))

        from post_process_hook import PostProcessHook as PPH
        PostProcessHook = PPH
        _POST_PROCESS_HOOK_IMPORTED = True
    return PostProcessHook


# ============================================================================
# å…¨å±€é…ç½®
# ============================================================================

DEFAULT_MAX_TURNS = 200
DEFAULT_ULTRATHINK = True
DEFAULT_TIMEOUT = 3600  # 1 hour

# Logger for BMad Orchestrator nodes
logger = logging.getLogger(__name__)


# ============================================================================
# BMad å·¥ä½œæµå¼ºåˆ¶æ‰§è¡Œè£…é¥°å™¨ (Phase 1: æ ¸å¿ƒå¼ºåˆ¶æœºåˆ¶)
# ============================================================================
# âœ… æ”¹è¿›ç‚¹ 1: ç¡®ä¿ *epic-develop å‘½ä»¤çœŸæ­£è°ƒç”¨ BMad å·¥ä½œæµ
# é˜²æ­¢ç»•è¿‡ SM â†’ PO â†’ DEV â†’ QA â†’ Commit æµç¨‹

def enforce_bmad_workflow(func: Callable) -> Callable:
    """
    è£…é¥°å™¨: å¼ºåˆ¶æ‰§è¡Œ BMad å·¥ä½œæµ

    åŠŸèƒ½:
    1. æ£€æŸ¥æ˜¯å¦æœ‰ç»•è¿‡æ ‡è®° (bypass_bmad_workflow)
    2. è®°å½•èŠ‚ç‚¹æ‰§è¡Œåˆ° executed_nodes åˆ—è¡¨
    3. æä¾›æ‰§è¡Œå®¡è®¡è·Ÿè¸ª

    ä½¿ç”¨ç¤ºä¾‹:
    ```python
    @enforce_bmad_workflow
    async def sm_node(state: BmadOrchestratorState) -> Dict[str, Any]:
        ...
    ```

    Raises:
        ValueError: å¦‚æœæ£€æµ‹åˆ°ç»•è¿‡ BMad å·¥ä½œæµçš„å°è¯•
    """
    @functools.wraps(func)
    async def wrapper(state: BmadOrchestratorState) -> Dict[str, Any]:
        node_name = func.__name__

        # ğŸ”´ æ£€æŸ¥æ˜¯å¦å°è¯•ç»•è¿‡ BMad å·¥ä½œæµ
        if state.get("bypass_bmad_workflow", False):
            error_msg = (
                f"âŒ BMad å·¥ä½œæµä¸å…è®¸è¢«ç»•è¿‡ï¼\n"
                f"èŠ‚ç‚¹: {node_name}\n"
                f"è¯·ä½¿ç”¨æ ‡å‡†æµç¨‹: SM â†’ PO â†’ DEV â†’ QA â†’ Commit"
            )
            logger.error(error_msg)
            raise ValueError(error_msg)

        # ğŸ“ è®°å½•èŠ‚ç‚¹æ‰§è¡Œå¼€å§‹
        executed_nodes = state.get("executed_nodes", [])
        execution_entry = {
            "node": node_name,
            "started_at": datetime.now().isoformat(),
            "status": "started"
        }

        logger.info(f"[BMad Workflow] å¼€å§‹æ‰§è¡ŒèŠ‚ç‚¹: {node_name}")

        try:
            # æ‰§è¡Œå®é™…èŠ‚ç‚¹å‡½æ•°
            result = await func(state)

            # æ›´æ–°æ‰§è¡Œè®°å½•
            execution_entry["status"] = "completed"
            execution_entry["completed_at"] = datetime.now().isoformat()
            executed_nodes.append(execution_entry)

            logger.info(f"[BMad Workflow] èŠ‚ç‚¹å®Œæˆ: {node_name}")

            # å°† executed_nodes åˆå¹¶åˆ°ç»“æœä¸­
            if "executed_nodes" not in result:
                result["executed_nodes"] = executed_nodes

            return result

        except Exception as e:
            # è®°å½•èŠ‚ç‚¹æ‰§è¡Œå¤±è´¥
            execution_entry["status"] = "failed"
            execution_entry["error"] = str(e)
            execution_entry["failed_at"] = datetime.now().isoformat()
            executed_nodes.append(execution_entry)

            logger.error(f"[BMad Workflow] èŠ‚ç‚¹å¤±è´¥: {node_name} - {e}")
            raise

    return wrapper


# ============================================================================
# Story æ–‡ä»¶éªŒè¯é—¨ç¦ (Phase 1: æ”¹è¿›ç‚¹ 2)
# ============================================================================
# âœ… ç¡®ä¿ DEV èŠ‚ç‚¹æ‰§è¡Œå‰ Story æ–‡ä»¶å¿…é¡»å­˜åœ¨
# é˜²æ­¢è·³è¿‡ SM/PO é˜¶æ®µç›´æ¥å¼€å‘

def validate_story_files_exist(
    project_root: Path,
    story_ids: List[str]
) -> Dict[str, Optional[Path]]:
    """
    éªŒè¯ Story æ–‡ä»¶æ˜¯å¦å­˜åœ¨

    Args:
        project_root: é¡¹ç›®æ ¹ç›®å½•
        story_ids: Story ID åˆ—è¡¨

    Returns:
        Dict[story_id, story_path] - å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨åˆ™å€¼ä¸º None

    Raises:
        FileNotFoundError: å¦‚æœä»»ä½• Story æ–‡ä»¶ä¸å­˜åœ¨
    """
    story_paths = {}
    missing_stories = []

    for story_id in story_ids:
        # å°è¯•å¤šç§è·¯å¾„æ¨¡å¼
        possible_paths = [
            project_root / "docs" / "stories" / f"{story_id}.story.md",
            project_root / "docs" / "stories" / f"story-{story_id}.md",
            project_root / "docs" / "stories" / f"{story_id.replace('.', '-')}.story.md",
        ]

        found = False
        for path in possible_paths:
            if path.exists():
                story_paths[story_id] = path
                found = True
                logger.debug(f"[StoryéªŒè¯] æ‰¾åˆ° Story æ–‡ä»¶: {story_id} -> {path}")
                break

        if not found:
            story_paths[story_id] = None
            missing_stories.append(story_id)
            logger.warning(f"[StoryéªŒè¯] Story æ–‡ä»¶ä¸å­˜åœ¨: {story_id}")

    if missing_stories:
        error_msg = (
            f"âŒ Story æ–‡ä»¶éªŒè¯å¤±è´¥ï¼\n"
            f"ç¼ºå¤±çš„ Story: {', '.join(missing_stories)}\n"
            f"è¯·å…ˆè¿è¡Œ SM Node åˆ›å»º Story æ–‡ä»¶ã€‚\n"
            f"å·¥ä½œæµ: SM (*draft) â†’ PO (*validate-story-draft) â†’ DEV (*develop-story)"
        )
        logger.error(error_msg)
        raise FileNotFoundError(error_msg)

    return story_paths


# ============================================================================
# Epic æ–‡ä»¶æŸ¥æ‰¾è¾…åŠ©å‡½æ•°
# ============================================================================

def find_epic_file(base_path: Path, epic_id: str) -> str:
    """
    æŸ¥æ‰¾ Epic æ–‡ä»¶ï¼Œæ”¯æŒå¤šç§å‘½åæ¨¡å¼ã€‚

    æœç´¢é¡ºåº:
    1. docs/prd/epic-{epic_id}.md (å°å†™)
    2. docs/prd/EPIC-{epic_id}.md (å¤§å†™æ— åç¼€)
    3. docs/prd/EPIC-{epic_id}-*.md (å¤§å†™å¸¦åç¼€ï¼Œå¦‚ EPIC-20-BACKEND-STABILITY-MULTI-PROVIDER.md)
    4. docs/prd/epics/epic-{epic_id}.md
    5. docs/prd/epics/EPIC-{epic_id}*.md

    Args:
        base_path: é¡¹ç›®æ ¹ç›®å½•
        epic_id: Epic ç¼–å· (å¦‚ "20")

    Returns:
        Epic æ–‡ä»¶çš„ç›¸å¯¹è·¯å¾„ï¼Œå¦‚æœæœªæ‰¾åˆ°åˆ™è¿”å›é»˜è®¤è·¯å¾„
    """
    import glob

    # å¯èƒ½çš„æ¨¡å¼åˆ—è¡¨
    patterns = [
        f"docs/prd/epic-{epic_id}.md",
        f"docs/prd/EPIC-{epic_id}.md",
        f"docs/prd/EPIC-{epic_id}-*.md",
        f"docs/prd/epics/epic-{epic_id}.md",
        f"docs/prd/epics/EPIC-{epic_id}*.md",
    ]

    for pattern in patterns:
        full_pattern = str(base_path / pattern)
        matches = glob.glob(full_pattern)
        if matches:
            # è¿”å›ç›¸å¯¹è·¯å¾„
            match_path = Path(matches[0])
            try:
                return str(match_path.relative_to(base_path))
            except ValueError:
                return matches[0]

    # æœªæ‰¾åˆ°ï¼Œè¿”å›é»˜è®¤è·¯å¾„ï¼ˆè®© SM Agent æŠ¥å‘Šæœªæ‰¾åˆ°ï¼‰
    return f"docs/prd/epic-{epic_id}.md"


# ============================================================================
# PostProcess è¾…åŠ©å‡½æ•° - æ„å»º .worktree-result.json æ ¼å¼
# ============================================================================
# âœ… Reference: scripts/daemon/story_file_updater.py:255-399
# å°† dev_outcome + qa_outcome åˆå¹¶ä¸º PostProcessHook æœŸæœ›çš„æ ¼å¼

def _build_worktree_result(
    story_id: str,
    dev_outcome: Dict[str, Any],
    qa_outcome: Dict[str, Any],
    story_drafts: List[Dict[str, Any]] = None,
) -> Dict[str, Any]:
    """
    æ„å»º .worktree-result.json æ ¼å¼çš„ç»“æœæ•°æ®ã€‚

    å°† BMad Orchestrator çš„ dev_outcome å’Œ qa_outcome åˆå¹¶ä¸º
    PostProcessHook æœŸæœ›çš„ç»Ÿä¸€æ ¼å¼ã€‚

    Args:
        story_id: Story ID (e.g., "14.4")
        dev_outcome: Dev Node è¿”å›çš„å¼€å‘ç»“æœ
        qa_outcome: QA Node è¿”å›çš„è´¨é‡å®¡æŸ¥ç»“æœ
        story_drafts: SM Node ç”Ÿæˆçš„ Story è‰ç¨¿åˆ—è¡¨ (å¯é€‰)

    Returns:
        ç¬¦åˆ PostProcessHook æœŸæœ›æ ¼å¼çš„å­—å…¸
    """
    # ä» story_drafts ä¸­æå– story_title
    story_title = f"Story {story_id}"
    if story_drafts:
        for draft in story_drafts:
            if draft.get("story_id") == story_id:
                story_title = draft.get("title", story_title)
                break

    # æ„å»º dev_record
    dev_record = {
        "agent_model": dev_outcome.get("agent_model", "Claude Sonnet 4.5 (*epic-develop)"),
        "duration_seconds": dev_outcome.get("duration_seconds", 0),
        "files_created": dev_outcome.get("files_created", []),
        "files_modified": dev_outcome.get("files_modified", []),
        "completion_notes": f"Story {story_id} è‡ªåŠ¨åŒ–å¼€å‘å®Œæˆ (*epic-develop)",
    }

    # æ„å»º qa_record
    qa_record = {
        "quality_score": qa_outcome.get("quality_score", 85),
        "ac_coverage": qa_outcome.get("ac_coverage", {}),
        "issues_found": qa_outcome.get("issues_found", []),
        "recommendations": qa_outcome.get("recommendations", []),
        "adr_compliance": qa_outcome.get("adr_compliance", True),
    }

    # æ„å»ºå®Œæ•´ç»“æœ
    result = {
        "story_id": story_id,
        "story_title": story_title,
        "dev_record": dev_record,
        "qa_record": qa_record,
        "qa_gate": qa_outcome.get("qa_gate", "PASS"),
        "timestamp": qa_outcome.get("timestamp", datetime.now().isoformat()),
        "commit_sha": "pending",  # å°†åœ¨ commit_node ä¸­æ›´æ–°
        "test_count": dev_outcome.get("tests_added", 0),
        "test_coverage": dev_outcome.get("test_coverage", 0),
        "tests_passed": dev_outcome.get("tests_passed", True),
        "fix_attempts": 0,
        "duration_seconds": dev_outcome.get("duration_seconds", 0) + qa_outcome.get("duration_seconds", 0),
    }

    return result


def _write_worktree_result(worktree_path: Path, result_data: Dict[str, Any]) -> bool:
    """
    å†™å…¥ .worktree-result.json æ–‡ä»¶ã€‚

    Args:
        worktree_path: Worktree è·¯å¾„
        result_data: ç»“æœæ•°æ®

    Returns:
        True if successful, False otherwise
    """
    result_file = worktree_path / ".worktree-result.json"
    try:
        with open(result_file, "w", encoding="utf-8") as f:
            json.dump(result_data, f, indent=2, ensure_ascii=False)
        print(f"[PostProcess] Wrote {result_file}")
        return True
    except Exception as e:
        print(f"[PostProcess] Error writing result file: {e}")
        return False


async def _run_post_process_hook(
    base_path: Path,
    story_id: str,
    worktree_path: Path,
    dev_outcome: Dict[str, Any],
    qa_outcome: Dict[str, Any],
    story_drafts: List[Dict[str, Any]] = None,
    session_id: str = "epic-develop",
) -> bool:
    """
    æ‰§è¡Œ PostProcessHook æ›´æ–° Story æ–‡ä»¶å’Œç”Ÿæˆ QA Gate YAMLã€‚

    Args:
        base_path: ä¸»ä»“åº“è·¯å¾„
        story_id: Story ID
        worktree_path: Worktree è·¯å¾„
        dev_outcome: å¼€å‘ç»“æœ
        qa_outcome: QA ç»“æœ
        story_drafts: Story è‰ç¨¿åˆ—è¡¨
        session_id: ä¼šè¯ ID

    Returns:
        True if successful, False otherwise
    """
    try:
        # æ„å»ºç»“æœæ•°æ®
        result_data = _build_worktree_result(
            story_id=story_id,
            dev_outcome=dev_outcome,
            qa_outcome=qa_outcome,
            story_drafts=story_drafts,
        )

        # å†™å…¥ .worktree-result.json
        if not _write_worktree_result(worktree_path, result_data):
            return False

        # è·å– PostProcessHook ç±»
        PPH = _ensure_post_process_hook()
        if not PPH:
            print("[PostProcess] Warning: PostProcessHook not available")
            return False

        # åˆ›å»º hook å®ä¾‹å¹¶æ‰§è¡Œ
        hook = PPH(base_path)
        result = hook.process(
            story_id=story_id,
            worktree_path=worktree_path,
            session_id=session_id,
        )

        if result.is_success():
            print(f"[PostProcess] âœ… Story {story_id} documents updated successfully")
            return True
        else:
            print(f"[PostProcess] âš ï¸ Story {story_id} post-process incomplete: {result.errors}")
            return False

    except Exception as e:
        print(f"[PostProcess] Error for Story {story_id}: {e}")
        return False


# ============================================================================
# Worktree ç®¡ç†è¾…åŠ©å‡½æ•°
# ============================================================================

async def create_worktree(
    base_path: Path,
    worktree_base: Path,
    story_id: str,
    branch_name: Optional[str] = None,
) -> Path:
    """
    åˆ›å»º Git Worktree

    Args:
        base_path: ä¸»ä»“åº“è·¯å¾„
        worktree_base: Worktree çˆ¶ç›®å½•
        story_id: Story ID
        branch_name: åˆ†æ”¯å (é»˜è®¤: develop-{story_id})

    Returns:
        Worktree è·¯å¾„

    Raises:
        RuntimeError: å¦‚æœ worktree åˆ›å»ºå¤±è´¥
    """
    import shutil

    if branch_name is None:
        branch_name = f"develop-{story_id}"

    worktree_path = worktree_base / f"Canvas-{branch_name}"
    print(f"[Worktree] Creating worktree: {worktree_path} (branch: {branch_name})")

    # å¦‚æœç›®å½•å·²å­˜åœ¨ï¼Œå…ˆæ¸…ç†
    if worktree_path.exists():
        git_file = worktree_path / ".git"
        if git_file.exists():
            # æ˜¯æœ‰æ•ˆçš„ git worktreeï¼Œä½¿ç”¨ git å‘½ä»¤ç§»é™¤
            print(f"[Worktree] Removing existing worktree: {worktree_path}")
            proc = await asyncio.create_subprocess_exec(
                'git', 'worktree', 'remove', str(worktree_path), '--force',
                cwd=str(base_path),
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
            )
            stdout, stderr = await proc.communicate()
            if proc.returncode != 0:
                print(f"[Worktree] WARN: git worktree remove failed: {stderr.decode()}")
                # å›é€€åˆ° shutil
                shutil.rmtree(worktree_path, ignore_errors=True)
        else:
            # å­¤ç«‹ç›®å½•ï¼ˆä¸æ˜¯æœ‰æ•ˆ worktreeï¼‰ï¼Œç›´æ¥åˆ é™¤
            print(f"[Worktree] Removing orphaned directory: {worktree_path}")
            shutil.rmtree(worktree_path, ignore_errors=True)

    # æ£€æŸ¥åˆ†æ”¯æ˜¯å¦å·²å­˜åœ¨
    proc = await asyncio.create_subprocess_exec(
        'git', 'branch', '--list', branch_name,
        cwd=str(base_path),
        stdout=asyncio.subprocess.PIPE,
        stderr=asyncio.subprocess.PIPE,
    )
    stdout, stderr = await proc.communicate()
    branch_exists = bool(stdout.decode().strip())

    # åˆ›å»ºæ–° worktree
    if branch_exists:
        # åˆ†æ”¯å·²å­˜åœ¨ï¼Œä¸ä½¿ç”¨ -b
        print(f"[Worktree] Branch '{branch_name}' exists, creating worktree without -b")
        proc = await asyncio.create_subprocess_exec(
            'git', 'worktree', 'add', str(worktree_path), branch_name,
            cwd=str(base_path),
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE,
        )
    else:
        # åˆ†æ”¯ä¸å­˜åœ¨ï¼Œä½¿ç”¨ -b åˆ›å»ºæ–°åˆ†æ”¯
        print(f"[Worktree] Creating new branch '{branch_name}'")
        proc = await asyncio.create_subprocess_exec(
            'git', 'worktree', 'add', '-b', branch_name, str(worktree_path),
            cwd=str(base_path),
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE,
        )

    stdout, stderr = await proc.communicate()

    if proc.returncode != 0:
        error_msg = stderr.decode() if stderr else "Unknown error"
        print(f"[Worktree] ERROR: git worktree add failed: {error_msg}")
        raise RuntimeError(f"Failed to create worktree: {error_msg}")

    # éªŒè¯ worktree åˆ›å»ºæˆåŠŸ
    git_file = worktree_path / ".git"
    if not git_file.exists():
        raise RuntimeError(f"Worktree created but .git file missing: {worktree_path}")

    print(f"[Worktree] SUCCESS: Created worktree at {worktree_path}")
    return worktree_path


async def remove_worktree(base_path: Path, worktree_path: Path) -> bool:
    """
    å®‰å…¨ç§»é™¤ Worktree

    å¥å£®æ€§æ”¹è¿›:
    1. æ£€æŸ¥ç›®å½•æ˜¯å¦æ˜¯æœ‰æ•ˆçš„ git worktree (.git æ–‡ä»¶å­˜åœ¨)
    2. å¦‚æœ git worktree remove å¤±è´¥ï¼Œå›é€€åˆ° shutil.rmtree
    3. è¿”å›æ“ä½œæ˜¯å¦æˆåŠŸ
    """
    import shutil

    # æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆçš„ git worktree
    git_file = worktree_path / ".git"
    if not worktree_path.exists():
        print(f"[Worktree] Directory does not exist: {worktree_path}")
        return True  # å·²ç»ä¸å­˜åœ¨ï¼Œè§†ä¸ºæˆåŠŸ

    if not git_file.exists():
        # å­¤ç«‹ç›®å½•ï¼ˆä¸æ˜¯æœ‰æ•ˆ worktreeï¼‰ï¼Œç›´æ¥åˆ é™¤
        try:
            shutil.rmtree(worktree_path)
            print(f"[Worktree] Removed orphaned directory: {worktree_path}")
            return True
        except Exception as e:
            print(f"[Worktree] [WARN] Failed to remove orphaned directory: {e}")
            return False

    # æ˜¯æœ‰æ•ˆ worktreeï¼Œä½¿ç”¨ git å‘½ä»¤
    proc = await asyncio.create_subprocess_exec(
        'git', 'worktree', 'remove', str(worktree_path), '--force',
        cwd=str(base_path),
        stdout=asyncio.subprocess.PIPE,
        stderr=asyncio.subprocess.PIPE,
    )
    stdout, stderr = await proc.communicate()

    if proc.returncode != 0:
        print(f"[Worktree] [WARN] git worktree remove failed: {stderr.decode()}")
        # å›é€€åˆ° shutil
        try:
            shutil.rmtree(worktree_path)
            print("[Worktree] Fallback: removed via shutil")
            return True
        except Exception as e:
            print(f"[Worktree] [ERROR] Failed to remove worktree: {e}")
            return False

    return True


async def git_add_and_commit(worktree_path: Path, message: str) -> bool:
    """
    åœ¨ worktree ä¸­æ‰§è¡Œ git add å’Œ commit

    Args:
        worktree_path: Worktree ç›®å½•è·¯å¾„
        message: Commit æ¶ˆæ¯

    Returns:
        æ˜¯å¦æˆåŠŸ
    """
    # git add -A
    proc = await asyncio.create_subprocess_exec(
        'git', 'add', '-A',
        cwd=str(worktree_path),
        stdout=asyncio.subprocess.PIPE,
        stderr=asyncio.subprocess.PIPE,
    )
    stdout, stderr = await proc.communicate()

    if proc.returncode != 0:
        print(f"[Git] [WARN] git add failed: {stderr.decode()}")
        # ç»§ç»­å°è¯• commit

    # git commit (skip pre-commit hooks in worktrees - they run on merge to main)
    proc = await asyncio.create_subprocess_exec(
        'git', 'commit', '-m', message, '--allow-empty', '--no-verify',
        cwd=str(worktree_path),
        stdout=asyncio.subprocess.PIPE,
        stderr=asyncio.subprocess.PIPE,
    )
    stdout, stderr = await proc.communicate()

    if proc.returncode != 0:
        stderr_text = stderr.decode()
        # "nothing to commit" ä¸æ˜¯é”™è¯¯
        if "nothing to commit" in stderr_text.lower():
            print("[Git] No changes to commit")
            return True
        print(f"[Git] [WARN] git commit failed: {stderr_text}")
        return False

    print(f"[Git] Committed: {message}")
    return True


async def merge_branch_to_main(base_path: Path, branch_name: str, message: str) -> bool:
    """
    å°†åˆ†æ”¯åˆå¹¶åˆ° main

    Args:
        base_path: ä¸»ä»“åº“è·¯å¾„
        branch_name: è¦åˆå¹¶çš„åˆ†æ”¯å
        message: Merge commit æ¶ˆæ¯

    Returns:
        æ˜¯å¦æˆåŠŸ
    """
    # ä¿å­˜å½“å‰åˆ†æ”¯
    proc = await asyncio.create_subprocess_exec(
        'git', 'rev-parse', '--abbrev-ref', 'HEAD',
        cwd=str(base_path),
        stdout=asyncio.subprocess.PIPE,
    )
    stdout, _ = await proc.communicate()
    original_branch = stdout.decode().strip()

    try:
        # checkout main
        proc = await asyncio.create_subprocess_exec(
            'git', 'checkout', 'main',
            cwd=str(base_path),
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE,
        )
        await proc.communicate()

        if proc.returncode != 0:
            print("[Git] [WARN] Failed to checkout main")
            return False

        # merge
        proc = await asyncio.create_subprocess_exec(
            'git', 'merge', branch_name, '--no-ff', '-m', message,
            cwd=str(base_path),
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE,
        )
        stdout, stderr = await proc.communicate()

        if proc.returncode != 0:
            print(f"[Git] [ERROR] Merge failed: {stderr.decode()}")
            # å°è¯• abort merge
            await asyncio.create_subprocess_exec(
                'git', 'merge', '--abort',
                cwd=str(base_path),
            )
            return False

        print(f"[Git] Merged {branch_name} to main")
        return True

    finally:
        # æ¢å¤åŸåˆ†æ”¯ï¼ˆå¦‚æœä¸æ˜¯ mainï¼‰
        if original_branch and original_branch != 'main':
            await asyncio.create_subprocess_exec(
                'git', 'checkout', original_branch,
                cwd=str(base_path),
            )


async def verify_story_file_exists(worktree_path: Path, story_file: str) -> bool:
    """
    éªŒè¯ Story æ–‡ä»¶å­˜åœ¨ä¸”éç©º

    Args:
        worktree_path: Worktree ç›®å½•è·¯å¾„
        story_file: Story æ–‡ä»¶ç›¸å¯¹è·¯å¾„

    Returns:
        æ–‡ä»¶æ˜¯å¦å­˜åœ¨ä¸”æœ‰æ•ˆ
    """
    file_path = worktree_path / story_file

    if not file_path.exists():
        print(f"[Verify] Story file does not exist: {file_path}")
        return False

    file_size = file_path.stat().st_size
    if file_size < 100:  # æœ€å°æœ‰æ•ˆå¤§å°
        print(f"[Verify] Story file too small ({file_size} bytes): {file_path}")
        return False

    print(f"[Verify] Story file verified: {file_path} ({file_size} bytes)")
    return True


def resolve_story_file_path(worktree_path: Path, story_id: str) -> str | None:
    """
    è§£æ Story æ–‡ä»¶è·¯å¾„ï¼Œæ”¯æŒå¤šç§å‘½åæ ¼å¼

    å‘½åæ ¼å¼ä¼˜å…ˆçº§:
    1. {story_id}.story.md (ä¾‹å¦‚: 14.1.story.md)
    2. story-{story_id}.story.md (ä¾‹å¦‚: story-14.1.story.md)
    3. {story_id}.md (ä¾‹å¦‚: 14.1.md)
    4. story-{story_id}.md (ä¾‹å¦‚: story-14.1.md)

    Args:
        worktree_path: Worktree ç›®å½•è·¯å¾„
        story_id: Story ID (ä¾‹å¦‚ "14.1")

    Returns:
        æ‰¾åˆ°çš„æ–‡ä»¶ç›¸å¯¹è·¯å¾„ï¼Œå¦‚æœéƒ½ä¸å­˜åœ¨åˆ™è¿”å› None
    """
    patterns = [
        f"docs/stories/{story_id}.story.md",
        f"docs/stories/story-{story_id}.story.md",
        f"docs/stories/{story_id}.md",
        f"docs/stories/story-{story_id}.md",
    ]

    for pattern in patterns:
        file_path = worktree_path / pattern
        if file_path.exists():
            file_size = file_path.stat().st_size
            if file_size >= 100:  # æœ€å°æœ‰æ•ˆå¤§å°
                print(f"[ResolveStoryPath] Found: {pattern} ({file_size} bytes)")
                return pattern
            else:
                print(f"[ResolveStoryPath] Found but too small ({file_size} bytes): {pattern}")

    print(f"[ResolveStoryPath] No valid story file found for {story_id}")
    print(f"[ResolveStoryPath] Searched patterns: {patterns}")
    return None


# ============================================================================
# Node 1: SM Agent - Story åˆ›å»º
# ============================================================================

@enforce_bmad_workflow
async def sm_node(state: BmadOrchestratorState) -> Dict[str, Any]:
    """
    SM (Scrum Master) Agent èŠ‚ç‚¹ - Story åˆ›å»º

    âœ… Verified from LangGraph Skill:
    - Node signature: async def node(state: State) -> dict
    - Return dict with state updates

    åŠŸèƒ½:
    1. ä¸ºæ¯ä¸ª story_id åˆ›å»º Story è‰ç¨¿
    2. è¯»å– core-config.yaml è·å–æ–‡æ¡£è·¯å¾„
    3. ç”ŸæˆåŒ…å« SDD å¼•ç”¨çš„ Story æ–‡ä»¶

    Args:
        state: å½“å‰ç¼–æ’çŠ¶æ€

    Returns:
        State æ›´æ–°:
        - story_drafts: List[StoryDraft]
        - sm_status: "completed" | "failed"
        - current_phase: "PO"
    """
    print(f"[SM Node] Starting SM phase for Epic {state['epic_id']}")
    print(f"[SM Node] Stories to create: {state['story_ids']}")

    base_path = Path(state["base_path"])
    worktree_base = Path(state["worktree_base"])
    story_ids = state["story_ids"]
    epic_id = state["epic_id"]

    # è·å– Epic æ–‡ä»¶è·¯å¾„ (æ”¯æŒå¤šç§å‘½åæ¨¡å¼)
    epic_file = find_epic_file(base_path, epic_id)
    print(f"[SM Node] Found Epic file: {epic_file}")

    spawner = BmadSessionSpawner(
        max_turns=state.get("max_turns", DEFAULT_MAX_TURNS),
        ultrathink=DEFAULT_ULTRATHINK,
        timeout_seconds=state.get("timeout", DEFAULT_TIMEOUT),
    )

    story_drafts: List[StoryDraft] = []
    blockers: List[BlockerInfo] = []

    # é¡ºåºæ‰§è¡Œ SM (Story åˆ›å»ºéœ€è¦æŒ‰é¡ºåº)
    for story_id in story_ids:
        print(f"[SM Node] Creating Story {story_id}...")

        # åˆ›å»ºä¸´æ—¶ worktree ç”¨äº SM
        worktree_path = await create_worktree(
            base_path=base_path,
            worktree_base=worktree_base,
            story_id=f"sm-{story_id}",
            branch_name=f"sm-draft-{story_id}",
        )

        try:
            # ç”Ÿæˆ SM ä¼šè¯
            session_id = await spawner.spawn_session(
                phase="SM",
                story_id=story_id,
                worktree_path=worktree_path,
                epic_id=epic_id,
                epic_file=epic_file,
            )

            # ç­‰å¾…å®Œæˆï¼ˆå¯ç”¨å¡ä½æ£€æµ‹ï¼‰
            # ä½¿ç”¨ timeout ä½œä¸º stuck æ£€æµ‹é˜ˆå€¼ï¼ˆé»˜è®¤ 300 ç§’å¤ªçŸ­ï¼‰
            stuck_threshold = state.get("timeout", DEFAULT_TIMEOUT)
            log_file = worktree_path / "sm-output.log"
            returncode, partial = await spawner.wait_for_session(
                session_id, log_file=log_file, stuck_threshold_seconds=stuck_threshold
            )

            # è·å–ç»“æœ
            result = await spawner.get_session_result("SM", worktree_path)

            # P0 ä¿®å¤: Fallback - å½“ .sm-result.json ä¸å­˜åœ¨æ—¶ï¼Œæ£€æŸ¥ Story æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            story_file_fallback = f"docs/stories/{story_id}.story.md"
            if result is None:
                print("[SM Node] [WARN] No .sm-result.json found, checking if Story file exists...")
                if await verify_story_file_exists(worktree_path, story_file_fallback):
                    print(f"[SM Node] [OK] Story file found via fallback: {story_file_fallback}")
                    # åˆ›å»ºåˆæˆçš„æˆåŠŸ result
                    result = SMResult(
                        story_id=story_id,
                        outcome="SUCCESS",
                        timestamp=datetime.now().isoformat(),
                        story_file=story_file_fallback,
                        title=f"Story {story_id}",
                        sdd_references=[],
                        adr_references=[],
                        checklist_passed=True,  # Fallback è§†ä¸ºé€šè¿‡
                        raw_data={"fallback": True},
                    )
                else:
                    print("[SM Node] [FAIL] No .sm-result.json and no Story file found")
                    blocker: BlockerInfo = {
                        "story_id": story_id,
                        "phase": "SM",
                        "blocker_type": "NO_OUTPUT",
                        "description": "SM session produced no .sm-result.json and no Story file",
                        "detected_at": datetime.now().isoformat(),
                        "retry_count": 0,
                        "resolution": None,
                    }
                    blockers.append(blocker)
                    continue  # è·³è¿‡è¿™ä¸ª Story

            if result and isinstance(result, SMResult):
                if result.outcome == "SUCCESS" and result.checklist_passed:
                    story_file = result.story_file or f"docs/stories/{story_id}.story.md"

                    # P0 ä¿®å¤: éªŒè¯ Story æ–‡ä»¶ç¡®å®å­˜åœ¨
                    if not await verify_story_file_exists(worktree_path, story_file):
                        blocker: BlockerInfo = {
                            "story_id": story_id,
                            "phase": "SM",
                            "blocker_type": "FILE_NOT_CREATED",
                            "description": f"SM claimed SUCCESS but Story file not found: {story_file}",
                            "detected_at": datetime.now().isoformat(),
                            "retry_count": 0,
                            "resolution": None,
                        }
                        blockers.append(blocker)
                        print(f"[SM Node] [FAIL] Story {story_id} file not found despite SUCCESS")
                        continue  # è·³è¿‡è¿™ä¸ª Storyï¼Œç»§ç»­ä¸‹ä¸€ä¸ª

                    # P0 ä¿®å¤: Git add + commit åœ¨ worktree ä¸­
                    branch_name = f"sm-draft-{story_id}"
                    commit_success = await git_add_and_commit(
                        worktree_path,
                        f"SM: Create Story {story_id} draft\n\n[BMad Orchestrator] Auto-generated Story draft"
                    )

                    if commit_success:
                        # P0 ä¿®å¤: Merge åˆ° main åˆ†æ”¯
                        merge_success = await merge_branch_to_main(
                            base_path,
                            branch_name,
                            f"Merge Story {story_id} draft from SM phase"
                        )

                        if not merge_success:
                            print(f"[SM Node] [WARN] Merge failed for {story_id}, attempting direct copy")
                            # åˆå¹¶å¤±è´¥æ—¶çš„å›é€€ç­–ç•¥ï¼šç›´æ¥å¤åˆ¶æ–‡ä»¶åˆ°ä¸»ä»“åº“
                            import shutil
                            src_file = worktree_path / story_file
                            dst_file = base_path / story_file
                            dst_file.parent.mkdir(parents=True, exist_ok=True)
                            try:
                                shutil.copy2(src_file, dst_file)
                                print(f"[SM Node] [OK] Story file copied directly: {dst_file}")
                            except Exception as copy_err:
                                print(f"[SM Node] [ERROR] Failed to copy file: {copy_err}")
                    else:
                        print(f"[SM Node] [WARN] Commit failed for {story_id}")

                    # åˆ›å»º StoryDraft è®°å½•
                    story_draft: StoryDraft = {
                        "story_id": story_id,
                        "story_file": story_file,
                        "epic_id": epic_id,
                        "title": result.title or f"Story {story_id}",
                        "created_at": datetime.now().isoformat(),
                        "sdd_references": result.sdd_references,
                        "adr_references": result.adr_references,
                        "status": "draft",
                    }
                    story_drafts.append(story_draft)
                    print(f"[SM Node] [OK] Story {story_id} created and persisted to main")
                else:
                    # P0 ä¿®å¤: Fallback - å½“ outcome=SUCCESS ä½† checklist_passed=False æ—¶
                    # æ£€æŸ¥ Story æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœå­˜åœ¨åˆ™å¼ºåˆ¶è§†ä¸ºæˆåŠŸ
                    if result.outcome == "SUCCESS" and not result.checklist_passed:
                        story_file_check = result.story_file or f"docs/stories/{story_id}.story.md"
                        print("[SM Node] [WARN] outcome=SUCCESS but checklist_passed=False, checking file...")
                        if await verify_story_file_exists(worktree_path, story_file_check):
                            print(f"[SM Node] [OK] Story file exists, forcing success: {story_file_check}")
                            # å¼ºåˆ¶è®¾ç½® checklist_passed=True å¹¶ç»§ç»­å¤„ç†
                            result.checklist_passed = True
                            # ä½¿ç”¨ goto æ¨¡å¼ - é‡æ–°è¿›å…¥æˆåŠŸåˆ†æ”¯
                            story_file = story_file_check

                            # å¤åˆ¶æˆåŠŸåˆ†æ”¯çš„é€»è¾‘
                            branch_name = f"sm-draft-{story_id}"
                            commit_success = await git_add_and_commit(
                                worktree_path,
                                f"SM: Create Story {story_id} draft\n\n[BMad Orchestrator] Auto-generated Story draft (fallback)"
                            )

                            if commit_success:
                                merge_success = await merge_branch_to_main(
                                    base_path,
                                    branch_name,
                                    f"Merge Story {story_id} draft from SM phase"
                                )
                                if not merge_success:
                                    print(f"[SM Node] [WARN] Merge failed for {story_id}, attempting direct copy")
                                    import shutil
                                    src_file = worktree_path / story_file
                                    dst_file = base_path / story_file
                                    dst_file.parent.mkdir(parents=True, exist_ok=True)
                                    try:
                                        shutil.copy2(src_file, dst_file)
                                        print(f"[SM Node] [OK] Story file copied directly: {dst_file}")
                                    except Exception as copy_err:
                                        print(f"[SM Node] [ERROR] Failed to copy file: {copy_err}")

                            story_draft: StoryDraft = {
                                "story_id": story_id,
                                "story_file": story_file,
                                "epic_id": epic_id,
                                "title": result.title or f"Story {story_id}",
                                "created_at": datetime.now().isoformat(),
                                "sdd_references": result.sdd_references,
                                "adr_references": result.adr_references,
                                "status": "draft",
                            }
                            story_drafts.append(story_draft)
                            print(f"[SM Node] [OK] Story {story_id} created via fallback and persisted")
                            continue  # æˆåŠŸï¼Œè·³åˆ°ä¸‹ä¸€ä¸ª Story

                    # çœŸæ­£çš„å¤±è´¥æƒ…å†µ
                    blocker: BlockerInfo = {
                        "story_id": story_id,
                        "phase": "SM",
                        "blocker_type": "MISSING_SPECS" if not result.checklist_passed else "ERROR",
                        "description": f"SM failed: {result.outcome}",
                        "detected_at": datetime.now().isoformat(),
                        "retry_count": 0,
                        "resolution": None,
                    }
                    blockers.append(blocker)
                    print(f"[SM Node] [FAIL] Story {story_id} failed: {result.outcome}")

        except asyncio.TimeoutError:
            blocker: BlockerInfo = {
                "story_id": story_id,
                "phase": "SM",
                "blocker_type": "TIMEOUT",
                "description": "SM session timed out",
                "detected_at": datetime.now().isoformat(),
                "retry_count": 0,
                "resolution": None,
            }
            blockers.append(blocker)
            print(f"[SM Node] â° Story {story_id} timed out")

        except Exception as e:
            blocker: BlockerInfo = {
                "story_id": story_id,
                "phase": "SM",
                "blocker_type": "ERROR",
                "description": str(e),
                "detected_at": datetime.now().isoformat(),
                "retry_count": 0,
                "resolution": None,
            }
            blockers.append(blocker)
            print(f"[SM Node] [ERROR] Story {story_id} error: {e}")

        finally:
            # æ¸…ç†ä¸´æ—¶ worktree
            await remove_worktree(base_path, worktree_path)

    # ç¡®å®šçŠ¶æ€
    if story_drafts:
        sm_status = "completed"
        next_phase = "PO"
    else:
        sm_status = "failed"
        next_phase = "HALT"

    print(f"[SM Node] Completed: {len(story_drafts)} drafts, {len(blockers)} blockers")

    return {
        "story_drafts": story_drafts,
        "sm_status": sm_status,
        "sm_error": blockers[0]["description"] if blockers else None,
        "blockers": blockers,
        "current_phase": next_phase,
    }


# ============================================================================
# Node 2: PO Agent - Story éªŒè¯ (SoT è‡ªåŠ¨è§£å†³)
# ============================================================================

@enforce_bmad_workflow
async def po_node(state: BmadOrchestratorState) -> Dict[str, Any]:
    """
    PO (Product Owner) Agent èŠ‚ç‚¹ - Story éªŒè¯

    âœ… Verified from LangGraph Skill:
    - Node signature: async def node(state: State) -> dict

    åŠŸèƒ½:
    1. éªŒè¯æ¯ä¸ª Story è‰ç¨¿
    2. æ£€æµ‹ SoT (Source of Truth) å†²çª
    3. è‡ªåŠ¨è§£å†³å†²çª (æŒ‰å±‚çº§è§„åˆ™)
    4. æ‰¹å‡†æˆ–æ‹’ç» Stories

    SoT Hierarchy:
    1. PRD (Level 1) - WHAT
    2. Architecture (Level 2) - HOW
    3. JSON Schema (Level 3)
    4. OpenAPI Spec (Level 4)
    5. Story (Level 5)
    6. Code (Level 6)

    Args:
        state: å½“å‰ç¼–æ’çŠ¶æ€

    Returns:
        State æ›´æ–°:
        - approved_stories: List[str]
        - rejected_stories: List[Dict]
        - sot_resolutions: List[SoTResolution]
        - po_status: "completed" | "failed"
        - current_phase: "ANALYSIS"
    """
    print("[PO Node] Starting PO validation phase")
    print(f"[PO Node] Stories to validate: {len(state['story_drafts'])}")

    base_path = Path(state["base_path"])
    worktree_base = Path(state["worktree_base"])
    story_drafts = state["story_drafts"]

    spawner = BmadSessionSpawner(
        max_turns=state.get("max_turns", DEFAULT_MAX_TURNS),
        ultrathink=DEFAULT_ULTRATHINK,
        timeout_seconds=state.get("timeout", DEFAULT_TIMEOUT),
    )

    approved_stories: List[str] = []
    rejected_stories: List[Dict[str, str]] = []
    sot_resolutions: List[SoTResolution] = []
    # Note: blockers list removed as it was unused

    # é¡ºåºæ‰§è¡Œ PO éªŒè¯
    for draft in story_drafts:
        story_id = draft["story_id"]
        story_file = draft["story_file"]

        print(f"[PO Node] Validating Story {story_id}...")

        # åˆ›å»ºä¸´æ—¶ worktree
        worktree_path = await create_worktree(
            base_path=base_path,
            worktree_base=worktree_base,
            story_id=f"po-{story_id}",
            branch_name=f"po-validate-{story_id}",
        )

        try:
            # ç”Ÿæˆ PO ä¼šè¯
            session_id = await spawner.spawn_session(
                phase="PO",
                story_id=story_id,
                worktree_path=worktree_path,
                story_file=story_file,
            )

            # ç­‰å¾…å®Œæˆï¼ˆå¯ç”¨å¡ä½æ£€æµ‹ï¼‰
            # ä½¿ç”¨ timeout ä½œä¸º stuck æ£€æµ‹é˜ˆå€¼ï¼ˆé»˜è®¤ 300 ç§’å¤ªçŸ­ï¼‰
            stuck_threshold = state.get("timeout", DEFAULT_TIMEOUT)
            log_file = worktree_path / "po-output.log"
            returncode, partial = await spawner.wait_for_session(
                session_id, log_file=log_file, stuck_threshold_seconds=stuck_threshold
            )

            # è·å–ç»“æœ
            result = await spawner.get_session_result("PO", worktree_path)

            if result and isinstance(result, POResult):
                if result.outcome in ("APPROVED", "AUTO_RESOLVED"):
                    approved_stories.append(story_id)

                    # è®°å½• SoT è§£å†³
                    for resolution_data in result.sot_resolutions:
                        resolution: SoTResolution = {
                            "story_id": story_id,
                            "conflict_type": resolution_data.get("conflict_type", "UNKNOWN"),
                            "source_a": resolution_data.get("source_a", ""),
                            "source_b": resolution_data.get("source_b", ""),
                            "field_name": resolution_data.get("field_name", ""),
                            "value_a": resolution_data.get("value_a", ""),
                            "value_b": resolution_data.get("value_b", ""),
                            "resolution": resolution_data.get("resolution", ""),
                            "sot_level_applied": resolution_data.get("sot_level_applied", "Story"),
                            "resolved_at": datetime.now().isoformat(),
                        }
                        sot_resolutions.append(resolution)

                    print(f"[PO Node] [OK] Story {story_id} approved (conflicts: {result.sot_conflicts_found})")

                else:  # REJECTED
                    rejected_stories.append({
                        "story_id": story_id,
                        "reason": result.rejection_reason or "Validation failed",
                    })
                    print(f"[PO Node] [FAIL] Story {story_id} rejected: {result.rejection_reason}")

        except asyncio.TimeoutError:
            rejected_stories.append({
                "story_id": story_id,
                "reason": "PO validation timed out",
            })
            print(f"[PO Node] â° Story {story_id} timed out")

        except Exception as e:
            rejected_stories.append({
                "story_id": story_id,
                "reason": str(e),
            })
            print(f"[PO Node] [ERROR] Story {story_id} error: {e}")

        finally:
            # æ¸…ç†ä¸´æ—¶ worktree
            await remove_worktree(base_path, worktree_path)

    # ç¡®å®šçŠ¶æ€
    if approved_stories:
        po_status = "completed"
        next_phase = "ANALYSIS"
    else:
        po_status = "failed"
        next_phase = "HALT"

    print(f"[PO Node] Completed: {len(approved_stories)} approved, {len(rejected_stories)} rejected")
    print(f"[PO Node] SoT resolutions: {len(sot_resolutions)}")

    return {
        "approved_stories": approved_stories,
        "rejected_stories": rejected_stories,
        "sot_resolutions": sot_resolutions,
        "po_status": po_status,
        "current_phase": next_phase,
    }


# ============================================================================
# Node 3: Analysis - ä¾èµ–åˆ†æå’Œæ¨¡å¼é€‰æ‹©
# ============================================================================

@enforce_bmad_workflow
async def analysis_node(state: BmadOrchestratorState) -> Dict[str, Any]:
    """
    Analysis èŠ‚ç‚¹ - ä¾èµ–åˆ†æå’Œæ‰§è¡Œæ¨¡å¼é€‰æ‹©

    âœ… Verified from LangGraph Skill:
    - Node signature: async def node(state: State) -> dict

    åŠŸèƒ½:
    1. åˆ†æ Stories ä¹‹é—´çš„æ–‡ä»¶ä¾èµ–
    2. æ£€æµ‹æ½œåœ¨å†²çª
    3. é€‰æ‹©æ‰§è¡Œæ¨¡å¼: parallel | linear | hybrid
    4. ç”Ÿæˆæ‰§è¡Œè®¡åˆ’å’Œæ‰¹æ¬¡

    Args:
        state: å½“å‰ç¼–æ’çŠ¶æ€

    Returns:
        State æ›´æ–°:
        - execution_plan: ExecutionPlan
        - execution_mode: "parallel" | "linear" | "hybrid"
        - parallel_batches: List[List[str]]
        - conflict_matrix: Dict[str, List[str]]
        - current_phase: "DEV"
    """
    print("[Analysis Node] Starting dependency analysis")

    approved_stories = state["approved_stories"]
    mode_override = state.get("mode_override")
    base_path = Path(state["base_path"])

    print(f"[Analysis Node] Approved stories: {approved_stories}")

    # å¦‚æœåªæœ‰ä¸€ä¸ª Storyï¼Œç›´æ¥ linear
    if len(approved_stories) <= 1:
        return {
            "execution_plan": {
                "mode": "linear",
                "parallel_batches": [approved_stories],
                "linear_sequence": approved_stories,
                "conflicts": {},
                "estimated_duration_minutes": 30 * len(approved_stories),
            },
            "execution_mode": "linear",
            "parallel_batches": [approved_stories],
            "conflict_matrix": {},
            "current_phase": "DEV",
        }

    # å¯¼å…¥ä¾èµ–åˆ†æå™¨ (å»¶è¿Ÿå¯¼å…¥é¿å…å¾ªç¯ä¾èµ–)
    from .dependency_analyzer import analyze_dependencies

    # åˆ†æä¾èµ–
    analysis_result = await analyze_dependencies(
        story_ids=approved_stories,
        base_path=base_path,
    )

    # ç¡®å®šæ‰§è¡Œæ¨¡å¼
    if mode_override:
        execution_mode = mode_override
    else:
        # è‡ªåŠ¨æ£€æµ‹
        if analysis_result["conflicts"]:
            # æœ‰å†²çªï¼Œä½¿ç”¨ hybrid æˆ– linear
            conflict_ratio = len(analysis_result["conflicts"]) / (len(approved_stories) * (len(approved_stories) - 1) / 2)
            if conflict_ratio > 0.5:
                execution_mode = "linear"
            else:
                execution_mode = "hybrid"
        else:
            # æ— å†²çªï¼Œå…¨éƒ¨å¹¶è¡Œ
            execution_mode = "parallel"

    print(f"[Analysis Node] Execution mode: {execution_mode}")
    print(f"[Analysis Node] Conflicts found: {len(analysis_result['conflicts'])}")

    # ç”Ÿæˆæ‰¹æ¬¡
    if execution_mode == "parallel":
        parallel_batches = [approved_stories]  # å…¨éƒ¨ä¸€æ‰¹
    elif execution_mode == "linear":
        parallel_batches = [[s] for s in approved_stories]  # æ¯ä¸ªä¸€æ‰¹
    else:  # hybrid
        parallel_batches = analysis_result.get("batches", [[s] for s in approved_stories])

    return {
        "execution_plan": {
            "mode": execution_mode,
            "parallel_batches": parallel_batches,
            "linear_sequence": approved_stories,
            "conflicts": analysis_result["conflicts"],
            "estimated_duration_minutes": 30 * len(approved_stories) // len(parallel_batches),
        },
        "execution_mode": execution_mode,
        "parallel_batches": parallel_batches,
        "conflict_matrix": analysis_result["conflicts"],
        "current_phase": "DEV",
    }


# ============================================================================
# Node 3.5: SDD Pre-Validation - å¼€å‘å‰ SDD éªŒè¯ (v1.1.0)
# ============================================================================

@enforce_bmad_workflow
async def sdd_pre_validation_node(state: BmadOrchestratorState) -> Dict[str, Any]:
    """
    SDD Pre-Validation èŠ‚ç‚¹ - å¼€å‘å‰ SDD éªŒè¯ (v1.1.0)

    âœ… Verified from LangGraph Skill:
    - Node signature: async def node(state: State) -> dict

    åŠŸèƒ½:
    1. Tier 1 (è¦†ç›–ç‡): PRDâ†’OpenAPI/Schema â‰¥80% (å¼ºåˆ¶é˜»æ­¢)
    2. Tier 2 (æ¥æºéªŒè¯): x-source-verification metadata
    3. Tier 3 (ä¸€è‡´æ€§): PRDâ†”Schemaâ†”OpenAPI

    è§„åˆ™:
    - Tier 1 è¦†ç›–ç‡ <80% â†’ é˜»å¡ (HALT) [ç”¨æˆ·ç¡®è®¤: å¼ºåˆ¶é˜»æ­¢]
    - Tier 2 æ¥æºæœªéªŒè¯ â†’ é˜»å¡ (HALT)
    - Tier 3 è­¦å‘Š â†’ ç»§ç»­ (ä½†è®°å½•)

    Args:
        state: å½“å‰ç¼–æ’çŠ¶æ€

    Returns:
        State æ›´æ–°:
        - sdd_pre_validation_result: Dict
        - sdd_pre_status: "passed" | "warnings" | "failed" | "skipped"
        - current_phase: "DEV" | "HALT"
    """
    print("[SDD Pre-Validation Node] Starting pre-development SDD validation")

    base_path = Path(state["base_path"])
    approved_stories = state.get("approved_stories", [])

    if not approved_stories:
        print("[SDD Pre-Validation Node] No stories to validate, skipping")
        return {
            "sdd_pre_status": "skipped",
            "sdd_pre_validation_result": None,
            "current_phase": "DEV",  # ç»§ç»­åˆ° DEV
        }

    print(f"[SDD Pre-Validation Node] Validating SDD coverage for {len(approved_stories)} stories")

    # SDD æ–‡ä»¶è·¯å¾„
    specs_path = base_path / "specs"
    openapi_files = list(specs_path.glob("**/*.openapi.yml")) + list(specs_path.glob("**/*.openapi.yaml"))
    schema_files = list(specs_path.glob("**/*.schema.json"))
    prd_path = base_path / "docs" / "prd"

    # æ£€æŸ¥ SDD æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    has_openapi = len(openapi_files) > 0
    has_schema = len(schema_files) > 0
    has_prd = prd_path.exists() and any(prd_path.glob("*.md"))

    if not (has_openapi and has_schema and has_prd):
        print("[SDD Pre-Validation Node] [WARN] SDD files incomplete:")
        print(f"  - OpenAPI: {has_openapi} ({len(openapi_files)} files)")
        print(f"  - Schema: {has_schema} ({len(schema_files)} files)")
        print(f"  - PRD: {has_prd}")
        # æ–‡ä»¶ä¸å®Œæ•´ï¼Œä»¥è­¦å‘Šæ¨¡å¼ç»§ç»­
        return {
            "sdd_pre_status": "warnings",
            "sdd_pre_validation_result": {
                "tier1_coverage": {
                    "openapi_count": len(openapi_files),
                    "schema_count": len(schema_files),
                    "prd_exists": has_prd,
                    "coverage_percent": 0,
                },
                "tier1_passed": True,  # é™çº§ä¸ºè­¦å‘Š
                "tier2_source_verified": True,  # é™çº§ä¸ºè­¦å‘Š
                "tier3_consistency": {
                    "warnings": ["SDD files incomplete - validation degraded"],
                    "conflicts": [],
                },
                "tier3_passed": True,
                "overall_passed": True,
                "validation_timestamp": datetime.now().isoformat(),
                "blocking_issues": [],
            },
            "current_phase": "DEV",  # ç»§ç»­
        }

    # Tier 1: è¦†ç›–ç‡éªŒè¯
    tier1_passed = True
    tier2_passed = True
    tier3_passed = True
    blocking_issues = []
    warnings = []
    coverage_percent = 0

    try:
        # æ£€æŸ¥è¦†ç›–ç‡éªŒè¯è„šæœ¬
        coverage_script = base_path / "scripts" / "verify-sdd-coverage.py"

        if coverage_script.exists():
            print("[SDD Pre-Validation Node] Running Tier 1: Coverage validation...")
            proc = await asyncio.create_subprocess_exec(
                'python', str(coverage_script),
                '--threshold', '80',  # 80% é˜ˆå€¼
                cwd=str(base_path),
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
            )
            stdout, stderr = await proc.communicate()

            if proc.returncode != 0:
                tier1_passed = False
                error_msg = stderr.decode()[:300]
                blocking_issues.append(f"Tier 1 Coverage <80%: {error_msg}")
                print("[SDD Pre-Validation Node] [FAIL] Tier 1 failed - Coverage below 80%")
            else:
                # å°è¯•è§£æè¦†ç›–ç‡
                try:
                    import json
                    output = stdout.decode()
                    if output.strip():
                        result_data = json.loads(output)
                        coverage_percent = result_data.get("coverage_percent", 80)
                except (json.JSONDecodeError, UnicodeDecodeError, OSError):
                    coverage_percent = 80
                print(f"[SDD Pre-Validation Node] [OK] Tier 1 passed - Coverage: {coverage_percent}%")
        else:
            # è„šæœ¬ä¸å­˜åœ¨ï¼Œä¼°ç®—è¦†ç›–ç‡
            print("[SDD Pre-Validation Node] [WARN] Coverage script not found, estimating...")
            # ç®€å•ä¼°ç®—: OpenAPI + Schema æ–‡ä»¶æ•° * 20
            estimated_coverage = min(100, (len(openapi_files) + len(schema_files)) * 20)
            coverage_percent = estimated_coverage

            if estimated_coverage < 80:
                tier1_passed = False
                blocking_issues.append(f"Estimated coverage {estimated_coverage}% < 80% threshold")
                print(f"[SDD Pre-Validation Node] [FAIL] Tier 1 failed - Estimated coverage: {estimated_coverage}%")
            else:
                print(f"[SDD Pre-Validation Node] [OK] Tier 1 passed - Estimated coverage: {estimated_coverage}%")

        # Tier 2: æ¥æºéªŒè¯ (x-source-verification)
        source_script = base_path / "scripts" / "verify-sdd-source.py"

        if source_script.exists():
            print("[SDD Pre-Validation Node] Running Tier 2: Source verification...")
            proc = await asyncio.create_subprocess_exec(
                'python', str(source_script),
                cwd=str(base_path),
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
            )
            stdout, stderr = await proc.communicate()

            if proc.returncode != 0:
                tier2_passed = False
                error_msg = stderr.decode()[:300]
                blocking_issues.append(f"Tier 2 Source verification failed: {error_msg}")
                print("[SDD Pre-Validation Node] [FAIL] Tier 2 failed")
            else:
                print("[SDD Pre-Validation Node] [OK] Tier 2 passed")
        else:
            print("[SDD Pre-Validation Node] [WARN] Source verification script not found, skipping...")
            # æ£€æŸ¥ OpenAPI æ–‡ä»¶æ˜¯å¦æœ‰ x-source-verification
            source_verified = True
            for openapi_file in openapi_files[:3]:  # åªæ£€æŸ¥å‰3ä¸ª
                try:
                    content = openapi_file.read_text(encoding='utf-8')
                    if 'x-source-verification' not in content and 'x-prd-reference' not in content:
                        source_verified = False
                        warnings.append(f"{openapi_file.name}: Missing source verification metadata")
                except (OSError, UnicodeDecodeError):
                    pass

            if not source_verified:
                warnings.append("Some OpenAPI files missing source verification metadata")
            print("[SDD Pre-Validation Node] [OK] Tier 2 passed (simplified)")

        # Tier 3: ä¸€è‡´æ€§éªŒè¯ (PRDâ†”Schemaâ†”OpenAPI)
        consistency_script = base_path / "scripts" / "verify-sdd-consistency.py"

        if consistency_script.exists():
            print("[SDD Pre-Validation Node] Running Tier 3: Consistency validation...")
            proc = await asyncio.create_subprocess_exec(
                'python', str(consistency_script),
                cwd=str(base_path),
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
            )
            stdout, stderr = await proc.communicate()

            if proc.returncode != 0:
                tier3_passed = False
                warnings.append(f"Tier 3 Consistency warning: {stderr.decode()[:200]}")
                print("[SDD Pre-Validation Node] [WARN] Tier 3 warnings")
            else:
                print("[SDD Pre-Validation Node] [OK] Tier 3 passed")
        else:
            print("[SDD Pre-Validation Node] [WARN] Consistency script not found, skipping...")
            tier3_passed = True  # é™çº§ä¸ºé€šè¿‡

    except Exception as e:
        print(f"[SDD Pre-Validation Node] [ERROR] Validation error: {e}")
        warnings.append(f"Validation error: {str(e)}")

    # ç¡®å®šæ•´ä½“ç»“æœ
    # ç”¨æˆ·ç¡®è®¤: Tier 1 <80% â†’ å¼ºåˆ¶é˜»æ­¢
    overall_passed = tier1_passed and tier2_passed  # Tier 3 ä¸é˜»å¡

    if not overall_passed:
        sdd_pre_status = "failed"
        next_phase = "HALT"
        print("[SDD Pre-Validation Node] [FAIL] Pre-validation FAILED - BLOCKING workflow")
        print(f"[SDD Pre-Validation Node] Blocking issues: {blocking_issues}")
    elif not tier3_passed or warnings:
        sdd_pre_status = "warnings"
        next_phase = "DEV"  # ç»§ç»­ä½†æœ‰è­¦å‘Š
        print("[SDD Pre-Validation Node] [WARN] Pre-validation passed with warnings")
    else:
        sdd_pre_status = "passed"
        next_phase = "DEV"
        print("[SDD Pre-Validation Node] [OK] Pre-validation PASSED")

    return {
        "sdd_pre_status": sdd_pre_status,
        "sdd_pre_validation_result": {
            "tier1_coverage": {
                "openapi_count": len(openapi_files),
                "schema_count": len(schema_files),
                "prd_exists": has_prd,
                "coverage_percent": coverage_percent,
            },
            "tier1_passed": tier1_passed,
            "tier2_source_verified": tier2_passed,
            "tier3_consistency": {
                "warnings": warnings,
                "conflicts": [],
            },
            "tier3_passed": tier3_passed,
            "overall_passed": overall_passed,
            "validation_timestamp": datetime.now().isoformat(),
            "blocking_issues": blocking_issues,
        },
        "current_phase": next_phase,
    }


# ============================================================================
# Node 4: Dev Agent - å¼€å‘å®ç° (æ”¯æŒå¹¶è¡Œ Send)
# ============================================================================

@enforce_bmad_workflow
async def dev_node(state: BmadOrchestratorState) -> Dict[str, Any]:
    """
    Dev Agent èŠ‚ç‚¹ - å¼€å‘å®ç°

    âœ… Verified from LangGraph Skill:
    - Node signature: async def node(state: State) -> dict
    - Use Send for parallel execution

    åŠŸèƒ½:
    1. ä¸ºå½“å‰æ‰¹æ¬¡çš„æ¯ä¸ª Story åˆ›å»º worktree
    2. å¹¶è¡Œè¿è¡Œ Dev Agent
    3. æ”¶é›†å¼€å‘ç»“æœ
    4. å¤„ç†æµ‹è¯•å¤±è´¥

    Args:
        state: å½“å‰ç¼–æ’çŠ¶æ€

    Returns:
        State æ›´æ–°:
        - dev_outcomes: List[DevOutcome] (ä½¿ç”¨ reducer åˆå¹¶)
        - active_sessions: List[SessionInfo]
        - worktree_paths: Dict[str, str]
        - dev_status: "completed" | "partially_failed" | "failed"
        - current_phase: "MERGE" | "HALT"
    """
    print("[Dev Node] Starting development phase")

    base_path = Path(state["base_path"])
    worktree_base = Path(state["worktree_base"])
    current_batch_index = state.get("current_batch_index", 0)
    parallel_batches = state["parallel_batches"]

    # è·å–å½“å‰æ‰¹æ¬¡
    if current_batch_index >= len(parallel_batches):
        return {
            "dev_status": "completed",
            "current_phase": "QA",
        }

    current_batch = parallel_batches[current_batch_index]
    print(f"[Dev Node] Processing batch {current_batch_index + 1}/{len(parallel_batches)}: {current_batch}")

    spawner = BmadSessionSpawner(
        max_turns=state.get("max_turns", DEFAULT_MAX_TURNS),
        ultrathink=DEFAULT_ULTRATHINK,
        timeout_seconds=state.get("timeout", DEFAULT_TIMEOUT),
    )

    dev_outcomes: List[DevOutcome] = []
    active_sessions: List[SessionInfo] = []
    worktree_paths: Dict[str, str] = {}
    blockers: List[BlockerInfo] = []

    # å¹¶è¡Œåˆ›å»º worktrees å’Œå¯åŠ¨ä¼šè¯
    # è·å– timeout å€¼ä¼ é€’ç»™ dev ä¼šè¯
    dev_timeout = state.get("timeout", DEFAULT_TIMEOUT)
    tasks = []
    for story_id in current_batch:
        task = _run_dev_session(
            spawner=spawner,
            story_id=story_id,
            base_path=base_path,
            worktree_base=worktree_base,
            timeout=dev_timeout,
        )
        tasks.append(task)

    # å¹¶è¡Œæ‰§è¡Œ
    results = await asyncio.gather(*tasks, return_exceptions=True)

    # å¤„ç†ç»“æœ
    for story_id, result in zip(current_batch, results):
        if isinstance(result, Exception):
            outcome: DevOutcome = {
                "story_id": story_id,
                "outcome": "ERROR",
                "status": "real_execution",  # ğŸ”’ G5: çœŸå®æ‰§è¡Œæ ‡è®° (å¼‚å¸¸ä¹Ÿæ˜¯çœŸå®æ‰§è¡Œ)
                "synthetic": False,           # ğŸ”’ G5: éåˆæˆç»“æœ
                "skipped": False,             # ğŸ”’ G5: æœªè·³è¿‡
                "tests_passed": False,
                "test_count": 0,
                "test_coverage": None,
                "files_created": [],
                "files_modified": [],
                "duration_seconds": 0,
                "blocking_reason": str(result),
                "completion_notes": None,
                "agent_model": "claude-sonnet-4-5",
                "timestamp": datetime.now().isoformat(),
            }
            dev_outcomes.append(outcome)
            blockers.append({
                "story_id": story_id,
                "phase": "DEV",
                "blocker_type": "ERROR",
                "description": str(result),
                "detected_at": datetime.now().isoformat(),
                "retry_count": 0,
                "resolution": None,
            })
            print(f"[Dev Node] [FAIL] Story {story_id} error: {result}")
        else:
            outcome, worktree_path = result
            dev_outcomes.append(outcome)
            worktree_paths[story_id] = str(worktree_path)

            if outcome["outcome"] == "SUCCESS":
                print(f"[Dev Node] [OK] Story {story_id} completed")
            else:
                print(f"[Dev Node] [FAIL] Story {story_id} failed: {outcome['outcome']}")
                blockers.append({
                    "story_id": story_id,
                    "phase": "DEV",
                    "blocker_type": "TEST_FAILURE" if not outcome["tests_passed"] else "ERROR",
                    "description": outcome["blocking_reason"] or "Development failed",
                    "detected_at": datetime.now().isoformat(),
                    "retry_count": 0,
                    "resolution": None,
                })

    # ç¡®å®šçŠ¶æ€
    success_count = sum(1 for o in dev_outcomes if o["outcome"] == "SUCCESS")
    if success_count == len(current_batch):
        dev_status = "completed"
        next_phase = "QA"
    elif success_count > 0:
        dev_status = "partially_failed"
        next_phase = "QA"  # ç»§ç»­ QA å·²å®Œæˆçš„
    else:
        dev_status = "failed"
        next_phase = "HALT"

    print(f"[Dev Node] Batch completed: {success_count}/{len(current_batch)} success")

    return {
        "dev_outcomes": dev_outcomes,
        "active_sessions": active_sessions,
        "worktree_paths": worktree_paths,
        "dev_status": dev_status,
        "blockers": blockers,
        "current_phase": next_phase,
    }


async def _run_dev_session(
    spawner: BmadSessionSpawner,
    story_id: str,
    base_path: Path,
    worktree_base: Path,
    timeout: int = DEFAULT_TIMEOUT,
) -> tuple:
    """è¿è¡Œå•ä¸ª Dev ä¼šè¯"""
    # åˆ›å»º worktree
    worktree_path = await create_worktree(
        base_path=base_path,
        worktree_base=worktree_base,
        story_id=story_id,
        branch_name=f"develop-{story_id}",
    )

    # P1 ä¿®å¤: æ”¯æŒå¤šç§ Story æ–‡ä»¶å‘½åæ ¼å¼
    story_file = resolve_story_file_path(worktree_path, story_id)
    if story_file is None:
        # å¦‚æœæ‰¾ä¸åˆ°æ–‡ä»¶ï¼Œè¿”å› DEV_BLOCKED çŠ¶æ€
        outcome: DevOutcome = {
            "story_id": story_id,
            "outcome": "DEV_BLOCKED",
            "status": "real_execution",  # ğŸ”’ G5: çœŸå®æ‰§è¡Œæ ‡è®° (æ–‡ä»¶æŸ¥æ‰¾æ˜¯çœŸå®æ“ä½œ)
            "synthetic": False,           # ğŸ”’ G5: éåˆæˆç»“æœ
            "skipped": False,             # ğŸ”’ G5: æœªè·³è¿‡
            "tests_passed": False,
            "test_count": 0,
            "test_coverage": None,
            "files_created": [],
            "files_modified": [],
            "duration_seconds": 0,
            "blocking_reason": f"Story file not found for {story_id} (searched multiple patterns)",
            "completion_notes": None,
            "agent_model": "claude-sonnet-4-5",
            "timestamp": datetime.now().isoformat(),
        }
        return outcome, worktree_path

    try:
        # å¯åŠ¨ä¼šè¯
        session_id = await spawner.spawn_session(
            phase="DEV",
            story_id=story_id,
            worktree_path=worktree_path,
            story_file=story_file,
        )

        # ç­‰å¾…å®Œæˆï¼ˆå¯ç”¨å¡ä½æ£€æµ‹ï¼‰
        # ä½¿ç”¨ timeout å‚æ•°ä½œä¸º stuck æ£€æµ‹é˜ˆå€¼ï¼ˆé»˜è®¤ 300 ç§’å¤ªçŸ­ï¼‰
        log_file = worktree_path / "dev-output.log"
        returncode, partial = await spawner.wait_for_session(
            session_id, log_file=log_file, stuck_threshold_seconds=timeout
        )

        # è·å–ç»“æœï¼ˆåŒ…å«å¡ä½æ—¶çš„éƒ¨åˆ†ç»“æœï¼‰
        result = await spawner.get_session_result("DEV", worktree_path)

        if result and isinstance(result, DevResult):
            outcome: DevOutcome = {
                "story_id": story_id,
                "outcome": result.outcome,
                "status": "real_execution",  # ğŸ”’ G5: çœŸå®æ‰§è¡Œæ ‡è®°
                "synthetic": False,           # ğŸ”’ G5: éåˆæˆç»“æœ
                "skipped": False,             # ğŸ”’ G5: æœªè·³è¿‡
                "tests_passed": result.tests_passed,
                "test_count": result.test_count,
                "test_coverage": result.test_coverage,
                "files_created": result.files_created,
                "files_modified": result.files_modified,
                "duration_seconds": result.duration_seconds,
                "blocking_reason": result.blocking_reason,
                "completion_notes": result.completion_notes,
                "agent_model": "claude-sonnet-4-5",
                "timestamp": datetime.now().isoformat(),
            }
        else:
            outcome: DevOutcome = {
                "story_id": story_id,
                "outcome": "ERROR",
                "status": "real_execution",  # ğŸ”’ G5: çœŸå®æ‰§è¡Œæ ‡è®°
                "synthetic": False,           # ğŸ”’ G5: éåˆæˆç»“æœ
                "skipped": False,             # ğŸ”’ G5: æœªè·³è¿‡
                "tests_passed": False,
                "test_count": 0,
                "test_coverage": None,
                "files_created": [],
                "files_modified": [],
                "duration_seconds": 0,
                "blocking_reason": "No result file found",
                "completion_notes": None,
                "agent_model": "claude-sonnet-4-5",
                "timestamp": datetime.now().isoformat(),
            }

        return outcome, worktree_path

    except asyncio.TimeoutError:
        outcome: DevOutcome = {
            "story_id": story_id,
            "outcome": "TIMEOUT",
            "status": "real_execution",  # ğŸ”’ G5: çœŸå®æ‰§è¡Œæ ‡è®° (è¶…æ—¶ä¹Ÿæ˜¯çœŸå®æ‰§è¡Œ)
            "synthetic": False,           # ğŸ”’ G5: éåˆæˆç»“æœ
            "skipped": False,             # ğŸ”’ G5: æœªè·³è¿‡
            "tests_passed": False,
            "test_count": 0,
            "test_coverage": None,
            "files_created": [],
            "files_modified": [],
            "duration_seconds": DEFAULT_TIMEOUT,
            "blocking_reason": "Session timed out",
            "completion_notes": None,
            "agent_model": "claude-sonnet-4-5",
            "timestamp": datetime.now().isoformat(),
        }
        return outcome, worktree_path


# ============================================================================
# Node 5: QA Agent - è´¨é‡å®¡æŸ¥ (æ”¯æŒå¹¶è¡Œ Send)
# ============================================================================

@enforce_bmad_workflow
async def qa_node(state: BmadOrchestratorState) -> Dict[str, Any]:
    """
    QA Agent èŠ‚ç‚¹ - è´¨é‡å®¡æŸ¥

    âœ… Verified from LangGraph Skill:
    - Node signature: async def node(state: State) -> dict

    åŠŸèƒ½:
    1. å¯¹å®Œæˆå¼€å‘çš„ Stories è¿è¡Œ QA
    2. å¹¶è¡Œæ‰§è¡Œ QA å®¡æŸ¥
    3. æ”¶é›† QA ç»“æœ
    4. å†³å®šä¸‹ä¸€æ­¥: COMMIT | FIX | HALT

    Args:
        state: å½“å‰ç¼–æ’çŠ¶æ€

    Returns:
        State æ›´æ–°:
        - qa_outcomes: List[QAOutcome] (ä½¿ç”¨ reducer åˆå¹¶)
        - current_qa_gate: "PASS" | "CONCERNS" | "FAIL" | "WAIVED"
        - qa_status: "completed" | "partially_failed" | "failed"
        - current_phase: "COMMIT" | "FIX" | "HALT"
    """
    print("[QA Node] Starting QA phase")

    worktree_paths = state["worktree_paths"]
    dev_outcomes = state.get("dev_outcomes", [])

    # åª QA å¼€å‘æˆåŠŸçš„ Stories
    successful_stories = [
        o["story_id"] for o in dev_outcomes
        if o["outcome"] == "SUCCESS"
    ]

    if not successful_stories:
        return {
            "qa_status": "failed",
            "current_qa_gate": "FAIL",
            "current_phase": "HALT",
        }

    print(f"[QA Node] Stories to review: {successful_stories}")

    spawner = BmadSessionSpawner(
        max_turns=state.get("max_turns", DEFAULT_MAX_TURNS),
        ultrathink=DEFAULT_ULTRATHINK,
        timeout_seconds=state.get("timeout", DEFAULT_TIMEOUT),
    )

    qa_outcomes: List[QAOutcome] = []
    # Note: blockers list removed as it was unused

    # å¹¶è¡Œè¿è¡Œ QA
    # è·å– timeout å€¼ä¼ é€’ç»™ QA ä¼šè¯
    qa_timeout = state.get("timeout", DEFAULT_TIMEOUT)
    tasks = []
    for story_id in successful_stories:
        worktree_path = worktree_paths.get(story_id)
        if worktree_path:
            task = _run_qa_session(
                spawner=spawner,
                story_id=story_id,
                worktree_path=Path(worktree_path),
                timeout=qa_timeout,
            )
            tasks.append(task)

    results = await asyncio.gather(*tasks, return_exceptions=True)

    # å¤„ç†ç»“æœ
    for story_id, result in zip(successful_stories, results):
        if isinstance(result, Exception):
            outcome: QAOutcome = {
                "story_id": story_id,
                "qa_gate": "FAIL",
                "quality_score": 0,
                "ac_coverage": {},
                "issues_found": [{"severity": "high", "description": str(result), "location": ""}],
                "recommendations": [],
                "adr_compliance": False,
                "duration_seconds": 0,
                "reviewer_model": "claude-sonnet-4-5",
                "timestamp": datetime.now().isoformat(),
            }
            qa_outcomes.append(outcome)
            print(f"[QA Node] [FAIL] Story {story_id} QA error: {result}")
        else:
            qa_outcomes.append(result)
            if result["qa_gate"] == "PASS":
                print(f"[QA Node] [OK] Story {story_id} QA passed")
            elif result["qa_gate"] == "CONCERNS":
                print(f"[QA Node] [WARN] Story {story_id} QA concerns")
            else:
                print(f"[QA Node] [FAIL] Story {story_id} QA failed")

    # ç¡®å®šæ•´ä½“ QA ç»“æœ
    pass_count = sum(1 for o in qa_outcomes if o["qa_gate"] in ("PASS", "WAIVED"))
    concerns_count = sum(1 for o in qa_outcomes if o["qa_gate"] == "CONCERNS")
    fail_count = sum(1 for o in qa_outcomes if o["qa_gate"] == "FAIL")

    if fail_count > 0:
        current_qa_gate = "FAIL"
        qa_status = "failed"
        next_phase = "HALT"
    elif concerns_count > 0 and state.get("fix_attempts", 0) == 0:
        current_qa_gate = "CONCERNS"
        qa_status = "completed"
        next_phase = "FIX"
    else:
        current_qa_gate = "PASS"
        qa_status = "completed"
        next_phase = "COMMIT"

    print(f"[QA Node] Results: {pass_count} pass, {concerns_count} concerns, {fail_count} fail")

    # ========================================================================
    # PostProcessHook: æ›´æ–° Story æ–‡æ¡£å’Œç”Ÿæˆ QA Gate YAML
    # ========================================================================
    # âœ… Reference: scripts/daemon/linear_develop_daemon.py:239-243
    # å¯¹ PASS/WAIVED/CONCERNS çš„ Stories æ‰§è¡Œåå¤„ç†
    print("[QA Node] Starting PostProcess hook for document sync...")

    base_path = Path(state.get("base_path", "."))
    story_drafts = state.get("story_drafts", [])
    session_id = state.get("session_id", "epic-develop")

    # åˆ›å»º dev_outcomes çš„ story_id -> outcome æ˜ å°„
    dev_outcome_map = {o["story_id"]: o for o in dev_outcomes}

    post_process_results = []
    for qa_outcome in qa_outcomes:
        story_id = qa_outcome["story_id"]

        # åªå¯¹ PASS/WAIVED/CONCERNS æ‰§è¡Œåå¤„ç† (ä¸åŒ…æ‹¬ FAIL)
        if qa_outcome["qa_gate"] in ("PASS", "WAIVED", "CONCERNS"):
            worktree_path_str = worktree_paths.get(story_id)
            dev_outcome = dev_outcome_map.get(story_id, {})

            if worktree_path_str:
                try:
                    success = await _run_post_process_hook(
                        base_path=base_path,
                        story_id=story_id,
                        worktree_path=Path(worktree_path_str),
                        dev_outcome=dev_outcome,
                        qa_outcome=qa_outcome,
                        story_drafts=story_drafts,
                        session_id=session_id,
                    )
                    post_process_results.append((story_id, success))
                except Exception as e:
                    print(f"[QA Node] PostProcess error for {story_id}: {e}")
                    post_process_results.append((story_id, False))
            else:
                print(f"[QA Node] No worktree path for {story_id}, skipping PostProcess")

    # æ±‡æ€»åå¤„ç†ç»“æœ
    pp_success = sum(1 for _, s in post_process_results if s)
    pp_total = len(post_process_results)
    print(f"[QA Node] PostProcess completed: {pp_success}/{pp_total} documents synced")

    return {
        "qa_outcomes": qa_outcomes,
        "current_qa_gate": current_qa_gate,
        "qa_status": qa_status,
        "current_phase": next_phase,
    }


async def _run_qa_session(
    spawner: BmadSessionSpawner,
    story_id: str,
    worktree_path: Path,
    timeout: int = DEFAULT_TIMEOUT,
) -> QAOutcome:
    """è¿è¡Œå•ä¸ª QA ä¼šè¯"""
    try:
        session_id = await spawner.spawn_session(
            phase="QA",
            story_id=story_id,
            worktree_path=worktree_path,
        )

        # ç­‰å¾…å®Œæˆï¼ˆå¯ç”¨å¡ä½æ£€æµ‹ï¼‰
        # ä½¿ç”¨ timeout å‚æ•°ä½œä¸º stuck æ£€æµ‹é˜ˆå€¼ï¼ˆé»˜è®¤ 300 ç§’å¤ªçŸ­ï¼‰
        log_file = worktree_path / "qa-output.log"
        returncode, partial = await spawner.wait_for_session(
            session_id, log_file=log_file, stuck_threshold_seconds=timeout
        )

        result = await spawner.get_session_result("QA", worktree_path)

        if result and isinstance(result, QAResult):
            return {
                "story_id": story_id,
                "qa_gate": result.qa_gate or "FAIL",
                "quality_score": result.quality_score,
                "ac_coverage": result.ac_coverage,
                "issues_found": result.issues_found,
                "recommendations": result.recommendations,
                "adr_compliance": result.adr_compliance,
                "duration_seconds": result.duration_seconds,
                "reviewer_model": "claude-sonnet-4-5",
                "timestamp": datetime.now().isoformat(),
            }
        else:
            return {
                "story_id": story_id,
                "qa_gate": "FAIL",
                "quality_score": 0,
                "ac_coverage": {},
                "issues_found": [{"severity": "high", "description": "No QA result file", "location": ""}],
                "recommendations": [],
                "adr_compliance": False,
                "duration_seconds": 0,
                "reviewer_model": "claude-sonnet-4-5",
                "timestamp": datetime.now().isoformat(),
            }

    except asyncio.TimeoutError:
        return {
            "story_id": story_id,
            "qa_gate": "FAIL",
            "quality_score": 0,
            "ac_coverage": {},
            "issues_found": [{"severity": "high", "description": "QA session timed out", "location": ""}],
            "recommendations": [],
            "adr_compliance": False,
            "duration_seconds": DEFAULT_TIMEOUT,
            "reviewer_model": "claude-sonnet-4-5",
            "timestamp": datetime.now().isoformat(),
        }


# ============================================================================
# Node 5.5: SDD Validation - ä¸‰å±‚ SDD éªŒè¯ (QA å)
# ============================================================================

@enforce_bmad_workflow
async def sdd_validation_node(state: BmadOrchestratorState) -> Dict[str, Any]:
    """
    SDD Validation èŠ‚ç‚¹ - å››å±‚ SDD éªŒè¯ (v1.1.0)

    âœ… Verified from LangGraph Skill:
    - Node signature: async def node(state: State) -> dict

    åŠŸèƒ½:
    1. Tier 1 (è¦†ç›–ç‡): PRDâ†’OpenAPI/Schema â‰¥80%
    2. Tier 2 (æ¥æºéªŒè¯): x-source-verification metadata
    3. Tier 3 (ä¸€è‡´æ€§): PRDâ†”Schemaâ†”OpenAPI
    4. Tier 4 (åˆçº¦æµ‹è¯•): Contract tests (tests/contract/) - v1.1.0 æ–°å¢

    è§„åˆ™:
    - Tier 1/2 å¤±è´¥ â†’ é˜»å¡ (HALT)
    - Tier 3/4 è­¦å‘Š â†’ ç»§ç»­ (ä½†è®°å½•)

    Args:
        state: å½“å‰ç¼–æ’çŠ¶æ€

    Returns:
        State æ›´æ–°:
        - sdd_validation_result: Dict
        - sdd_status: "passed" | "warnings" | "failed" | "skipped"
        - current_phase: "MERGE" | "HALT"
    """
    print("[SDD Node] Starting SDD validation")

    base_path = Path(state["base_path"])
    qa_outcomes = state.get("qa_outcomes", [])

    # åªå¯¹ QA é€šè¿‡çš„ Stories è¿›è¡Œ SDD éªŒè¯
    passed_stories = [
        o["story_id"] for o in qa_outcomes
        if o["qa_gate"] in ("PASS", "WAIVED")
    ]

    if not passed_stories:
        print("[SDD Node] No stories to validate, skipping")
        return {
            "sdd_status": "skipped",
            "sdd_validation_result": None,
            "current_phase": "HALT",
        }

    print(f"[SDD Node] Validating SDD for stories: {passed_stories}")

    # æ£€æŸ¥ SDD éªŒè¯è„šæœ¬æ˜¯å¦å­˜åœ¨
    sdd_scripts = [
        base_path / "scripts" / "verify-sdd-coverage.py",
        base_path / "scripts" / "verify-sdd-source.py",
        base_path / "scripts" / "verify-sdd-consistency.py",
    ]

    scripts_exist = all(script.exists() for script in sdd_scripts)

    if not scripts_exist:
        # SDD è„šæœ¬ä¸å­˜åœ¨ï¼Œè·³è¿‡éªŒè¯ (è­¦å‘Šæ¨¡å¼)
        print("[SDD Node] [WARN] SDD validation scripts not found, skipping validation")
        return {
            "sdd_status": "skipped",
            "sdd_validation_result": {
                "tier1_coverage": {},
                "tier1_passed": True,  # é»˜è®¤é€šè¿‡
                "tier2_source_verified": True,  # é»˜è®¤é€šè¿‡
                "tier3_consistency": {"warnings": ["SDD scripts not found"], "conflicts": []},
                "tier3_passed": True,
                "overall_passed": True,
                "validation_timestamp": datetime.now().isoformat(),
                "blocking_issues": [],
            },
            "current_phase": "MERGE",  # ç»§ç»­åˆ° MERGE
        }

    # è¿è¡Œä¸‰å±‚éªŒè¯
    tier1_passed = True
    tier2_passed = True
    tier3_passed = True
    blocking_issues = []
    warnings = []

    try:
        # Tier 1: è¦†ç›–ç‡éªŒè¯
        print("[SDD Node] Running Tier 1: Coverage validation...")
        proc = await asyncio.create_subprocess_exec(
            'python', str(sdd_scripts[0]),
            cwd=str(base_path),
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE,
        )
        stdout, stderr = await proc.communicate()
        if proc.returncode != 0:
            tier1_passed = False
            blocking_issues.append(f"Tier 1 Coverage failed: {stderr.decode()[:200]}")
            print("[SDD Node] [FAIL] Tier 1 failed")
        else:
            print("[SDD Node] [OK] Tier 1 passed")

        # Tier 2: æ¥æºéªŒè¯
        print("[SDD Node] Running Tier 2: Source verification...")
        proc = await asyncio.create_subprocess_exec(
            'python', str(sdd_scripts[1]),
            cwd=str(base_path),
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE,
        )
        stdout, stderr = await proc.communicate()
        if proc.returncode != 0:
            tier2_passed = False
            blocking_issues.append(f"Tier 2 Source verification failed: {stderr.decode()[:200]}")
            print("[SDD Node] [FAIL] Tier 2 failed")
        else:
            print("[SDD Node] [OK] Tier 2 passed")

        # Tier 3: ä¸€è‡´æ€§éªŒè¯
        print("[SDD Node] Running Tier 3: Consistency validation...")
        proc = await asyncio.create_subprocess_exec(
            'python', str(sdd_scripts[2]),
            cwd=str(base_path),
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE,
        )
        stdout, stderr = await proc.communicate()
        if proc.returncode != 0:
            # Tier 3 æ˜¯è­¦å‘Šçº§åˆ«ï¼Œä¸é˜»å¡
            tier3_passed = False
            warnings.append(f"Tier 3 Consistency warning: {stderr.decode()[:200]}")
            print("[SDD Node] [WARN] Tier 3 warnings")
        else:
            print("[SDD Node] [OK] Tier 3 passed")

        # Tier 4: Contract Testing (v1.1.0 - æ–°å¢)
        tier4_passed = True
        contract_test_path = base_path / "tests" / "contract"

        if contract_test_path.exists() and any(contract_test_path.glob("test_*.py")):
            print("[SDD Node] Running Tier 4: Contract testing...")
            proc = await asyncio.create_subprocess_exec(
                'python', '-m', 'pytest',
                str(contract_test_path),
                '-v', '--tb=short',
                '-q',  # Quiet mode
                cwd=str(base_path),
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
            )
            stdout, stderr = await proc.communicate()

            if proc.returncode != 0:
                tier4_passed = False
                error_output = stdout.decode()[-500:] + stderr.decode()[-200:]
                warnings.append(f"Tier 4 Contract tests failed: {error_output}")
                print("[SDD Node] [WARN] Tier 4 contract tests failed")
            else:
                print("[SDD Node] [OK] Tier 4 passed - Contract tests succeeded")
        else:
            print("[SDD Node] [WARN] Contract test path not found, skipping Tier 4")
            tier4_passed = True  # é™çº§ä¸ºé€šè¿‡

    except Exception as e:
        print(f"[SDD Node] [ERROR] SDD validation error: {e}")
        warnings.append(f"SDD validation error: {str(e)}")
        tier4_passed = True  # å¼‚å¸¸æ—¶é™çº§

    # ç¡®å®šæ•´ä½“ç»“æœ
    # Tier 4 æ˜¯è­¦å‘Šçº§åˆ«ï¼ˆå’ŒTier 3ä¸€æ ·ï¼‰ï¼Œä¸é˜»å¡
    overall_passed = tier1_passed and tier2_passed  # Tier 3/4 ä¸é˜»å¡

    if not overall_passed:
        sdd_status = "failed"
        next_phase = "HALT"
        print("[SDD Node] [FAIL] SDD validation FAILED - blocking issues found")
    elif not tier3_passed or not tier4_passed:
        sdd_status = "warnings"
        next_phase = "MERGE"  # ç»§ç»­ä½†æœ‰è­¦å‘Š
        print(f"[SDD Node] [WARN] SDD validation PASSED with warnings (Tier3={tier3_passed}, Tier4={tier4_passed})")
    else:
        sdd_status = "passed"
        next_phase = "MERGE"
        print("[SDD Node] [OK] SDD validation PASSED (all 4 tiers)")

    return {
        "sdd_validation_result": {
            "tier1_coverage": {},  # TODO: Parse actual coverage
            "tier1_passed": tier1_passed,
            "tier2_source_verified": tier2_passed,
            "tier3_consistency": {"warnings": warnings, "conflicts": []},
            "tier3_passed": tier3_passed,
            "tier4_contract_tests": tier4_passed,  # v1.1.0 æ–°å¢
            "overall_passed": overall_passed,
            "validation_timestamp": datetime.now().isoformat(),
            "blocking_issues": blocking_issues,
        },
        "sdd_status": sdd_status,
        "current_phase": next_phase,
    }


# ============================================================================
# Node 6: Merge - Worktree åˆå¹¶
# ============================================================================

@enforce_bmad_workflow
async def merge_node(state: BmadOrchestratorState) -> Dict[str, Any]:
    """
    Merge èŠ‚ç‚¹ - Worktree åˆå¹¶ (é›†æˆ Commit Gate v2)

    åŠŸèƒ½:
    1. ğŸ”’ æ‰§è¡Œ Commit Gate v2 éªŒè¯ (G1-G10) - ç¡¬æ€§æŒ‡æ ‡
    2. å°†å®Œæˆ QA çš„ worktrees åˆå¹¶åˆ°ä¸»åˆ†æ”¯
    3. å¤„ç†åˆå¹¶å†²çª
    4. æ¸…ç† worktrees

    âš ï¸ Commit Gate æ˜¯ç¡¬æ€§æŒ‡æ ‡:
    - ä»»ä½•æ£€æŸ¥å¤±è´¥éƒ½ä¼šé˜»æ­¢ merge
    - Gate å¤±è´¥çš„ Story ä¼šè¢«è·¯ç”±å› DEV/QA
    - æ‰€æœ‰éªŒè¯ç»“æœè®°å½•åˆ°å®¡è®¡æ—¥å¿—

    Args:
        state: å½“å‰ç¼–æ’çŠ¶æ€

    Returns:
        State æ›´æ–°:
        - merge_status: "completed" | "conflict_detected" | "failed" | "gate_blocked"
        - merge_conflicts: List[Dict]
        - gate_results: List[Dict] - Commit Gate éªŒè¯ç»“æœ
        - current_phase: "COMMIT" | "HALT" | "DEV"
    """
    print("[Merge Node] Starting merge phase")

    base_path = Path(state["base_path"])
    worktree_paths = state["worktree_paths"]
    qa_outcomes = state.get("qa_outcomes", [])

    # åªåˆå¹¶ QA é€šè¿‡çš„
    passed_stories = [
        o["story_id"] for o in qa_outcomes
        if o["qa_gate"] in ("PASS", "WAIVED")
    ]

    if not passed_stories:
        return {
            "merge_status": "failed",
            "current_phase": "HALT",
        }

    print(f"[Merge Node] Stories to merge: {passed_stories}")

    # ========================================
    # ğŸ”’ Commit Gate v2 - é›¶å¹»è§‰å¼ºåˆ¶éªŒè¯
    # ========================================
    print("[Merge Node] ğŸ”’ Executing Commit Gate v2 (G1-G10 verification)")

    gate_results: List[Dict[str, Any]] = []
    gate_passed_stories: List[str] = []
    gate_failed_stories: List[str] = []

    # è·å– dev_outcomes å’Œ story_drafts ç”¨äº Gate éªŒè¯
    dev_outcomes = state.get("dev_outcomes", [])
    story_drafts = state.get("story_drafts", [])

    # æ„å»º story_id â†’ outcome æ˜ å°„
    dev_outcome_map = {o["story_id"]: o for o in dev_outcomes}
    qa_outcome_map = {o["story_id"]: o for o in qa_outcomes}
    story_draft_map = {d["story_id"]: d for d in story_drafts}

    for story_id in passed_stories:
        worktree_path = worktree_paths.get(story_id)
        if not worktree_path:
            continue

        # è·å–è¯¥ Story çš„ dev/qa outcome
        dev_outcome = dev_outcome_map.get(story_id, {})
        qa_outcome = qa_outcome_map.get(story_id, {})
        story_draft = story_draft_map.get(story_id)

        try:
            # ğŸ”’ æ‰§è¡Œ Commit Gate v2 éªŒè¯ (G1-G10)
            gate = CommitGate(story_id, worktree_path, base_path=base_path)
            await gate.execute_gate(
                dev_outcome=dev_outcome,
                qa_outcome=qa_outcome,
                story_draft=story_draft,
            )

            # Gate é€šè¿‡
            gate_results.append({
                "story_id": story_id,
                "gate_passed": True,
                "checks_passed": gate.results,
                "timestamp": datetime.now().isoformat(),
            })
            gate_passed_stories.append(story_id)
            print(f"[Merge Node] [GATE PASS] Story {story_id}")

        except CommitGateError as e:
            # Gate å¤±è´¥ - è®°å½•å¤±è´¥åŸå› 
            gate_results.append({
                "story_id": story_id,
                "gate_passed": False,
                "error": str(e),
                "failed_checks": e.failed_checks if hasattr(e, 'failed_checks') else [],
                "timestamp": datetime.now().isoformat(),
            })
            gate_failed_stories.append(story_id)
            print(f"[Merge Node] [GATE FAIL] Story {story_id}: {str(e)[:100]}")

        except Exception as e:
            # å…¶ä»–å¼‚å¸¸ä¹Ÿè§†ä¸º Gate å¤±è´¥
            gate_results.append({
                "story_id": story_id,
                "gate_passed": False,
                "error": f"Unexpected error: {str(e)}",
                "timestamp": datetime.now().isoformat(),
            })
            gate_failed_stories.append(story_id)
            print(f"[Merge Node] [GATE ERROR] Story {story_id}: {str(e)[:100]}")

    # å¦‚æœæœ‰ Gate å¤±è´¥çš„ Storyï¼Œé˜»æ­¢ merge å¹¶è·¯ç”±å› DEV
    if gate_failed_stories:
        print(f"[Merge Node] ğŸ”’ GATE BLOCKED: {len(gate_failed_stories)} stories failed verification")
        return {
            "merge_status": "gate_blocked",
            "gate_results": gate_results,
            "gate_failed_stories": gate_failed_stories,
            "current_phase": "DEV",  # è·¯ç”±å› DEV ä¿®å¤
        }

    # æ‰€æœ‰ Gate é€šè¿‡ï¼Œç»§ç»­åˆå¹¶
    print(f"[Merge Node] âœ… All {len(gate_passed_stories)} stories passed Commit Gate")

    merge_conflicts: List[Dict[str, str]] = []

    for story_id in passed_stories:
        worktree_path = worktree_paths.get(story_id)
        if not worktree_path:
            continue

        branch_name = f"develop-{story_id}"

        try:
            # ========================================
            # æ£€æŸ¥åˆ†æ”¯æ˜¯å¦æœ‰æ–° commits (re-QA åœºæ™¯å¤„ç†)
            # ========================================
            check_commits = await asyncio.create_subprocess_exec(
                'git', 'rev-list', '--count', f'main..{branch_name}',
                cwd=str(base_path),
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
            )
            stdout, stderr = await check_commits.communicate()

            if check_commits.returncode == 0:
                commit_count = int(stdout.decode().strip()) if stdout else 0
                if commit_count == 0:
                    # åˆ†æ”¯ä¸ main ç›¸åŒï¼Œè·³è¿‡ merge (re-QA åœºæ™¯)
                    print(f"[Merge Node] [SKIP] {branch_name} has no new commits (re-QA mode)")
                    continue

            # åœ¨ worktree ä¸­è¿è¡Œ merge
            proc = await asyncio.create_subprocess_exec(
                'git', 'checkout', 'main',
                cwd=str(base_path),
            )
            await proc.wait()

            # Merge åˆ†æ”¯
            proc = await asyncio.create_subprocess_exec(
                'git', 'merge', branch_name, '--no-ff',
                '-m', f"Merge Story {story_id}",
                cwd=str(base_path),
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
            )
            stdout, stderr = await proc.communicate()

            if proc.returncode != 0:
                merge_conflicts.append({
                    "story_id": story_id,
                    "conflicting_files": stderr.decode() if stderr else "Unknown",
                    "conflict_type": "MERGE_CONFLICT",
                })
                print(f"[Merge Node] [FAIL] Story {story_id} merge conflict")
            else:
                print(f"[Merge Node] [OK] Story {story_id} merged")

        except Exception as e:
            merge_conflicts.append({
                "story_id": story_id,
                "conflicting_files": str(e),
                "conflict_type": "ERROR",
            })

    # ç¡®å®šçŠ¶æ€
    if merge_conflicts:
        merge_status = "conflict_detected"
        next_phase = "HALT"
    else:
        merge_status = "completed"
        next_phase = "COMMIT"

    return {
        "merge_status": merge_status,
        "merge_conflicts": merge_conflicts,
        "gate_results": gate_results,  # ğŸ”’ Commit Gate éªŒè¯ç»“æœ
        "current_phase": next_phase,
    }


# ============================================================================
# Node 7: Commit - Git æäº¤
# ============================================================================

@enforce_bmad_workflow
async def commit_node(state: BmadOrchestratorState) -> Dict[str, Any]:
    """
    Commit èŠ‚ç‚¹ - Git æäº¤

    åŠŸèƒ½:
    1. åˆ›å»ºæœ€ç»ˆ commit
    2. è®°å½• commit SHA
    3. è·¯ç”±åˆ° cleanup_node (v1.1.0: æ¸…ç†ç§»è‡³ cleanup_node)

    æ³¨æ„: Worktree æ¸…ç†ç”± cleanup_node ç»Ÿä¸€å¤„ç†ï¼Œç¡®ä¿æ— è®ºæˆåŠŸæˆ–å¤±è´¥éƒ½ä¼šæ‰§è¡Œã€‚

    Args:
        state: å½“å‰ç¼–æ’çŠ¶æ€

    Returns:
        State æ›´æ–°:
        - commit_shas: List[str] (ä½¿ç”¨ reducer åˆå¹¶)
        - current_phase: "COMMIT"
    """
    print("[Commit Node] Creating final commit")

    base_path = Path(state["base_path"])
    qa_outcomes = state.get("qa_outcomes", [])

    commit_shas: List[str] = []

    # è·å–å½“å‰ commit SHA
    proc = await asyncio.create_subprocess_exec(
        'git', 'log', '-1', '--format=%H',
        cwd=str(base_path),
        stdout=asyncio.subprocess.PIPE,
    )
    stdout, _ = await proc.communicate()
    if stdout:
        commit_sha = stdout.decode().strip()
        commit_shas.append(commit_sha)
        print(f"[Commit Node] [OK] Commit SHA: {commit_sha}")

    print("[Commit Node] Routing to cleanup_node for resource cleanup")

    return {
        "commit_shas": commit_shas,
        "final_status": "success",
        "current_phase": "COMMIT",
        "completion_summary": f"Successfully completed {len(qa_outcomes)} stories",
    }


# ============================================================================
# Node 8: Fix - CONCERNS ä¿®å¤å¾ªç¯
# ============================================================================

@enforce_bmad_workflow
async def fix_node(state: BmadOrchestratorState) -> Dict[str, Any]:
    """
    Fix èŠ‚ç‚¹ - CONCERNS ä¿®å¤

    åŠŸèƒ½:
    1. å¯¹ CONCERNS çš„ Stories è¿›è¡Œä¿®å¤
    2. æœ€å¤šå°è¯• 1 æ¬¡
    3. é‡æ–°è¿è¡Œ QA

    Args:
        state: å½“å‰ç¼–æ’çŠ¶æ€

    Returns:
        State æ›´æ–°:
        - fix_attempts: int
        - current_phase: "QA" | "HALT"
    """
    print("[Fix Node] Starting fix cycle")

    fix_attempts = state.get("fix_attempts", 0) + 1

    if fix_attempts > 1:
        print("[Fix Node] Max fix attempts reached")
        return {
            "fix_attempts": fix_attempts,
            "final_status": "partial_success",
            "current_phase": "COMMIT",  # æäº¤å·²é€šè¿‡çš„
        }

    # TODO: å®ç°è‡ªåŠ¨ä¿®å¤é€»è¾‘
    # ç°åœ¨ç®€å•åœ°é‡æ–°è¿è¡Œ QA
    print(f"[Fix Node] Fix attempt {fix_attempts}, returning to QA")

    return {
        "fix_attempts": fix_attempts,
        "current_phase": "QA",
    }


# ============================================================================
# Node 9: Halt - å¤±è´¥å¤„ç†
# ============================================================================

@enforce_bmad_workflow
async def halt_node(state: BmadOrchestratorState) -> Dict[str, Any]:
    """
    Halt èŠ‚ç‚¹ - å¤±è´¥å¤„ç†

    åŠŸèƒ½:
    1. è®°å½•å¤±è´¥åŸå› 
    2. ç”Ÿæˆå¤±è´¥æŠ¥å‘Š
    3. è·¯ç”±åˆ° cleanup_node (v1.1.0: æ¸…ç†ç§»è‡³ cleanup_node)

    æ³¨æ„: Worktree æ¸…ç†ç”± cleanup_node ç»Ÿä¸€å¤„ç†ï¼Œç¡®ä¿æ— è®ºæˆåŠŸæˆ–å¤±è´¥éƒ½ä¼šæ‰§è¡Œã€‚

    Args:
        state: å½“å‰ç¼–æ’çŠ¶æ€

    Returns:
        State æ›´æ–°:
        - final_status: "halted"
        - completion_summary: str
    """
    print("[Halt Node] Workflow halted")

    blockers = state.get("blockers", [])

    # ç”Ÿæˆå¤±è´¥æ‘˜è¦
    if blockers:
        summary = f"Halted with {len(blockers)} blockers: "
        summary += ", ".join([f"{b['story_id']}({b['blocker_type']})" for b in blockers[:3]])
    else:
        summary = "Workflow halted due to failures"

    print(f"[Halt Node] {summary}")
    print("[Halt Node] Routing to cleanup_node for resource cleanup")

    # === Status Persistence: Save partial status on HALT ===
    try:
        from pathlib import Path

        from .status_persister import StatusPersister

        base_path = state.get("base_path", ".")
        epic_id = state.get("epic_id", "unknown")

        persister = StatusPersister(Path(base_path))
        await persister.persist_workflow_result(
            final_state=dict(state),
            epic_id=epic_id,
        )
        print("[Halt Node] Partial status persisted to YAML")
    except Exception as e:
        print(f"[Halt Node] [WARN] Status persistence error: {e} (non-blocking)")
    # === End Status Persistence ===

    return {
        "final_status": "halted",
        "completion_summary": summary,
        "current_phase": "HALT",
    }


# ============================================================================
# Send å‡½æ•° (ç”¨äºå¹¶è¡Œ Dev/QA)
# ============================================================================

def create_dev_sends(state: BmadOrchestratorState) -> List[Send]:
    """
    åˆ›å»ºå¹¶è¡Œ Dev Send

    âœ… Verified from LangGraph Skill (Pattern: Send for parallel execution)

    ç”¨äº StateGraph çš„æ¡ä»¶è¾¹ï¼Œä¸ºæ¯ä¸ª Story åˆ›å»ºå¹¶è¡Œ Dev ä»»åŠ¡ã€‚
    """
    current_batch_index = state.get("current_batch_index", 0)
    parallel_batches = state.get("parallel_batches", [])

    if current_batch_index >= len(parallel_batches):
        return []

    current_batch = parallel_batches[current_batch_index]
    return [Send("dev_node", {"story_id": story_id}) for story_id in current_batch]


def create_qa_sends(state: BmadOrchestratorState) -> List[Send]:
    """
    åˆ›å»ºå¹¶è¡Œ QA Send

    âœ… Verified from LangGraph Skill (Pattern: Send for parallel execution)

    ç”¨äº StateGraph çš„æ¡ä»¶è¾¹ï¼Œä¸ºæ¯ä¸ªå®Œæˆå¼€å‘çš„ Story åˆ›å»ºå¹¶è¡Œ QA ä»»åŠ¡ã€‚
    """
    dev_outcomes = state.get("dev_outcomes", [])
    successful = [o["story_id"] for o in dev_outcomes if o["outcome"] == "SUCCESS"]
    return [Send("qa_node", {"story_id": story_id}) for story_id in successful]


# ============================================================================
# Node 10: Cleanup - å·¥ä½œæ ‘æ¸…ç† (Always executes before END)
# ============================================================================

async def cleanup_node(state: BmadOrchestratorState) -> Dict[str, Any]:
    """
    Cleanup èŠ‚ç‚¹ - å·¥ä½œæ ‘æ¸…ç†

    æ­¤èŠ‚ç‚¹åœ¨å·¥ä½œæµç»“æŸå‰ **æ€»æ˜¯** æ‰§è¡Œï¼Œç¡®ä¿:
    1. æ¸…ç†æ‰€æœ‰ worktrees (é™¤é gate_blocked éœ€è¦ä¿ç•™)
    2. æ‰§è¡Œ git worktree prune (æ¸…ç†å­¤ç«‹å¼•ç”¨)
    3. è®¾ç½® cleanup_completed æ ‡å¿—

    æ— è®ºå·¥ä½œæµæˆåŠŸ (COMMIT â†’ CLEANUP â†’ END) è¿˜æ˜¯å¤±è´¥ (HALT â†’ CLEANUP â†’ END)ï¼Œ
    cleanup_node éƒ½ä¼šæ‰§è¡Œä»¥ç¡®ä¿èµ„æºè¢«æ­£ç¡®é‡Šæ”¾ã€‚

    **ç‰¹æ®Šå¤„ç†**: å½“ merge_status == "gate_blocked" æ—¶ï¼Œä¿ç•™ worktrees ä»¥ä¾¿ç”¨æˆ·æ‰‹åŠ¨ä¿®å¤ã€‚

    Args:
        state: å½“å‰ç¼–æ’çŠ¶æ€

    Returns:
        State æ›´æ–°:
        - cleanup_completed: True
        - current_phase: "CLEANUP"
        - worktrees_preserved: bool (æ˜¯å¦å›  gate_blocked ä¿ç•™äº† worktrees)
    """
    print("[Cleanup Node] Starting cleanup")

    base_path = Path(state["base_path"])
    worktree_paths = state.get("worktree_paths", {})
    merge_status = state.get("merge_status", "")
    cleanup_errors: List[str] = []
    worktrees_preserved = False

    # æ£€æŸ¥æ˜¯å¦å›  Gate å¤±è´¥éœ€è¦ä¿ç•™ worktrees
    if merge_status == "gate_blocked":
        print("[Cleanup Node] âš ï¸ Gate blocked - PRESERVING worktrees for manual fix")
        print(f"[Cleanup Node] Preserved worktrees: {list(worktree_paths.keys())}")
        for story_id, worktree_path in worktree_paths.items():
            print(f"[Cleanup Node]   â†’ {story_id}: {worktree_path}")
        print("[Cleanup Node] To fix: Navigate to worktree, fix issues, commit changes")
        worktrees_preserved = True
    else:
        # æ­£å¸¸æ¸…ç†: åˆ é™¤æ‰€æœ‰ worktrees
        for story_id, worktree_path in worktree_paths.items():
            try:
                success = await remove_worktree(base_path, Path(worktree_path))
                if success:
                    print(f"[Cleanup Node] [OK] Removed worktree for {story_id}")
                else:
                    print(f"[Cleanup Node] [WARN] Could not remove worktree for {story_id}")
            except Exception as e:
                cleanup_errors.append(f"{story_id}: {str(e)}")
                print(f"[Cleanup Node] [ERROR] Failed to clean worktree for {story_id}: {e}")

    # 2. æ‰§è¡Œ git worktree prune (æ¸…ç†å­¤ç«‹å¼•ç”¨) - è·³è¿‡å¦‚æœ worktrees è¢«ä¿ç•™
    if not worktrees_preserved:
        try:
            proc = await asyncio.create_subprocess_exec(
                'git', 'worktree', 'prune',
                cwd=str(base_path),
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
            )
            _, stderr = await proc.communicate()
            if proc.returncode == 0:
                print("[Cleanup Node] [OK] Git worktree prune completed")
            else:
                print(f"[Cleanup Node] [WARN] Git worktree prune failed: {stderr.decode()}")
        except Exception as e:
            print(f"[Cleanup Node] [WARN] Git worktree prune exception: {e}")
    else:
        print("[Cleanup Node] [SKIP] Git worktree prune skipped (worktrees preserved)")

    # 3. ç”Ÿæˆæ¸…ç†æ‘˜è¦
    if worktrees_preserved:
        summary = f"Worktrees preserved for manual fix ({len(worktree_paths)} stories)"
    elif cleanup_errors:
        summary = f"Cleanup completed with {len(cleanup_errors)} errors"
    else:
        summary = f"Cleanup completed successfully, removed {len(worktree_paths)} worktrees"

    print(f"[Cleanup Node] {summary}")

    return {
        "cleanup_completed": True,
        "current_phase": "CLEANUP",
        "worktrees_preserved": worktrees_preserved,
    }
