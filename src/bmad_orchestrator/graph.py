"""
BMad Orchestrator StateGraph æ„å»º

æ„å»ºå…¨è‡ªåŠ¨åŒ– SMâ†’POâ†’Devâ†’QA å·¥ä½œæµçš„ LangGraph StateGraphã€‚

âœ… Verified from LangGraph Skill:
- Pattern: StateGraph(State) construction
- add_node() for nodes
- add_edge() for unconditional transitions
- add_conditional_edges() for routing
- set_entry_point() for start node
- compile() with checkpointer for persistence

12ä¸ªèŠ‚ç‚¹å·¥ä½œæµ (v1.1.0 - å¢åŠ  SDD_PRE å’Œ CLEANUP):
SM â†’ PO â†’ Analysis â†’ SDD_PRE â†’ DEV â†’ QA â†’ SDD â†’ MERGE â†’ COMMIT â†’ CLEANUP â†’ END
                        â†“                   â†“                           â†‘
                      HALT              FIX (CONCERNSå¾ªç¯)               â”‚
                    (è¦†ç›–ç‡<80%)           â†“                           â”‚
                                        HALT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

å…³é”®æ”¹è¿› (v1.1.0):
- SDD_PRE èŠ‚ç‚¹: å¼€å‘å‰éªŒè¯ SDD è¦†ç›–ç‡ â‰¥80%ï¼Œå¦åˆ™å¼ºåˆ¶é˜»æ­¢
- CLEANUP èŠ‚ç‚¹: ç¡®ä¿å·¥ä½œæ ‘æ€»æ˜¯è¢«æ¸…ç† (æ— è®ºæˆåŠŸæˆ–å¤±è´¥)
- Fail-Forward è®¾è®¡: éƒ¨åˆ†å¤±è´¥æ—¶ä»ç»§ç»­æ‰§è¡Œåˆ° COMMIT
- å¢å¼ºè·¯ç”±: ä¼˜å…ˆæ£€æŸ¥å®é™…ç»“æœåˆ—è¡¨ï¼Œè€ŒéçŠ¶æ€æ ‡å¿—
- å¡ä½æ£€æµ‹: 5åˆ†é’Ÿæ— æ—¥å¿—æ´»åŠ¨åˆ™ç»ˆæ­¢ä¼šè¯å¹¶æå–éƒ¨åˆ†ç»“æœ

Author: Canvas Learning System Team
Version: 1.1.0
Created: 2025-11-30
Updated: 2025-12-01 - Added cleanup_node, fail-forward design
"""

import asyncio
from pathlib import Path
from typing import Any, List, Literal

# âœ… Verified from Context7 (LangGraph persistence.md):
# - SqliteSaver for persistent checkpointing (state survives restarts)
# - AsyncSqliteSaver for async workflows with ainvoke/astream
# - MemorySaver for in-memory (state lost on restart)
from langgraph.checkpoint.sqlite.aio import AsyncSqliteSaver
from langgraph.graph import END, StateGraph

from .nodes import (
    analysis_node,
    cleanup_node,
    commit_node,
    dev_node,
    fix_node,
    halt_node,
    merge_node,
    po_node,
    qa_node,
    sdd_pre_validation_node,  # NEW: v1.1.0 - Pre-dev SDD validation
    sdd_validation_node,
    sm_node,
)
from .state import BmadOrchestratorState, create_initial_state

# ============================================================================
# è¾…åŠ©å‡½æ•°
# ============================================================================

async def create_worktrees_from_main(
    base_path: Path,
    worktree_base: Path,
    story_ids: List[str],
) -> dict:
    """
    ä¸º re-QA åœºæ™¯ä» main åˆ†æ”¯åˆ›å»º worktrees

    å½“ä½¿ç”¨ --skip-dev å‚æ•°æ—¶ï¼Œä»£ç å·²åœ¨ main åˆ†æ”¯ä¸Šã€‚
    æ­¤å‡½æ•°åˆ›å»º worktrees ä»¥ä¾¿ QA å¯ä»¥åœ¨ç‹¬ç«‹ç¯å¢ƒä¸­è¿è¡Œã€‚

    Args:
        base_path: é¡¹ç›®æ ¹ç›®å½• (main ä»“åº“)
        worktree_base: Worktree çˆ¶ç›®å½•
        story_ids: Story IDs åˆ—è¡¨

    Returns:
        worktree_paths: {story_id: worktree_path} å­—å…¸
    """
    worktree_paths = {}

    for story_id in story_ids:
        branch_name = f"develop-{story_id}"
        worktree_path = worktree_base / f"Canvas-{branch_name}"

        # æ£€æŸ¥ worktree æ˜¯å¦å·²å­˜åœ¨
        if worktree_path.exists():
            print(f"[Worktree] {worktree_path} already exists, reusing")
            worktree_paths[story_id] = str(worktree_path)
            continue

        # æ£€æŸ¥åˆ†æ”¯æ˜¯å¦å·²å­˜åœ¨
        check_branch = await asyncio.create_subprocess_exec(
            'git', 'rev-parse', '--verify', branch_name,
            cwd=str(base_path),
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE,
        )
        _, _ = await check_branch.communicate()
        branch_exists = check_branch.returncode == 0

        if branch_exists:
            # åˆ†æ”¯å­˜åœ¨ï¼Œä½¿ç”¨ç°æœ‰åˆ†æ”¯åˆ›å»º worktree
            proc = await asyncio.create_subprocess_exec(
                'git', 'worktree', 'add', str(worktree_path), branch_name,
                cwd=str(base_path),
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
            )
        else:
            # åˆ†æ”¯ä¸å­˜åœ¨ï¼Œä» main åˆ›å»ºæ–°åˆ†æ”¯
            proc = await asyncio.create_subprocess_exec(
                'git', 'worktree', 'add', '-b', branch_name, str(worktree_path), 'main',
                cwd=str(base_path),
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
            )

        stdout, stderr = await proc.communicate()

        if proc.returncode == 0:
            print(f"[Worktree] Created: {worktree_path}")
            worktree_paths[story_id] = str(worktree_path)
        else:
            error_msg = stderr.decode() if stderr else "Unknown error"
            print(f"[Worktree] [WARN] Failed to create {worktree_path}: {error_msg}")
            # å³ä½¿å¤±è´¥ä¹Ÿè®°å½•è·¯å¾„ï¼Œè®©åç»­æµç¨‹å†³å®šæ˜¯å¦ç»§ç»­
            worktree_paths[story_id] = str(worktree_path)

    return worktree_paths


# ============================================================================
# è·¯ç”±å‡½æ•°
# ============================================================================

def route_after_sm(state: BmadOrchestratorState) -> Literal["po_node", "halt_node"]:
    """
    SM èŠ‚ç‚¹åçš„è·¯ç”±

    âœ… Verified from LangGraph Skill (Pattern: Conditional routing)

    è§„åˆ™:
    - sm_status == "completed" â†’ PO
    - sm_status == "failed" â†’ HALT
    """
    if state.get("sm_status") == "completed":
        return "po_node"
    return "halt_node"


def route_after_po(state: BmadOrchestratorState) -> Literal["analysis_node", "halt_node"]:
    """
    PO èŠ‚ç‚¹åçš„è·¯ç”±

    è§„åˆ™:
    - æœ‰ approved_stories â†’ Analysis
    - æ—  approved_stories â†’ HALT
    """
    if state.get("approved_stories"):
        return "analysis_node"
    return "halt_node"


def route_after_analysis(state: BmadOrchestratorState) -> Literal["sdd_pre_validation_node"]:
    """
    Analysis èŠ‚ç‚¹åçš„è·¯ç”± (v1.1.0)

    æ€»æ˜¯è¿›å…¥ SDD Pre-Validation é˜¶æ®µ (å¼€å‘å‰éªŒè¯)
    """
    return "sdd_pre_validation_node"


def route_after_sdd_pre(state: BmadOrchestratorState) -> Literal["dev_node", "halt_node"]:
    """
    SDD Pre-Validation èŠ‚ç‚¹åçš„è·¯ç”± (v1.1.0)

    è§„åˆ™:
    - sdd_pre_status == "passed" or "warnings" â†’ DEV
    - sdd_pre_status == "failed" â†’ HALT (å¼ºåˆ¶é˜»æ­¢)
    """
    sdd_pre_status = state.get("sdd_pre_status", "pending")

    if sdd_pre_status in ("passed", "warnings", "skipped"):
        print(f"[Router] SDD_PREâ†’DEV: sdd_pre_status={sdd_pre_status}")
        return "dev_node"

    # å¤±è´¥æ—¶é˜»æ­¢å¼€å‘
    print(f"[Router] SDD_PREâ†’HALT: sdd_pre_status={sdd_pre_status}")
    return "halt_node"


def route_after_dev(state: BmadOrchestratorState) -> Literal["qa_node", "halt_node"]:
    """
    Dev èŠ‚ç‚¹åçš„è·¯ç”± - FAIL-FORWARD è®¾è®¡

    âœ… Enhanced: æ£€æŸ¥å®é™… dev_outcomes åˆ—è¡¨ï¼Œè€Œéä»…æ£€æŸ¥çŠ¶æ€æ ‡å¿—

    è§„åˆ™:
    - å¦‚æœæœ‰ä»»ä½• Story å¼€å‘æˆåŠŸ â†’ QA (å³ä½¿éƒ¨åˆ†å¤±è´¥ä¹Ÿç»§ç»­)
    - åªæœ‰å½“æ‰€æœ‰ Story éƒ½å¤±è´¥æ—¶ â†’ HALT

    è¿™ç¡®ä¿å³ä½¿éƒ¨åˆ† Story å¤±è´¥ï¼Œå·¥ä½œæµä¹Ÿèƒ½ç»§ç»­åˆ° QA é˜¶æ®µã€‚
    """
    # é¦–å…ˆæ£€æŸ¥å®é™…çš„ dev_outcomes åˆ—è¡¨
    dev_outcomes = state.get("dev_outcomes", [])

    if dev_outcomes:
        # è®¡ç®—æˆåŠŸçš„ Story æ•°é‡
        success_count = sum(
            1 for o in dev_outcomes
            if o.get("outcome") == "SUCCESS"
        )

        if success_count > 0:
            # è‡³å°‘æœ‰ä¸€ä¸ª Story æˆåŠŸ - ç»§ç»­åˆ° QA
            print(f"[Router] DEVâ†’QA: {success_count}/{len(dev_outcomes)} stories succeeded (fail-forward)")
            return "qa_node"

    # å›é€€åˆ°çŠ¶æ€æ ‡å¿—æ£€æŸ¥ï¼ˆå‘åå…¼å®¹ï¼‰
    dev_status = state.get("dev_status", "unknown")
    if dev_status in ("completed", "partially_failed"):
        print(f"[Router] DEVâ†’QA: dev_status={dev_status}")
        return "qa_node"

    # åªæœ‰å½“çœŸçš„æ²¡æœ‰æˆåŠŸæ—¶æ‰ HALT
    print(f"[Router] DEVâ†’HALT: No successful stories (dev_status={dev_status}, outcomes={len(dev_outcomes)})")
    return "halt_node"


def route_after_qa(
    state: BmadOrchestratorState
) -> Literal["sdd_validation_node", "fix_node", "halt_node"]:
    """
    QA èŠ‚ç‚¹åçš„è·¯ç”± - FAIL-FORWARD è®¾è®¡

    ä¼˜å…ˆæ£€æŸ¥ qa_outcomes åˆ—è¡¨çš„å®é™…ç»“æœï¼Œè€Œä¸æ˜¯ä»…ä¾èµ– current_qa_gate çŠ¶æ€æ ‡å¿—ã€‚

    Fail-Forward è§„åˆ™:
    1. é¦–å…ˆæ£€æŸ¥ qa_outcomes åˆ—è¡¨
    2. å¦‚æœæœ‰ä»»ä½• PASS/WAIVED çš„ Story â†’ ç»§ç»­åˆ° SDD_VALIDATION
    3. å¦‚æœåªæœ‰ CONCERNS ä¸”æœªä¿®å¤ â†’ å°è¯• FIX (æœ€å¤š1æ¬¡)
    4. åªæœ‰å½“å…¨éƒ¨ FAIL æ—¶æ‰ HALT
    5. å›é€€åˆ° current_qa_gate çŠ¶æ€æ ‡å¿—ï¼ˆå‘åå…¼å®¹ï¼‰
    """
    # é¦–å…ˆæ£€æŸ¥å®é™…çš„ qa_outcomes åˆ—è¡¨
    qa_outcomes = state.get("qa_outcomes", [])
    fix_attempts = state.get("fix_attempts", 0)

    if qa_outcomes:
        # ç»Ÿè®¡å„ç§ç»“æœ
        pass_count = sum(
            1 for o in qa_outcomes
            if o.get("qa_gate") in ("PASS", "WAIVED")
        )
        concerns_count = sum(
            1 for o in qa_outcomes
            if o.get("qa_gate") == "CONCERNS"
        )
        fail_count = sum(
            1 for o in qa_outcomes
            if o.get("qa_gate") == "FAIL"
        )
        total = len(qa_outcomes)

        # Fail-forward: è‡³å°‘æœ‰ä¸€ä¸ª PASS/WAIVED å°±ç»§ç»­
        if pass_count > 0:
            print(f"[Router] QAâ†’SDD: {pass_count}/{total} stories passed (fail-forward)")
            return "sdd_validation_node"

        # åªæœ‰ CONCERNSï¼Œå°è¯•ä¿®å¤ä¸€æ¬¡
        if concerns_count > 0 and fail_count == 0 and fix_attempts < 1:
            print(f"[Router] QAâ†’FIX: {concerns_count} stories with concerns, attempting fix")
            return "fix_node"

        # å¦‚æœæœ‰ CONCERNS ä½†å·²ç»å°è¯•ä¿®å¤è¿‡ï¼Œç»§ç»­åˆ° SDDï¼ˆfail-forwardï¼‰
        if concerns_count > 0 and fix_attempts >= 1:
            print(f"[Router] QAâ†’SDD: {concerns_count} concerns after fix attempt (fail-forward)")
            return "sdd_validation_node"

        # å…¨éƒ¨ FAIL æ‰ HALT
        if fail_count == total:
            print(f"[Router] QAâ†’HALT: All {fail_count} stories failed")
            return "halt_node"

    # å›é€€åˆ°çŠ¶æ€æ ‡å¿—æ£€æŸ¥ï¼ˆå‘åå…¼å®¹ï¼‰
    qa_gate = state.get("current_qa_gate", "unknown")

    if qa_gate in ("PASS", "WAIVED"):
        print(f"[Router] QAâ†’SDD: qa_gate={qa_gate}")
        return "sdd_validation_node"
    elif qa_gate == "CONCERNS" and fix_attempts < 1:
        print(f"[Router] QAâ†’FIX: qa_gate={qa_gate}, fix_attempts={fix_attempts}")
        return "fix_node"
    elif qa_gate == "CONCERNS" and fix_attempts >= 1:
        # Fail-forward: CONCERNS ä¿®å¤åç»§ç»­
        print(f"[Router] QAâ†’SDD: qa_gate={qa_gate} after fix (fail-forward)")
        return "sdd_validation_node"

    # åªæœ‰å½“çœŸçš„å¤±è´¥æ—¶æ‰ HALT
    print(f"[Router] QAâ†’HALT: qa_gate={qa_gate}, outcomes={len(qa_outcomes)}")
    return "halt_node"


def route_after_sdd_validation(
    state: BmadOrchestratorState
) -> Literal["merge_node", "halt_node"]:
    """
    SDD Validation èŠ‚ç‚¹åçš„è·¯ç”± - FAIL-FORWARD è®¾è®¡

    è§„åˆ™:
    - sdd_status in ("passed", "warnings", "skipped") â†’ MERGE
    - sdd_status == "failed" â†’ HALT (ä½† warnings ä»ç»§ç»­)

    Fail-Forward: é»˜è®¤ä¸º "skipped"ï¼Œç¡®ä¿å³ä½¿ SDD éªŒè¯æœªè¿è¡Œä¹Ÿèƒ½ç»§ç»­
    """
    sdd_status = state.get("sdd_status", "skipped")

    if sdd_status in ("passed", "warnings", "skipped"):
        print(f"[Router] SDDâ†’MERGE: sdd_status={sdd_status}")
        return "merge_node"

    # åªæœ‰æ˜ç¡®å¤±è´¥æ‰ HALT
    print(f"[Router] SDDâ†’HALT: sdd_status={sdd_status}")
    return "halt_node"


def route_after_merge(state: BmadOrchestratorState) -> Literal["commit_node", "halt_node"]:
    """
    Merge èŠ‚ç‚¹åçš„è·¯ç”± - FAIL-FORWARD è®¾è®¡

    è§„åˆ™:
    - merge_status == "conflict_detected" â†’ HALT
    - merge_status == "gate_blocked" â†’ HALT (preserve worktrees for manual fix)
    - å…¶ä»–æƒ…å†µ (åŒ…æ‹¬ "completed", "partial") â†’ COMMIT

    Fail-Forward: åªæœ‰æ˜ç¡®çš„å†²çªå’Œ Gate é˜»å¡æ‰åœæ­¢ï¼Œéƒ¨åˆ†åˆå¹¶ç»§ç»­
    """
    merge_status = state.get("merge_status", "unknown")

    # å†²çªå’Œ Gate é˜»å¡éƒ½ HALT
    if merge_status == "conflict_detected":
        print(f"[Router] MERGEâ†’HALT: merge_status={merge_status}")
        return "halt_node"

    if merge_status == "gate_blocked":
        print(f"[Router] MERGEâ†’HALT: merge_status={merge_status} (Commit Gate failed, preserving worktrees)")
        return "halt_node"

    # Fail-forward: å…¶ä»–æƒ…å†µç»§ç»­åˆ° COMMIT
    print(f"[Router] MERGEâ†’COMMIT: merge_status={merge_status}")
    return "commit_node"


def route_after_fix(state: BmadOrchestratorState) -> Literal["qa_node", "commit_node"]:
    """
    Fix èŠ‚ç‚¹åçš„è·¯ç”±

    è§„åˆ™:
    - fix_attempts == 1 â†’ é‡æ–° QA
    - fix_attempts > 1 â†’ æäº¤å·²é€šè¿‡çš„
    """
    if state.get("fix_attempts", 0) <= 1:
        return "qa_node"
    return "commit_node"


def route_after_commit(state: BmadOrchestratorState) -> Literal["cleanup_node", "dev_node"]:
    """
    Commit èŠ‚ç‚¹åçš„è·¯ç”±

    è§„åˆ™:
    - re-QA æ¨¡å¼ (skip_dev) â†’ ç›´æ¥ CLEANUP (ä¸å¾ªç¯å›DEV)
    - è¿˜æœ‰æ›´å¤šæ‰¹æ¬¡ â†’ è¿”å› DEV
    - æ‰€æœ‰æ‰¹æ¬¡å®Œæˆ â†’ CLEANUP (ç„¶ååˆ° END)
    """
    # æ£€æŸ¥æ˜¯å¦ä¸º re-QA æ¨¡å¼ (skip_dev)
    re_qa_mode = state.get("re_qa_mode", False)
    if re_qa_mode:
        print("[Router] COMMITâ†’CLEANUP: re-QA mode - skip batch checking")
        return "cleanup_node"

    current_batch_index = state.get("current_batch_index", 0)
    parallel_batches = state.get("parallel_batches", [])

    if current_batch_index + 1 < len(parallel_batches):
        # è¿˜æœ‰æ›´å¤šæ‰¹æ¬¡ï¼Œæ›´æ–°ç´¢å¼•å¹¶è¿”å› DEV
        print(f"[Router] COMMITâ†’DEV: More batches remaining ({current_batch_index + 1}/{len(parallel_batches)})")
        return "dev_node"

    # æ‰€æœ‰æ‰¹æ¬¡å®Œæˆï¼Œè¿›å…¥æ¸…ç†
    print("[Router] COMMITâ†’CLEANUP: All batches completed")
    return "cleanup_node"


# ============================================================================
# StateGraph æ„å»º
# ============================================================================

def build_graph(
    checkpointer_path: str = "bmad_orchestrator.db",
    skip_sm: bool = False,
    skip_po: bool = False,
    skip_analysis: bool = False,
    skip_dev: bool = False,
    skip_qa: bool = False,
    skip_sdd: bool = False,
) -> StateGraph:
    """
    æ„å»º BMad Orchestrator StateGraph (æ”¯æŒå¤šé˜¶æ®µè·³è¿‡)

    âœ… Verified from LangGraph Skill:
    - Pattern: StateGraph(State) construction
    - add_node() for nodes
    - add_conditional_edges() for routing

    Args:
        checkpointer_path: SqliteSaver æ•°æ®åº“è·¯å¾„
        skip_sm: æ˜¯å¦è·³è¿‡SMé˜¶æ®µ
        skip_po: æ˜¯å¦è·³è¿‡POé˜¶æ®µ
        skip_analysis: æ˜¯å¦è·³è¿‡Analysisé˜¶æ®µ
        skip_dev: æ˜¯å¦è·³è¿‡DEVé˜¶æ®µ (ç”¨äºre-QAåœºæ™¯)
        skip_qa: æ˜¯å¦è·³è¿‡QAé˜¶æ®µ
        skip_sdd: æ˜¯å¦è·³è¿‡SDDé˜¶æ®µ

    Returns:
        ç¼–è¯‘åçš„ StateGraph
    """
    # åˆ›å»º StateGraph
    graph = StateGraph(BmadOrchestratorState)

    # ========================================
    # æ·»åŠ èŠ‚ç‚¹
    # ========================================

    graph.add_node("sm_node", sm_node)
    graph.add_node("po_node", po_node)
    graph.add_node("analysis_node", analysis_node)
    graph.add_node("sdd_pre_validation_node", sdd_pre_validation_node)  # NEW: v1.1.0 - Pre-dev SDD
    graph.add_node("dev_node", dev_node)
    graph.add_node("qa_node", qa_node)
    graph.add_node("sdd_validation_node", sdd_validation_node)  # SDD Validation (post-QA)
    graph.add_node("merge_node", merge_node)
    graph.add_node("commit_node", commit_node)
    graph.add_node("fix_node", fix_node)
    graph.add_node("halt_node", halt_node)
    graph.add_node("cleanup_node", cleanup_node)  # Cleanup (v1.1.0)

    # ========================================
    # è®¾ç½®å…¥å£ç‚¹ (æ ¹æ® skip å‚æ•°æ¡ä»¶è®¾ç½®)
    # ========================================

    if skip_sm and skip_po and skip_analysis and skip_dev:
        graph.set_entry_point("qa_node")
        print("[INFO] Skip SM+PO+Analysis+DEV - starting from QA node (re-QA mode)")
    elif skip_sm and skip_po and skip_analysis:
        graph.set_entry_point("dev_node")
        print("[INFO] Skip SM+PO+Analysis - starting from DEV node")
    elif skip_sm and skip_po:
        graph.set_entry_point("analysis_node")
        print("[INFO] Skip SM+PO - starting from Analysis node")
    elif skip_sm:
        graph.set_entry_point("po_node")
        print("[INFO] Skip SM - starting from PO node")
    else:
        graph.set_entry_point("sm_node")

    # ========================================
    # æ·»åŠ æ¡ä»¶è¾¹ (è·¯ç”±)
    # ========================================

    # SM â†’ PO or HALT
    graph.add_conditional_edges(
        "sm_node",
        route_after_sm,
        {
            "po_node": "po_node",
            "halt_node": "halt_node",
        }
    )

    # PO â†’ Analysis or HALT
    graph.add_conditional_edges(
        "po_node",
        route_after_po,
        {
            "analysis_node": "analysis_node",
            "halt_node": "halt_node",
        }
    )

    # Analysis â†’ SDD_PRE (v1.1.0)
    graph.add_conditional_edges(
        "analysis_node",
        route_after_analysis,
        {
            "sdd_pre_validation_node": "sdd_pre_validation_node",
        }
    )

    # SDD_PRE â†’ DEV or HALT (v1.1.0)
    graph.add_conditional_edges(
        "sdd_pre_validation_node",
        route_after_sdd_pre,
        {
            "dev_node": "dev_node",
            "halt_node": "halt_node",
        }
    )

    # DEV â†’ QA or HALT
    graph.add_conditional_edges(
        "dev_node",
        route_after_dev,
        {
            "qa_node": "qa_node",
            "halt_node": "halt_node",
        }
    )

    # QA â†’ SDD_VALIDATION or FIX or HALT
    graph.add_conditional_edges(
        "qa_node",
        route_after_qa,
        {
            "sdd_validation_node": "sdd_validation_node",  # NEW: QA â†’ SDD
            "fix_node": "fix_node",
            "halt_node": "halt_node",
        }
    )

    # SDD_VALIDATION â†’ MERGE or HALT
    graph.add_conditional_edges(
        "sdd_validation_node",
        route_after_sdd_validation,
        {
            "merge_node": "merge_node",
            "halt_node": "halt_node",
        }
    )

    # MERGE â†’ COMMIT or HALT
    graph.add_conditional_edges(
        "merge_node",
        route_after_merge,
        {
            "commit_node": "commit_node",
            "halt_node": "halt_node",
        }
    )

    # FIX â†’ QA or COMMIT
    graph.add_conditional_edges(
        "fix_node",
        route_after_fix,
        {
            "qa_node": "qa_node",
            "commit_node": "commit_node",
        }
    )

    # COMMIT â†’ CLEANUP or DEV (for next batch)
    graph.add_conditional_edges(
        "commit_node",
        route_after_commit,
        {
            "cleanup_node": "cleanup_node",  # Changed: COMMIT â†’ CLEANUP (v1.1.0)
            "dev_node": "dev_node",
        }
    )

    # HALT â†’ CLEANUP (Changed from HALT â†’ END in v1.1.0)
    # Ensures cleanup always happens even on failure
    graph.add_edge("halt_node", "cleanup_node")

    # CLEANUP â†’ END (Always the final step)
    graph.add_edge("cleanup_node", END)

    return graph


def compile_graph(
    checkpointer_path: str = "bmad_orchestrator.db",
    skip_sm: bool = False,
    skip_po: bool = False,
    skip_analysis: bool = False,
    skip_dev: bool = False,
    skip_qa: bool = False,
    skip_sdd: bool = False,
) -> Any:
    """
    ç¼–è¯‘ StateGraph (ä¸åŒ…å« checkpointerï¼Œé€‚ç”¨äºåŒæ­¥æµ‹è¯•åœºæ™¯)

    âš ï¸ NOTE: æ­¤å‡½æ•°ç¼–è¯‘å›¾ä½†ä¸è®¾ç½® checkpointerã€‚
    å¯¹äºå¼‚æ­¥å·¥ä½œæµ (ainvoke/astream)ï¼Œè¯·ä½¿ç”¨ run_epic_workflow()ï¼Œ
    å®ƒä¼šåœ¨ async with AsyncSqliteSaver ä¸Šä¸‹æ–‡ä¸­æ­£ç¡®å¤„ç† checkpointerã€‚

    âœ… Verified from Context7 (LangGraph persistence.md):
    - AsyncSqliteSaver requires 'async with' context manager
    - For async workflows, checkpointer must be set within async context

    Args:
        checkpointer_path: æ•°æ®åº“è·¯å¾„ (ä¼ é€’ç»™ build_graphï¼Œæœªä½¿ç”¨äº checkpointer)
        skip_sm: æ˜¯å¦è·³è¿‡SMé˜¶æ®µ
        skip_po: æ˜¯å¦è·³è¿‡POé˜¶æ®µ
        skip_analysis: æ˜¯å¦è·³è¿‡Analysisé˜¶æ®µ
        skip_dev: æ˜¯å¦è·³è¿‡DEVé˜¶æ®µ (ç”¨äºre-QAåœºæ™¯)
        skip_qa: æ˜¯å¦è·³è¿‡QAé˜¶æ®µ
        skip_sdd: æ˜¯å¦è·³è¿‡SDDé˜¶æ®µ

    Returns:
        ç¼–è¯‘åçš„ Graph (æ—  checkpointerï¼Œé€‚ç”¨äºåŒæ­¥/æµ‹è¯•åœºæ™¯)
    """
    graph = build_graph(
        checkpointer_path,
        skip_sm=skip_sm,
        skip_po=skip_po,
        skip_analysis=skip_analysis,
        skip_dev=skip_dev,
        skip_qa=skip_qa,
        skip_sdd=skip_sdd,
    )

    # âœ… Verified from Context7 (LangGraph persistence.md):
    # For async workflows (ainvoke/astream), use AsyncSqliteSaver
    # AsyncSqliteSaver must be used with 'async with' context manager
    # Therefore, we only build the graph here and let the caller handle checkpointer

    # ğŸ”’ ç¼–è¯‘å›¾ä½†ä¸è®¾ç½® checkpointer
    # checkpointer å°†ç”±è°ƒç”¨æ–¹ (run_epic_workflow) åœ¨ async with ä¸Šä¸‹æ–‡ä¸­è®¾ç½®
    compiled = graph.compile()

    return compiled


# ============================================================================
# è¿è¡Œå…¥å£
# ============================================================================

async def run_epic_workflow(
    epic_id: str,
    story_ids: List[str],
    base_path: str,
    worktree_base: str = None,
    max_turns: int = 200,
    mode_override: Literal["parallel", "linear", "hybrid"] = None,
    checkpointer_path: str = "bmad_orchestrator.db",
    skip_sm: bool = False,
    skip_po: bool = False,
    skip_analysis: bool = False,
    skip_dev: bool = False,
    skip_qa: bool = False,
    skip_sdd: bool = False,
    timeout: int = 3600,
) -> dict:
    """
    è¿è¡Œ Epic å…¨è‡ªåŠ¨åŒ–å·¥ä½œæµ

    âœ… Verified from LangGraph Skill:
    - Pattern: compiled_graph.ainvoke(initial_state)

    Args:
        epic_id: Epic ID
        story_ids: è¦å¤„ç†çš„ Story IDs
        base_path: é¡¹ç›®æ ¹ç›®å½•
        worktree_base: Worktree çˆ¶ç›®å½• (é»˜è®¤: base_path çˆ¶ç›®å½•)
        max_turns: æ¯ä¸ª Claude ä¼šè¯çš„æœ€å¤§è½®æ•°
        mode_override: å¼ºåˆ¶æ‰§è¡Œæ¨¡å¼
        checkpointer_path: SqliteSaver æ•°æ®åº“è·¯å¾„
        skip_sm: æ˜¯å¦è·³è¿‡SMé˜¶æ®µ
        skip_po: æ˜¯å¦è·³è¿‡POé˜¶æ®µ
        skip_analysis: æ˜¯å¦è·³è¿‡ä¾èµ–åˆ†æé˜¶æ®µ
        skip_dev: æ˜¯å¦è·³è¿‡DEVé˜¶æ®µ (ç”¨äºre-QAåœºæ™¯)
        skip_qa: æ˜¯å¦è·³è¿‡QAé˜¶æ®µ
        skip_sdd: æ˜¯å¦è·³è¿‡SDDéªŒè¯
        timeout: è¶…æ—¶æ—¶é—´(ç§’)

    Returns:
        æœ€ç»ˆçŠ¶æ€å­—å…¸
    """
    print("=" * 60)
    print(f"BMad Orchestrator - Epic {epic_id} Workflow")
    print("=" * 60)
    print(f"Stories: {story_ids}")
    print(f"Base Path: {base_path}")
    print(f"Mode Override: {mode_override or 'Auto-detect'}")
    print(f"Max Turns: {max_turns}")
    print(f"Timeout: {timeout}s")
    print("\nPhase Skip Settings:")
    print(f"  Skip SM: {skip_sm}")
    print(f"  Skip PO: {skip_po}")
    print(f"  Skip Analysis: {skip_analysis}")
    print(f"  Skip DEV: {skip_dev}")
    print(f"  Skip QA: {skip_qa}")
    print(f"  Skip SDD: {skip_sdd}")
    if any([skip_sm, skip_po, skip_analysis, skip_dev, skip_qa, skip_sdd]):
        entry_point = "SM"
        if skip_sm:
            entry_point = "PO"
        if skip_sm and skip_po:
            entry_point = "Analysis"
        if skip_sm and skip_po and skip_analysis:
            entry_point = "DEV"
        if skip_sm and skip_po and skip_analysis and skip_dev:
            entry_point = "QA"
        print(f"  ** Entry Point: {entry_point} **")
    print("=" * 60)

    # åˆ›å»ºåˆå§‹çŠ¶æ€
    initial_state = create_initial_state(
        epic_id=epic_id,
        story_ids=story_ids,
        base_path=base_path,
        worktree_base=worktree_base,
        max_turns=max_turns,
        mode_override=mode_override,
    )

    # æ·»åŠ  timeout åˆ° state (ä¾› nodes ä½¿ç”¨)
    initial_state["timeout"] = timeout

    # é¢„å¡«å…… story_drafts (å¤šä¸ªè·³è¿‡æ¨¡å¼éƒ½éœ€è¦)
    base_path_obj = Path(base_path)
    story_drafts = []
    for story_id in story_ids:
        # å°è¯•æ‰¾åˆ° Story æ–‡ä»¶
        story_file_patterns = [
            f"docs/stories/story-{story_id}.story.md",
            f"docs/stories/{story_id}.story.md",
        ]
        story_file = None
        for pattern in story_file_patterns:
            full_path = base_path_obj / pattern
            if full_path.exists():
                story_file = pattern
                break

        if story_file:
            story_drafts.append({
                "story_id": story_id,
                "story_file": story_file,
                "outcome": "SUCCESS",
                "checklist_passed": True,
            })
        else:
            story_drafts.append({
                "story_id": story_id,
                "story_file": f"docs/stories/story-{story_id}.story.md",
                "outcome": "SUCCESS",
                "checklist_passed": True,
            })

    # ========================================
    # Skip SM çŠ¶æ€é¢„å¡«å……
    # ========================================
    if skip_sm:
        initial_state["sm_status"] = "completed"
        initial_state["current_phase"] = "po"
        initial_state["story_drafts"] = story_drafts
        print(f"[INFO] Skip SM - Pre-filled {len(story_drafts)} story drafts")

    # ========================================
    # Skip PO çŠ¶æ€é¢„å¡«å……
    # ========================================
    if skip_po:
        initial_state["po_status"] = "completed"
        initial_state["approved_stories"] = story_ids  # å‡è®¾å…¨éƒ¨æ‰¹å‡†
        initial_state["sot_resolutions"] = []
        if not skip_sm:
            initial_state["story_drafts"] = story_drafts
        initial_state["current_phase"] = "analysis"
        print(f"[INFO] Skip PO - All {len(story_ids)} stories pre-approved")

    # ========================================
    # Skip Analysis çŠ¶æ€é¢„å¡«å……
    # ========================================
    if skip_analysis:
        mode = mode_override or "parallel"
        if mode == "linear":
            batches = [[s] for s in story_ids]
        else:
            batches = [story_ids]  # å•æ‰¹æ¬¡å¹¶è¡Œ
        initial_state["parallel_batches"] = batches
        initial_state["current_batch_index"] = 0
        initial_state["execution_mode"] = mode
        if not skip_po:
            initial_state["approved_stories"] = story_ids
        initial_state["current_phase"] = "dev"
        print(f"[INFO] Skip Analysis - Using {mode} mode with {len(batches)} batch(es)")

    # ========================================
    # Skip DEV çŠ¶æ€é¢„å¡«å…… (re-QA åœºæ™¯)
    # ========================================
    if skip_dev:
        from datetime import datetime

        # åˆ›å»º worktrees ä» main åˆ†æ”¯
        worktree_base_path = Path(worktree_base) if worktree_base else base_path_obj.parent
        worktree_paths = await create_worktrees_from_main(base_path_obj, worktree_base_path, story_ids)

        # æ„é€ åˆæˆ dev_outcomes (å‡è®¾æ‰€æœ‰ Story å¼€å‘æˆåŠŸ)
        # âš ï¸ å¿…é¡»æ ‡è®°ä¸º syntheticï¼Œä»¥ä¾¿ Commit Gate G5 æ£€æµ‹
        synthetic_dev_outcomes = []
        for story_id in story_ids:
            synthetic_dev_outcomes.append({
                "story_id": story_id,
                "outcome": "SUCCESS",
                "status": "synthetic_success",  # ğŸ”’ G5 æ£€æµ‹æ ‡è®°
                "synthetic": True,               # ğŸ”’ G5 æ£€æµ‹æ ‡è®°
                "skipped": True,                 # ğŸ”’ G5 æ£€æµ‹æ ‡è®°
                "tests_passed": True,
                "test_count": 0,
                "test_coverage": None,
                "files_created": [],
                "files_modified": [],
                "duration_seconds": 0,
                "blocking_reason": None,
                "completion_notes": "âš ï¸ SYNTHETIC: Pre-filled for re-QA (--skip-dev) - NOT real execution",
                "agent_model": "claude-sonnet-4-5",
                "timestamp": datetime.now().isoformat(),
            })

        initial_state["dev_outcomes"] = synthetic_dev_outcomes
        initial_state["dev_status"] = "completed"
        initial_state["worktree_paths"] = worktree_paths
        initial_state["current_phase"] = "qa"
        initial_state["re_qa_mode"] = True  # æ ‡è®°ä¸º re-QA æ¨¡å¼ï¼Œç”¨äºè·¯ç”±å†³ç­–

        # ç¡®ä¿å‰ç½®çŠ¶æ€ä¹Ÿæ­£ç¡®è®¾ç½®
        if not skip_analysis:
            mode = mode_override or "linear"
            if mode == "linear":
                batches = [[s] for s in story_ids]
            else:
                batches = [story_ids]
            initial_state["parallel_batches"] = batches
            initial_state["current_batch_index"] = 0
            initial_state["execution_mode"] = mode

        print(f"[INFO] Skip DEV - Created {len(worktree_paths)} worktrees from main branch")
        print(f"[INFO] Skip DEV - Pre-filled {len(synthetic_dev_outcomes)} dev_outcomes (re-QA mode)")

    # æ„å»º Graph (ä¼ é€’æ‰€æœ‰ skip å‚æ•°)
    graph = build_graph(
        checkpointer_path,
        skip_sm=skip_sm,
        skip_po=skip_po,
        skip_analysis=skip_analysis,
        skip_dev=skip_dev,
        skip_qa=skip_qa,
        skip_sdd=skip_sdd,
    )

    # âœ… Verified from Context7 (LangGraph persistence.md):
    # AsyncSqliteSaver requires 'async with' context manager for async workflows
    # Pattern: async with AsyncSqliteSaver.from_conn_string("db.sqlite") as checkpointer
    config = {"configurable": {"thread_id": f"epic-{epic_id}"}}

    async with AsyncSqliteSaver.from_conn_string(checkpointer_path) as checkpointer:
        print(f"[INFO] Using AsyncSqliteSaver for persistent checkpointing: {checkpointer_path}")

        # ç¼–è¯‘ Graph å¹¶é™„åŠ  checkpointer
        compiled = graph.compile(checkpointer=checkpointer)

        # è¿è¡Œ
        final_state = await compiled.ainvoke(initial_state, config)

    # === Status Persistence: Persist workflow results to YAML ===
    try:
        from .status_persister import StatusPersister

        persister = StatusPersister(Path(base_path))
        success = await persister.persist_workflow_result(
            final_state=final_state,
            epic_id=epic_id,
        )

        if success:
            print(f"[Orchestrator] Status persisted to {persister.YAML_PATH}")
        else:
            print("[Orchestrator] [WARN] Failed to persist status (non-blocking)")
    except Exception as e:
        print(f"[Orchestrator] [WARN] Status persistence error: {e} (non-blocking)")
    # === End Status Persistence ===

    print("=" * 60)
    print("Workflow Complete!")
    print(f"Final Status: {final_state.get('final_status', 'unknown')}")
    print(f"Commits: {len(final_state.get('commit_shas', []))}")
    print(f"Blockers: {len(final_state.get('blockers', []))}")
    print("=" * 60)

    return final_state


async def resume_workflow(
    thread_id: str,
    checkpointer_path: str = "bmad_orchestrator.db",
) -> dict:
    """
    æ¢å¤ä¸­æ–­çš„å·¥ä½œæµ

    âœ… Verified from LangGraph Skill:
    - Pattern: Use thread_id to resume from checkpoint

    âœ… Verified from Context7 (LangGraph persistence.md):
    - AsyncSqliteSaver requires 'async with' context manager

    Args:
        thread_id: å·¥ä½œæµçº¿ç¨‹ ID (é€šå¸¸æ˜¯ "epic-{epic_id}")
        checkpointer_path: AsyncSqliteSaver æ•°æ®åº“è·¯å¾„

    Returns:
        æœ€ç»ˆçŠ¶æ€å­—å…¸
    """
    print(f"Resuming workflow: {thread_id}")

    # æ„å»º Graph (ä½¿ç”¨é»˜è®¤å‚æ•°)
    graph = build_graph(checkpointer_path)

    config = {"configurable": {"thread_id": thread_id}}

    # âœ… Verified from Context7: AsyncSqliteSaver requires async with
    async with AsyncSqliteSaver.from_conn_string(checkpointer_path) as checkpointer:
        compiled = graph.compile(checkpointer=checkpointer)

        # è·å–å½“å‰çŠ¶æ€
        state = await compiled.aget_state(config)

        if state.values:
            print(f"Current Phase: {state.values.get('current_phase', 'unknown')}")
            print("Resuming...")

            # ç»§ç»­æ‰§è¡Œ
            final_state = await compiled.ainvoke(None, config)
            return final_state
        else:
            print(f"No checkpoint found for thread: {thread_id}")
            return {}


# ============================================================================
# CLI å…¥å£
# ============================================================================

if __name__ == "__main__":
    import argparse
    import asyncio

    parser = argparse.ArgumentParser(description="BMad Orchestrator - Epic Workflow")
    parser.add_argument("epic_id", help="Epic ID to develop")
    parser.add_argument("--stories", nargs="+", required=True, help="Story IDs")
    parser.add_argument("--base-path", default=".", help="Project base path")
    parser.add_argument("--worktree-base", help="Worktree parent directory")
    parser.add_argument("--max-turns", type=int, default=200, help="Max turns per session")
    parser.add_argument("--mode", choices=["parallel", "linear", "hybrid"], help="Execution mode override")
    parser.add_argument("--resume", help="Resume from thread ID")

    args = parser.parse_args()

    async def main():
        if args.resume:
            result = await resume_workflow(args.resume)
        else:
            result = await run_epic_workflow(
                epic_id=args.epic_id,
                story_ids=args.stories,
                base_path=args.base_path,
                worktree_base=args.worktree_base,
                max_turns=args.max_turns,
                mode_override=args.mode,
            )

        print("\nFinal Result:")
        print(f"  Status: {result.get('final_status', 'unknown')}")
        print(f"  Summary: {result.get('completion_summary', 'N/A')}")

    asyncio.run(main())
