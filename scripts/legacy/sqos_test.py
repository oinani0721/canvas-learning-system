# sqos_test.py

import yaml
import re

class SQOS:
    def __init__(self, main_prompt_path, confusion_analyzer_path, question_generator_path, case_library_path):
        self.main_prompt_content = self._read_file_content(main_prompt_path)
        self.confusion_analyzer_content = self._read_file_content(confusion_analyzer_path)
        self.question_generator_content = self._read_file_content(question_generator_path)
        self.case_library = self._load_yaml(case_library_path)

        self.main_prompt_config = self._parse_main_prompt_for_question_types(self.main_prompt_content)
        self.confusion_analyzer_config = self._extract_yaml_from_markdown(self.confusion_analyzer_content)
        self.question_generator_config = self._extract_yaml_from_markdown(self.question_generator_content)

    def _read_file_content(self, file_path):
        with open(file_path, 'r', encoding='utf-8') as f:
            return f.read()

    def _load_yaml(self, file_path):
        with open(file_path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)

    def _extract_yaml_from_markdown(self, markdown_content):
        # Extracts YAML blocks from markdown files
        yaml_blocks = re.findall(r'```yaml\n([\s\S]*?)\n```', markdown_content)
        full_yaml = "\n".join(yaml_blocks)
        return yaml.safe_load(full_yaml) if full_yaml else {}

    def _parse_main_prompt_for_question_types(self, markdown_content):
        # Manually parse the 'é—®é¢˜ç±»å‹å®šä¹‰' section from the main prompt markdown
        question_type_definitions = {}
        # Pattern to find '### X. YYYå‹é—®é¢˜' sections and their 'ç›®çš„'
        pattern = r'### \d\. (.*?å‹é—®é¢˜)\n.*?-\s\*\*ç›®çš„\*\*ï¼š\s*(.*?)\n'
        matches = re.findall(pattern, markdown_content, re.DOTALL)
        for q_type, purpose in matches:
            question_type_definitions[q_type] = {'ç›®çš„': purpose.strip()}
        return {'é—®é¢˜ç±»å‹å®šä¹‰': question_type_definitions}

    def analyze_confusion(self, user_input):
        # Implement confusion analysis based on confusion_analyzer_config
        # This is a simplified placeholder. A full implementation would involve
        # concept extraction, misconception detection, multi-question separation, etc.
        
        core_concepts = []
        domain = ""
        user_level = "åˆå­¦è€…"
        confusion_type = "æ¦‚å¿µç†è§£"
        misconceptions = []

        # Simple keyword-based concept extraction and domain inference
        if re.search(r'static|class|method|public|void|String\[\]', user_input, re.IGNORECASE):
            domain = "Javaç¼–ç¨‹"
            if re.search(r'static', user_input, re.IGNORECASE):
                core_concepts.append({'concept': 'static', 'context': 'å…³é”®å­—ä½¿ç”¨', 'confidence': 0.9})
                # Use misconception patterns from config if available
                static_misconceptions = self.confusion_analyzer_config.get('è¯¯è§£æ¨¡å¼åº“', {}).get('staticç›¸å…³', [])
                for mc_pattern in static_misconceptions:
                    if re.search(mc_pattern['æ¨¡å¼'], user_input):
                        misconceptions.append({'type': mc_pattern['è¯¯è§£ç±»å‹'], 'detail': mc_pattern['æ­£ç¡®ç†è§£'], 'severity': 'high'})
            if re.search(r'class', user_input, re.IGNORECASE):
                core_concepts.append({'concept': 'class', 'context': 'å®šä¹‰å¯¹è±¡æ¨¡æ¿', 'confidence': 0.9})
                class_misconceptions = self.confusion_analyzer_config.get('è¯¯è§£æ¨¡å¼åº“', {}).get('classç›¸å…³', [])
                for mc_pattern in class_misconceptions:
                    if re.search(mc_pattern['æ¨¡å¼'], user_input):
                        misconceptions.append({'type': mc_pattern['è¯¯è§£ç±»å‹'], 'detail': mc_pattern['æ­£ç¡®ç†è§£'], 'severity': 'medium'})
            if re.search(r'main method|main', user_input, re.IGNORECASE):
                core_concepts.append({'concept': 'main method', 'context': 'ç¨‹åºå…¥å£', 'confidence': 1.0})
            if re.search(r'String\[\] args|args', user_input, re.IGNORECASE):
                core_concepts.append({'concept': 'String[] args', 'context': 'å‘½ä»¤è¡Œå‚æ•°', 'confidence': 1.0})
                args_misconceptions = self.confusion_analyzer_config.get('è¯¯è§£æ¨¡å¼åº“', {}).get('å‚æ•°ç›¸å…³', [])
                for mc_pattern in args_misconceptions:
                    if re.search(mc_pattern['æ¨¡å¼'], user_input):
                        misconceptions.append({'type': mc_pattern['è¯¯è§£ç±»å‹'], 'detail': mc_pattern['æ­£ç¡®ç†è§£'], 'severity': 'high'})
        elif re.search(r'é€’å½’', user_input):
            domain = "ç®—æ³•æ€ç»´"
            core_concepts.append({'concept': 'é€’å½’', 'context': 'ç®—æ³•æ¦‚å¿µ', 'confidence': 1.0})
            if re.search(r'åœä¸‹æ¥', user_input): # This specific misconception is not in the YAML, so keep it for now
                misconceptions.append({'type': 'å®ç°å›°éš¾', 'detail': 'ä¸ç†è§£é€’å½’ç»ˆæ­¢æ¡ä»¶', 'severity': 'high'})
        elif re.search(r'æ•°ç»„|é“¾è¡¨', user_input):
            domain = "æ•°æ®ç»“æ„"
            core_concepts.append({'concept': 'æ•°ç»„/é“¾è¡¨', 'context': 'æ•°æ®ç»“æ„', 'confidence': 1.0})
        elif re.search(r'ç»§æ‰¿', user_input):
            domain = "é¢å‘å¯¹è±¡ç¼–ç¨‹"
            core_concepts.append({'concept': 'ç»§æ‰¿', 'context': 'OOPæ¦‚å¿µ', 'confidence': 1.0})

        # Determine confusion type based on keywords
        if re.search(r'åŒºåˆ«|ä¸åŒ|å¯¹æ¯”', user_input):
            confusion_type = "å¯¹æ¯”å·®å¼‚"
        elif re.search(r'æ€ä¹ˆç”¨|å¦‚ä½•å®ç°|ä¾‹å­', user_input):
            confusion_type = "ä½¿ç”¨æ–¹æ³•"
        elif re.search(r'ä¸ºä»€ä¹ˆ|åŸç†|è®¾è®¡åˆè¡·', user_input):
            confusion_type = "åŸç†æ¢ç©¶"

        return {
            'raw_input': user_input,
            'core_concepts': core_concepts,
            'misconceptions': misconceptions,
            'confusion_type': confusion_type,
            'domain': domain,
            'user_level': user_level,
            'sub_questions': [], # Simplified, not implementing multi-question separation yet
            'focus_suggestion': ""
        }

    def generate_questions(self, confusion_analysis_result):
        # Implement question generation based on question_generator_config
        # This is a simplified placeholder
        
        core_concept = confusion_analysis_result['core_concepts'][0]['concept'] if confusion_analysis_result['core_concepts'] else "X"
        confusion_type = confusion_analysis_result['confusion_type']
        domain = confusion_analysis_result['domain']

        generated_questions = []
        # Accessing type_weights_by_confusion from the extracted YAML config
        question_type_weights = self.question_generator_config.get('type_weights_by_confusion', {})
        
        # Prioritize question types based on confusion type
        sorted_question_types = sorted(question_type_weights.items(), key=lambda item: item[1], reverse=True)
        
        # Ensure diversity by picking from different types
        selected_types_count = 0
        for q_type, _ in sorted_question_types:
            # Ensure we don't add more than 5 questions total
            if len(generated_questions) >= 5:
                break

            # Only add if this type hasn't been added or if we need more unique types
            if q_type not in [q['type'] for q in generated_questions] or selected_types_count < 3:
                selected_types_count += 1
                
                # Get templates from the config, adjusting key name to match YAML structure
                template_key = f'{q_type.replace("å‹é—®é¢˜", "").lower()}_templates' # e.g., 'å¯¹æ¯”å‹é—®é¢˜' -> 'å¯¹æ¯”_templates' -> 'comparison_templates'
                templates = self.question_generator_config.get(template_key, {})
                
                # Pick one template from each category for diversity, or just the first if category not explicit
                added_for_type = 0
                for template_category in templates:
                    if len(generated_questions) >= 5 or added_for_type >= 1: # Limit to 1 question per type from initial pass for diversity
                        break
                    template_str = templates[template_category][0]
                    question_text = template_str.replace('{concept}', core_concept).replace('{concept1}', core_concept).replace('{concept2}', 'Y').replace('{action}', 'æ­¤æ¦‚å¿µ').replace('{feature}', core_concept).replace('{language/framework}', domain)
                    generated_questions.append({
                        'text': question_text,
                        'type': q_type,
                        'explanation': self.main_prompt_config['é—®é¢˜ç±»å‹å®šä¹‰'][q_type]['ç›®çš„'],
                    })
                    added_for_type += 1

        # Fallback to generic if not enough questions generated or if specific domain templates are not found
        if len(generated_questions) < 5:
            generic_types = ["å¯¹æ¯”å‹", "æ¼”ç¤ºå‹", "å®šä¹‰å‹", "åŸå› å‹", "åº”ç”¨å‹"]
            for q_type_short in generic_types:
                q_type_full = f'{q_type_short}é—®é¢˜' # Reconstruct full type name to match main_prompt_config
                if len(generated_questions) >= 5:
                    break
                if q_type_full not in [q['type'] for q in generated_questions]:
                    template_key = f'{q_type_short.lower()}_templates'
                    templates = self.question_generator_config.get(template_key, {})
                    if templates:
                        template_str = list(templates.values())[0][0] # Get first template from first category
                        question_text = template_str.replace('{concept}', core_concept).replace('{concept1}', core_concept).replace('{concept2}', 'Y').replace('{action}', 'æ­¤æ¦‚å¿µ').replace('{feature}', core_concept).replace('{language/framework}', domain)
                        generated_questions.append({
                            'text': question_text,
                            'type': q_type_full,
                            'explanation': self.main_prompt_config['é—®é¢˜ç±»å‹å®šä¹‰'][q_type_full]['ç›®çš„'],
                        })
        
        return generated_questions[:5]

    def format_output(self, user_input, questions):
        output = f"åŸºäºä½ çš„å›°æƒ‘ï¼š\"{user_input}\"\n\næˆ‘ä¸ºä½ ç”Ÿæˆäº†ä»¥ä¸‹ç²¾å‡†é—®é¢˜ï¼Œè¯·é€‰æ‹©æœ€ç¬¦åˆä½ éœ€æ±‚çš„ï¼š\n\n"
        
        # Retrieve explanations from the parsed main_prompt_config
        type_explanations = {}
        for q_type_full, details in self.main_prompt_config['é—®é¢˜ç±»å‹å®šä¹‰'].items():
            # Remove the "é—®é¢˜" suffix for matching with q['type']
            short_type = q_type_full.replace("é—®é¢˜", "")
            type_explanations[short_type] = f"ğŸ’¡ {details['ç›®çš„']}"

        for i, q in enumerate(questions):
            # q['type'] will be something like "å¯¹æ¯”å‹é—®é¢˜", but our keys are "å¯¹æ¯”å‹"
            display_type = q['type'].replace("é—®é¢˜", "") # Get "å¯¹æ¯”å‹" from "å¯¹æ¯”å‹é—®é¢˜" for display
            output += f"{i+1}. ã€{display_type}ã€‘{q['text']}\n"
            output += f"   {type_explanations.get(display_type, '')}\n\n"
        output += "è¾“å…¥æ•°å­—é€‰æ‹©é—®é¢˜ï¼Œæˆ–è¾“å…¥0è‡ªå®šä¹‰ä½ çš„é—®é¢˜ã€‚"
        return output

# Example Usage
if __name__ == "__main__":
    sqos_system = SQOS(
        main_prompt_path='prompts/sqos-main-prompt.md',
        confusion_analyzer_path='prompts/modules/confusion-analyzer.md',
        question_generator_path='prompts/modules/question-generator.md',
        case_library_path='prompts/data/cs61b-case-library.yaml'
    )

    test_confusions = [
        "æˆ‘æä¸æ‡‚static",
        "main methodé‡Œé¢çš„String[] argsæ˜¯ä»€ä¹ˆ",
        "æˆ‘ä¸æ‡‚classå’Œç±»çš„difference",
        "é€’å½’æ€ä¹ˆç†è§£",
        "æ•°ç»„å’Œé“¾è¡¨æœ‰ä»€ä¹ˆåŒºåˆ«",
        "ä¸ºä»€ä¹ˆJavaè¦try-catchï¼Œç›´æ¥è®©ç¨‹åºå´©æºƒä¸è¡Œå—ï¼Ÿ"
    ]

    for confusion in test_confusions:
        print("\n" + "="*50)
        print(f"Processing confusion: \"{confusion}\"")
        analysis = sqos_system.analyze_confusion(confusion)
        questions = sqos_system.generate_questions(analysis)
        formatted_output = sqos_system.format_output(confusion, questions)
        print(formatted_output)
