#!/usr/bin/env python3
"""
UltraThink v3.0 æ‰¹å¤„ç†ç®¡ç†å™¨
æ”¯æŒå¤§è§„æ¨¡é—®é¢˜åˆ†æå’Œæ™ºèƒ½è°ƒåº¦
"""

import asyncio
import json
import os
import sys
import time
from datetime import datetime
from typing import List, Dict, Optional, Callable
import logging
from dataclasses import asdict
import pandas as pd
from tqdm import tqdm
import argparse

# å¯¼å…¥ä¸»ç³»ç»Ÿ
from ultrathink_v3 import UltraThinkV3, Question, AnalysisResult

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('ultrathink_batch.log'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger('UltraThink_BatchManager')


class UltraThinkBatchProcessor:
    """å¢å¼ºç‰ˆæ‰¹å¤„ç†å™¨"""
    
    def __init__(self, config_path: str = 'ultrathink_config.json'):
        """åˆå§‹åŒ–æ‰¹å¤„ç†å™¨"""
        self.ultrathink = UltraThinkV3(config_path)
        self.results_cache = {}
        self.statistics = {
            'total_processed': 0,
            'successful': 0,
            'failed': 0,
            'total_time': 0,
            'quality_distribution': {}
        }
    
    async def process_file(self, input_file: str, output_dir: str = None) -> Dict:
        """å¤„ç†è¾“å…¥æ–‡ä»¶ä¸­çš„é—®é¢˜"""
        logger.info(f"å¼€å§‹å¤„ç†æ–‡ä»¶: {input_file}")
        
        # è¯»å–é—®é¢˜åˆ—è¡¨
        questions = self._load_questions_from_file(input_file)
        logger.info(f"å·²åŠ è½½ {len(questions)} ä¸ªé—®é¢˜")
        
        # è®¾ç½®è¾“å‡ºç›®å½•
        if output_dir is None:
            output_dir = f"./batch_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        os.makedirs(output_dir, exist_ok=True)
        
        # æ‰¹é‡å¤„ç†
        start_time = time.time()
        results = await self._process_questions_with_progress(questions)
        total_time = time.time() - start_time
        
        # ç”ŸæˆæŠ¥å‘Š
        report = self._generate_batch_report(results, total_time)
        
        # ä¿å­˜ç»“æœ
        self._save_batch_results(results, report, output_dir)
        
        logger.info(f"æ‰¹å¤„ç†å®Œæˆï¼æ€»ç”¨æ—¶: {total_time:.2f}ç§’")
        return report
    
    def _load_questions_from_file(self, file_path: str) -> List[Question]:
        """ä»æ–‡ä»¶åŠ è½½é—®é¢˜"""
        questions = []
        
        if file_path.endswith('.json'):
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                for item in data:
                    if isinstance(item, dict):
                        questions.append(Question(
                            id=str(item.get('id', len(questions) + 1)),
                            content=item['content'],
                            category=item.get('category')
                        ))
                    else:
                        questions.append(Question(
                            id=str(len(questions) + 1),
                            content=str(item)
                        ))
        
        elif file_path.endswith('.txt'):
            with open(file_path, 'r', encoding='utf-8') as f:
                for i, line in enumerate(f):
                    line = line.strip()
                    if line:
                        questions.append(Question(
                            id=str(i + 1).zfill(3),
                            content=line
                        ))
        
        elif file_path.endswith('.csv'):
            df = pd.read_csv(file_path)
            for i, row in df.iterrows():
                questions.append(Question(
                    id=str(row.get('id', i + 1)),
                    content=row['content'],
                    category=row.get('category')
                ))
        
        return questions
    
    async def _process_questions_with_progress(self, questions: List[Question]) -> List[AnalysisResult]:
        """å¸¦è¿›åº¦æ¡çš„æ‰¹é‡å¤„ç†"""
        results = []
        
        # åˆ›å»ºè¿›åº¦æ¡
        with tqdm(total=len(questions), desc="åˆ†æè¿›åº¦", unit="é¢˜") as pbar:
            # åˆ†æ‰¹å¤„ç†
            batch_size = self.ultrathink.config['batch_settings']['batch_size']
            
            for i in range(0, len(questions), batch_size):
                batch = questions[i:i+batch_size]
                
                # å¼‚æ­¥å¤„ç†å½“å‰æ‰¹æ¬¡
                tasks = []
                for q in batch:
                    task = self._process_single_question_safe(q)
                    tasks.append(task)
                
                batch_results = await asyncio.gather(*tasks)
                
                # æ›´æ–°ç»“æœå’Œç»Ÿè®¡
                for result in batch_results:
                    if result:
                        results.append(result)
                        self.statistics['successful'] += 1
                        
                        # æ›´æ–°è´¨é‡åˆ†å¸ƒ
                        quality_level = self._get_quality_level(result.quality_score)
                        self.statistics['quality_distribution'][quality_level] = \
                            self.statistics['quality_distribution'].get(quality_level, 0) + 1
                    else:
                        self.statistics['failed'] += 1
                
                # æ›´æ–°è¿›åº¦æ¡
                pbar.update(len(batch))
                
                # æ˜¾ç¤ºå½“å‰ç»Ÿè®¡
                pbar.set_postfix({
                    'æˆåŠŸ': self.statistics['successful'],
                    'å¤±è´¥': self.statistics['failed'],
                    'å¹³å‡è´¨é‡': self._calculate_average_quality(results)
                })
        
        self.statistics['total_processed'] = len(questions)
        return results
    
    async def _process_single_question_safe(self, question: Question) -> Optional[AnalysisResult]:
        """å®‰å…¨åœ°å¤„ç†å•ä¸ªé—®é¢˜ï¼ˆå¸¦é”™è¯¯å¤„ç†ï¼‰"""
        try:
            # æ£€æŸ¥ç¼“å­˜
            cache_key = self._generate_cache_key(question)
            if cache_key in self.results_cache:
                logger.debug(f"ä»ç¼“å­˜è¿”å›é—®é¢˜ {question.id}")
                return self.results_cache[cache_key]
            
            # å¤„ç†é—®é¢˜
            result = await self.ultrathink.analyze_question(question)
            
            # ç¼“å­˜ç»“æœ
            self.results_cache[cache_key] = result
            
            return result
            
        except Exception as e:
            logger.error(f"å¤„ç†é—®é¢˜ {question.id} æ—¶å‡ºé”™: {str(e)}")
            return None
    
    def _generate_cache_key(self, question: Question) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        import hashlib
        content_hash = hashlib.md5(question.content.encode()).hexdigest()
        return f"{question.category}_{content_hash}"
    
    def _get_quality_level(self, score: float) -> str:
        """è·å–è´¨é‡çº§åˆ«"""
        if score >= 9:
            return "ä¸“å®¶çº§"
        elif score >= 7:
            return "æ·±åº¦"
        elif score >= 5:
            return "æ ‡å‡†"
        else:
            return "åŸºç¡€"
    
    def _calculate_average_quality(self, results: List[AnalysisResult]) -> float:
        """è®¡ç®—å¹³å‡è´¨é‡åˆ†æ•°"""
        if not results:
            return 0.0
        total_score = sum(r.quality_score for r in results)
        return round(total_score / len(results), 2)
    
    def _generate_batch_report(self, results: List[AnalysisResult], total_time: float) -> Dict:
        """ç”Ÿæˆæ‰¹å¤„ç†æŠ¥å‘Š"""
        report = {
            'summary': {
                'total_questions': self.statistics['total_processed'],
                'successful': self.statistics['successful'],
                'failed': self.statistics['failed'],
                'success_rate': f"{(self.statistics['successful'] / self.statistics['total_processed'] * 100):.1f}%",
                'total_time': f"{total_time:.2f}ç§’",
                'average_time_per_question': f"{total_time / self.statistics['total_processed']:.2f}ç§’"
            },
            'quality_analysis': {
                'average_score': self._calculate_average_quality(results),
                'distribution': self.statistics['quality_distribution'],
                'highest_score': max((r.quality_score for r in results), default=0),
                'lowest_score': min((r.quality_score for r in results), default=0)
            },
            'performance_metrics': {
                'questions_per_minute': round(self.statistics['total_processed'] / (total_time / 60), 2),
                'cache_hit_rate': f"{(len(self.results_cache) / self.statistics['total_processed'] * 100):.1f}%"
            },
            'timestamp': datetime.now().isoformat()
        }
        
        return report
    
    def _save_batch_results(self, results: List[AnalysisResult], report: Dict, output_dir: str):
        """ä¿å­˜æ‰¹å¤„ç†ç»“æœ"""
        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        report_path = os.path.join(output_dir, 'batch_report.json')
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜ç»“æœæ±‡æ€»
        summary_data = []
        for result in results:
            summary_data.append({
                'question_id': result.question_id,
                'quality_score': result.quality_score,
                'processing_time': result.processing_time,
                'save_path': result.save_path
            })
        
        summary_df = pd.DataFrame(summary_data)
        summary_df.to_csv(os.path.join(output_dir, 'results_summary.csv'), index=False)
        
        # ç”ŸæˆHTMLæŠ¥å‘Š
        self._generate_html_report(report, results, output_dir)
        
        logger.info(f"ç»“æœå·²ä¿å­˜åˆ°: {output_dir}")
    
    def _generate_html_report(self, report: Dict, results: List[AnalysisResult], output_dir: str):
        """ç”ŸæˆHTMLæ ¼å¼çš„æŠ¥å‘Š"""
        html_content = f"""
<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>UltraThink v3.0 æ‰¹å¤„ç†æŠ¥å‘Š</title>
    <style>
        body {{
            font-family: Arial, sans-serif;
            margin: 20px;
            background-color: #f5f5f5;
        }}
        .container {{
            max-width: 1200px;
            margin: 0 auto;
            background-color: white;
            padding: 20px;
            border-radius: 10px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }}
        h1, h2 {{
            color: #333;
        }}
        .summary-grid {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 20px;
            margin: 20px 0;
        }}
        .metric-card {{
            background-color: #f8f9fa;
            padding: 15px;
            border-radius: 8px;
            text-align: center;
        }}
        .metric-value {{
            font-size: 24px;
            font-weight: bold;
            color: #007bff;
        }}
        .metric-label {{
            font-size: 14px;
            color: #666;
            margin-top: 5px;
        }}
        table {{
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }}
        th, td {{
            padding: 10px;
            text-align: left;
            border-bottom: 1px solid #ddd;
        }}
        th {{
            background-color: #007bff;
            color: white;
        }}
        tr:hover {{
            background-color: #f5f5f5;
        }}
        .quality-badge {{
            padding: 3px 8px;
            border-radius: 12px;
            font-size: 12px;
            font-weight: bold;
        }}
        .quality-expert {{
            background-color: #28a745;
            color: white;
        }}
        .quality-deep {{
            background-color: #17a2b8;
            color: white;
        }}
        .quality-standard {{
            background-color: #ffc107;
            color: black;
        }}
        .quality-basic {{
            background-color: #6c757d;
            color: white;
        }}
    </style>
</head>
<body>
    <div class="container">
        <h1>ğŸš€ UltraThink v3.0 æ‰¹å¤„ç†æŠ¥å‘Š</h1>
        <p>ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
        
        <h2>ğŸ“Š æ€»ä½“ç»Ÿè®¡</h2>
        <div class="summary-grid">
            <div class="metric-card">
                <div class="metric-value">{report['summary']['total_questions']}</div>
                <div class="metric-label">æ€»é—®é¢˜æ•°</div>
            </div>
            <div class="metric-card">
                <div class="metric-value">{report['summary']['success_rate']}</div>
                <div class="metric-label">æˆåŠŸç‡</div>
            </div>
            <div class="metric-card">
                <div class="metric-value">{report['quality_analysis']['average_score']:.1f}</div>
                <div class="metric-label">å¹³å‡è´¨é‡åˆ†</div>
            </div>
            <div class="metric-card">
                <div class="metric-value">{report['performance_metrics']['questions_per_minute']}</div>
                <div class="metric-label">é—®é¢˜/åˆ†é’Ÿ</div>
            </div>
        </div>
        
        <h2>ğŸ“ˆ è´¨é‡åˆ†å¸ƒ</h2>
        <table>
            <tr>
                <th>è´¨é‡çº§åˆ«</th>
                <th>æ•°é‡</th>
                <th>å æ¯”</th>
            </tr>
"""
        
        # æ·»åŠ è´¨é‡åˆ†å¸ƒæ•°æ®
        total = report['summary']['total_questions']
        for level, count in report['quality_analysis']['distribution'].items():
            percentage = (count / total * 100) if total > 0 else 0
            html_content += f"""
            <tr>
                <td><span class="quality-badge quality-{level.lower()}">{level}</span></td>
                <td>{count}</td>
                <td>{percentage:.1f}%</td>
            </tr>
"""
        
        html_content += """
        </table>
        
        <h2>ğŸ“ å¤„ç†è¯¦æƒ…</h2>
        <table>
            <tr>
                <th>é—®é¢˜ID</th>
                <th>è´¨é‡åˆ†æ•°</th>
                <th>å¤„ç†æ—¶é—´</th>
                <th>è´¨é‡çº§åˆ«</th>
            </tr>
"""
        
        # æ·»åŠ å‰20ä¸ªç»“æœçš„è¯¦æƒ…
        for result in results[:20]:
            quality_level = self._get_quality_level(result.quality_score)
            html_content += f"""
            <tr>
                <td>{result.question_id}</td>
                <td>{result.quality_score:.1f}</td>
                <td>{result.processing_time:.2f}ç§’</td>
                <td><span class="quality-badge quality-{quality_level.lower()}">{quality_level}</span></td>
            </tr>
"""
        
        if len(results) > 20:
            html_content += f"""
            <tr>
                <td colspan="4" style="text-align: center; padding: 20px;">
                    ... è¿˜æœ‰ {len(results) - 20} ä¸ªç»“æœï¼Œè¯·æŸ¥çœ‹å®Œæ•´CSVæ–‡ä»¶
                </td>
            </tr>
"""
        
        html_content += """
        </table>
    </div>
</body>
</html>
"""
        
        # ä¿å­˜HTMLæ–‡ä»¶
        html_path = os.path.join(output_dir, 'batch_report.html')
        with open(html_path, 'w', encoding='utf-8') as f:
            f.write(html_content)


async def main():
    """ä¸»ç¨‹åºå…¥å£"""
    parser = argparse.ArgumentParser(description='UltraThink v3.0 æ‰¹å¤„ç†å·¥å…·')
    parser.add_argument('input_file', help='è¾“å…¥æ–‡ä»¶è·¯å¾„ (æ”¯æŒ.json, .txt, .csv)')
    parser.add_argument('-o', '--output', help='è¾“å‡ºç›®å½•è·¯å¾„', default=None)
    parser.add_argument('-c', '--config', help='é…ç½®æ–‡ä»¶è·¯å¾„', default='ultrathink_config.json')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not os.path.exists(args.input_file):
        logger.error(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {args.input_file}")
        return
    
    # åˆ›å»ºæ‰¹å¤„ç†å™¨
    processor = UltraThinkBatchProcessor(args.config)
    
    # å¤„ç†æ–‡ä»¶
    try:
        report = await processor.process_file(args.input_file, args.output)
        
        # æ‰“å°æ‘˜è¦
        print("\n" + "="*50)
        print("æ‰¹å¤„ç†å®Œæˆï¼")
        print(f"æ€»é—®é¢˜æ•°: {report['summary']['total_questions']}")
        print(f"æˆåŠŸç‡: {report['summary']['success_rate']}")
        print(f"å¹³å‡è´¨é‡åˆ†: {report['quality_analysis']['average_score']:.1f}")
        print(f"æ€»ç”¨æ—¶: {report['summary']['total_time']}")
        print("="*50)
        
    except Exception as e:
        logger.error(f"æ‰¹å¤„ç†å¤±è´¥: {str(e)}")
        raise


if __name__ == "__main__":
    asyncio.run(main())