"""
æ€§èƒ½ç›‘æ§ä»ªè¡¨æ¿ - Canvaså­¦ä¹ ç³»ç»Ÿ

æœ¬æ¨¡å—å®ç°é›†æˆçš„æ€§èƒ½ç›‘æ§ä»ªè¡¨æ¿ï¼Œæ•´åˆï¼š
- æ€§èƒ½ç›‘æ§
- åŠ¨æ€å®ä¾‹ç®¡ç†
- æ™ºèƒ½ç¼“å­˜
- é…ç½®ç®¡ç†
- åŸºå‡†æµ‹è¯•

Author: Canvas Learning System Team
Version: 1.0
Created: 2025-01-27
Story: 10.6 - Integration
"""

import asyncio
import json
import time
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, field
from pathlib import Path
import threading

# å¯¼å…¥æ€§èƒ½ç»„ä»¶
from performance_monitor import PerformanceMonitor
from dynamic_instance_manager import DynamicInstanceManager
from intelligent_cache_manager import IntelligentCacheManager, CacheEntryType
from configuration_manager import ConfigurationManager, PerformanceConfig
from performance_benchmark_system import PerformanceBenchmarkSystem


@dataclass
class DashboardMetrics:
    """ä»ªè¡¨æ¿æŒ‡æ ‡"""
    timestamp: datetime = field(default_factory=datetime.now)
    system_health: str = "good"  # good, warning, critical
    active_instances: int = 0
    queue_length: int = 0
    cpu_usage: float = 0.0
    memory_usage: float = 0.0
    cache_hit_rate: float = 0.0
    cache_size_mb: float = 0.0
    requests_per_second: float = 0.0
    average_response_time: float = 0.0
    error_rate: float = 0.0
    efficiency_ratio: float = 1.0
    alerts_count: int = 0

    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            "timestamp": self.timestamp.isoformat(),
            "system_health": self.system_health,
            "active_instances": self.active_instances,
            "queue_length": self.queue_length,
            "cpu_usage": self.cpu_usage,
            "memory_usage": self.memory_usage,
            "cache_hit_rate": self.cache_hit_rate,
            "cache_size_mb": self.cache_size_mb,
            "requests_per_second": self.requests_per_second,
            "average_response_time": self.average_response_time,
            "error_rate": self.error_rate,
            "efficiency_ratio": self.efficiency_ratio,
            "alerts_count": self.alerts_count
        }


@dataclass
class DashboardAlert:
    """ä»ªè¡¨æ¿å‘Šè­¦"""
    alert_id: str
    severity: str  # info, warning, critical
    category: str  # performance, resource, cache, system
    title: str
    message: str
    timestamp: datetime = field(default_factory=datetime.now)
    acknowledged: bool = False
    auto_resolved: bool = False

    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            "alert_id": self.alert_id,
            "severity": self.severity,
            "category": self.category,
            "title": self.title,
            "message": self.message,
            "timestamp": self.timestamp.isoformat(),
            "acknowledged": self.acknowledged,
            "auto_resolved": self.auto_resolved
        }


class PerformanceDashboard:
    """æ€§èƒ½ç›‘æ§ä»ªè¡¨æ¿

    é›†æˆæ‰€æœ‰æ€§èƒ½ç›‘æ§ç»„ä»¶ï¼Œæä¾›ç»Ÿä¸€çš„æ€§èƒ½è§†å›¾å’Œæ§åˆ¶æ¥å£ã€‚
    """

    def __init__(self, config_file: str = "config/performance_config.yaml"):
        """åˆå§‹åŒ–æ€§èƒ½ä»ªè¡¨æ¿

        Args:
            config_file: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.config_file = Path(config_file)
        self.config_dir = self.config_file.parent

        # åˆå§‹åŒ–é…ç½®ç®¡ç†å™¨
        self.config_manager = ConfigurationManager(str(self.config_dir))
        self.current_config: PerformanceConfig

        # æ€§èƒ½ç»„ä»¶
        self.performance_monitor: Optional[PerformanceMonitor] = None
        self.instance_manager: Optional[DynamicInstanceManager] = None
        self.cache_manager: Optional[IntelligentCacheManager] = None
        self.benchmark_system: Optional[PerformanceBenchmarkSystem] = None

        # æ¨¡æ‹Ÿå®ä¾‹æ± ï¼ˆç”¨äºæµ‹è¯•ï¼‰
        self.mock_instance_pool = None

        # ä»ªè¡¨æ¿çŠ¶æ€
        self.is_running = False
        self.metrics_history: List[DashboardMetrics] = []
        self.active_alerts: List[DashboardAlert] = []
        self.alert_history: List[DashboardAlert] = []

        # ç›‘æ§ä»»åŠ¡
        self.dashboard_task: Optional[asyncio.Task] = None
        self.metrics_update_interval = 5  # ç§’

        # å‘Šè­¦é˜ˆå€¼
        self.alert_thresholds = {
            "cpu_warning": 70,
            "cpu_critical": 90,
            "memory_warning": 75,
            "memory_critical": 90,
            "response_time_warning": 3000,  # ms
            "response_time_critical": 5000,  # ms
            "error_rate_warning": 0.05,  # 5%
            "error_rate_critical": 0.10,  # 10%
            "cache_hit_rate_warning": 0.40,  # 40%
            "efficiency_warning": 2.0,
            "efficiency_critical": 1.5
        }

    async def start(self) -> bool:
        """å¯åŠ¨æ€§èƒ½ä»ªè¡¨æ¿

        Returns:
            bool: æ˜¯å¦æˆåŠŸå¯åŠ¨
        """
        try:
            print("Starting Performance Dashboard...")

            # 1. åŠ è½½é…ç½®
            await self._load_configuration()

            # 2. åˆå§‹åŒ–æ€§èƒ½ç»„ä»¶
            await self._initialize_components()

            # 3. å¯åŠ¨æ‰€æœ‰ç»„ä»¶
            await self._start_components()

            # 4. å¯åŠ¨ä»ªè¡¨æ¿ç›‘æ§
            self.is_running = True
            self.dashboard_task = asyncio.create_task(self._dashboard_loop())

            print("âœ“ Performance Dashboard started successfully")
            return True

        except Exception as e:
            print(f"âœ— Failed to start Performance Dashboard: {e}")
            import traceback
            traceback.print_exc()
            return False

    async def stop(self) -> None:
        """åœæ­¢æ€§èƒ½ä»ªè¡¨æ¿"""
        print("Stopping Performance Dashboard...")

        self.is_running = False

        # åœæ­¢ä»ªè¡¨æ¿ä»»åŠ¡
        if self.dashboard_task:
            self.dashboard_task.cancel()
            try:
                await self.dashboard_task
            except asyncio.CancelledError:
                pass

        # åœæ­¢æ‰€æœ‰ç»„ä»¶
        await self._stop_components()

        print("âœ“ Performance Dashboard stopped")

    async def _load_configuration(self) -> None:
        """åŠ è½½é…ç½®"""
        # è·å–å½“å‰æœ‰æ•ˆé…ç½®
        self.current_config = await self.config_manager.get_current_config()

        # åˆ›å»ºé…ç½®ç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        self.config_dir.mkdir(parents=True, exist_ok=True)

    async def _initialize_components(self) -> None:
        """åˆå§‹åŒ–æ€§èƒ½ç»„ä»¶"""
        config_dict = self.current_config.to_dict()

        # 1. åˆå§‹åŒ–æ€§èƒ½ç›‘æ§å™¨
        self.performance_monitor = PerformanceMonitor({
            "enabled": self.current_config.monitoring_enabled,
            "collect_metrics": True,
            "log_performance_data": self.current_config.log_performance_metrics,
            "slow_execution_threshold_seconds": self.current_config.slow_execution_threshold_seconds,
            "memory_usage_alert_threshold_mb": self.current_config.memory_usage_alert_threshold_mb,
            "cpu_usage_alert_threshold_percent": self.current_config.cpu_usage_alert_threshold_percent,
            "metrics_collection_interval_seconds": self.current_config.metrics_collection_interval_seconds
        })

        # 2. åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨
        self.cache_manager = IntelligentCacheManager({
            "max_cache_size_mb": self.current_config.cache_max_size_mb,
            "max_entries": self.current_config.cache_max_entries,
            "default_ttl_seconds": self.current_config.cache_ttl_seconds,
            "enable_compression": self.current_config.cache_compression_enabled,
            "similarity_threshold": self.current_config.cache_similarity_threshold,
            "eviction_policy": self.current_config.cache_eviction_policy,
            "cleanup_interval_seconds": 60
        })

        # 3. åˆ›å»ºæ¨¡æ‹Ÿå®ä¾‹æ± ï¼ˆç”¨äºæ¼”ç¤ºï¼‰
        self.mock_instance_pool = MockInstancePool(self.current_config.max_concurrent_instances)

        # 4. åˆå§‹åŒ–åŠ¨æ€å®ä¾‹ç®¡ç†å™¨
        self.instance_manager = DynamicInstanceManager(
            instance_pool=self.mock_instance_pool,
            performance_monitor=self.performance_monitor,
            config={
                "min_instances": self.current_config.min_instances,
                "max_instances": self.current_config.max_concurrent_instances,
                "scale_up_threshold": self.current_config.scaling_threshold_cpu / 100,
                "scale_down_threshold": 0.3,
                "adjustment_cooldown_seconds": self.current_config.adjustment_cooldown_seconds,
                "adjustment_strategy": self.current_config.adjustment_strategy,
                "auto_adjustment_enabled": self.current_config.auto_scaling_enabled
            }
        )

        # 5. åˆå§‹åŒ–åŸºå‡†æµ‹è¯•ç³»ç»Ÿ
        self.benchmark_system = PerformanceBenchmarkSystem(
            performance_monitor=self.performance_monitor,
            cache_manager=self.cache_manager,
            config_manager=self.config_manager
        )

    async def _start_components(self) -> None:
        """å¯åŠ¨æ‰€æœ‰ç»„ä»¶"""
        # å¯åŠ¨æ€§èƒ½ç›‘æ§å™¨
        if self.current_config.monitoring_enabled:
            await self.performance_monitor.start_monitoring()

        # å¯åŠ¨ç¼“å­˜ç®¡ç†å™¨
        if self.current_config.cache_enabled:
            await self.cache_manager.start_monitoring()

        # å¯åŠ¨åŠ¨æ€å®ä¾‹ç®¡ç†å™¨
        if self.current_config.auto_scaling_enabled:
            await self.instance_manager.start_monitoring()

    async def _stop_components(self) -> None:
        """åœæ­¢æ‰€æœ‰ç»„ä»¶"""
        # åœæ­¢æ€§èƒ½ç›‘æ§å™¨
        if self.performance_monitor:
            await self.performance_monitor.stop_monitoring()

        # åœæ­¢ç¼“å­˜ç®¡ç†å™¨
        if self.cache_manager:
            await self.cache_manager.stop_monitoring()

        # åœæ­¢åŠ¨æ€å®ä¾‹ç®¡ç†å™¨
        if self.instance_manager:
            await self.instance_manager.stop_monitoring()

    async def _dashboard_loop(self) -> None:
        """ä»ªè¡¨æ¿ä¸»å¾ªç¯"""
        print("Dashboard monitoring loop started")

        while self.is_running:
            try:
                # æ”¶é›†æŒ‡æ ‡
                metrics = await self._collect_metrics()

                # æ›´æ–°å†å²è®°å½•
                self.metrics_history.append(metrics)
                if len(self.metrics_history) > 1000:  # ä¿ç•™æœ€è¿‘1000æ¡è®°å½•
                    self.metrics_history = self.metrics_history[-1000:]

                # æ£€æŸ¥å‘Šè­¦
                await self._check_alerts(metrics)

                # è¾“å‡ºä»ªè¡¨æ¿çŠ¶æ€
                if len(self.metrics_history) % 12 == 0:  # æ¯åˆ†é’Ÿè¾“å‡ºä¸€æ¬¡
                    self._print_dashboard_status(metrics)

                await asyncio.sleep(self.metrics_update_interval)

            except Exception as e:
                print(f"Dashboard loop error: {e}")
                await asyncio.sleep(5)

    async def _collect_metrics(self) -> DashboardMetrics:
        """æ”¶é›†æ€§èƒ½æŒ‡æ ‡"""
        metrics = DashboardMetrics()

        # ç³»ç»Ÿèµ„æºæŒ‡æ ‡
        if self.performance_monitor and self.performance_monitor.resource_metrics_history:
            latest_resource = self.performance_monitor.resource_metrics_history[-1]
            metrics.cpu_usage = latest_resource.cpu_percent
            metrics.memory_usage = latest_resource.memory_percent

        # å®ä¾‹æŒ‡æ ‡
        if self.mock_instance_pool:
            metrics.active_instances = self.mock_instance_pool.get_active_instance_count()
            metrics.queue_length = self.mock_instance_pool.get_queue_length()

        # ç¼“å­˜æŒ‡æ ‡
        if self.cache_manager:
            cache_stats = await self.cache_manager.get_cache_statistics()
            metrics.cache_hit_rate = cache_stats.hit_rate
            metrics.cache_size_mb = cache_stats.total_size_mb

        # æ€§èƒ½æŒ‡æ ‡
        if self.performance_monitor and self.performance_monitor.execution_metrics_history:
            recent_executions = self.performance_monitor.execution_metrics_history[-5:]
            if recent_executions:
                metrics.average_response_time = statistics.mean(
                    m.average_task_time_ms for m in recent_executions
                )
                metrics.requests_per_second = statistics.mean(
                    m.throughput_tasks_per_second for m in recent_executions
                )

                # è®¡ç®—é”™è¯¯ç‡
                total_tasks = sum(m.task_count for m in recent_executions)
                failed_tasks = sum(m.failed_tasks for m in recent_executions)
                metrics.error_rate = failed_tasks / total_tasks if total_tasks > 0 else 0

                # è®¡ç®—æ•ˆç‡æ¯”
                metrics.efficiency_ratio = statistics.mean(
                    m.parallel_efficiency for m in recent_executions
                )

        # å‘Šè­¦è®¡æ•°
        metrics.alerts_count = len([a for a in self.active_alerts if not a.acknowledged])

        # è¯„ä¼°ç³»ç»Ÿå¥åº·çŠ¶æ€
        metrics.system_health = self._evaluate_system_health(metrics)

        return metrics

    def _evaluate_system_health(self, metrics: DashboardMetrics) -> str:
        """è¯„ä¼°ç³»ç»Ÿå¥åº·çŠ¶æ€"""
        # æ£€æŸ¥å…³é”®æŒ‡æ ‡
        critical_issues = 0
        warning_issues = 0

        if metrics.cpu_usage > self.alert_thresholds["cpu_critical"]:
            critical_issues += 1
        elif metrics.cpu_usage > self.alert_thresholds["cpu_warning"]:
            warning_issues += 1

        if metrics.memory_usage > self.alert_thresholds["memory_critical"]:
            critical_issues += 1
        elif metrics.memory_usage > self.alert_thresholds["memory_warning"]:
            warning_issues += 1

        if metrics.error_rate > self.alert_thresholds["error_rate_critical"]:
            critical_issues += 1
        elif metrics.error_rate > self.alert_thresholds["error_rate_warning"]:
            warning_issues += 1

        if metrics.average_response_time > self.alert_thresholds["response_time_critical"]:
            critical_issues += 1
        elif metrics.average_response_time > self.alert_thresholds["response_time_warning"]:
            warning_issues += 1

        if metrics.efficiency_ratio < self.alert_thresholds["efficiency_critical"]:
            critical_issues += 1
        elif metrics.efficiency_ratio < self.alert_thresholds["efficiency_warning"]:
            warning_issues += 1

        # ç¡®å®šå¥åº·çŠ¶æ€
        if critical_issues > 0:
            return "critical"
        elif warning_issues > 2:
            return "warning"
        else:
            return "good"

    async def _check_alerts(self, metrics: DashboardMetrics) -> None:
        """æ£€æŸ¥å¹¶ç”Ÿæˆå‘Šè­¦"""
        new_alerts = []

        # CPUå‘Šè­¦
        if metrics.cpu_usage > self.alert_thresholds["cpu_critical"]:
            new_alerts.append(DashboardAlert(
                alert_id=f"cpu-critical-{int(time.time())}",
                severity="critical",
                category="resource",
                title="CPUä½¿ç”¨ç‡è¿‡é«˜",
                message=f"CPUä½¿ç”¨ç‡è¾¾åˆ°{metrics.cpu_usage:.1f}%ï¼Œè¶…è¿‡{self.alert_thresholds['cpu_critical']}%é˜ˆå€¼"
            ))
        elif metrics.cpu_usage > self.alert_thresholds["cpu_warning"]:
            new_alerts.append(DashboardAlert(
                alert_id=f"cpu-warning-{int(time.time())}",
                severity="warning",
                category="resource",
                title="CPUä½¿ç”¨ç‡åé«˜",
                message=f"CPUä½¿ç”¨ç‡è¾¾åˆ°{metrics.cpu_usage:.1f}%ï¼Œè¶…è¿‡{self.alert_thresholds['cpu_warning']}%é˜ˆå€¼"
            ))

        # å†…å­˜å‘Šè­¦
        if metrics.memory_usage > self.alert_thresholds["memory_critical"]:
            new_alerts.append(DashboardAlert(
                alert_id=f"memory-critical-{int(time.time())}",
                severity="critical",
                category="resource",
                title="å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜",
                message=f"å†…å­˜ä½¿ç”¨ç‡è¾¾åˆ°{metrics.memory_usage:.1f}%ï¼Œè¶…è¿‡{self.alert_thresholds['memory_critical']}%é˜ˆå€¼"
            ))

        # å“åº”æ—¶é—´å‘Šè­¦
        if metrics.average_response_time > self.alert_thresholds["response_time_critical"]:
            new_alerts.append(DashboardAlert(
                alert_id=f"response-critical-{int(time.time())}",
                severity="critical",
                category="performance",
                title="å“åº”æ—¶é—´è¿‡é•¿",
                message=f"å¹³å‡å“åº”æ—¶é—´è¾¾åˆ°{metrics.average_response_time:.0f}msï¼Œè¶…è¿‡{self.alert_thresholds['response_time_critical']}msé˜ˆå€¼"
            ))

        # æ•ˆç‡æ¯”å‘Šè­¦
        if metrics.efficiency_ratio < self.alert_thresholds["efficiency_critical"]:
            new_alerts.append(DashboardAlert(
                alert_id=f"efficiency-critical-{int(time.time())}",
                severity="critical",
                category="performance",
                title="å¹¶è¡Œæ•ˆç‡è¿‡ä½",
                message=f"å¹¶è¡Œæ•ˆç‡ä»…ä¸º{metrics.efficiency_ratio:.1f}xï¼Œä½äº{self.alert_thresholds['efficiency_critical']}xé˜ˆå€¼"
            ))

        # æ›´æ–°å‘Šè­¦åˆ—è¡¨
        for alert in new_alerts:
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸ä¼¼å‘Šè­¦
            existing = self._find_similar_alert(alert)
            if not existing:
                self.active_alerts.append(alert)
                self.alert_history.append(alert)
                print(f"ğŸš¨ ALERT: {alert.title} - {alert.message}")

    def _find_similar_alert(self, new_alert: DashboardAlert) -> Optional[DashboardAlert]:
        """æŸ¥æ‰¾ç›¸ä¼¼çš„æ´»è·ƒå‘Šè­¦"""
        for alert in self.active_alerts:
            if (alert.category == new_alert.category and
                alert.title == new_alert.title and
                not alert.acknowledged):
                return alert
        return None

    def _print_dashboard_status(self, metrics: DashboardMetrics) -> None:
        """æ‰“å°ä»ªè¡¨æ¿çŠ¶æ€"""
        # æ¸…å±
        os.system('cls' if os.name == 'nt' else 'clear')

        # æ‰“å°çŠ¶æ€
        print("=" * 80)
        print("ğŸ¯ Canvas Learning System - Performance Dashboard")
        print("=" * 80)
        print(f"ğŸ“… Time: {metrics.timestamp.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"ğŸ¥ System Health: {metrics.system_health.upper()}")
        print("-" * 80)

        # èµ„æºä½¿ç”¨
        print(f"\nğŸ“Š Resource Usage:")
        print(f"  CPU: {metrics.cpu_usage:.1f}% {'â–“' * int(metrics.cpu_usage/5)}{'â–‘' * (20-int(metrics.cpu_usage/5))}")
        print(f"  Memory: {metrics.memory_usage:.1f}% {'â–“' * int(metrics.memory_usage/5)}{'â–‘' * (20-int(metrics.memory_usage/5))}")

        # å®ä¾‹çŠ¶æ€
        print(f"\nğŸ”„ Instance Status:")
        print(f"  Active Instances: {metrics.active_instances}/{self.current_config.max_concurrent_instances}")
        print(f"  Queue Length: {metrics.queue_length}")
        print(f"  Efficiency Ratio: {metrics.efficiency_ratio:.1f}x {'âœ“' if metrics.efficiency_ratio >= 3 else 'âœ—'}")

        # ç¼“å­˜çŠ¶æ€
        print(f"\nğŸ’¾ Cache Status:")
        print(f"  Hit Rate: {metrics.cache_hit_rate:.1%} {'âœ“' if metrics.cache_hit_rate > 0.6 else 'âœ—'}")
        print(f"  Size: {metrics.cache_size_mb:.1f} MB")

        # æ€§èƒ½æŒ‡æ ‡
        print(f"\nâš¡ Performance:")
        print(f"  Requests/sec: {metrics.requests_per_second:.1f}")
        print(f"  Avg Response: {metrics.average_response_time:.0f}ms {'âœ“' if metrics.average_response_time < 2000 else 'âœ—'}")
        print(f"  Error Rate: {metrics.error_rate:.1%}")

        # æ´»è·ƒå‘Šè­¦
        if metrics.alerts_count > 0:
            print(f"\nğŸš¨ Active Alerts ({metrics.alerts_count}):")
            for alert in self.active_alerts[-5:]:  # æ˜¾ç¤ºæœ€è¿‘5ä¸ª
                if not alert.acknowledged:
                    print(f"  â€¢ {alert.title}: {alert.message}")

        print("\n" + "=" * 80)
        print("Commands: 'run_benchmark', 'config', 'alerts', 'help', 'quit'")
        print("=" * 80)

    async def run_benchmark(self) -> Dict[str, Any]:
        """è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•"""
        if not self.benchmark_system:
            return {"error": "Benchmark system not initialized"}

        print("\nğŸ Running performance benchmark...")
        try:
            report = await self.benchmark_system.run_comprehensive_benchmark()
            return {
                "success": True,
                "report_id": report.report_id,
                "summary": report.summary,
                "recommendations": report.recommendations
            }
        except Exception as e:
            return {"error": str(e)}

    async def update_configuration(self, changes: Dict[str, Any]) -> bool:
        """æ›´æ–°é…ç½®"""
        try:
            success = await self.config_manager.apply_config_changes(changes)
            if success:
                # é‡æ–°åŠ è½½é…ç½®
                await self._load_configuration()
                # é‡å¯ç»„ä»¶ä»¥åº”ç”¨æ–°é…ç½®
                await self._restart_components()
                print("âœ“ Configuration updated successfully")
            return success
        except Exception as e:
            print(f"âœ— Failed to update configuration: {e}")
            return False

    async def _restart_components(self) -> None:
        """é‡å¯ç»„ä»¶ä»¥åº”ç”¨æ–°é…ç½®"""
        await self._stop_components()
        await self._initialize_components()
        await self._start_components()

    async def acknowledge_alert(self, alert_id: str) -> bool:
        """ç¡®è®¤å‘Šè­¦"""
        for alert in self.active_alerts:
            if alert.alert_id == alert_id:
                alert.acknowledged = True
                return True
        return False

    def get_metrics_summary(self, hours: int = 1) -> Dict[str, Any]:
        """è·å–æŒ‡æ ‡æ‘˜è¦"""
        cutoff_time = datetime.now() - timedelta(hours=hours)
        recent_metrics = [
            m for m in self.metrics_history
            if m.timestamp >= cutoff_time
        ]

        if not recent_metrics:
            return {"error": "No metrics available"}

        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        import statistics
        avg_cpu = statistics.mean(m.cpu_usage for m in recent_metrics)
        avg_memory = statistics.mean(m.memory_usage for m in recent_metrics)
        avg_response = statistics.mean(m.average_response_time for m in recent_metrics)
        avg_efficiency = statistics.mean(m.efficiency_ratio for m in recent_metrics)

        return {
            "time_range_hours": hours,
            "sample_count": len(recent_metrics),
            "averages": {
                "cpu_usage": avg_cpu,
                "memory_usage": avg_memory,
                "response_time_ms": avg_response,
                "efficiency_ratio": avg_efficiency
            },
            "peaks": {
                "cpu_usage": max(m.cpu_usage for m in recent_metrics),
                "memory_usage": max(m.memory_usage for m in recent_metrics),
                "response_time_ms": max(m.average_response_time for m in recent_metrics)
            },
            "system_health": recent_metrics[-1].system_health if recent_metrics else "unknown"
        }

    def get_active_alerts(self) -> List[DashboardAlert]:
        """è·å–æ´»è·ƒå‘Šè­¦"""
        return [a for a in self.active_alerts if not a.acknowledged]

    async def export_dashboard_data(self, file_path: str) -> bool:
        """å¯¼å‡ºä»ªè¡¨æ¿æ•°æ®"""
        try:
            export_data = {
                "export_timestamp": datetime.now().isoformat(),
                "current_config": self.current_config.to_dict(),
                "metrics_history": [m.to_dict() for m in self.metrics_history[-100:]],
                "active_alerts": [a.to_dict() for a in self.get_active_alerts()],
                "alert_history": [a.to_dict() for a in self.alert_history[-50:]]
            }

            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(export_data, f, ensure_ascii=False, indent=2)

            return True
        except Exception as e:
            print(f"Failed to export dashboard data: {e}")
            return False


class MockInstancePool:
    """æ¨¡æ‹Ÿå®ä¾‹æ± ï¼ˆç”¨äºæ¼”ç¤ºï¼‰"""

    def __init__(self, max_instances: int = 6):
        self.max_instances = max_instances
        self.active_instances = max_instances // 2
        self.queue_length = 0
        self.last_update = time.time()

    def get_active_instance_count(self) -> int:
        """è·å–æ´»è·ƒå®ä¾‹æ•°"""
        # æ¨¡æ‹ŸåŠ¨æ€å˜åŒ–
        if time.time() - self.last_update > 10:
            import random
            self.active_instances = max(1, min(self.max_instances,
                self.active_instances + random.randint(-2, 2)))
            self.last_update = time.time()
        return self.active_instances

    def get_queue_length(self) -> int:
        """è·å–é˜Ÿåˆ—é•¿åº¦"""
        import random
        if random.random() < 0.1:  # 10%æ¦‚ç‡å˜åŒ–
            self.queue_length = max(0, self.queue_length + random.randint(-3, 3))
        return self.queue_length

    async def set_max_concurrent_instances(self, max_instances: int) -> None:
        """è®¾ç½®æœ€å¤§å¹¶å‘å®ä¾‹æ•°"""
        self.max_instances = max_instances
        print(f"[Mock] Max concurrent instances set to: {max_instances}")


# ä¸»ç¨‹åºå…¥å£
async def main():
    """ä¸»ç¨‹åº"""
    import os

    dashboard = PerformanceDashboard()

    # å¯åŠ¨ä»ªè¡¨æ¿
    if await dashboard.start():
        print("\nDashboard is running. Press Ctrl+C to stop.")

        try:
            # ç®€å•çš„äº¤äº’ç•Œé¢
            while True:
                await asyncio.sleep(1)
        except KeyboardInterrupt:
            print("\nShutting down...")
            await dashboard.stop()
    else:
        print("Failed to start dashboard")


if __name__ == "__main__":
    asyncio.run(main())