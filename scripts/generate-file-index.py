#!/usr/bin/env python3
"""
Project File Index Generator

生成项目文件索引，防止AI编造不存在的文件路径。
包含源文件、API端点、数据模型的完整列表。

Usage:
    python scripts/generate-file-index.py
    python scripts/generate-file-index.py --output custom-path.md
"""

import os
import json
import yaml
import argparse
from pathlib import Path
from datetime import datetime
from collections import defaultdict

# 项目根目录
PROJECT_ROOT = Path(__file__).parent.parent

# 配置
CONFIG = {
    "source_dirs": ["src", "scripts", "tests"],
    "source_extensions": [".py", ".ts", ".js", ".tsx", ".jsx"],
    "ignore_patterns": ["__pycache__", ".git", "node_modules", ".venv", "venv", "dist", "build"],
    "openapi_path": "specs/api/canvas-api.openapi.yml",
    "schema_dir": "specs/data",
}


def scan_source_files():
    """扫描所有源文件"""
    files_by_dir = defaultdict(list)

    for source_dir in CONFIG["source_dirs"]:
        dir_path = PROJECT_ROOT / source_dir
        if not dir_path.exists():
            continue

        for root, dirs, files in os.walk(dir_path):
            # 过滤忽略的目录
            dirs[:] = [d for d in dirs if d not in CONFIG["ignore_patterns"]]

            rel_root = Path(root).relative_to(PROJECT_ROOT)

            for file in files:
                if any(file.endswith(ext) for ext in CONFIG["source_extensions"]):
                    file_path = rel_root / file
                    files_by_dir[str(rel_root)].append(str(file_path))

    return files_by_dir


def extract_openapi_endpoints():
    """从OpenAPI spec提取所有端点"""
    spec_path = PROJECT_ROOT / CONFIG["openapi_path"]
    if not spec_path.exists():
        return []

    with open(spec_path, 'r', encoding='utf-8') as f:
        spec = yaml.safe_load(f)

    endpoints = []
    paths = spec.get("paths", {})

    for path, methods in paths.items():
        for method, operation in methods.items():
            if method in ["get", "post", "put", "delete", "patch"]:
                operation_id = operation.get("operationId", "unknown")
                summary = operation.get("summary", "")
                tags = operation.get("tags", [])

                endpoints.append({
                    "method": method.upper(),
                    "path": path,
                    "operation_id": operation_id,
                    "summary": summary,
                    "tags": tags
                })

    return endpoints


def extract_json_schemas():
    """从JSON Schema目录提取所有数据模型"""
    schema_dir = PROJECT_ROOT / CONFIG["schema_dir"]
    if not schema_dir.exists():
        return []

    schemas = []

    for schema_file in schema_dir.glob("*.json"):
        with open(schema_file, 'r', encoding='utf-8') as f:
            schema = json.load(f)

        title = schema.get("title", schema_file.stem)
        description = schema.get("description", "")

        schemas.append({
            "name": title,
            "file": schema_file.name,
            "description": description[:100] + "..." if len(description) > 100 else description
        })

    return schemas


def generate_markdown(files_by_dir, endpoints, schemas):
    """生成Markdown格式的索引"""
    lines = []

    # Header
    lines.append("# Project File Index (Auto-generated)")
    lines.append("")
    lines.append(f"**Generated**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    lines.append("")
    lines.append("> This file is auto-generated by `scripts/generate-file-index.py`")
    lines.append("> Used to prevent AI hallucination of non-existent files/endpoints")
    lines.append("")
    lines.append("---")
    lines.append("")

    # Source Files
    total_files = sum(len(files) for files in files_by_dir.values())
    lines.append(f"## Source Files ({total_files} files)")
    lines.append("")

    for dir_name in sorted(files_by_dir.keys()):
        files = files_by_dir[dir_name]
        lines.append(f"### {dir_name}/")
        for file_path in sorted(files):
            lines.append(f"- `{file_path}` ✓")
        lines.append("")

    # API Endpoints
    if endpoints:
        lines.append("---")
        lines.append("")
        lines.append(f"## API Endpoints ({len(endpoints)} endpoints)")
        lines.append("")
        lines.append("From: `specs/api/canvas-api.openapi.yml`")
        lines.append("")

        # Group by tags
        endpoints_by_tag = defaultdict(list)
        for ep in endpoints:
            tag = ep["tags"][0] if ep["tags"] else "Untagged"
            endpoints_by_tag[tag].append(ep)

        for tag in sorted(endpoints_by_tag.keys()):
            lines.append(f"### {tag}")
            for ep in endpoints_by_tag[tag]:
                lines.append(f"- `{ep['method']} {ep['path']}` - {ep['operation_id']}")
                if ep['summary']:
                    lines.append(f"  - {ep['summary']}")
            lines.append("")

    # Data Models
    if schemas:
        lines.append("---")
        lines.append("")
        lines.append(f"## Data Models ({len(schemas)} schemas)")
        lines.append("")
        lines.append("From: `specs/data/*.json`")
        lines.append("")

        for schema in sorted(schemas, key=lambda x: x["name"]):
            lines.append(f"### {schema['name']}")
            lines.append(f"- File: `{schema['file']}`")
            if schema['description']:
                lines.append(f"- {schema['description']}")
            lines.append("")

    # Anti-Hallucination Note
    lines.append("---")
    lines.append("")
    lines.append("## Anti-Hallucination Protocol")
    lines.append("")
    lines.append("When referencing files, endpoints, or models:")
    lines.append("1. **Verify existence** in this index before referencing")
    lines.append("2. If not found here, use `Glob` or `Grep` to confirm")
    lines.append("3. If still not found, **explicitly state uncertainty**")
    lines.append("")
    lines.append("**DO NOT** invent file paths, API endpoints, or data models not listed here.")
    lines.append("")

    return "\n".join(lines)


def main():
    parser = argparse.ArgumentParser(description="Generate project file index")
    parser.add_argument("--output", "-o", default="project-file-index.md",
                       help="Output file path (default: project-file-index.md)")
    args = parser.parse_args()

    print("[*] Scanning project files...")
    files_by_dir = scan_source_files()

    print("[*] Extracting OpenAPI endpoints...")
    endpoints = extract_openapi_endpoints()

    print("[*] Extracting JSON schemas...")
    schemas = extract_json_schemas()

    print("[*] Generating markdown...")
    markdown = generate_markdown(files_by_dir, endpoints, schemas)

    output_path = PROJECT_ROOT / args.output
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(markdown)

    # Summary
    total_files = sum(len(files) for files in files_by_dir.values())
    print(f"\n[OK] Generated {output_path}")
    print(f"   - Source files: {total_files}")
    print(f"   - API endpoints: {len(endpoints)}")
    print(f"   - Data models: {len(schemas)}")


if __name__ == "__main__":
    main()
