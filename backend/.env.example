# Canvas Learning System - Environment Variables
# ✅ Verified from Context7:/websites/fastapi_tiangolo (topic: settings .env file)
#
# Copy this file to .env and modify the values as needed.
# cp .env.example .env
#
# [Source: docs/architecture/EPIC-11-BACKEND-ARCHITECTURE.md#配置管理]

# ═══════════════════════════════════════════════════════════════════════════════
# Application Settings
# ═══════════════════════════════════════════════════════════════════════════════

# Application name displayed in API responses and Swagger UI
PROJECT_NAME="Canvas Learning System API"

# Application description for API documentation
PROJECT_DESCRIPTION="Multi-agent learning system backend using Feynman method"

# Application version (semver format)
VERSION="1.0.0"

# Debug mode (enables Swagger/ReDoc docs, detailed error messages)
# Set to False in production
DEBUG=True

# Logging level: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL="INFO"

# ═══════════════════════════════════════════════════════════════════════════════
# CORS Settings
# ═══════════════════════════════════════════════════════════════════════════════

# Allowed origins for CORS (comma-separated list)
# Example: "http://localhost:3000,http://127.0.0.1:3000,app://obsidian.md"
# Use "*" to allow all origins (not recommended for production)
CORS_ORIGINS="http://localhost:3000,http://127.0.0.1:3000,app://obsidian.md"

# ═══════════════════════════════════════════════════════════════════════════════
# Canvas Settings
# ═══════════════════════════════════════════════════════════════════════════════

# Base path to Canvas files directory
# This should point to your Obsidian vault or notes directory containing .canvas files
CANVAS_BASE_PATH="../笔记库"

# ═══════════════════════════════════════════════════════════════════════════════
# API Settings
# ═══════════════════════════════════════════════════════════════════════════════

# API version prefix
API_V1_PREFIX="/api/v1"

# Maximum concurrent requests (rate limiting)
MAX_CONCURRENT_REQUESTS=100

# ═══════════════════════════════════════════════════════════════════════════════
# AI Provider Configuration (Required for Agent functionality)
# [Source: docs/prd/EPIC-20-BACKEND-STABILITY.md#Multi-Provider-Support]
# ═══════════════════════════════════════════════════════════════════════════════
#
# Supported providers:
#   - google: Google Gemini (gemini-2.0-flash-exp, gemini-1.5-pro)
#   - openai: OpenAI (gpt-4o, gpt-4-turbo)
#   - anthropic: Anthropic Claude (claude-3-5-sonnet-20241022)
#   - custom: OpenAI-compatible API (e.g., OpenRouter, local models)
#
# ═══════════════════════════════════════════════════════════════════════════════

# Primary AI Provider: google, openai, anthropic, or custom
AI_PROVIDER=google

# API Base URL (required for custom provider, optional for others)
# Examples:
#   - OpenRouter: https://openrouter.ai/api/v1
#   - Local LLM: http://localhost:8080/v1
# AI_BASE_URL=

# API Key (required) - Get from your provider's dashboard
# ⚠️ NEVER commit .env with a real API key!
AI_API_KEY=your-api-key-here

# Model name (depends on provider)
AI_MODEL_NAME=gemini-2.0-flash-exp

# Request timeout in seconds (default: 120)
AI_TIMEOUT=120

# ═══════════════════════════════════════════════════════════════════════════════
# Multi-Provider Failover (Optional, for high availability)
# ═══════════════════════════════════════════════════════════════════════════════
#
# AI_PROVIDER_1_NAME=google
# AI_PROVIDER_1_API_KEY=your-google-api-key
# AI_PROVIDER_1_MODEL=gemini-2.0-flash-exp
# AI_PROVIDER_1_PRIORITY=1
# AI_PROVIDER_1_ENABLED=true
#
# AI_PROVIDER_2_NAME=openai
# AI_PROVIDER_2_API_KEY=your-openai-api-key
# AI_PROVIDER_2_MODEL=gpt-4o
# AI_PROVIDER_2_PRIORITY=2
# AI_PROVIDER_2_ENABLED=true
