# Canvas Learning System - Environment Variables
# ✅ Verified from Context7:/websites/fastapi_tiangolo (topic: settings .env file)
#
# Copy this file to .env and modify the values as needed.
# cp .env.example .env
#
# [Source: docs/architecture/EPIC-11-BACKEND-ARCHITECTURE.md#配置管理]

# ═══════════════════════════════════════════════════════════════════════════════
# Application Settings
# ═══════════════════════════════════════════════════════════════════════════════

# Application name displayed in API responses and Swagger UI
PROJECT_NAME="Canvas Learning System API"

# Application description for API documentation
PROJECT_DESCRIPTION="Multi-agent learning system backend using Feynman method"

# Application version (semver format)
VERSION="1.0.0"

# Debug mode (enables Swagger/ReDoc docs, detailed error messages)
# Set to False in production
DEBUG=True

# Logging level: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL="INFO"

# Enable detailed Agent API response logging for debugging (Story 12.G.1)
# When true, logs detailed response content from Gemini API and extractor attempts
# Useful for diagnosing "无法从AI响应中提取有效内容" errors
# Default: false (disabled to avoid performance impact)
DEBUG_AGENT_RESPONSE=false

# ═══════════════════════════════════════════════════════════════════════════════
# CORS Settings
# ═══════════════════════════════════════════════════════════════════════════════

# Allowed origins for CORS (comma-separated list)
# Example: "http://localhost:3000,http://127.0.0.1:3000,app://obsidian.md"
# Use "*" to allow all origins (not recommended for production)
CORS_ORIGINS="http://localhost:3000,http://127.0.0.1:3000,app://obsidian.md"

# ═══════════════════════════════════════════════════════════════════════════════
# Canvas Settings
# ═══════════════════════════════════════════════════════════════════════════════

# Base path to Canvas files directory
# This should point to your Obsidian vault or notes directory containing .canvas files
CANVAS_BASE_PATH="../笔记库"

# ═══════════════════════════════════════════════════════════════════════════════
# API Settings
# ═══════════════════════════════════════════════════════════════════════════════

# API version prefix
API_V1_PREFIX="/api/v1"

# Maximum concurrent requests (rate limiting)
MAX_CONCURRENT_REQUESTS=100

# ═══════════════════════════════════════════════════════════════════════════════
# AI Provider Configuration (Required for Agent functionality)
# [Source: docs/prd/EPIC-20-BACKEND-STABILITY.md#Multi-Provider-Support]
# ═══════════════════════════════════════════════════════════════════════════════
#
# Supported providers:
#   - google: Google Gemini (gemini-2.0-flash-exp, gemini-1.5-pro)
#   - openai: OpenAI (gpt-4o, gpt-4-turbo)
#   - anthropic: Anthropic Claude (claude-3-5-sonnet-20241022)
#   - custom: OpenAI-compatible API (e.g., OpenRouter, local models)
#
# ═══════════════════════════════════════════════════════════════════════════════

# Primary AI Provider: google, openai, anthropic, or custom
AI_PROVIDER=google

# API Base URL (required for custom provider, optional for others)
# Examples:
#   - OpenRouter: https://openrouter.ai/api/v1
#   - Local LLM: http://localhost:8080/v1
# AI_BASE_URL=

# API Key (required) - Get from your provider's dashboard
# ⚠️ NEVER commit .env with a real API key!
AI_API_KEY=your-api-key-here

# Model name (depends on provider)
AI_MODEL_NAME=gemini-2.0-flash-exp

# Request timeout in seconds (default: 120)
AI_TIMEOUT=120

# ═══════════════════════════════════════════════════════════════════════════════
# Multi-Provider Failover (Optional, for high availability)
# ═══════════════════════════════════════════════════════════════════════════════
#
# AI_PROVIDER_1_NAME=google
# AI_PROVIDER_1_API_KEY=your-google-api-key
# AI_PROVIDER_1_MODEL=gemini-2.0-flash-exp
# AI_PROVIDER_1_PRIORITY=1
# AI_PROVIDER_1_ENABLED=true
#
# AI_PROVIDER_2_NAME=openai
# AI_PROVIDER_2_API_KEY=your-openai-api-key
# AI_PROVIDER_2_MODEL=gpt-4o
# AI_PROVIDER_2_PRIORITY=2
# AI_PROVIDER_2_ENABLED=true

# ═══════════════════════════════════════════════════════════════════════════════
# Neo4j Configuration (Story 30.1)
# [Source: docs/stories/30.1.story.md - AC 2]
# ═══════════════════════════════════════════════════════════════════════════════
#
# Neo4j is used as the graph database backend for the 3-layer memory system.
# Start the Neo4j container with: docker-compose up -d neo4j
#
# ═══════════════════════════════════════════════════════════════════════════════

# Enable/disable Neo4j connection (set to false to use JSON fallback)
NEO4J_ENABLED=true

# Neo4j Bolt connection URI
# Default: bolt://localhost:7688 (local Docker container)
# NOTE: Using 7688 to avoid conflict with spring-2026-courses (7687)
NEO4J_URI=bolt://localhost:7688

# Neo4j authentication username
NEO4J_USER=neo4j

# Neo4j authentication password
# ⚠️ CHANGE THIS! Use a secure password in production.
# This must match the NEO4J_PASSWORD in docker-compose.yml
NEO4J_PASSWORD=your_secure_password_here

# Neo4j database name (default database for Neo4j Community Edition)
NEO4J_DATABASE=neo4j

# ═══════════════════════════════════════════════════════════════════════════════
# Neo4j Connection Pool Configuration (Story 30.2)
# [Source: docs/stories/30.2.story.md - AC 2]
# ═══════════════════════════════════════════════════════════════════════════════

# Maximum connections in Neo4j connection pool
NEO4J_MAX_CONNECTION_POOL_SIZE=50

# Connection acquisition timeout in seconds
NEO4J_CONNECTION_TIMEOUT=30

# Maximum connection lifetime in seconds (connections are recycled after this)
NEO4J_MAX_CONNECTION_LIFETIME=3600

# ═══════════════════════════════════════════════════════════════════════════════
# Neo4j Retry Configuration (Story 30.2)
# [Source: docs/stories/30.2.story.md - AC 5]
# ═══════════════════════════════════════════════════════════════════════════════

# Number of retry attempts for Neo4j operations
NEO4J_RETRY_ATTEMPTS=3

# Base delay for exponential backoff (1s, 2s, 4s)
NEO4J_RETRY_DELAY_BASE=1.0

# Maximum retry delay in seconds
NEO4J_RETRY_MAX_DELAY=10.0

# ═══════════════════════════════════════════════════════════════════════════════
# Graphiti JSON Dual-Write Configuration (Story 36.9)
# [Source: docs/stories/36.9.story.md - AC-36.9.5]
# ═══════════════════════════════════════════════════════════════════════════════
#
# Enable dual-write of learning events to both Neo4j AND Graphiti JSON storage.
# When enabled, learning events are stored to:
#   1. Neo4j (primary, synchronous)
#   2. Graphiti JSON file (secondary, fire-and-forget, 500ms timeout)
#
# This provides redundancy and enables MCP-compatible tooling access to learning history.
# The JSON file is stored at: backend/data/learning_memories.json
#
# ═══════════════════════════════════════════════════════════════════════════════

# Enable/disable Graphiti JSON dual-write (default: false)
# Set to true to enable dual-write for redundancy
ENABLE_GRAPHITI_JSON_DUAL_WRITE=false
