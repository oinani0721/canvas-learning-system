# ‚úÖ Verified structure from docs/architecture/EPIC-11-BACKEND-ARCHITECTURE.md#Layer-2-ÊúçÂä°Â±Ç
# ‚úÖ Epic 20 Story 20.4: AgentServiceÈáçÂÜô - ‰ΩøÁî®ÁúüÂÆûGemini APIË∞ÉÁî®
# ‚úÖ Verified from Context7:/googleapis/python-genai (topic: async generate content)
"""
Agent Service - Business logic for Agent operations.

This service provides async methods for Agent calls,
wrapping the Gemini API functionality with async support.

[Source: docs/architecture/EPIC-11-BACKEND-ARCHITECTURE.md#Layer-2-ÊúçÂä°Â±Ç]
[Source: docs/prd/sprint-change-proposal-20251208.md - Story 20.4]
[Source: Gemini Migration - Using google.genai instead of anthropic]
"""
import asyncio
import json
import logging
import os
import re
import time
import uuid
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
from typing import TYPE_CHECKING, Any, Dict, List, Optional, Tuple

if TYPE_CHECKING:
    from app.clients.gemini_client import GeminiClient
    from app.clients.graphiti_client import LearningMemoryClient

logger = logging.getLogger(__name__)


class AgentType(str, Enum):
    """
    AgentÁ±ªÂûãÊûö‰∏æ
    [Source: helpers.md#Section-1-14-agentsËØ¶ÁªÜËØ¥Êòé]
    """
    BASIC_DECOMPOSITION = "basic-decomposition"
    DEEP_DECOMPOSITION = "deep-decomposition"
    QUESTION_DECOMPOSITION = "question-decomposition"
    ORAL_EXPLANATION = "oral-explanation"
    FOUR_LEVEL_EXPLANATION = "four-level-explanation"
    FOUR_LEVEL = "four-level"  # Alias for FOUR_LEVEL_EXPLANATION
    CLARIFICATION_PATH = "clarification-path"
    COMPARISON_TABLE = "comparison-table"
    EXAMPLE_TEACHING = "example-teaching"
    MEMORY_ANCHOR = "memory-anchor"
    SCORING_AGENT = "scoring-agent"
    SCORING = "scoring"  # Alias for SCORING_AGENT
    VERIFICATION_QUESTION = "verification-question-agent"
    CANVAS_ORCHESTRATOR = "canvas-orchestrator"


@dataclass
class AgentResult:
    """
    AgentË∞ÉÁî®ÁªìÊûúÊï∞ÊçÆÁ±ª
    [Source: docs/architecture/EPIC-11-BACKEND-ARCHITECTURE.md#Layer-2-ÊúçÂä°Â±Ç]
    """
    agent_type: AgentType
    node_id: str = ""
    success: bool = True
    result: Optional[Dict[str, Any]] = None
    data: Optional[Dict[str, Any]] = None  # Alias for result
    error: Optional[str] = None
    duration_ms: float = 0.0
    created_at: datetime = field(default_factory=datetime.now)

    def __post_init__(self):
        """Ensure data and result are synchronized"""
        if self.data is None and self.result is not None:
            self.data = self.result
        elif self.result is None and self.data is not None:
            self.result = self.data

    def to_dict(self) -> Dict[str, Any]:
        """ËΩ¨Êç¢‰∏∫Â≠óÂÖ∏"""
        return {
            "agent_type": self.agent_type.value,
            "node_id": self.node_id,
            "success": self.success,
            "result": self.result,
            "data": self.data,
            "error": self.error,
            "duration_ms": self.duration_ms,
            "created_at": self.created_at.isoformat(),
        }


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# Story 21.3: Â§öÈáçfallbackÊñáÊú¨ÊèêÂèñÂíåÂèãÂ•ΩÈîôËØØÂ§ÑÁêÜ
# [Source: docs/prd/EPIC-21-AGENT-E2E-FLOW-FIX.md#story-21-3]
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def extract_explanation_text(response: Any) -> Tuple[str, bool]:
    """
    ÊèêÂèñexplanation_textÔºåÂ∏¶Â§öÈáçfallbackÊú∫Âà∂„ÄÇ

    ‰ΩøÁî®Â§ö‰∏™ÊèêÂèñÂô®Êåâ‰ºòÂÖàÁ∫ßÂ∞ùËØïÊèêÂèñÊñáÊú¨ÂÜÖÂÆπÔºåÁ°Æ‰øùÂç≥‰Ωø
    APIÂìçÂ∫îÊ†ºÂºè‰∏çÂÆåÂÖ®ÂåπÈÖç‰πüËÉΩÂ∞ΩÊúÄÂ§ßÂä™ÂäõÊèêÂèñÊúâÊïàÂÜÖÂÆπ„ÄÇ

    Args:
        response: AI APIÂìçÂ∫îÂØπË±°ÔºàÂèØËÉΩÊòØdict„ÄÅÂØπË±°ÊàñÂ≠óÁ¨¶‰∏≤Ôºâ

    Returns:
        Tuple[str, bool]: (ÊèêÂèñÁöÑÊñáÊú¨, ÊòØÂê¶ÊàêÂäü)
            - text: ÊèêÂèñÁöÑÊñáÊú¨ÂÜÖÂÆπÔºàÂéªÈô§È¶ñÂ∞æÁ©∫ÁôΩÔºâ
            - success: TrueÂ¶ÇÊûúÊàêÂäüÊèêÂèñÂà∞ÈùûÁ©∫ÊñáÊú¨

    [Source: docs/prd/EPIC-21-AGENT-E2E-FLOW-FIX.md#story-21-3]
    """
    if response is None:
        logger.warning("extract_explanation_text: response is None")
        return "", False

    extractors = [
        # ‰ºòÂÖàÁ∫ß1: responseÂ≠óÊÆµÔºàGeminiÂ∏∏Áî®Ê†ºÂºèÔºâ
        ("response", lambda r: r.get("response") if isinstance(r, dict) else None),
        # ‰ºòÂÖàÁ∫ß2: textÂ±ûÊÄßÔºàÊüê‰∫õSDKÁöÑÂìçÂ∫îÂØπË±°Ôºâ
        ("text_attr", lambda r: r.text if hasattr(r, "text") and r.text else None),
        # ‰ºòÂÖàÁ∫ß3: dictÁöÑtextÂ≠óÊÆµ
        ("text_key", lambda r: r.get("text") if isinstance(r, dict) else None),
        # ‰ºòÂÖàÁ∫ß4: dictÁöÑcontentÂ≠óÊÆµ
        ("content", lambda r: r.get("content") if isinstance(r, dict) else None),
        # ‰ºòÂÖàÁ∫ß5: dictÁöÑexplanationÂ≠óÊÆµ
        ("explanation", lambda r: r.get("explanation") if isinstance(r, dict) else None),
        # ‰ºòÂÖàÁ∫ß6: dictÁöÑmessageÂ≠óÊÆµ
        ("message", lambda r: r.get("message") if isinstance(r, dict) else None),
        # ‰ºòÂÖàÁ∫ß7: dictÁöÑoutputÂ≠óÊÆµ
        ("output", lambda r: r.get("output") if isinstance(r, dict) else None),
        # ‰ºòÂÖàÁ∫ß8: Â¶ÇÊûúresponseÊú¨Ë∫´ÊòØÂ≠óÁ¨¶‰∏≤
        ("direct_str", lambda r: r if isinstance(r, str) else None),
        # ‰ºòÂÖàÁ∫ß9: Â≠óÁ¨¶‰∏≤ÂåñÔºàÊúÄÁªàfallbackÔºâ
        ("stringify", lambda r: str(r) if r else None),
    ]

    for name, extractor in extractors:
        try:
            text = extractor(response)
            if text and isinstance(text, str) and text.strip():
                if name != "response" and name != "text_attr":
                    logger.info(
                        f"extract_explanation_text: Used fallback extractor '{name}' "
                        f"for response type: {type(response).__name__}"
                    )
                return text.strip(), True
        except Exception as e:
            logger.debug(f"extract_explanation_text: Extractor '{name}' failed: {e}")

    # ÊâÄÊúâÊèêÂèñÂô®ÈÉΩÂ§±Ë¥•
    logger.error(
        f"extract_explanation_text: All extractors failed for response type: {type(response).__name__}",
        extra={
            "response_type": type(response).__name__,
            "response_preview": str(response)[:500] if response else "None"
        }
    )
    return "", False


def create_error_response(
    error_message: str,
    source_node_id: str,
    agent_type: str,
    source_x: int = 0,
    source_y: int = 0,
    source_width: int = 400,
    source_height: int = 200,
) -> Dict[str, Any]:
    """
    ÂàõÂª∫ÂèãÂ•ΩÁöÑÈîôËØØÂìçÂ∫îÔºåÂú®Canvas‰∏≠ÊòæÁ§∫ÈîôËØØÊèêÁ§∫ËäÇÁÇπ„ÄÇ

    ÂΩìAgentÂ§ÑÁêÜÂ§±Ë¥•Êó∂ÔºåÂàõÂª∫‰∏Ä‰∏™Á∫¢Ëâ≤ÈîôËØØËäÇÁÇπËÄå‰∏çÊòØËøîÂõûÁ©∫ÁªìÊûúÔºå
    ËÆ©Áî®Êà∑Ê∏ÖÊ•öÂú∞Áü•ÈÅìÂèëÁîü‰∫Ü‰ªÄ‰πàÈóÆÈ¢ò„ÄÇ

    Args:
        error_message: ÈîôËØØÊèèËø∞‰ø°ÊÅØ
        source_node_id: Ê∫êËäÇÁÇπID
        agent_type: AgentÁ±ªÂûã
        source_x, source_y: Ê∫êËäÇÁÇπ‰ΩçÁΩÆ
        source_width, source_height: Ê∫êËäÇÁÇπÂ∞∫ÂØ∏

    Returns:
        ÂåÖÂê´ÈîôËØØËäÇÁÇπÂíåÂÖÉÊï∞ÊçÆÁöÑÂìçÂ∫îÂ≠óÂÖ∏

    [Source: docs/prd/EPIC-21-AGENT-E2E-FLOW-FIX.md#story-21-3]
    """
    error_node_id = f"error-{agent_type}-{uuid.uuid4().hex[:8]}"

    # ‰ΩçÁΩÆÂú®Ê∫êËäÇÁÇπÂè≥‰æß
    error_x = source_x + source_width + 50
    error_y = source_y

    logger.warning(
        f"create_error_response: Creating error node for {agent_type}",
        extra={
            "source_node_id": source_node_id,
            "agent_type": agent_type,
            "error_message": error_message
        }
    )

    return {
        "created_nodes": [{
            "id": error_node_id,
            "type": "text",
            "text": f"‚ö†Ô∏è AgentÂ§ÑÁêÜÂ§±Ë¥•\n\n**ÈîôËØØ**: {error_message}\n\n**AgentÁ±ªÂûã**: {agent_type}\n\nËØ∑Ê£ÄÊü•ËæìÂÖ•ÂÜÖÂÆπÂêéÈáçËØï„ÄÇ",
            "color": "1",  # Á∫¢Ëâ≤Ë°®Á§∫ÈîôËØØ
            "x": error_x,
            "y": error_y,
            "width": 400,
            "height": 200
        }],
        "created_edges": [{
            "id": f"edge-error-{uuid.uuid4().hex[:8]}",
            "fromNode": source_node_id,
            "toNode": error_node_id,
            "fromSide": "right",
            "toSide": "left",
            "label": "ÈîôËØØ",
            "color": "1"  # Á∫¢Ëâ≤
        }],
        "error": True,
        "error_message": error_message,
        "agent_type": agent_type,
        "source_node_id": source_node_id
    }


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# Story 21.4: ÁªìÊûÑÂåñÊó•ÂøóËÆ∞ÂΩïÂô®
# [Source: docs/prd/EPIC-21-AGENT-E2E-FLOW-FIX.md#story-21-4]
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

# Êó•ÂøóÈÖçÁΩÆÔºàÂèØÈÄöËøáÁéØÂ¢ÉÂèòÈáèË¶ÜÁõñÔºâ
AGENT_LOG_TRUNCATE_LENGTH = int(os.getenv("AGENT_LOG_TRUNCATE_LENGTH", "500"))


class AgentCallLogger:
    """
    AgentË∞ÉÁî®Êó•ÂøóËÆ∞ÂΩïÂô®„ÄÇ

    Êèê‰æõÁªìÊûÑÂåñÊó•ÂøóËÆ∞ÂΩïÔºåËøΩË∏™ÊØèÊ¨°AgentË∞ÉÁî®ÁöÑÂÆåÊï¥ÊµÅÁ®ãÔºå
    ÂåÖÊã¨ËØ∑Ê±ÇÂèÇÊï∞„ÄÅÂìçÂ∫îÂÜÖÂÆπ„ÄÅÂª∂ËøüÊó∂Èó¥ÂíåÈîôËØØ‰ø°ÊÅØ„ÄÇ

    Attributes:
        agent_type: AgentÁ±ªÂûãÔºàÂ¶Ç "decompose_basic"Ôºâ
        node_id: ÁõÆÊ†áËäÇÁÇπID
        canvas_name: CanvasÊñá‰ª∂Âêç
        start_time: Ë∞ÉÁî®ÂºÄÂßãÊó∂Èó¥

    [Source: docs/prd/EPIC-21-AGENT-E2E-FLOW-FIX.md#story-21-4]
    """

    def __init__(self, agent_type: str, node_id: str, canvas_name: str = ""):
        """ÂàùÂßãÂåñÊó•ÂøóËÆ∞ÂΩïÂô®„ÄÇ"""
        self.agent_type = agent_type
        self.node_id = node_id
        self.canvas_name = canvas_name
        self.start_time = time.time()
        self.extra = {
            "agent_type": agent_type,
            "node_id": node_id,
            "canvas_name": canvas_name,
        }

    def log_request(self, request_params: Dict[str, Any]) -> None:
        """
        ËÆ∞ÂΩïËØ∑Ê±Ç‰ø°ÊÅØ„ÄÇ

        Args:
            request_params: ËØ∑Ê±ÇÂèÇÊï∞Â≠óÂÖ∏ÔºàËá™Âä®Êà™Êñ≠ÈïøÊñáÊú¨Ôºâ
        """
        logger.info(
            f"[Agent Call START] {self.agent_type} for node {self.node_id}",
            extra={
                **self.extra,
                "event": "agent_call_start",
                "request_params": self._summarize(request_params)
            }
        )

    def log_response(
        self,
        response: Any,
        success: bool,
        response_length: int = 0
    ) -> None:
        """
        ËÆ∞ÂΩïÂìçÂ∫î‰ø°ÊÅØ„ÄÇ

        Args:
            response: APIÂìçÂ∫îÂØπË±°
            success: ÊòØÂê¶ÊàêÂäü
            response_length: ÂìçÂ∫îÊñáÊú¨ÈïøÂ∫¶
        """
        latency_ms = int((time.time() - self.start_time) * 1000)

        log_data = {
            **self.extra,
            "event": "agent_call_end",
            "success": success,
            "latency_ms": latency_ms,
            "response_length": response_length
        }

        if success:
            logger.info(
                f"[Agent Call SUCCESS] {self.agent_type} completed in {latency_ms}ms",
                extra=log_data
            )
        else:
            # Â§±Ë¥•Êó∂ËÆ∞ÂΩïÊõ¥Â§öËØ¶ÊÉÖ
            log_data["response_preview"] = self._truncate(str(response))
            logger.warning(
                f"[Agent Call FAILED] {self.agent_type} failed after {latency_ms}ms",
                extra=log_data
            )

    def log_error(self, error: Exception) -> None:
        """
        ËÆ∞ÂΩïÂºÇÂ∏∏‰ø°ÊÅØ„ÄÇ

        Args:
            error: ÊçïËé∑ÁöÑÂºÇÂ∏∏ÂØπË±°
        """
        latency_ms = int((time.time() - self.start_time) * 1000)

        logger.error(
            f"[Agent Call ERROR] {self.agent_type} raised {type(error).__name__}",
            extra={
                **self.extra,
                "event": "agent_call_error",
                "latency_ms": latency_ms,
                "error_type": type(error).__name__,
                "error_message": str(error)[:500]
            },
            exc_info=True
        )

    @staticmethod
    def _summarize(data: Dict[str, Any], max_length: int = 200) -> str:
        """ÁîüÊàêÊï∞ÊçÆÊëòË¶Å„ÄÇ"""
        summary = str(data)
        if len(summary) > max_length:
            return summary[:max_length] + "..."
        return summary

    @staticmethod
    def _truncate(text: str, max_length: int = None) -> str:
        """Êà™Êñ≠ÊñáÊú¨„ÄÇ"""
        if max_length is None:
            max_length = AGENT_LOG_TRUNCATE_LENGTH
        if len(text) > max_length:
            return text[:max_length] + "... [truncated]"
        return text


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# Story 21.5: ‰∏™‰∫∫ÁêÜËß£ËäÇÁÇπËá™Âä®ÂàõÂª∫
# [Source: docs/prd/EPIC-21-AGENT-E2E-FLOW-FIX.md#story-21-5]
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

# ÈÖçÁΩÆÂ∏∏ÈáèÔºàÂèØÈÄöËøáÁéØÂ¢ÉÂèòÈáèË¶ÜÁõñÔºâ
AUTO_CREATE_PERSONAL_NODE = os.getenv("AUTO_CREATE_PERSONAL_NODE", "true").lower() == "true"
PERSONAL_NODE_VERTICAL_OFFSET = int(os.getenv("PERSONAL_NODE_VERTICAL_OFFSET", "50"))
PERSONAL_NODE_PROMPT_TEXT = os.getenv(
    "PERSONAL_NODE_PROMPT_TEXT",
    "Âú®Ê≠§Â°´ÂÜô‰Ω†ÁöÑ‰∏™‰∫∫ÁêÜËß£...\n\nüí° ÊèêÁ§∫ÔºöÁî®Ëá™Â∑±ÁöÑËØùËß£ÈáäËøô‰∏™Ê¶ÇÂøµ"
)

# ‰∏™‰∫∫ÁêÜËß£ËäÇÁÇπÊ®°Êùø
PERSONAL_UNDERSTANDING_TEMPLATE = {
    "type": "text",
    "text": PERSONAL_NODE_PROMPT_TEXT,
    "width": 400,
    "height": 150,
    "color": "3"  # ÈªÑËâ≤ - Ë°®Á§∫ÂæÖÂ°´ÂÜôÁöÑ‰∏™‰∫∫ÁêÜËß£Âå∫Âüü
}


def create_personal_understanding_node(
    explanation_node_id: str,
    explanation_x: int,
    explanation_y: int,
    explanation_height: int,
    vertical_offset: int = None,
    custom_prompt: str = None
) -> Tuple[Dict[str, Any], str]:
    """
    ÂàõÂª∫‰∏™‰∫∫ÁêÜËß£ËäÇÁÇπ„ÄÇ

    Âú®Ëß£ÈáäËäÇÁÇπ‰∏ãÊñπÂàõÂª∫‰∏Ä‰∏™ÈªÑËâ≤ÁöÑ‰∏™‰∫∫ÁêÜËß£Âå∫ÂüüÔºåÁî®‰∫éÁî®Êà∑Â°´ÂÜôËá™Â∑±ÁöÑÁêÜËß£Ôºå
    Ë∑µË°åË¥πÊõºÂ≠¶‰π†Ê≥ï"Áî®Ëá™Â∑±ÁöÑËØùËß£Èáä"ÁöÑÊ†∏ÂøÉÂéüÂàô„ÄÇ

    Args:
        explanation_node_id: Ëß£ÈáäËäÇÁÇπID (Áî®‰∫éÂëΩÂêçÂíåÂÖ≥ËÅî)
        explanation_x: Ëß£ÈáäËäÇÁÇπXÂùêÊ†á
        explanation_y: Ëß£ÈáäËäÇÁÇπYÂùêÊ†á
        explanation_height: Ëß£ÈáäËäÇÁÇπÈ´òÂ∫¶
        vertical_offset: ÂûÇÁõ¥Èó¥Ë∑ù (ÈªòËÆ§‰ΩøÁî®ÈÖçÁΩÆÂÄº)
        custom_prompt: Ëá™ÂÆö‰πâÊèêÁ§∫ÊñáÂ≠ó (ÈªòËÆ§‰ΩøÁî®ÈÖçÁΩÆÂÄº)

    Returns:
        Tuple[node_dict, node_id]: ËäÇÁÇπÊï∞ÊçÆÂ≠óÂÖ∏ÂíåËäÇÁÇπID

    [Source: docs/prd/EPIC-21-AGENT-E2E-FLOW-FIX.md#story-21-5]
    """
    if vertical_offset is None:
        vertical_offset = PERSONAL_NODE_VERTICAL_OFFSET

    personal_node_id = f"personal-{explanation_node_id[:12]}-{uuid.uuid4().hex[:6]}"

    node = {
        "id": personal_node_id,
        **PERSONAL_UNDERSTANDING_TEMPLATE,
        "x": explanation_x,  # ‰∏éËß£ÈáäËäÇÁÇπÊ∞¥Âπ≥ÂØπÈΩê
        "y": explanation_y + explanation_height + vertical_offset  # Âú®Ëß£ÈáäËäÇÁÇπ‰∏ãÊñπ
    }

    # ‰ΩøÁî®Ëá™ÂÆö‰πâÊèêÁ§∫ÊñáÂ≠ó
    if custom_prompt:
        node["text"] = custom_prompt

    logger.debug(f"Created personal understanding node {personal_node_id} at ({node['x']}, {node['y']})")
    return node, personal_node_id


def create_personal_understanding_edge(
    explanation_node_id: str,
    personal_node_id: str,
    label: str = "‰∏™‰∫∫ÁêÜËß£"
) -> Dict[str, Any]:
    """
    ÂàõÂª∫Ëß£ÈáäËäÇÁÇπÂà∞‰∏™‰∫∫ÁêÜËß£ËäÇÁÇπÁöÑEdgeËøûÊé•„ÄÇ

    Args:
        explanation_node_id: Ëß£ÈáäËäÇÁÇπID (fromNode)
        personal_node_id: ‰∏™‰∫∫ÁêÜËß£ËäÇÁÇπID (toNode)
        label: EdgeÊ†áÁ≠æ (ÈªòËÆ§"‰∏™‰∫∫ÁêÜËß£")

    Returns:
        EdgeÊï∞ÊçÆÂ≠óÂÖ∏

    [Source: docs/prd/EPIC-21-AGENT-E2E-FLOW-FIX.md#story-21-5]
    """
    edge = {
        "id": f"edge-personal-{uuid.uuid4().hex[:8]}",
        "fromNode": explanation_node_id,
        "toNode": personal_node_id,
        "fromSide": "bottom",
        "toSide": "top",
        "label": label,
        "color": "3"  # ÈªÑËâ≤Ôºå‰∏é‰∏™‰∫∫ÁêÜËß£ËäÇÁÇπÂåπÈÖç
    }

    logger.debug(f"Created personal understanding edge: {explanation_node_id} ‚Üí {personal_node_id}")
    return edge


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# Story 21.6: EdgeËøûÊé•ÈÄªËæë‰øÆÂ§ç - Áªü‰∏ÄEdgeÂàõÂª∫ÂáΩÊï∞
# [Source: docs/prd/EPIC-21-AGENT-E2E-FLOW-FIX.md#story-21-6]
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

class EdgeLabels:
    """
    EdgeÊ†áÁ≠æÂ∏∏ÈáèÁ±ª„ÄÇ

    ÂÆö‰πâÊâÄÊúâAgentÁ±ªÂûãÂØπÂ∫îÁöÑEdgeÊ†áÁ≠æÔºåÁ°Æ‰øù‰∏ÄËá¥ÊÄß„ÄÇ

    [Source: docs/prd/EPIC-21-AGENT-E2E-FLOW-FIX.md#story-21-6]
    """
    BASIC_DECOMPOSE = "Âü∫Á°ÄÊãÜËß£"
    DEEP_DECOMPOSE = "Ê∑±Â∫¶ÊãÜËß£"
    ORAL_EXPLANATION = "Âè£ËØ≠Ëß£Èáä"
    FOUR_LEVEL = "ÂõõÂ±ÇËß£Èáä"
    CLARIFICATION = "ÊæÑÊ∏ÖË∑ØÂæÑ"
    COMPARISON = "ÂØπÊØîË°®Ê†º"
    MEMORY_ANCHOR = "ËÆ∞ÂøÜÈîöÁÇπ"
    EXAMPLE = "‰æãÈ¢òÊïôÂ≠¶"
    PERSONAL_UNDERSTANDING = "‰∏™‰∫∫ÁêÜËß£"
    QUESTION = "ÈóÆÈ¢òÊãÜËß£"
    VERIFICATION = "Ê£ÄÈ™åÈóÆÈ¢ò"

    @classmethod
    def get_label_for_type(cls, explanation_type: str) -> str:
        """
        Ê†πÊçÆËß£ÈáäÁ±ªÂûãËé∑ÂèñÂØπÂ∫îÁöÑEdgeÊ†áÁ≠æ„ÄÇ

        Args:
            explanation_type: AgentËß£ÈáäÁ±ªÂûã (Â¶Ç "oral", "four-level", "clarification")

        Returns:
            ÂØπÂ∫îÁöÑ‰∏≠ÊñáEdgeÊ†áÁ≠æ
        """
        type_label_map = {
            "oral": cls.ORAL_EXPLANATION,
            "four-level": cls.FOUR_LEVEL,
            "four_level": cls.FOUR_LEVEL,
            "four-level-explanation": cls.FOUR_LEVEL,
            "clarification": cls.CLARIFICATION,
            "comparison": cls.COMPARISON,
            "memory": cls.MEMORY_ANCHOR,
            "example": cls.EXAMPLE,
            "basic": cls.BASIC_DECOMPOSE,
            "deep": cls.DEEP_DECOMPOSE,
            "question": cls.QUESTION,
            "verification": cls.VERIFICATION,
        }
        return type_label_map.get(explanation_type.lower(), explanation_type)


class EdgeColors:
    """
    EdgeÈ¢úËâ≤Â∏∏ÈáèÁ±ª„ÄÇ

    Obsidian CanvasÈ¢úËâ≤‰ª£Á†Å:
    - "1": Á∫¢Ëâ≤ (ÈîôËØØ/ÈóÆÈ¢ò)
    - "2": Ê©ôËâ≤
    - "3": ÈªÑËâ≤ (ÂæÖÂ§ÑÁêÜ/‰∏™‰∫∫ÁêÜËß£)
    - "4": ÁªøËâ≤ (Ëß£Èáä/ÂÆåÊàê)
    - "5": ËìùËâ≤ (ÈªòËÆ§)
    - "6": Á¥´Ëâ≤ (ÈÉ®ÂàÜÁêÜËß£)

    [Source: docs/prd/EPIC-21-AGENT-E2E-FLOW-FIX.md#story-21-6]
    """
    DEFAULT = "5"       # ËìùËâ≤ - ÈªòËÆ§
    EXPLANATION = "4"   # ÁªøËâ≤ - Ëß£ÈáäÁ±ªEdge
    PERSONAL = "3"      # ÈªÑËâ≤ - ‰∏™‰∫∫ÁêÜËß£Edge
    ERROR = "1"         # Á∫¢Ëâ≤ - ÈîôËØØ/ÈóÆÈ¢ò
    PURPLE = "6"        # Á¥´Ëâ≤ - ÈÉ®ÂàÜÁêÜËß£


def create_edge(
    from_node_id: str,
    to_node_id: str,
    label: str = "",
    color: str = EdgeColors.DEFAULT,
    from_side: str = "right",
    to_side: str = "left"
) -> Dict[str, Any]:
    """
    Áªü‰∏ÄÁöÑEdgeÂàõÂª∫ÂáΩÊï∞„ÄÇ

    ÂàõÂª∫Á¨¶ÂêàObsidian CanvasÊ†ºÂºèÁöÑEdgeÊï∞ÊçÆ„ÄÇ

    Args:
        from_node_id: Ê∫êËäÇÁÇπID
        to_node_id: ÁõÆÊ†áËäÇÁÇπID
        label: EdgeÊ†áÁ≠æÊñáÂ≠ó
        color: EdgeÈ¢úËâ≤‰ª£Á†Å (‰ΩøÁî®EdgeColorsÂ∏∏Èáè)
        from_side: Ê∫êËäÇÁÇπËøûÊé•ÁÇπ ("top", "bottom", "left", "right")
        to_side: ÁõÆÊ†áËäÇÁÇπËøûÊé•ÁÇπ ("top", "bottom", "left", "right")

    Returns:
        EdgeÊï∞ÊçÆÂ≠óÂÖ∏ÔºåÂèØÁõ¥Êé•Ê∑ªÂä†Âà∞CanvasÁöÑedgesÊï∞ÁªÑ

    [Source: docs/prd/EPIC-21-AGENT-E2E-FLOW-FIX.md#story-21-6]
    """
    edge = {
        "id": f"edge-{uuid.uuid4().hex[:8]}",
        "fromNode": from_node_id,
        "toNode": to_node_id,
        "fromSide": from_side,
        "toSide": to_side,
    }

    # Âè™Âú®ÊúâÂÄºÊó∂Ê∑ªÂä†ÂèØÈÄâÂ≠óÊÆµ
    if label:
        edge["label"] = label
    if color and color != EdgeColors.DEFAULT:
        edge["color"] = color

    logger.debug(f"Created edge: {from_node_id} ‚Üí {to_node_id} [{label}]")
    return edge


def edge_exists(
    edges: List[Dict[str, Any]],
    from_node_id: str,
    to_node_id: str,
    label: Optional[str] = None
) -> bool:
    """
    Ê£ÄÊü•EdgeÊòØÂê¶Â∑≤Â≠òÂú®„ÄÇ

    Áî®‰∫éÈÅøÂÖçÂàõÂª∫ÈáçÂ§çÁöÑEdgeËøûÊé•„ÄÇ

    Args:
        edges: Áé∞ÊúâEdgeÂàóË°®
        from_node_id: Ê∫êËäÇÁÇπID
        to_node_id: ÁõÆÊ†áËäÇÁÇπID
        label: EdgeÊ†áÁ≠æ (ÂèØÈÄâÔºåÂ¶ÇÊûúÊèê‰æõÂàôÂêåÊó∂Ê£ÄÊü•Ê†áÁ≠æÂåπÈÖç)

    Returns:
        TrueÂ¶ÇÊûúÂ≠òÂú®Áõ∏ÂêåÁöÑEdgeÔºåFalseÂê¶Âàô

    [Source: docs/prd/EPIC-21-AGENT-E2E-FLOW-FIX.md#story-21-6]
    """
    for edge in edges:
        if (edge.get("fromNode") == from_node_id and
            edge.get("toNode") == to_node_id):
            # Â¶ÇÊûú‰∏çÊ£ÄÊü•Ê†áÁ≠æÔºåÊàñÊ†áÁ≠æÂåπÈÖçÔºåÂàôËÆ§‰∏∫Â≠òÂú®
            if label is None or edge.get("label") == label:
                return True
    return False


def create_edge_if_not_exists(
    existing_edges: List[Dict[str, Any]],
    from_node_id: str,
    to_node_id: str,
    label: str = "",
    color: str = EdgeColors.DEFAULT,
    from_side: str = "right",
    to_side: str = "left"
) -> Optional[Dict[str, Any]]:
    """
    Â¶ÇÊûúEdge‰∏çÂ≠òÂú®ÂàôÂàõÂª∫„ÄÇ

    Èò≤Ê≠¢ÈáçÂ§çEdgeÁöÑÂÆâÂÖ®ÂàõÂª∫ÂáΩÊï∞„ÄÇ

    Args:
        existing_edges: Áé∞ÊúâEdgeÂàóË°®
        from_node_id: Ê∫êËäÇÁÇπID
        to_node_id: ÁõÆÊ†áËäÇÁÇπID
        label: EdgeÊ†áÁ≠æ
        color: EdgeÈ¢úËâ≤
        from_side: Ê∫êËäÇÁÇπËøûÊé•ÁÇπ
        to_side: ÁõÆÊ†áËäÇÁÇπËøûÊé•ÁÇπ

    Returns:
        Êñ∞ÂàõÂª∫ÁöÑEdgeÊï∞ÊçÆÂ≠óÂÖ∏ÔºåÂ¶ÇÊûúEdgeÂ∑≤Â≠òÂú®ÂàôËøîÂõûNone

    [Source: docs/prd/EPIC-21-AGENT-E2E-FLOW-FIX.md#story-21-6]
    """
    if edge_exists(existing_edges, from_node_id, to_node_id, label if label else None):
        logger.debug(f"Edge already exists: {from_node_id} ‚Üí {to_node_id} [{label}], skipping")
        return None

    return create_edge(
        from_node_id=from_node_id,
        to_node_id=to_node_id,
        label=label,
        color=color,
        from_side=from_side,
        to_side=to_side
    )


class AgentService:
    """
    Agent call business logic service.

    Provides async methods for invoking various learning agents
    (basic-decomposition, scoring, oral-explanation, etc.).

    [Source: docs/architecture/EPIC-11-BACKEND-ARCHITECTURE.md#Layer-2-ÊúçÂä°Â±Ç]
    [Source: docs/prd/sprint-change-proposal-20251208.md - Story 20.4]
    """

    def __init__(
        self,
        gemini_client: Optional["GeminiClient"] = None,
        memory_client: Optional["LearningMemoryClient"] = None,
        canvas_service: Optional[Any] = None,  # ‚úÖ FIX: CanvasÂÜôÂÖ•ÊîØÊåÅ
        max_concurrent: int = 10,
        ai_config: Optional[Any] = None  # AIConfig from dependencies.py
    ):
        """
        Initialize AgentService with optional GeminiClient and LearningMemoryClient.

        ‚úÖ Verified from Context7:/googleapis/python-genai (topic: async generate content)

        Args:
            gemini_client: GeminiClient instance for real Gemini API calls.
                          If None or not configured, raises error (no mock fallback).
            memory_client: LearningMemoryClient for querying historical learning memories.
                          If None, historical context is not included.
            canvas_service: CanvasService instance for writing nodes to Canvas.
                           If None, nodes are returned but not written to Canvas.
            max_concurrent: Maximum concurrent agent calls (default: 10)
            ai_config: AIConfig dataclass with provider, model, base_url, api_key.
                      Used for future multi-provider support.

        [Source: docs/architecture/EPIC-11-BACKEND-ARCHITECTURE.md#‰æùËµñÊ≥®ÂÖ•ËÆæËÆ°]
        [Source: docs/prd/sprint-change-proposal-20251208.md - Story 20.4]
        [Source: FIX-4.2 AgentË∞ÉÁî®Êó∂Êü•ËØ¢ÂéÜÂè≤]
        [Source: Multi-Provider AI Architecture]
        [Source: FIX-Canvas-Write: BackendÁõ¥Êé•ÂÜôÂÖ•CanvasÊñá‰ª∂]
        """
        self._gemini_client = gemini_client
        self._memory_client = memory_client
        self._canvas_service = canvas_service  # ‚úÖ FIX: Store canvas_service
        self._max_concurrent = max_concurrent
        self._ai_config = ai_config  # Store for future multi-provider support
        self._semaphore = asyncio.Semaphore(max_concurrent)
        self._total_calls = 0
        self._active_calls = 0
        self._initialized = True
        self._use_real_api = gemini_client is not None and gemini_client.is_configured()

        # Log AI configuration
        if ai_config:
            provider = getattr(ai_config, 'provider', 'unknown')
            model = getattr(ai_config, 'model_name', 'unknown')
            logger.info(f"AgentService AI config: provider={provider}, model={model}")

        if self._use_real_api:
            logger.info("AgentService initialized with REAL AI API calls")
        else:
            logger.warning("AgentService initialized without configured AI client - API calls will fail")

        if self._memory_client:
            logger.info("AgentService will use LearningMemoryClient for historical context")

        if self._canvas_service:
            logger.info("AgentService will use CanvasService for writing nodes to Canvas")
        else:
            logger.warning("AgentService initialized without CanvasService - nodes will not be written to Canvas")

        logger.debug(f"AgentService max_concurrent={max_concurrent}")

    @property
    def total_calls(self) -> int:
        """Total number of agent calls made"""
        return self._total_calls

    @property
    def active_calls(self) -> int:
        """Current number of active agent calls"""
        return self._active_calls

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # FIX-Canvas-Write: ÂêéÁ´ØÁõ¥Êé•ÂÜôÂÖ•ËäÇÁÇπÂà∞CanvasÊñá‰ª∂
    # [Source: Plan cozy-sniffing-shamir.md - ‰øÆÂ§çAgentÁªìÊûúÊó†Ê≥ïÊòæÁ§∫Âú®Canvas‰∏äÁöÑÈóÆÈ¢ò]
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    async def _write_nodes_to_canvas(
        self,
        canvas_name: str,
        nodes: List[Dict[str, Any]],
        edges: List[Dict[str, Any]]
    ) -> bool:
        """
        Â∞ÜÁîüÊàêÁöÑËäÇÁÇπÂíåËæπÂÜôÂÖ• Canvas Êñá‰ª∂„ÄÇ

        ËøôÊòØËß£ÂÜ≥ "Agent ÁîüÊàêÊàêÂäü‰ΩÜ Canvas ‰∏çÊòæÁ§∫‰ªª‰ΩïËäÇÁÇπ" ÈóÆÈ¢òÁöÑÊ†∏ÂøÉ‰øÆÂ§ç„ÄÇ
        ÂêéÁ´ØÁõ¥Êé•ÂÜôÂÖ• Canvas Êñá‰ª∂ÔºåËÄå‰∏çÊòØ‰æùËµñÂâçÁ´ØÂ§ÑÁêÜËøîÂõûÁöÑËäÇÁÇπÊï∞ÊçÆ„ÄÇ

        Args:
            canvas_name: Canvas Êñá‰ª∂Âêç (‰∏çÂê´ .canvas Êâ©Â±ïÂêç)
            nodes: Ë¶ÅÊ∑ªÂä†ÁöÑËäÇÁÇπÂàóË°®
            edges: Ë¶ÅÊ∑ªÂä†ÁöÑËæπÂàóË°®

        Returns:
            True Â¶ÇÊûúÂÜôÂÖ•ÊàêÂäüÔºåFalse Â¶ÇÊûúÂ§±Ë¥•ÊàñÊó† canvas_service

        [Source: Plan cozy-sniffing-shamir.md]
        [Source: FIX-Canvas-Write: BackendÁõ¥Êé•ÂÜôÂÖ•CanvasÊñá‰ª∂]
        """
        if not self._canvas_service:
            logger.warning("[FIX-Canvas-Write] No canvas_service available, nodes will not be written to Canvas")
            return False

        if not nodes and not edges:
            logger.debug("[FIX-Canvas-Write] No nodes or edges to write")
            return True

        try:
            # ËØªÂèñÁé∞Êúâ Canvas Êï∞ÊçÆ
            canvas_data = await self._canvas_service.read_canvas(canvas_name)
            logger.debug(f"[FIX-Canvas-Write] Read canvas {canvas_name}, has {len(canvas_data.get('nodes', []))} existing nodes")

            # Ê∑ªÂä†Êñ∞ËäÇÁÇπ
            if nodes:
                canvas_data.setdefault("nodes", []).extend(nodes)
                logger.info(f"[FIX-Canvas-Write] Adding {len(nodes)} new nodes to {canvas_name}")

            # Ê∑ªÂä†Êñ∞Ëæπ
            if edges:
                canvas_data.setdefault("edges", []).extend(edges)
                logger.info(f"[FIX-Canvas-Write] Adding {len(edges)} new edges to {canvas_name}")

            # ÂÜôÂõû Canvas Êñá‰ª∂
            await self._canvas_service.write_canvas(canvas_name, canvas_data)
            logger.info(f"[FIX-Canvas-Write] ‚úÖ Successfully written {len(nodes)} nodes and {len(edges)} edges to {canvas_name}")

            return True

        except Exception as e:
            logger.error(f"[FIX-Canvas-Write] ‚ùå Failed to write nodes to canvas {canvas_name}: {e}")
            return False

    async def _call_gemini_api(
        self,
        agent_type: AgentType,
        prompt: str,
        context: Optional[str] = None,
        canvas_name: Optional[str] = None,
        node_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Call Gemini API through GeminiClient.

        ‚úÖ Verified from Context7:/googleapis/python-genai (topic: async generate content)

        Makes a real Gemini API call. No mock fallback - raises error if not configured.

        FIX-4.2: Now queries historical learning memories and enriches context.

        Args:
            agent_type: Type of agent to invoke
            prompt: User prompt to send
            context: Optional additional context (e.g., adjacent nodes, textbook refs)
            canvas_name: Optional Canvas name for memory lookup
            node_id: Optional node ID for memory lookup

        Returns:
            Dict with agent response

        [Source: docs/prd/sprint-change-proposal-20251208.md - Story 20.4]
        [Source: FIX-4.2 AgentË∞ÉÁî®Êó∂Êü•ËØ¢ÂéÜÂè≤]
        [Source: Gemini Migration - Using google.genai instead of anthropic]
        """
        # FIX-4.2: Query historical memories before calling Claude
        enriched_context = context or ""

        if self._memory_client and (canvas_name or prompt):
            try:
                # Search for relevant memories
                query = prompt[:100] if prompt else ""
                memories = await self._memory_client.search_memories(
                    query=query,
                    canvas_name=canvas_name,
                    node_id=node_id,
                    limit=5
                )

                if memories:
                    memory_context = self._memory_client.format_for_context(memories)
                    if memory_context:
                        enriched_context = f"{enriched_context}\n\n{memory_context}" if enriched_context else memory_context
                        logger.debug(f"Added {len(memories)} historical memories to context")

            except Exception as e:
                logger.warning(f"Failed to query historical memories: {e}")
                # Continue without memories - graceful degradation

        # Phase 1 FIX: Âº∫Âà∂ÁúüÂÆûAPIË∞ÉÁî®Ôºå‰∏çÂÜçÂõûÈÄÄÂà∞Mock
        # [Source: C:\Users\ROG\.claude\plans\wild-purring-umbrella.md - Phase 1]
        # [Gemini Migration] Now uses GOOGLE_API_KEY instead of ANTHROPIC_API_KEY
        if not self._use_real_api:
            # FIX: Provide diagnostic information in error message
            ai_config_info = ""
            if self._ai_config:
                ai_config_info = (
                    f" (Received config: provider={getattr(self._ai_config, 'provider', 'unknown')}, "
                    f"model={getattr(self._ai_config, 'model_name', 'unknown')}, "
                    f"has_api_key={bool(getattr(self._ai_config, 'api_key', ''))}, "
                    f"base_url={getattr(self._ai_config, 'base_url', 'none')!r})"
                )
            raise RuntimeError(
                f"AI API not configured!{ai_config_info} "
                "For custom providers, ensure base_url is set. "
                "Check: 1) API key in plugin settings, 2) base_url for custom providers, "
                "3) Headers being sent (browser dev tools)."
            )

        if not self._gemini_client:
            raise RuntimeError(
                "GeminiClient not initialized. "
                "This is a configuration error - check backend startup logs."
            )

        # Real Gemini API call - ‰∏çÂÜçÊúâMockÂõûÈÄÄ
        # ‚úÖ Verified from Context7:/googleapis/python-genai (topic: async generate content)
        logger.info(f"Making REAL Gemini API call for agent: {agent_type.value}")
        try:
            result = await self._gemini_client.call_agent(
                agent_type=agent_type.value,
                user_prompt=prompt,
                context=enriched_context if enriched_context else None,
                temperature=0.7
            )

            # ‚úÖ FIX: Parse AI response JSON from string
            # [Source: Plan - unified-knitting-crane.md - Root Cause Analysis]
            # GeminiClient returns {"response": "JSON_STRING", ...}
            # We need to parse the JSON string and merge it into result
            if "response" in result and isinstance(result["response"], str):
                response_text = result["response"]
                try:
                    json_text = response_text.strip()

                    # Handle markdown code block wrappers
                    if "```json" in json_text:
                        json_text = json_text.split("```json")[1].split("```")[0].strip()
                    elif "```" in json_text:
                        # Handle code blocks without language marker
                        parts = json_text.split("```")
                        if len(parts) >= 3:
                            json_text = parts[1].strip()

                    # Parse JSON (json module imported at file top, line 15)
                    parsed = json.loads(json_text)

                    # Merge parsed data into result
                    if isinstance(parsed, dict):
                        result.update(parsed)
                        logger.debug(f"Parsed AI response JSON keys: {list(parsed.keys())}")
                except (json.JSONDecodeError, IndexError, ValueError) as e:
                    # Catch more exception types for robustness
                    logger.warning(f"Failed to parse AI response as JSON: {e}")
                    logger.debug(f"Raw response (first 500 chars): {response_text[:500] if response_text else 'empty'}...")

            # Ê∑ªÂä†Êù•Ê∫êÊ†áËÆ∞Ôºå‰æø‰∫éÈ™åËØÅÊòØÁúüÂÆûAPIË∞ÉÁî®
            result["_source"] = "gemini_api"
            result["_mock"] = False
            logger.info(f"Gemini API success: {result.get('usage', {})}")
            return result
        except FileNotFoundError as e:
            # Prompt template not found - ‰∏çÂÜçÂõûÈÄÄÂà∞MockÔºåÁõ¥Êé•Êä•Èîô
            logger.error(f"Prompt template not found for {agent_type.value}: {e}")
            raise FileNotFoundError(
                f"Agent prompt template missing: {agent_type.value}. "
                f"Please ensure .claude/agents/{agent_type.value}.md exists."
            ) from e
        except Exception as e:
            logger.error(f"Gemini API error for {agent_type.value}: {e}")
            raise

    # Phase 1 FIX: _mock_response removed - all calls must use real Gemini API
    # [Source: C:\Users\ROG\.claude\plans\wild-purring-umbrella.md - Phase 1]
    # [Source: Gemini Migration - Using google.genai instead of anthropic]

    def _enrich_prompt_with_neighbors(
        self,
        original_content: str,
        adjacent_data: Dict[str, Any],
        max_context_length: int = 200
    ) -> str:
        """
        Enrich agent prompt with adjacent node content.

        Phase 4.1: Option A - Adjacent Node Enrichment
        Injects parent and child node context into the Agent prompt.

        Args:
            original_content: Original node content
            adjacent_data: Result from CanvasService.get_adjacent_nodes()
            max_context_length: Maximum characters per adjacent node

        Returns:
            Enriched prompt with neighbor context

        [Source: docs/architecture/EPIC-11-BACKEND-ARCHITECTURE.md#Phase-4-Edge-Enhancement]
        """
        if not adjacent_data:
            return original_content

        parents = adjacent_data.get("parents", [])
        children = adjacent_data.get("children", [])

        # If no neighbors, return original
        if not parents and not children:
            return original_content

        context_parts = [original_content, "\n\n### Adjacent Concept Context (Áõ∏ÈÇªÊ¶ÇÂøµ‰∏ä‰∏ãÊñá):"]

        # Add parent context
        for parent_info in parents:
            parent_node = parent_info.get("node", {})
            label = parent_info.get("label", "related")
            text = parent_node.get("text", "")[:max_context_length]
            node_type = parent_node.get("type", "text")

            if text:
                context_parts.append(f"\n- Parent [{label or 'prerequisite'}] ({node_type}): {text}")

        # Add child context
        for child_info in children:
            child_node = child_info.get("node", {})
            label = child_info.get("label", "related")
            text = child_node.get("text", "")[:max_context_length]
            node_type = child_node.get("type", "text")

            if text:
                context_parts.append(f"\n- Child [{label or 'extends'}] ({node_type}): {text}")

        return "\n".join(context_parts)

    async def call_agent(
        self,
        agent_type: AgentType,
        prompt: str,
        timeout: Optional[float] = None,
        context: Optional[str] = None
    ) -> AgentResult:
        """
        Call a single agent with concurrency control.

        Uses real Claude API if ClaudeClient is configured, otherwise mock.

        Args:
            agent_type: Type of agent to call
            prompt: Input prompt for the agent
            timeout: Optional timeout in seconds
            context: Optional additional context for the agent

        Returns:
            AgentResult with the response

        [Source: docs/prd/sprint-change-proposal-20251208.md - Story 20.4]
        """
        start_time = datetime.now()
        async with self._semaphore:
            self._active_calls += 1
            self._total_calls += 1
            try:
                if timeout:
                    data = await asyncio.wait_for(
                        self._call_gemini_api(agent_type, prompt, context),
                        timeout=timeout
                    )
                else:
                    data = await self._call_gemini_api(agent_type, prompt, context)

                duration_ms = (datetime.now() - start_time).total_seconds() * 1000
                return AgentResult(
                    agent_type=agent_type,
                    success=True,
                    result=data,
                    data=data,
                    duration_ms=duration_ms,
                )
            except asyncio.TimeoutError:
                return AgentResult(
                    agent_type=agent_type,
                    success=False,
                    error="Agent call timed out",
                )
            except Exception as e:
                logger.error(f"Agent call failed: {agent_type.value} - {e}")
                return AgentResult(
                    agent_type=agent_type,
                    success=False,
                    error=str(e),
                )
            finally:
                self._active_calls -= 1

    async def call_agent_with_images(
        self,
        agent_type: AgentType,
        prompt: str,
        images: Optional[List[Dict[str, Any]]] = None,
        timeout: Optional[float] = None,
        context: Optional[str] = None
    ) -> AgentResult:
        """
        Call a single agent with images (multimodal support).

        Uses real Claude API with vision capabilities.

        Args:
            agent_type: Type of agent to call
            prompt: Input prompt for the agent
            images: List of image dicts with 'data' (base64) and 'media_type'
            timeout: Optional timeout in seconds
            context: Optional additional context for the agent

        Returns:
            AgentResult with the response

        [Source: FIX-2.1 ÂÆûÁé∞Claude VisionÂ§öÊ®°ÊÄÅÊîØÊåÅ]
        """
        start_time = datetime.now()
        async with self._semaphore:
            self._active_calls += 1
            self._total_calls += 1
            try:
                if self._gemini_client:
                    # ‚úÖ Verified from Context7:/googleapis/python-genai
                    # Gemini natively supports multimodal input
                    agent_type_str = agent_type.value if isinstance(agent_type, AgentType) else agent_type

                    if timeout:
                        data = await asyncio.wait_for(
                            self._gemini_client.call_agent_with_images(
                                agent_type_str, prompt, images=images, context=context
                            ),
                            timeout=timeout
                        )
                    else:
                        data = await self._gemini_client.call_agent_with_images(
                            agent_type_str, prompt, images=images, context=context
                        )
                else:
                    # Phase 1 FIX: No fallback to mock - raise error if not configured
                    # [Source: C:\Users\ROG\.claude\plans\wild-purring-umbrella.md - Phase 1]
                    # [Gemini Migration] Now uses GOOGLE_API_KEY
                    raise RuntimeError(
                        "GeminiClient not configured for multimodal! "
                        "Please set GOOGLE_API_KEY in backend/.env or plugin settings."
                    )

                duration_ms = (datetime.now() - start_time).total_seconds() * 1000
                return AgentResult(
                    agent_type=agent_type,
                    success=True,
                    result=data,
                    data=data,
                    duration_ms=duration_ms,
                )
            except asyncio.TimeoutError:
                return AgentResult(
                    agent_type=agent_type,
                    success=False,
                    error="Agent call with images timed out",
                )
            except Exception as e:
                logger.error(f"Agent call with images failed: {agent_type.value} - {e}")
                return AgentResult(
                    agent_type=agent_type,
                    success=False,
                    error=str(e),
                )
            finally:
                self._active_calls -= 1

    async def call_agents_batch(
        self,
        requests: List[Dict[str, Any]],
        return_exceptions: bool = False
    ) -> List[AgentResult]:
        """
        Call multiple agents concurrently.

        Args:
            requests: List of dicts with 'agent_type' and 'prompt' keys
            return_exceptions: If True, return exceptions as results

        Returns:
            List of AgentResult objects
        """
        tasks = [
            self.call_agent(req["agent_type"], req["prompt"])
            for req in requests
        ]

        if return_exceptions:
            results = await asyncio.gather(*tasks, return_exceptions=True)
            # Convert exceptions to AgentResult with error
            processed = []
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    processed.append(AgentResult(
                        agent_type=requests[i]["agent_type"],
                        success=False,
                        error=str(result),
                    ))
                else:
                    processed.append(result)
            return processed
        else:
            return await asyncio.gather(*tasks)

    async def call_decomposition(
        self,
        content: str,
        deep: bool = False
    ) -> AgentResult:
        """
        Call decomposition agent.

        Args:
            content: Content to decompose
            deep: If True, use deep decomposition

        Returns:
            AgentResult with decomposition
        """
        agent_type = AgentType.DEEP_DECOMPOSITION if deep else AgentType.BASIC_DECOMPOSITION
        return await self.call_agent(agent_type, content)

    async def call_scoring(
        self,
        node_content: str,
        user_understanding: str
    ) -> AgentResult:
        """
        Call scoring agent.

        Args:
            node_content: Original node content
            user_understanding: User's explanation

        Returns:
            AgentResult with scores
        """
        prompt = f"Content: {node_content}\nUser: {user_understanding}"
        return await self.call_agent(AgentType.SCORING, prompt)

    async def call_explanation(
        self,
        content: str,
        explanation_type: str = "oral",
        context: Optional[str] = None,
        images: Optional[List[Dict[str, Any]]] = None
    ) -> AgentResult:
        """
        Call explanation agent with optional multimodal support.

        Args:
            content: Content to explain
            explanation_type: Type of explanation
            context: Optional additional context (adjacent nodes, textbook refs, etc.)
            images: Optional list of images for multimodal analysis

        Returns:
            AgentResult with explanation

        [Source: FIX-1.1 ‰øÆÂ§ç‰∏ä‰∏ãÊñá‰º†ÈÄíÈìæ]
        [Source: FIX-2.1 ÂÆûÁé∞Claude VisionÂ§öÊ®°ÊÄÅÊîØÊåÅ]
        """
        type_map = {
            "oral": AgentType.ORAL_EXPLANATION,
            "clarification": AgentType.CLARIFICATION_PATH,
            "comparison": AgentType.COMPARISON_TABLE,
            "memory": AgentType.MEMORY_ANCHOR,
            "four_level": AgentType.FOUR_LEVEL,
            "four-level": AgentType.FOUR_LEVEL,  # ‚úÖ Support both formats
            "example": AgentType.EXAMPLE_TEACHING,
        }
        agent_type = type_map.get(explanation_type, AgentType.ORAL_EXPLANATION)

        # ‚úÖ FIX-2.1: Use multimodal call if images are provided
        if images and len(images) > 0:
            logger.info(f"Calling {agent_type.value} with {len(images)} images")
            return await self.call_agent_with_images(
                agent_type, content, images=images, context=context
            )

        # ‚úÖ FIX-1.1: Pass context to call_agent for adjacent node enrichment
        return await self.call_agent(agent_type, content, context=context)

    async def decompose_basic(
        self,
        canvas_name: str,
        node_id: str,
        content: str,
        source_x: float = 0,
        source_y: float = 0
    ) -> Dict[str, Any]:
        """
        Perform basic decomposition on a concept node.

        Args:
            canvas_name: Target canvas name
            node_id: ID of node to decompose
            content: Node content to decompose
            source_x: X coordinate of source node (for positioning new nodes)
            source_y: Y coordinate of source node (for positioning new nodes)

        Returns:
            Decomposition result with guiding questions and created_nodes for Canvas

        [Source: docs/architecture/EPIC-11-BACKEND-ARCHITECTURE.md#Layer-2-ÊúçÂä°Â±Ç]
        [FIX: Return created_nodes array for frontend to write to Canvas]
        """
        import uuid

        logger.debug(f"Basic decomposition for node {node_id}")
        result = await self.call_decomposition(content, deep=False)

        # Extract questions from AI result
        questions = []
        created_nodes = []

        if result.success and result.data:
            sub_questions = result.data.get("sub_questions", [])

            # Generate Canvas nodes for each question
            # Position nodes below and to the right of source node
            node_width = 280
            node_height = 100
            gap_x = 20
            gap_y = 30
            nodes_per_row = 2

            for idx, q in enumerate(sub_questions):
                question_text = q.get("text", "")
                question_type = q.get("type", "")
                guidance = q.get("guidance", "")

                # Format node text with type and guidance
                node_text = question_text
                if question_type:
                    node_text = f"[{question_type}] {node_text}"
                if guidance:
                    node_text = f"{node_text}\n\n{guidance}"

                questions.append(question_text)

                # Calculate position (grid layout below source node)
                row = idx // nodes_per_row
                col = idx % nodes_per_row
                x = source_x + col * (node_width + gap_x)
                y = source_y + 150 + row * (node_height + gap_y)  # 150px below source

                # Create node object matching frontend NodeRead type
                created_nodes.append({
                    "id": f"q-{node_id}-{idx}-{uuid.uuid4().hex[:8]}",
                    "type": "text",
                    "text": node_text,
                    "x": x,
                    "y": y,
                    "width": node_width,
                    "height": node_height,
                    "color": "3",  # Yellow - indicates question/understanding area
                })

        logger.info(f"Basic decomposition created {len(created_nodes)} nodes")
        return {
            "node_id": node_id,
            "questions": questions,
            "created_nodes": created_nodes,
            "status": "completed",
            "result": result.data,
        }

    async def decompose_deep(
        self,
        canvas_name: str,
        node_id: str,
        content: str,
        source_x: float = 0,
        source_y: float = 0
    ) -> Dict[str, Any]:
        """
        Perform deep decomposition for verification questions.

        Args:
            canvas_name: Target canvas name
            node_id: ID of node to decompose
            content: Node content to decompose
            source_x: X coordinate of source node for positioning
            source_y: Y coordinate of source node for positioning

        Returns:
            Deep verification questions and created nodes

        [Source: docs/architecture/EPIC-11-BACKEND-ARCHITECTURE.md#Layer-2-ÊúçÂä°Â±Ç]
        """
        import uuid
        logger.debug(f"Deep decomposition for node {node_id}")
        result = await self.call_decomposition(content, deep=True)

        verification_questions = []
        created_nodes = []

        if result.success and result.data:
            # Extract sub_questions from AI result
            sub_questions = result.data.get("sub_questions", [])

            # Node positioning parameters
            node_width = 300
            node_height = 120
            gap_x = 20
            gap_y = 40
            nodes_per_row = 2

            for idx, q in enumerate(sub_questions):
                question_text = q.get("text", "")
                question_type = q.get("type", "")
                guidance = q.get("guidance", "")

                # Build node text with type and guidance
                node_text = question_text
                if question_type:
                    node_text = f"[{question_type}] {node_text}"
                if guidance:
                    node_text = f"{node_text}\n\n{guidance}"

                verification_questions.append(question_text)

                # Calculate position (grid layout below source node)
                row = idx // nodes_per_row
                col = idx % nodes_per_row
                x = source_x + col * (node_width + gap_x)
                y = source_y + 180 + row * (node_height + gap_y)  # 180px below source

                # Create node object matching frontend NodeRead type
                # Deep decomposition uses purple color (indicating "partially understood")
                created_nodes.append({
                    "id": f"vq-{node_id}-{idx}-{uuid.uuid4().hex[:8]}",
                    "type": "text",
                    "text": node_text,
                    "x": x,
                    "y": y,
                    "width": node_width,
                    "height": node_height,
                    "color": "6",  # Purple - indicates verification/deep question
                })

        logger.info(f"Deep decomposition created {len(created_nodes)} nodes")
        return {
            "node_id": node_id,
            "verification_questions": verification_questions,
            "created_nodes": created_nodes,
            "status": "completed",
            "result": result.data,
        }

    async def score_node(
        self,
        canvas_name: str,
        node_ids: List[str],
        node_contents: Dict[str, str]
    ) -> Dict[str, Any]:
        """
        Score multiple nodes' understanding.

        Args:
            canvas_name: Target canvas name
            node_ids: List of node IDs to score
            node_contents: Dict mapping node_id to content text

        Returns:
            {"scores": [{"node_id": ..., "accuracy": ..., ...}]}

        [Source: docs/architecture/EPIC-11-BACKEND-ARCHITECTURE.md#Layer-2-ÊúçÂä°Â±Ç]
        [Source: FIX-4.2 ËÆ∞ÂΩïÂ≠¶‰π†ÂéÜÁ®ã]
        [Source: FIX-5.1 ‰øÆÂ§çscore_nodeÁ≠æÂêç‰∏çÂåπÈÖç]
        """
        logger.debug(f"Scoring {len(node_ids)} nodes")

        scores = []
        for node_id in node_ids:
            content = node_contents.get(node_id, "")

            # Call scoring for each node
            result = await self.call_scoring("", content)

            # Determine color based on score
            total_score = 0.0
            if result.success and result.data:
                total_score = result.data.get("total", 0.0)

            # Color mapping: 0-40% red, 40-70% yellow, 70%+ green
            if total_score >= 0.7:
                new_color = "4"  # green
            elif total_score >= 0.4:
                new_color = "3"  # yellow
            else:
                new_color = "1"  # red

            scores.append({
                "node_id": node_id,
                "accuracy": result.data.get("accuracy", 0.0) if result.data else 0.0,
                "imagery": result.data.get("imagery", 0.0) if result.data else 0.0,
                "completeness": result.data.get("completeness", 0.0) if result.data else 0.0,
                "originality": result.data.get("originality", 0.0) if result.data else 0.0,
                "total": total_score,
                "new_color": new_color,
            })

            # Record learning episode
            await self.record_learning_episode(
                canvas_name=canvas_name,
                node_id=node_id,
                concept=content[:50] if content else "Unknown",
                user_understanding=content,
                score=total_score,
                agent_feedback=str(result.data) if result.data else None
            )

        return {"scores": scores}

    async def find_related_understanding_content(
        self,
        canvas_name: str,
        source_node_id: str,
        canvas_data: Dict[str, Any]
    ) -> List[str]:
        """
        FIX-4.4: Êü•Êâæ‰∏éÊ∫êËäÇÁÇπÂÖ≥ËÅîÁöÑÊâÄÊúâÈªÑËâ≤ÁêÜËß£ËäÇÁÇπÂÜÖÂÆπ„ÄÇ

        ÈÄöËøá Canvas ÁöÑ edges ËøΩË∏™Ôºö
        Ê∫êËäÇÁÇπ ‚Üí Ëß£ÈáäËäÇÁÇπ ‚Üí ÈªÑËâ≤ÁêÜËß£ËäÇÁÇπ

        Áî®‰∫éÂú®ÁîüÊàêÊñ∞Ëß£ÈáäÊó∂ÔºåÁªìÂêàÁî®Êà∑‰πãÂâçÂ°´ÂÜôÁöÑ‰∏™‰∫∫ÁêÜËß£Ôºå
        ÂÆûÁé∞‰∏™ÊÄßÂåñ„ÄÅÊ∏êËøõÂºèÁöÑÂ≠¶‰π†Ëß£Èáä„ÄÇ

        Args:
            canvas_name: Canvas Êñá‰ª∂Âêç
            source_node_id: Ê∫êËäÇÁÇπ ID
            canvas_data: Canvas Êï∞ÊçÆÂ≠óÂÖ∏

        Returns:
            Áî®Êà∑Â°´ÂÜôÁöÑÁêÜËß£ÂÜÖÂÆπÂàóË°®ÔºàÊéíÈô§Á©∫ÂÜÖÂÆπÂíåÂç†‰ΩçÁ¨¶Ôºâ
        """
        understanding_contents = []

        # ÈÅçÂéÜÊâÄÊúâ edgesÔºåÊâæÂà∞‰ªéÊ∫êËäÇÁÇπÂá∫ÂèëÁöÑËß£ÈáäÈìæ
        edges = canvas_data.get("edges", [])
        nodes = {n["id"]: n for n in canvas_data.get("nodes", [])}

        # Êü•ÊâæÊâÄÊúâ‰∏éÊ∫êËäÇÁÇπÁõ∏ÂÖ≥ÁöÑËäÇÁÇπÔºàÈÄöËøá edge ÂÖ≥Á≥ªÔºåBFSÈÅçÂéÜÔºâ
        related_node_ids = set()
        queue = [source_node_id]
        visited = {source_node_id}

        while queue:
            current_id = queue.pop(0)
            for edge in edges:
                if edge.get("fromNode") == current_id:
                    to_node_id = edge.get("toNode")
                    if to_node_id and to_node_id not in visited:
                        related_node_ids.add(to_node_id)
                        visited.add(to_node_id)
                        queue.append(to_node_id)

        logger.info(f"[FIX-4.4] Found {len(related_node_ids)} related nodes for {source_node_id}")

        # ‰ªéÂÖ≥ËÅîËäÇÁÇπ‰∏≠ÊâæÂá∫ÈªÑËâ≤ËäÇÁÇπÔºàcolor: "3"ÔºâÂπ∂ËØªÂèñÂÜÖÂÆπ
        for node_id in related_node_ids:
            node = nodes.get(node_id)
            if node and node.get("color") == "3":  # Yellow node
                logger.debug(f"[FIX-4.4] Found yellow node: {node_id}, type={node.get('type')}")

                if node.get("type") == "file" and node.get("file"):
                    # FIX-4.4: Read file content from vault
                    content = await self.canvas_service.read_file_content(node["file"])
                    if content:
                        understanding_contents.append(content)
                        logger.info(f"[FIX-4.4] Read understanding from file: {node['file']}")
                elif node.get("type") == "text" and node.get("text"):
                    text = node["text"].strip()
                    if text and "[ËØ∑Âú®Ê≠§Â°´ÂÜô" not in text:
                        understanding_contents.append(text)
                        logger.info(f"[FIX-4.4] Read understanding from text node: {node_id}")

        logger.info(f"[FIX-4.4] Total user understandings found: {len(understanding_contents)}")
        return understanding_contents

    async def generate_explanation(
        self,
        canvas_name: str,
        node_id: str,
        content: str,
        explanation_type: str = "oral",
        adjacent_context: Optional[str] = None,
        images: Optional[List[Dict[str, Any]]] = None,
        source_x: float = 0,
        source_y: float = 0,
        source_width: float = 400,
        source_height: float = 200
    ) -> Dict[str, Any]:
        """
        Generate an explanation for a concept with adjacent node context and optional images.

        Args:
            canvas_name: Target canvas name
            node_id: ID of node to explain
            content: Node content to explain
            explanation_type: Type of explanation (oral, four-level, etc.)
            adjacent_context: Context from adjacent nodes (1-hop neighbors)
            images: Optional list of images for multimodal analysis
            source_x: X coordinate of source node (for positioning new nodes)
            source_y: Y coordinate of source node (for positioning new nodes)
            source_width: Width of source node
            source_height: Height of source node

        Returns:
            Generated explanation with created_nodes for Canvas

        [Source: docs/architecture/EPIC-11-BACKEND-ARCHITECTURE.md#Layer-2-ÊúçÂä°Â±Ç]
        [Source: FIX-1.1 ‰øÆÂ§ç‰∏ä‰∏ãÊñá‰º†ÈÄíÈìæ - adjacent_context now flows through]
        [Source: FIX-2.1 ÂÆûÁé∞Claude VisionÂ§öÊ®°ÊÄÅÊîØÊåÅ]
        [Source: FIX-3.0 ËøîÂõûcreated_nodesÊï∞ÁªÑÁî®‰∫éCanvasÂÜôÂÖ•]
        """
        import json as json_module  # Avoid conflict with local variables
        import uuid

        context_len = len(adjacent_context) if adjacent_context else 0
        images_count = len(images) if images else 0
        logger.debug(f"Generating {explanation_type} explanation for node {node_id}, context_len={context_len}, images={images_count}")

        # ‚úÖ FIX-4.4: ËØªÂèñÁî®Êà∑‰πãÂâçÂ°´ÂÜôÁöÑ‰∏™‰∫∫ÁêÜËß£
        user_understandings = []
        try:
            from app.config import settings
            canvas_path = os.path.join(settings.CANVAS_BASE_PATH, canvas_name)
            if not canvas_path.endswith('.canvas'):
                canvas_path += '.canvas'

            if os.path.exists(canvas_path):
                with open(canvas_path, 'r', encoding='utf-8') as f:
                    canvas_data = json_module.load(f)

                # Êü•ÊâæÂÖ≥ËÅîÁöÑÈªÑËâ≤ÁêÜËß£ËäÇÁÇπÂÜÖÂÆπ
                user_understandings = await self.find_related_understanding_content(
                    canvas_name, node_id, canvas_data
                )
                logger.info(f"[FIX-4.4] Found {len(user_understandings)} user understandings for node {node_id}")
        except Exception as e:
            logger.warning(f"[FIX-4.4] Failed to read user understandings: {e}")

        # ‚úÖ FIX-4.4: ÊûÑÂª∫ÂåÖÂê´Áî®Êà∑ÁêÜËß£ÁöÑÂ¢ûÂº∫‰∏ä‰∏ãÊñá
        enhanced_context = adjacent_context or ""
        if user_understandings:
            user_context = "\n\n## Áî®Êà∑‰πãÂâçÁöÑ‰∏™‰∫∫ÁêÜËß£\n\n"
            for i, understanding in enumerate(user_understandings, 1):
                user_context += f"### ÁêÜËß£ {i}\n{understanding}\n\n"
            user_context += "ËØ∑ÁªìÂêàÁî®Êà∑ÁöÑËøô‰∫õÁêÜËß£ÔºåÁîüÊàêÊõ¥Ë¥¥ÂêàÁî®Êà∑ËÆ§Áü•Ê∞¥Âπ≥ÁöÑËß£Èáä„ÄÇÂ¶ÇÊûúÁî®Êà∑ÁêÜËß£ÊúâËØØÔºåËØ∑Âú®Ëß£Èáä‰∏≠ÂßîÂ©âÁ∫†Ê≠£„ÄÇ\n"
            enhanced_context = (enhanced_context + user_context) if enhanced_context else user_context
            logger.info(f"[FIX-4.4] Enhanced context with {len(user_understandings)} user understandings")

        # ‚úÖ FIX-1.1: Pass adjacent_context to call_explanation
        # ‚úÖ FIX-2.1: Pass images for multimodal support
        # ‚úÖ FIX-4.4: Pass enhanced_context with user understandings
        result = await self.call_explanation(
            content, explanation_type, context=enhanced_context, images=images
        )

        # ‚úÖ Story 21.3: Use multi-fallback extraction with friendly error handling
        # [Source: docs/prd/EPIC-21-AGENT-E2E-FLOW-FIX.md#story-21-3]
        explanation_text, extraction_success = extract_explanation_text(result.data)

        if not extraction_success or not explanation_text:
            logger.error(
                f"generate_explanation: Failed to extract text for {explanation_type}",
                extra={
                    "node_id": node_id,
                    "explanation_type": explanation_type,
                    "result_data_type": type(result.data).__name__ if result.data else "None"
                }
            )
            # ‚úÖ FIX: Create error response and write to Canvas before returning
            error_response = create_error_response(
                error_message="Êó†Ê≥ï‰ªéAIÂìçÂ∫î‰∏≠ÊèêÂèñÊúâÊïàÂÜÖÂÆπ",
                source_node_id=node_id,
                agent_type=explanation_type,
                source_x=source_x,
                source_y=source_y,
                source_width=source_width,
                source_height=source_height,
            )

            # ‚úÖ FIX: Write error nodes to Canvas so user can see the error
            if canvas_name:
                await self._write_nodes_to_canvas(
                    canvas_name=canvas_name,
                    nodes=error_response.get("created_nodes", []),
                    edges=error_response.get("created_edges", [])
                )

            return error_response

        # ‚úÖ FIX: Generate created_node_id (required by ExplainResponse)
        created_node_id = f"explain-{explanation_type}-{node_id}-{uuid.uuid4().hex[:8]}"

        # ‚úÖ FIX-4.3: Create file nodes instead of text nodes
        # All explanation nodes will be .md files stored in {canvas}-explanations/ folder
        created_nodes = []
        created_edges = []

        # ‚úÖ FIX-4.3: Calculate explanations directory path
        from datetime import datetime

        from app.config import settings

        vault_path = settings.CANVAS_BASE_PATH  # e.g., "C:/Users/ROG/ÊâòÁ¶è/Canvas/Á¨îËÆ∞Â∫ì"
        # canvas_name is relative path like "Canvas/Math53/Lecture5.canvas"
        canvas_dir = os.path.dirname(canvas_name)  # "Canvas/Math53"
        canvas_basename = os.path.splitext(os.path.basename(canvas_name))[0]  # "Lecture5"
        explanations_dir = f"{canvas_dir}/{canvas_basename}-explanations"  # "Canvas/Math53/Lecture5-explanations"

        # Create explanations directory if it doesn't exist
        full_explanations_dir = os.path.join(vault_path, explanations_dir)
        os.makedirs(full_explanations_dir, exist_ok=True)
        logger.info(f"[FIX-4.3] Explanations directory: {full_explanations_dir}")

        timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")

        if explanation_text:
            # Check if this is a four-level explanation (contains level headers)
            is_four_level = explanation_type in ["four-level", "four_level", "four-level-explanation"]
            level_pattern = r'##\s*(Êñ∞ÊâãÂ±Ç|ËøõÈò∂Â±Ç|‰∏ìÂÆ∂Â±Ç|ÂàõÊñ∞Â±Ç).*?\n'

            if is_four_level and re.search(level_pattern, explanation_text):
                # ‚úÖ FIX-4.8: Create ONE .md file containing ALL 4 levels
                # (User feedback: should be 1 file, not 4 separate files)

                # Layout configuration
                node_width = 500
                node_height = 600  # Taller to accommodate all 4 levels
                yellow_height = 150
                gap_y = 40

                # Position below source node
                node_x = source_x
                node_y = source_y + source_height + gap_y

                # ‚úÖ FIX-4.8: Create single .md file with all 4 levels
                explain_node_id = f"explain-four-level-{node_id[:8]}-{uuid.uuid4().hex[:4]}"
                explain_filename = f"ÂõõÂ±ÇÊ¨°Ëß£Èáä-{node_id[:8]}-{timestamp}.md"
                explain_file_path = f"{explanations_dir}/{explain_filename}"
                explain_full_path = os.path.join(vault_path, explain_file_path)

                # Write ALL levels to single .md file
                with open(explain_full_path, 'w', encoding='utf-8') as f:
                    f.write(f"# ÂõõÂ±ÇÊ¨°Ëß£Èáä\n\n{explanation_text}")
                logger.info(f"[FIX-4.8] Created SINGLE four-level explanation file: {explain_file_path}")

                # Create file type node (green for explanation)
                created_nodes.append({
                    "id": explain_node_id,
                    "type": "file",
                    "file": explain_file_path,
                    "x": node_x,
                    "y": node_y,
                    "width": node_width,
                    "height": node_height,
                    "color": "4",  # Green - explanation
                })
                logger.info(f"[FIX-4.8] Created four-level file node {explain_node_id}")

                # Create edge: Source ‚Üí Explanation
                edge1_id = f"edge-src-explain-{uuid.uuid4().hex[:8]}"
                created_edges.append({
                    "id": edge1_id,
                    "fromNode": node_id,
                    "toNode": explain_node_id,
                    "fromSide": "bottom",
                    "toSide": "top",
                    "label": "ÂõõÂ±ÇÊ¨°Ëß£Èáä",
                })

                # ‚úÖ Story 21.5: Conditionally create personal understanding node
                # [Source: docs/prd/EPIC-21-AGENT-E2E-FLOW-FIX.md#story-21-5]
                if AUTO_CREATE_PERSONAL_NODE:
                    # ‚úÖ FIX-4.9: Create ONE yellow understanding node
                    yellow_node_id = f"understand-four-level-{node_id[:8]}-{uuid.uuid4().hex[:4]}"
                    yellow_y = node_y + node_height + gap_y
                    created_nodes.append({
                        "id": yellow_node_id,
                        "type": "text",
                        "text": "# üí° ÊàëÁöÑÁêÜËß£\n\n[ËØ∑Âú®Ê≠§Â°´ÂÜô‰Ω†ÂØπÂõõÂ±ÇÊ¨°Ëß£ÈáäÁöÑÊï¥‰ΩìÁêÜËß£...]\n\n## Êñ∞ÊâãÂ±ÇÁêÜËß£\n\n\n## ËøõÈò∂Â±ÇÁêÜËß£\n\n\n## ‰∏ìÂÆ∂Â±ÇÁêÜËß£\n\n\n## ÂàõÊñ∞Â±ÇÁêÜËß£\n\n",
                        "x": node_x,
                        "y": yellow_y,
                        "width": node_width,
                        "height": yellow_height,
                        "color": "6",  # ‚úÖ FIX-4.11: Purple (matches user's reference node)
                    })
                    logger.info(f"[Story 21.5] Created PURPLE understanding node {yellow_node_id}")

                    # Create edge: Explanation ‚Üí Personal Understanding
                    edge2_id = f"edge-explain-yellow-{uuid.uuid4().hex[:8]}"
                    created_edges.append({
                        "id": edge2_id,
                        "fromNode": explain_node_id,
                        "toNode": yellow_node_id,
                        "fromSide": "bottom",
                        "toSide": "top",
                        "label": "‰∏™‰∫∫ÁêÜËß£",  # ‚úÖ FIX-4.10: Add edge label
                        "color": "6",  # ‚úÖ Purple edge (matches user's reference)
                    })
                else:
                    logger.debug("[Story 21.5] AUTO_CREATE_PERSONAL_NODE=False, skipping personal node")

                created_node_id = explain_node_id
                logger.info("[FIX-4.8/4.9] Four-level explanation complete: 1 file, 1 yellow node, 2 edges")

            else:
                # ‚úÖ FIX-4.3: Standard single-node explanation (oral, basic, etc.) - create .md files
                node_width = 500
                node_height = max(300, min(800, len(explanation_text) // 3))

                # Position below source node
                gap_y = 50
                node_x = source_x
                node_y = source_y + source_height + gap_y

                # ‚úÖ FIX-4.3: Create .md file for explanation node
                explain_filename = f"{explanation_type}-Ëß£Èáä-{node_id[:8]}-{timestamp}.md"
                explain_file_path = f"{explanations_dir}/{explain_filename}"
                explain_full_path = os.path.join(vault_path, explain_file_path)

                # Write explanation content to .md file
                with open(explain_full_path, 'w', encoding='utf-8') as f:
                    f.write(f"# {explanation_type.title()} Ëß£Èáä\n\n{explanation_text}")
                logger.info(f"[FIX-4.3] Created explanation file: {explain_file_path}")

                # Create file type node
                created_nodes.append({
                    "id": created_node_id,
                    "type": "file",
                    "file": explain_file_path,  # Relative to vault
                    "x": node_x,
                    "y": node_y,
                    "width": node_width,
                    "height": node_height,
                    "color": "4",  # Green - indicates explanation
                })
                logger.info(f"[FIX-4.3] Created explanation file node {created_node_id} at ({node_x}, {node_y})")

                # Create edge: Source ‚Üí Explanation
                edge1_id = f"edge-{node_id[:8]}-{created_node_id[:8]}-{uuid.uuid4().hex[:4]}"
                created_edges.append({
                    "id": edge1_id,
                    "fromNode": node_id,
                    "toNode": created_node_id,
                    "fromSide": "bottom",
                    "toSide": "top",
                    "label": explanation_type,
                })

                # ‚úÖ Story 21.5: Conditionally create personal understanding node
                # [Source: docs/prd/EPIC-21-AGENT-E2E-FLOW-FIX.md#story-21-5]
                if AUTO_CREATE_PERSONAL_NODE:
                    # ‚úÖ FIX-4.5: Create text type yellow understanding node (directly editable in Canvas)
                    yellow_node_id = f"understand-{node_id[:8]}-{uuid.uuid4().hex[:4]}"
                    yellow_y = node_y + node_height + PERSONAL_NODE_VERTICAL_OFFSET
                    created_nodes.append({
                        "id": yellow_node_id,
                        "type": "text",
                        "text": PERSONAL_NODE_PROMPT_TEXT,  # ‚úÖ Story 21.5: Use configurable prompt
                        "x": node_x,
                        "y": yellow_y,
                        "width": node_width,
                        "height": 120,
                        "color": "3",  # Yellow - ÂæÖÂ°´ÂÜôÁöÑ‰∏™‰∫∫ÁêÜËß£Âå∫Âüü
                    })
                    logger.info(f"[Story 21.5] Created yellow text node {yellow_node_id}")

                    # Create edge: Explanation ‚Üí Personal Understanding
                    edge2_id = f"edge-{created_node_id[:8]}-{yellow_node_id[:8]}-{uuid.uuid4().hex[:4]}"
                    created_edges.append({
                        "id": edge2_id,
                        "fromNode": created_node_id,
                        "toNode": yellow_node_id,
                        "fromSide": "bottom",
                        "toSide": "top",
                        "label": "‰∏™‰∫∫ÁêÜËß£",  # ‚úÖ Story 21.5: Add edge label
                        "color": "3",  # Yellow edge matches node
                    })
                    logger.info("[Story 21.5] Created edges for standard explanation with personal node")
                else:
                    logger.debug("[Story 21.5] AUTO_CREATE_PERSONAL_NODE=False, skipping personal node")

        # ‚úÖ FIX-Canvas-Write: ÂÜôÂÖ•ËäÇÁÇπÂà∞ Canvas Êñá‰ª∂ÔºàÊ†∏ÂøÉ‰øÆÂ§çÔºâ
        # [Source: Plan cozy-sniffing-shamir.md]
        # ËøôÊòØËß£ÂÜ≥ "Agent ÁîüÊàêÊàêÂäü‰ΩÜ Canvas ‰∏çÊòæÁ§∫‰ªª‰ΩïËäÇÁÇπ" ÈóÆÈ¢òÁöÑÂÖ≥ÈîÆ
        write_success = await self._write_nodes_to_canvas(
            canvas_name=canvas_name,
            nodes=created_nodes,
            edges=created_edges
        )
        if write_success:
            logger.info(f"[FIX-Canvas-Write] ‚úÖ Canvas {canvas_name} updated with {len(created_nodes)} nodes and {len(created_edges)} edges")
        else:
            logger.warning(f"[FIX-Canvas-Write] ‚ö†Ô∏è Failed to write to Canvas {canvas_name}, nodes returned in response but not persisted")

        return {
            "node_id": node_id,
            "explanation_type": explanation_type,
            "explanation": explanation_text,  # ‚úÖ Now contains actual AI response
            "created_node_id": created_node_id,  # ‚úÖ Required by ExplainResponse
            "created_nodes": created_nodes,  # ‚úÖ FIX-3.0: Nodes for Canvas
            "created_edges": created_edges,  # ‚úÖ FIX-4.1: Edges for Canvas
            "canvas_write_success": write_success,  # ‚úÖ FIX-Canvas-Write: Indicate if canvas was updated
            "status": "completed",
            "result": result.data,
            "context_used": context_len > 0,  # Track if context was provided
            "images_processed": images_count,  # Track images processed
        }

    async def record_learning_episode(
        self,
        canvas_name: str,
        node_id: str,
        concept: str,
        user_understanding: Optional[str] = None,
        score: Optional[float] = None,
        agent_feedback: Optional[str] = None
    ) -> bool:
        """
        Record a learning episode to the memory system.

        This method should be called after any Agent analysis to track
        the user's learning progress over time.

        Args:
            canvas_name: Canvas file name
            node_id: Node ID that was analyzed
            concept: Concept name or summary
            user_understanding: User's answer or understanding text
            score: Score from scoring agent (0-40)
            agent_feedback: Feedback from the agent

        Returns:
            True if successfully recorded, False otherwise

        [Source: FIX-4.2 AgentË∞ÉÁî®Êó∂Êü•ËØ¢ÂéÜÂè≤]
        """
        if not self._memory_client:
            logger.debug("Memory client not available, skipping episode recording")
            return False

        try:
            from app.clients.graphiti_client import LearningMemory

            memory = LearningMemory(
                canvas_name=canvas_name,
                node_id=node_id,
                concept=concept,
                user_understanding=user_understanding,
                score=score,
                agent_feedback=agent_feedback
            )

            success = await self._memory_client.add_learning_episode(memory)
            if success:
                logger.debug(f"Recorded learning episode: {canvas_name}/{node_id}")
            return success

        except Exception as e:
            logger.warning(f"Failed to record learning episode: {e}")
            return False

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # Story 21.5.4: AI Connection Health Check
    # [Source: docs/stories/21.5.4.story.md]
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    async def test_ai_connection(self) -> Dict[str, Any]:
        """
        Test AI API connection with a minimal request.

        Sends a simple test prompt to verify:
        1. API key is valid
        2. Network connectivity is working
        3. Model is available

        Returns:
            Dict with connection status:
            - status: "ok" or "error"
            - model: Current model name
            - provider: Current provider (google/anthropic/openai/custom)
            - latency_ms: Response time (if successful)
            - error: Error message (if failed)
            - error_code: Error code for diagnosis

        [Source: docs/stories/21.5.4.story.md#AC-21.5.4.2]
        [Source: docs/prd/EPIC-21.5-AGENT-RELIABILITY-FIX.md#story-21-5-4]
        """
        import time

        from app.config import settings

        # Get current AI configuration
        provider = getattr(settings, 'AI_PROVIDER', 'google')
        model = getattr(settings, 'AI_MODEL_NAME', 'gemini-2.0-flash-exp')

        # Check if client is configured
        if not self._use_real_api or not self._gemini_client:
            return {
                "status": "error",
                "model": model,
                "provider": provider,
                "error": "AI client not configured. Check API key settings.",
                "error_code": "LLM_AUTH_FAILED"
            }

        start_time = time.time()

        # [FIX] Bug #4: 400ms was too short for actual AI API calls (typically 2-10s)
        # Changed from 0.4s to 15s to allow real API response
        # Health check latency will be reported separately for monitoring
        AI_HEALTH_CHECK_TIMEOUT = 15.0  # 15s timeout for AI API call

        try:
            # Send minimal test request with timeout
            # ‚úÖ Verified from Context7:/googleapis/python-genai (topic: async generate content)
            # ‚úÖ Verified from Python docs: asyncio.wait_for() for Python 3.9+ timeout
            # [FIX] Bug #2: Method name was 'generate' but should be 'call_agent'
            # [FIX] Bug #3: Parameter name was 'prompt' but should be 'user_prompt'
            response = await asyncio.wait_for(
                self._gemini_client.call_agent(
                    agent_type="basic-decomposition",  # Use simplest agent
                    user_prompt="Reply with exactly: OK",
                    context=None
                ),
                timeout=AI_HEALTH_CHECK_TIMEOUT
            )

            latency_ms = (time.time() - start_time) * 1000

            # Check if we got a valid response
            if response and (response.get("response") or response.get("text")):
                logger.info(
                    f"AI connection test successful: provider={provider}, "
                    f"model={model}, latency={latency_ms:.0f}ms"
                )
                return {
                    "status": "ok",
                    "model": model,
                    "provider": provider,
                    "latency_ms": round(latency_ms, 2)
                }
            else:
                return {
                    "status": "error",
                    "model": model,
                    "provider": provider,
                    "error": "Empty response from AI API",
                    "error_code": "LLM_EMPTY_RESPONSE"
                }

        except asyncio.TimeoutError:
            # P0 Fix: Explicit timeout handling for NFR-3 compliance
            # [Source: docs/qa/gates/21.5.4-agent-health-check-enhancement.yml#PERF-001]
            latency_ms = (time.time() - start_time) * 1000
            logger.warning(
                f"AI connection test timeout: provider={provider}, "
                f"model={model}, timeout={AI_HEALTH_CHECK_TIMEOUT}s, "
                f"latency={latency_ms:.0f}ms"
            )
            return {
                "status": "error",
                "model": model,
                "provider": provider,
                "error": f"AI health check timeout ({AI_HEALTH_CHECK_TIMEOUT}s exceeded)",
                "error_code": "LLM_TIMEOUT",
                "latency_ms": round(latency_ms, 2)
            }

        except Exception as e:
            latency_ms = (time.time() - start_time) * 1000
            error_message = str(e)

            # Classify error type
            error_code = "LLM_UNKNOWN_ERROR"
            if "api key" in error_message.lower() or "authentication" in error_message.lower():
                error_code = "LLM_AUTH_FAILED"
            elif "timeout" in error_message.lower():
                error_code = "LLM_TIMEOUT"
            elif "rate limit" in error_message.lower():
                error_code = "LLM_RATE_LIMITED"
            elif "quota" in error_message.lower():
                error_code = "LLM_QUOTA_EXCEEDED"

            logger.warning(
                f"AI connection test failed: provider={provider}, "
                f"model={model}, error={error_message}, code={error_code}"
            )

            return {
                "status": "error",
                "model": model,
                "provider": provider,
                "error": error_message,
                "error_code": error_code,
                "latency_ms": round(latency_ms, 2)
            }

    async def cleanup(self) -> None:
        """
        Cleanup resources when service is no longer needed.

        Waits for all active calls to complete before cleanup.

        [Source: docs/architecture/EPIC-11-BACKEND-ARCHITECTURE.md#‰æùËµñÊ≥®ÂÖ•ËÆæËÆ°]
        """
        # Wait for active calls to complete
        while self._active_calls > 0:
            await asyncio.sleep(0.01)

        self._initialized = False
        logger.debug("AgentService cleanup completed")
