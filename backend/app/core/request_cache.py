# Canvas Learning System - Request Deduplication Cache
# Story 12.H.5: Backend Dedup - Prevent duplicate Agent requests
# [Source: docs/stories/story-12.H.5-backend-dedup.md]
"""
Request deduplication cache for Agent endpoints.

This module provides a thread-safe cache to prevent duplicate Agent requests
from being processed simultaneously. When the same request is detected within
the TTL window, subsequent requests are rejected with HTTP 409 Conflict.

Design Alignment:
- ADR-007: Uses in-memory cache with TTL mechanism for auto-cleanup
- ADR-009: Returns NON_RETRYABLE error (409 Conflict) for duplicate requests

[Source: docs/architecture/decisions/ADR-007-cache-strategy.md]
[Source: docs/architecture/decisions/ADR-009-error-handling.md]
"""

import hashlib
import logging
import threading
import time
from typing import Any, Dict, Tuple

logger = logging.getLogger(__name__)


class RequestCache:
    """
    Thread-safe request deduplication cache.

    Story 12.H.5: Prevents duplicate Agent requests from being processed
    simultaneously by tracking in-progress requests with a TTL mechanism.

    Key Features:
    - MD5-based cache key generation (ADR-007 aligned)
    - TTL-based automatic expiration (default: 60 seconds)
    - Periodic cleanup of expired entries
    - Thread-safe operations with locking

    Usage:
        cache = RequestCache(ttl=60)
        key = cache.get_key(canvas_name, node_id, agent_type)

        if cache.is_duplicate(key):
            raise HTTPException(409, "Duplicate request")

        cache.mark_in_progress(key)
        try:
            # Process request
            result = await process_request()
            cache.mark_completed(key)
            return result
        except Exception:
            cache.remove(key)  # Allow retry on failure
            raise

    [Source: docs/stories/story-12.H.5-backend-dedup.md#技术方案]
    """

    def __init__(self, ttl: int = 60, cleanup_interval: int = 30):
        """
        Initialize the request cache.

        Args:
            ttl: Time-to-live for cache entries in seconds (default: 60)
                 Per ADR-007, TTL mechanism ensures automatic cleanup.
            cleanup_interval: Interval for cleanup checks in seconds (default: 30)

        [Source: docs/stories/story-12.H.5-backend-dedup.md#AC1]
        """
        self.ttl = ttl
        self.cleanup_interval = cleanup_interval
        self._cache: Dict[str, Tuple[float, Any]] = {}
        self._lock = threading.Lock()
        self._last_cleanup = time.time()

    def get_key(
        self,
        canvas_name: str,
        node_id: str,
        agent_type: str
    ) -> str:
        """
        Generate a unique cache key for a request.

        Key format: MD5({canvas_name}:{node_id}:{agent_type})

        Per ADR-007, cache keys use MD5 hash for consistent key generation
        regardless of input length or special characters.

        Args:
            canvas_name: Canvas file name (e.g., "math.canvas")
            node_id: Target node ID (e.g., "abc123")
            agent_type: Agent type (e.g., "oral", "four-level")

        Returns:
            MD5 hash string of the combined key

        [Source: docs/stories/story-12.H.5-backend-dedup.md#AC3]
        """
        raw = f"{canvas_name}:{node_id}:{agent_type}"
        return hashlib.md5(raw.encode()).hexdigest()

    def is_duplicate(self, key: str) -> bool:
        """
        Check if a request is a duplicate (already in progress or recently completed).

        A request is considered duplicate if:
        1. The key exists in the cache
        2. The entry has not expired (within TTL window)

        This method also triggers periodic cleanup of expired entries.

        Args:
            key: Cache key generated by get_key()

        Returns:
            True if this is a duplicate request within the TTL window,
            False if this is a new/unique request

        [Source: docs/stories/story-12.H.5-backend-dedup.md#验收标准]
        """
        self._maybe_cleanup()

        with self._lock:
            if key in self._cache:
                timestamp, _ = self._cache[key]
                if time.time() - timestamp < self.ttl:
                    logger.warning(
                        f"[Story 12.H.5] Duplicate request detected: key={key[:16]}..."
                    )
                    return True
                else:
                    # Entry has expired, remove it
                    del self._cache[key]
                    logger.debug(
                        f"[Story 12.H.5] Expired cache entry removed: key={key[:16]}..."
                    )
            return False

    def mark_in_progress(self, key: str, data: Any = None) -> None:
        """
        Mark a request as in-progress.

        This should be called immediately after confirming the request
        is not a duplicate, before starting the actual processing.

        Args:
            key: Cache key generated by get_key()
            data: Optional metadata to store with the entry

        [Source: docs/stories/story-12.H.5-backend-dedup.md#Task-1.4]
        """
        with self._lock:
            self._cache[key] = (time.time(), data)
            logger.debug(f"[Story 12.H.5] Request marked in progress: key={key[:16]}...")

    def mark_completed(self, key: str) -> None:
        """
        Mark a request as completed.

        Note: The entry is NOT immediately removed. Instead, the timestamp
        is refreshed to restart the TTL window. This prevents immediate
        re-submission of the same request.

        Args:
            key: Cache key generated by get_key()

        [Source: docs/stories/story-12.H.5-backend-dedup.md#Task-1.5]
        """
        with self._lock:
            if key in self._cache:
                _, data = self._cache[key]
                # Refresh timestamp to start new TTL window
                self._cache[key] = (time.time(), data)
                logger.debug(f"[Story 12.H.5] Request marked completed: key={key[:16]}...")

    def remove(self, key: str) -> None:
        """
        Immediately remove a cache entry.

        Use this when a request fails or is cancelled, to allow
        immediate retry of the same request.

        Args:
            key: Cache key generated by get_key()

        [Source: docs/stories/story-12.H.5-backend-dedup.md#Task-1.6]
        """
        with self._lock:
            if key in self._cache:
                del self._cache[key]
                logger.debug(f"[Story 12.H.5] Request removed from cache: key={key[:16]}...")

    def _maybe_cleanup(self) -> None:
        """
        Periodically clean up expired cache entries.

        This method is called by is_duplicate() and only performs
        cleanup if enough time has passed since the last cleanup
        (controlled by cleanup_interval).

        Per ADR-007: TTL mechanism ensures automatic cleanup of stale entries.

        [Source: docs/stories/story-12.H.5-backend-dedup.md#AC5]
        """
        now = time.time()
        if now - self._last_cleanup < self.cleanup_interval:
            return

        with self._lock:
            self._last_cleanup = now
            expired_keys = [
                k for k, (ts, _) in self._cache.items()
                if now - ts >= self.ttl
            ]
            for k in expired_keys:
                del self._cache[k]

            if expired_keys:
                logger.debug(
                    f"[Story 12.H.5] Cleaned up {len(expired_keys)} expired cache entries"
                )

    def clear(self) -> None:
        """
        Clear all cache entries.

        Primarily used for testing to reset state between tests.
        """
        with self._lock:
            self._cache.clear()
            logger.debug("[Story 12.H.5] Request cache cleared")

    def __len__(self) -> int:
        """Return the number of entries in the cache."""
        with self._lock:
            return len(self._cache)


# Global singleton instance
# Per Story 12.H.5: Single shared cache instance for all endpoints
# TTL defaults from config will be applied at import time
request_cache = RequestCache(ttl=60, cleanup_interval=30)
