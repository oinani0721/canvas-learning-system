# Langgraph - Llms-Txt

**Pages:** 952

---

## 1. Create and/or select your dataset

**URL:** llms-txt#1.-create-and/or-select-your-dataset

ls_client = Client()
dataset = ls_client.clone_public_dataset(
    "https://smith.langchain.com/public/a63525f9-bdf2-4512-83e3-077dc9417f96/d"
)

---

## 1. Read current authentication module code

**URL:** llms-txt#1.-read-current-authentication-module-code

---

## 1. Specify config schema

**URL:** llms-txt#1.-specify-config-schema

class ContextSchema(TypedDict):
    my_runtime_value: str

---

## 2. Define an evaluator

**URL:** llms-txt#2.-define-an-evaluator

def is_concise(outputs: dict, reference_outputs: dict) -> bool:
    return len(outputs["answer"]) < (3 * len(reference_outputs["answer"]))

---

## 2. Define a graph that accesses the config in a node

**URL:** llms-txt#2.-define-a-graph-that-accesses-the-config-in-a-node

class State(TypedDict):
    my_state_value: str

def node(state: State, runtime: Runtime[ContextSchema]):  # [!code highlight]
    if runtime.context["my_runtime_value"] == "a":  # [!code highlight]
        return {"my_state_value": 1}
    elif runtime.context["my_runtime_value"] == "b":  # [!code highlight]
        return {"my_state_value": 2}
    else:
        raise ValueError("Unknown values.")

builder = StateGraph(State, context_schema=ContextSchema)  # [!code highlight]
builder.add_node(node)
builder.add_edge(START, "node")
builder.add_edge("node", END)

graph = builder.compile()

---

## 2. Identify functions that need async conversion

**URL:** llms-txt#2.-identify-functions-that-need-async-conversion

---

## 3. Define the interface to your app

**URL:** llms-txt#3.-define-the-interface-to-your-app

def chatbot(inputs: dict) -> dict:
    return {"answer": inputs["question"] + " is a good question. I don't know the answer."}

---

## 3. Pass in configuration at runtime:

**URL:** llms-txt#3.-pass-in-configuration-at-runtime:

**Contents:**
- Add retry policies
- Add node caching
- Create a sequence of steps

print(graph.invoke({}, context={"my_runtime_value": "a"}))  # [!code highlight]
print(graph.invoke({}, context={"my_runtime_value": "b"}))  # [!code highlight]

{'my_state_value': 1}
{'my_state_value': 2}
python  theme={null}
  from dataclasses import dataclass

from langchain.chat_models import init_chat_model
  from langgraph.graph import MessagesState, END, StateGraph, START
  from langgraph.runtime import Runtime
  from typing_extensions import TypedDict

@dataclass
  class ContextSchema:
      model_provider: str = "anthropic"

MODELS = {
      "anthropic": init_chat_model("claude-haiku-4-5-20251001"),
      "openai": init_chat_model("gpt-4.1-mini"),
  }

def call_model(state: MessagesState, runtime: Runtime[ContextSchema]):
      model = MODELS[runtime.context.model_provider]
      response = model.invoke(state["messages"])
      return {"messages": [response]}

builder = StateGraph(MessagesState, context_schema=ContextSchema)
  builder.add_node("model", call_model)
  builder.add_edge(START, "model")
  builder.add_edge("model", END)

graph = builder.compile()

# Usage
  input_message = {"role": "user", "content": "hi"}
  # With no configuration, uses default (Anthropic)
  response_1 = graph.invoke({"messages": [input_message]}, context=ContextSchema())["messages"][-1]
  # Or, can set OpenAI
  response_2 = graph.invoke({"messages": [input_message]}, context={"model_provider": "openai"})["messages"][-1]

print(response_1.response_metadata["model_name"])
  print(response_2.response_metadata["model_name"])
  
  claude-haiku-4-5-20251001
  gpt-4.1-mini-2025-04-14
  python  theme={null}
  from dataclasses import dataclass
  from langchain.chat_models import init_chat_model
  from langchain.messages import SystemMessage
  from langgraph.graph import END, MessagesState, StateGraph, START
  from langgraph.runtime import Runtime
  from typing_extensions import TypedDict

@dataclass
  class ContextSchema:
      model_provider: str = "anthropic"
      system_message: str | None = None

MODELS = {
      "anthropic": init_chat_model("claude-haiku-4-5-20251001"),
      "openai": init_chat_model("gpt-4.1-mini"),
  }

def call_model(state: MessagesState, runtime: Runtime[ContextSchema]):
      model = MODELS[runtime.context.model_provider]
      messages = state["messages"]
      if (system_message := runtime.context.system_message):
          messages = [SystemMessage(system_message)] + messages
      response = model.invoke(messages)
      return {"messages": [response]}

builder = StateGraph(MessagesState, context_schema=ContextSchema)
  builder.add_node("model", call_model)
  builder.add_edge(START, "model")
  builder.add_edge("model", END)

graph = builder.compile()

# Usage
  input_message = {"role": "user", "content": "hi"}
  response = graph.invoke({"messages": [input_message]}, context={"model_provider": "openai", "system_message": "Respond in Italian."})
  for message in response["messages"]:
      message.pretty_print()
  
  ================================ Human Message ================================

hi
  ================================== Ai Message ==================================

Ciao! Come posso aiutarti oggi?
  python  theme={null}
from langgraph.types import RetryPolicy

builder.add_node(
    "node_name",
    node_function,
    retry_policy=RetryPolicy(),
)
python  theme={null}
  import sqlite3
  from typing_extensions import TypedDict
  from langchain.chat_models import init_chat_model
  from langgraph.graph import END, MessagesState, StateGraph, START
  from langgraph.types import RetryPolicy
  from langchain_community.utilities import SQLDatabase
  from langchain.messages import AIMessage

db = SQLDatabase.from_uri("sqlite:///:memory:")
  model = init_chat_model("claude-haiku-4-5-20251001")

def query_database(state: MessagesState):
      query_result = db.run("SELECT * FROM Artist LIMIT 10;")
      return {"messages": [AIMessage(content=query_result)]}

def call_model(state: MessagesState):
      response = model.invoke(state["messages"])
      return {"messages": [response]}

# Define a new graph
  builder = StateGraph(MessagesState)
  builder.add_node(
      "query_database",
      query_database,
      retry_policy=RetryPolicy(retry_on=sqlite3.OperationalError),
  )
  builder.add_node("model", call_model, retry_policy=RetryPolicy(max_attempts=5))
  builder.add_edge(START, "model")
  builder.add_edge("model", "query_database")
  builder.add_edge("query_database", END)
  graph = builder.compile()
  python  theme={null}
from langgraph.types import CachePolicy

builder.add_node(
    "node_name",
    node_function,
    cache_policy=CachePolicy(ttl=120),
)
python  theme={null}
from langgraph.cache.memory import InMemoryCache

graph = builder.compile(cache=InMemoryCache())
python  theme={null}
from langgraph.graph import START, StateGraph

builder = StateGraph(State)

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
<Accordion title="Extended example: specifying LLM at runtime">
  Below we demonstrate a practical example in which we configure what LLM to use at runtime. We will use both OpenAI and Anthropic models.
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</Accordion>

<Accordion title="Extended example: specifying model and system message at runtime">
  Below we demonstrate a practical example in which we configure two parameters: the LLM and system message to use at runtime.
```

---

## 3. Refactor functions to async/await

**URL:** llms-txt#3.-refactor-functions-to-async/await

---

## 4. Run an evaluation

**URL:** llms-txt#4.-run-an-evaluation

experiment = ls_client.evaluate(
    chatbot,
    data=dataset,
    evaluators=[is_concise],
    experiment_prefix="my-first-experiment",
    # 'upload_results' is the relevant arg.
    upload_results=False
)

---

## 4. Update function calls throughout codebase

**URL:** llms-txt#4.-update-function-calls-throughout-codebase

---

## 5. Analyze results locally

**URL:** llms-txt#5.-analyze-results-locally

results = list(experiment)

---

## 5. Run tests and fix any failures

**URL:** llms-txt#5.-run-tests-and-fix-any-failures

**Contents:**
  - LLM tool selector
  - Tool retry
  - LLM tool emulator
  - Context editing
- Custom middleware
- Decorator-based middleware

print(result["todos"])  # Track the agent's progress through each step
python  theme={null}
from langchain.agents import create_agent
from langchain.agents.middleware import LLMToolSelectorMiddleware

agent = create_agent(
    model="gpt-4o",
    tools=[tool1, tool2, tool3, tool4, tool5, ...],  # Many tools
    middleware=[
        LLMToolSelectorMiddleware(
            model="gpt-4o-mini",  # Use cheaper model for selection
            max_tools=3,  # Limit to 3 most relevant tools
            always_include=["search"],  # Always include certain tools
        ),
    ],
)
python  theme={null}
from langchain.agents import create_agent
from langchain.agents.middleware import ToolRetryMiddleware

agent = create_agent(
    model="gpt-4o",
    tools=[search_tool, database_tool],
    middleware=[
        ToolRetryMiddleware(
            max_retries=3,  # Retry up to 3 times
            backoff_factor=2.0,  # Exponential backoff multiplier
            initial_delay=1.0,  # Start with 1 second delay
            max_delay=60.0,  # Cap delays at 60 seconds
            jitter=True,  # Add random jitter to avoid thundering herd
        ),
    ],
)
python  theme={null}
from langchain.agents import create_agent
from langchain.agents.middleware import LLMToolEmulator

agent = create_agent(
    model="gpt-4o",
    tools=[get_weather, search_database, send_email],
    middleware=[
        # Emulate all tools by default
        LLMToolEmulator(),

# Or emulate specific tools
        # LLMToolEmulator(tools=["get_weather", "search_database"]),

# Or use a custom model for emulation
        # LLMToolEmulator(model="claude-sonnet-4-5-20250929"),
    ],
)
python  theme={null}
from langchain.agents import create_agent
from langchain.agents.middleware import ContextEditingMiddleware, ClearToolUsesEdit

agent = create_agent(
    model="gpt-4o",
    tools=[...],
    middleware=[
        ContextEditingMiddleware(
            edits=[
                ClearToolUsesEdit(trigger=1000),  # Clear old tool uses
            ],
        ),
    ],
)
python  theme={null}
from langchain.agents.middleware import before_model, after_model, wrap_model_call
from langchain.agents.middleware import AgentState, ModelRequest, ModelResponse, dynamic_prompt
from langchain.messages import AIMessage
from langchain.agents import create_agent
from langgraph.runtime import Runtime
from typing import Any, Callable

**Examples:**

Example 1 (unknown):
```unknown
<Accordion title="Configuration options">
  <ParamField body="system_prompt" type="string">
    Custom system prompt for guiding todo usage. Uses built-in prompt if not specified.
  </ParamField>

  <ParamField body="tool_description" type="string">
    Custom description for the `write_todos` tool. Uses built-in description if not specified.
  </ParamField>
</Accordion>

### LLM tool selector

Use an LLM to intelligently select relevant tools before calling the main model.

<Tip>
  **Perfect for:**

  * Agents with many tools (10+) where most aren't relevant per query
  * Reducing token usage by filtering irrelevant tools
  * Improving model focus and accuracy
</Tip>
```

Example 2 (unknown):
```unknown
<Accordion title="Configuration options">
  <ParamField body="model" type="string | BaseChatModel">
    Model for tool selection. Can be a model string or `BaseChatModel` instance. Defaults to the agent's main model.
  </ParamField>

  <ParamField body="system_prompt" type="string">
    Instructions for the selection model. Uses built-in prompt if not specified.
  </ParamField>

  <ParamField body="max_tools" type="number">
    Maximum number of tools to select. Defaults to no limit.
  </ParamField>

  <ParamField body="always_include" type="list[string]">
    List of tool names to always include in the selection
  </ParamField>
</Accordion>

### Tool retry

Automatically retry failed tool calls with configurable exponential backoff.

<Tip>
  **Perfect for:**

  * Handling transient failures in external API calls
  * Improving reliability of network-dependent tools
  * Building resilient agents that gracefully handle temporary errors
</Tip>
```

Example 3 (unknown):
```unknown
<Accordion title="Configuration options">
  <ParamField body="max_retries" type="number" default="2">
    Maximum number of retry attempts after the initial call (3 total attempts with default)
  </ParamField>

  <ParamField body="tools" type="list[BaseTool | str]">
    Optional list of tools or tool names to apply retry logic to. If `None`, applies to all tools.
  </ParamField>

  <ParamField body="retry_on" type="tuple[type[Exception], ...] | callable" default="(Exception,)">
    Either a tuple of exception types to retry on, or a callable that takes an exception and returns `True` if it should be retried.
  </ParamField>

  <ParamField body="on_failure" type="string | callable" default="return_message">
    Behavior when all retries are exhausted. Options:

    * `"return_message"` - Return a ToolMessage with error details (allows LLM to handle failure)
    * `"raise"` - Re-raise the exception (stops agent execution)
    * Custom callable - Function that takes the exception and returns a string for the ToolMessage content
  </ParamField>

  <ParamField body="backoff_factor" type="number" default="2.0">
    Multiplier for exponential backoff. Each retry waits `initial_delay * (backoff_factor ** retry_number)` seconds. Set to 0.0 for constant delay.
  </ParamField>

  <ParamField body="initial_delay" type="number" default="1.0">
    Initial delay in seconds before first retry
  </ParamField>

  <ParamField body="max_delay" type="number" default="60.0">
    Maximum delay in seconds between retries (caps exponential backoff growth)
  </ParamField>

  <ParamField body="jitter" type="boolean" default="true">
    Whether to add random jitter (±25%) to delay to avoid thundering herd
  </ParamField>
</Accordion>

### LLM tool emulator

Emulate tool execution using an LLM for testing purposes, replacing actual tool calls with AI-generated responses.

<Tip>
  **Perfect for:**

  * Testing agent behavior without executing real tools
  * Developing agents when external tools are unavailable or expensive
  * Prototyping agent workflows before implementing actual tools
</Tip>
```

Example 4 (unknown):
```unknown
<Accordion title="Configuration options">
  <ParamField body="tools" type="list[str | BaseTool]">
    List of tool names (str) or BaseTool instances to emulate. If `None` (default), ALL tools will be emulated. If empty list, no tools will be emulated.
  </ParamField>

  <ParamField body="model" type="string | BaseChatModel" default="anthropic:claude-3-5-sonnet-latest">
    Model to use for generating emulated tool responses. Can be a model identifier string or BaseChatModel instance.
  </ParamField>
</Accordion>

### Context editing

Manage conversation context by trimming, summarizing, or clearing tool uses.

<Tip>
  **Perfect for:**

  * Long conversations that need periodic context cleanup
  * Removing failed tool attempts from context
  * Custom context management strategies
</Tip>
```

---

## A2A endpoint in Agent Server

**URL:** llms-txt#a2a-endpoint-in-agent-server

**Contents:**
- Agent Card Discovery
- Requirements
- Usage overview
- Creating an A2A-compatible agent

Source: https://docs.langchain.com/langsmith/server-a2a

[Agent2Agent (A2A)](https://a2a-protocol.org/latest/) is Google's protocol for enabling communication between conversational AI agents. [LangSmith implements A2A support](https://langchain-ai.github.io/langgraph/cloud/reference/api/api_ref.html#tag/a2a/post/a2a/\{assistant_id}), allowing your agents to communicate with other A2A-compatible agents through a standardized protocol.

The A2A endpoint is available in [Agent Server](/langsmith/agent-server) at `/a2a/{assistant_id}`.

## Agent Card Discovery

Each assistant automatically exposes an A2A Agent Card that describes its capabilities and provides the information needed for other agents to connect. You can retrieve the agent card for any assistant using:

The agent card includes the assistant's name, description, available skills, supported input/output modes, and the A2A endpoint URL for communication.

To use A2A, ensure you have the following dependencies installed:

* `langgraph-api >= 0.4.9`

* Upgrade to use langgraph-api>=0.4.9.
* Deploy your agent with message-based state structure.
* Connect with other A2A-compatible agents using the endpoint.

## Creating an A2A-compatible agent

This example creates an A2A-compatible agent that processes incoming messages using OpenAI's API and maintains conversational state. The agent defines a message-based state structure and handles the A2A protocol's message format.

To be compatible with the [A2A "text" parts](https://a2a-protocol.org/dev/specification/#651-textpart-object), the agent must have a `messages` key in state. Here's an example:

```python  theme={null}
"""LangGraph A2A conversational agent.

Supports the A2A protocol with messages input for conversational interactions.
"""

from __future__ import annotations

import os
from dataclasses import dataclass
from typing import Any, Dict, List, TypedDict

from langgraph.graph import StateGraph
from langgraph.runtime import Runtime
from openai import AsyncOpenAI

class Context(TypedDict):
    """Context parameters for the agent."""
    my_configurable_param: str

@dataclass
class State:
    """Input state for the agent.

Defines the initial structure for A2A conversational messages.
    """
    messages: List[Dict[str, Any]]

async def call_model(state: State, runtime: Runtime[Context]) -> Dict[str, Any]:
    """Process conversational messages and returns output using OpenAI."""
    # Initialize OpenAI client
    client = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# Process the incoming messages
    latest_message = state.messages[-1] if state.messages else {}
    user_content = latest_message.get("content", "No message content")

# Create messages for OpenAI API
    openai_messages = [
        {
            "role": "system",
            "content": "You are a helpful conversational agent. Keep responses brief and engaging."
        },
        {
            "role": "user",
            "content": user_content
        }
    ]

try:
        # Make OpenAI API call
        response = await client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=openai_messages,
            max_tokens=100,
            temperature=0.7
        )

ai_response = response.choices[0].message.content

except Exception as e:
        ai_response = f"I received your message but had trouble processing it. Error: {str(e)[:50]}..."

# Create a response message
    response_message = {
        "role": "assistant",
        "content": ai_response
    }

return {
        "messages": state.messages + [response_message]
    }

**Examples:**

Example 1 (unknown):
```unknown
GET /.well-known/agent-card.json?assistant_id={assistant_id}
```

Example 2 (unknown):
```unknown
## Usage overview

To enable A2A:

* Upgrade to use langgraph-api>=0.4.9.
* Deploy your agent with message-based state structure.
* Connect with other A2A-compatible agents using the endpoint.

## Creating an A2A-compatible agent

This example creates an A2A-compatible agent that processes incoming messages using OpenAI's API and maintains conversational state. The agent defines a message-based state structure and handles the A2A protocol's message format.

To be compatible with the [A2A "text" parts](https://a2a-protocol.org/dev/specification/#651-textpart-object), the agent must have a `messages` key in state. Here's an example:
```

---

## Access custom state fields

**URL:** llms-txt#access-custom-state-fields

@tool
def get_user_preference(
    pref_name: str,
    runtime: ToolRuntime  # ToolRuntime parameter is not visible to the model
) -> str:
    """Get a user preference value."""
    preferences = runtime.state.get("user_preferences", {})
    return preferences.get(pref_name, "Not set")
python  theme={null}
from langgraph.types import Command
from langchain.messages import RemoveMessage
from langgraph.graph.message import REMOVE_ALL_MESSAGES
from langchain.tools import tool, ToolRuntime

**Examples:**

Example 1 (unknown):
```unknown
<Warning>
  The `tool_runtime` parameter is hidden from the model. For the example above, the model only sees `pref_name` in the tool schema - `tool_runtime` is *not* included in the request.
</Warning>

**Updating state:**

Use [`Command`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.Command) to update the agent's state or control the graph's execution flow:
```

---

## Access memory

**URL:** llms-txt#access-memory

@tool
def get_user_info(user_id: str, runtime: ToolRuntime) -> str:
    """Look up user info."""
    store = runtime.store
    user_info = store.get(("users",), user_id)
    return str(user_info.value) if user_info else "Unknown user"

---

## Access the current conversation state

**URL:** llms-txt#access-the-current-conversation-state

@tool
def summarize_conversation(
    runtime: ToolRuntime
) -> str:
    """Summarize the conversation so far."""
    messages = runtime.state["messages"]

human_msgs = sum(1 for m in messages if m.__class__.__name__ == "HumanMessage")
    ai_msgs = sum(1 for m in messages if m.__class__.__name__ == "AIMessage")
    tool_msgs = sum(1 for m in messages if m.__class__.__name__ == "ToolMessage")

return f"Conversation has {human_msgs} user messages, {ai_msgs} AI responses, and {tool_msgs} tool results"

---

## Add custom authentication

**URL:** llms-txt#add-custom-authentication

**Contents:**
- Add custom authentication to your deployment
- Enable agent authentication
  - Authorizing a user for Studio

Source: https://docs.langchain.com/langsmith/custom-auth

This guide shows you how to add custom authentication to your LangSmith application. The steps on this page apply to both [cloud](/langsmith/cloud) and [self-hosted](/langsmith/self-hosted) deployments. It does not apply to isolated usage of the [LangGraph open source library](/oss/python/langgraph/overview) in your own custom server.

## Add custom authentication to your deployment

To leverage custom authentication and access user-level metadata in your deployments, set up custom authentication to automatically populate the `config["configurable"]["langgraph_auth_user"]` object through a custom authentication handler. You can then access this object in your graph with the `langgraph_auth_user` key to [allow an agent to perform authenticated actions on behalf of the user](#enable-agent-authentication).

1. Implement authentication:

<Note>
     Without a custom `@auth.authenticate` handler, LangGraph sees only the API-key owner (usually the developer), so requests aren’t scoped to individual end-users. To propagate custom tokens, you must implement your own handler.
   </Note>

* This handler receives the request (headers, etc.), validates the user, and returns a dictionary with at least an identity field.
* You can add any custom fields you want (e.g., OAuth tokens, roles, org IDs, etc.).

2. In your [`langgraph.json`](/langsmith/application-structure#configuration-file), add the path to your auth file:

3. Once you've set up authentication in your server, requests must include the required authorization information based on your chosen scheme. Assuming you are using JWT token authentication, you could access your deployments using any of the following methods:

<Tabs>
     <Tab title="Python Client">
       
     </Tab>

<Tab title="Python RemoteGraph">
       
     </Tab>

<Tab title="JavaScript Client">
       
     </Tab>

<Tab title="JavaScript RemoteGraph">
       
     </Tab>

<Tab title="CURL">
       
     </Tab>
   </Tabs>

For more details on RemoteGraph, refer to the [Use RemoteGraph](/langsmith/use-remote-graph) guide.

## Enable agent authentication

After [authentication](#add-custom-authentication-to-your-deployment), the platform creates a special configuration object (`config`) that is passed to LangSmith deployment. This object contains information about the current user, including any custom fields you return from your `@auth.authenticate` handler.

To allow an agent to perform authenticated actions on behalf of the user, access this object in your graph with the `langgraph_auth_user` key:

<Note>
  Fetch user credentials from a secure secret store. Storing secrets in graph state is not recommended.
</Note>

### Authorizing a user for Studio

By default, if you add custom authorization on your resources, this will also apply to interactions made from [Studio](/langsmith/studio). If you want, you can handle logged-in Studio users differently by checking [is\_studio\_user()](https://langchain-ai.github.io/langgraph/cloud/reference/sdk/python_sdk_ref/#langgraph_sdk.auth.types.StudioUser).

<Note>
  `is_studio_user` was added in version 0.1.73 of the langgraph-sdk. If you're on an older version, you can still check whether `isinstance(ctx.user, StudioUser)`.
</Note>

```python  theme={null}
from langgraph_sdk.auth import is_studio_user, Auth
auth = Auth()

**Examples:**

Example 1 (unknown):
```unknown
* This handler receives the request (headers, etc.), validates the user, and returns a dictionary with at least an identity field.
* You can add any custom fields you want (e.g., OAuth tokens, roles, org IDs, etc.).

2. In your [`langgraph.json`](/langsmith/application-structure#configuration-file), add the path to your auth file:
```

Example 2 (unknown):
```unknown
3. Once you've set up authentication in your server, requests must include the required authorization information based on your chosen scheme. Assuming you are using JWT token authentication, you could access your deployments using any of the following methods:

   <Tabs>
     <Tab title="Python Client">
```

Example 3 (unknown):
```unknown
</Tab>

     <Tab title="Python RemoteGraph">
```

Example 4 (unknown):
```unknown
</Tab>

     <Tab title="JavaScript Client">
```

---

## Add edges

**URL:** llms-txt#add-edges

**Contents:**
- Create branches
  - Run graph nodes in parallel
  - Defer node execution
  - Conditional branching
- Map-Reduce and the Send API

builder.add_edge(START, "step_1")
builder.add_edge("step_1", "step_2")
builder.add_edge("step_2", "step_3")
python  theme={null}
builder = StateGraph(State).add_sequence([step_1, step_2, step_3])
builder.add_edge(START, "step_1")
python  theme={null}
  from typing_extensions import TypedDict

class State(TypedDict):
      value_1: str
      value_2: int
  python  theme={null}
  def step_1(state: State):
      return {"value_1": "a"}

def step_2(state: State):
      current_value_1 = state["value_1"]
      return {"value_1": f"{current_value_1} b"}

def step_3(state: State):
      return {"value_2": 10}
  python  theme={null}
  from langgraph.graph import START, StateGraph

builder = StateGraph(State)

# Add nodes
  builder.add_node(step_1)
  builder.add_node(step_2)
  builder.add_node(step_3)

# Add edges
  builder.add_edge(START, "step_1")
  builder.add_edge("step_1", "step_2")
  builder.add_edge("step_2", "step_3")
  python  theme={null}
    builder.add_node("my_node", step_1)
    python  theme={null}
  graph = builder.compile()
  python  theme={null}
  from IPython.display import Image, display

display(Image(graph.get_graph().draw_mermaid_png()))
  python  theme={null}
  graph.invoke({"value_1": "c"})
  
  {'value_1': 'a b', 'value_2': 10}
  python  theme={null}
    builder = StateGraph(State).add_sequence([step_1, step_2, step_3])  # [!code highlight]
    builder.add_edge(START, "step_1")

graph = builder.compile()

graph.invoke({"value_1": "c"})
    python  theme={null}
import operator
from typing import Annotated, Any
from typing_extensions import TypedDict
from langgraph.graph import StateGraph, START, END

class State(TypedDict):
    # The operator.add reducer fn makes this append-only
    aggregate: Annotated[list, operator.add]

def a(state: State):
    print(f'Adding "A" to {state["aggregate"]}')
    return {"aggregate": ["A"]}

def b(state: State):
    print(f'Adding "B" to {state["aggregate"]}')
    return {"aggregate": ["B"]}

def c(state: State):
    print(f'Adding "C" to {state["aggregate"]}')
    return {"aggregate": ["C"]}

def d(state: State):
    print(f'Adding "D" to {state["aggregate"]}')
    return {"aggregate": ["D"]}

builder = StateGraph(State)
builder.add_node(a)
builder.add_node(b)
builder.add_node(c)
builder.add_node(d)
builder.add_edge(START, "a")
builder.add_edge("a", "b")
builder.add_edge("a", "c")
builder.add_edge("b", "d")
builder.add_edge("c", "d")
builder.add_edge("d", END)
graph = builder.compile()
python  theme={null}
from IPython.display import Image, display

display(Image(graph.get_graph().draw_mermaid_png()))
python  theme={null}
graph.invoke({"aggregate": []}, {"configurable": {"thread_id": "foo"}})

Adding "A" to []
Adding "B" to ['A']
Adding "C" to ['A']
Adding "D" to ['A', 'B', 'C']
python  theme={null}
  graph.invoke({"value_1": "c"}, {"configurable": {"max_concurrency": 10}})
  python  theme={null}
import operator
from typing import Annotated, Any
from typing_extensions import TypedDict
from langgraph.graph import StateGraph, START, END

class State(TypedDict):
    # The operator.add reducer fn makes this append-only
    aggregate: Annotated[list, operator.add]

def a(state: State):
    print(f'Adding "A" to {state["aggregate"]}')
    return {"aggregate": ["A"]}

def b(state: State):
    print(f'Adding "B" to {state["aggregate"]}')
    return {"aggregate": ["B"]}

def b_2(state: State):
    print(f'Adding "B_2" to {state["aggregate"]}')
    return {"aggregate": ["B_2"]}

def c(state: State):
    print(f'Adding "C" to {state["aggregate"]}')
    return {"aggregate": ["C"]}

def d(state: State):
    print(f'Adding "D" to {state["aggregate"]}')
    return {"aggregate": ["D"]}

builder = StateGraph(State)
builder.add_node(a)
builder.add_node(b)
builder.add_node(b_2)
builder.add_node(c)
builder.add_node(d, defer=True)  # [!code highlight]
builder.add_edge(START, "a")
builder.add_edge("a", "b")
builder.add_edge("a", "c")
builder.add_edge("b", "b_2")
builder.add_edge("b_2", "d")
builder.add_edge("c", "d")
builder.add_edge("d", END)
graph = builder.compile()
python  theme={null}
from IPython.display import Image, display

display(Image(graph.get_graph().draw_mermaid_png()))
python  theme={null}
graph.invoke({"aggregate": []})

Adding "A" to []
Adding "B" to ['A']
Adding "C" to ['A']
Adding "B_2" to ['A', 'B', 'C']
Adding "D" to ['A', 'B', 'C', 'B_2']
python  theme={null}
import operator
from typing import Annotated, Literal, Sequence
from typing_extensions import TypedDict
from langgraph.graph import StateGraph, START, END

class State(TypedDict):
    aggregate: Annotated[list, operator.add]
    # Add a key to the state. We will set this key to determine
    # how we branch.
    which: str

def a(state: State):
    print(f'Adding "A" to {state["aggregate"]}')
    return {"aggregate": ["A"], "which": "c"}  # [!code highlight]

def b(state: State):
    print(f'Adding "B" to {state["aggregate"]}')
    return {"aggregate": ["B"]}

def c(state: State):
    print(f'Adding "C" to {state["aggregate"]}')
    return {"aggregate": ["C"]}

builder = StateGraph(State)
builder.add_node(a)
builder.add_node(b)
builder.add_node(c)
builder.add_edge(START, "a")
builder.add_edge("b", END)
builder.add_edge("c", END)

def conditional_edge(state: State) -> Literal["b", "c"]:
    # Fill in arbitrary logic here that uses the state
    # to determine the next node
    return state["which"]

builder.add_conditional_edges("a", conditional_edge)  # [!code highlight]

graph = builder.compile()
python  theme={null}
from IPython.display import Image, display

display(Image(graph.get_graph().draw_mermaid_png()))
python  theme={null}
result = graph.invoke({"aggregate": []})
print(result)

Adding "A" to []
Adding "C" to ['A']
{'aggregate': ['A', 'C'], 'which': 'c'}
python  theme={null}
  def route_bc_or_cd(state: State) -> Sequence[str]:
      if state["which"] == "cd":
          return ["c", "d"]
      return ["b", "c"]
  python  theme={null}
from langgraph.graph import StateGraph, START, END
from langgraph.types import Send
from typing_extensions import TypedDict, Annotated
import operator

class OverallState(TypedDict):
    topic: str
    subjects: list[str]
    jokes: Annotated[list[str], operator.add]
    best_selected_joke: str

def generate_topics(state: OverallState):
    return {"subjects": ["lions", "elephants", "penguins"]}

def generate_joke(state: OverallState):
    joke_map = {
        "lions": "Why don't lions like fast food? Because they can't catch it!",
        "elephants": "Why don't elephants use computers? They're afraid of the mouse!",
        "penguins": "Why don't penguins like talking to strangers at parties? Because they find it hard to break the ice."
    }
    return {"jokes": [joke_map[state["subject"]]]}

def continue_to_jokes(state: OverallState):
    return [Send("generate_joke", {"subject": s}) for s in state["subjects"]]

def best_joke(state: OverallState):
    return {"best_selected_joke": "penguins"}

builder = StateGraph(OverallState)
builder.add_node("generate_topics", generate_topics)
builder.add_node("generate_joke", generate_joke)
builder.add_node("best_joke", best_joke)
builder.add_edge(START, "generate_topics")
builder.add_conditional_edges("generate_topics", continue_to_jokes, ["generate_joke"])
builder.add_edge("generate_joke", "best_joke")
builder.add_edge("best_joke", END)
graph = builder.compile()
python  theme={null}
from IPython.display import Image, display

display(Image(graph.get_graph().draw_mermaid_png()))
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
We can also use the built-in shorthand `.add_sequence`:
```

Example 2 (unknown):
```unknown
<Accordion title="Why split application steps into a sequence with LangGraph?">
  LangGraph makes it easy to add an underlying persistence layer to your application.
  This allows state to be checkpointed in between the execution of nodes, so your LangGraph nodes govern:

  * How state updates are [checkpointed](/oss/python/langgraph/persistence)
  * How interruptions are resumed in [human-in-the-loop](/oss/python/langgraph/interrupts) workflows
  * How we can "rewind" and branch-off executions using LangGraph's [time travel](/oss/python/langgraph/use-time-travel) features

  They also determine how execution steps are [streamed](/oss/python/langgraph/streaming), and how your application is visualized and debugged using [Studio](/langsmith/studio).

  Let's demonstrate an end-to-end example. We will create a sequence of three steps:

  1. Populate a value in a key of the state
  2. Update the same value
  3. Populate a different value

  Let's first define our [state](/oss/python/langgraph/graph-api#state). This governs the [schema of the graph](/oss/python/langgraph/graph-api#schema), and can also specify how to apply updates. See [this section](#process-state-updates-with-reducers) for more detail.

  In our case, we will just keep track of two values:
```

Example 3 (unknown):
```unknown
Our [nodes](/oss/python/langgraph/graph-api#nodes) are just Python functions that read our graph's state and make updates to it. The first argument to this function will always be the state:
```

Example 4 (unknown):
```unknown
<Note>
    Note that when issuing updates to the state, each node can just specify the value of the key it wishes to update.

    By default, this will **overwrite** the value of the corresponding key. You can also use [reducers](/oss/python/langgraph/graph-api#reducers) to control how updates are processed— for example, you can append successive updates to a key instead. See [this section](#process-state-updates-with-reducers) for more detail.
  </Note>

  Finally, we define the graph. We use [StateGraph](/oss/python/langgraph/graph-api#stategraph) to define a graph that operates on this state.

  We will then use [`add_node`](/oss/python/langgraph/graph-api#messagesstate) and [`add_edge`](/oss/python/langgraph/graph-api#edges) to populate our graph and define its control flow.
```

---

## Add edges to connect nodes

**URL:** llms-txt#add-edges-to-connect-nodes

orchestrator_worker_builder.add_edge(START, "orchestrator")
orchestrator_worker_builder.add_conditional_edges(
    "orchestrator", assign_workers, ["llm_call"]
)
orchestrator_worker_builder.add_edge("llm_call", "synthesizer")
orchestrator_worker_builder.add_edge("synthesizer", END)

---

## Add metadata and tags to traces

**URL:** llms-txt#add-metadata-and-tags-to-traces

Source: https://docs.langchain.com/langsmith/add-metadata-tags

LangSmith supports sending arbitrary metadata and tags along with traces.

Tags are strings that can be used to categorize or label a trace. Metadata is a dictionary of key-value pairs that can be used to store additional information about a trace.

Both are useful for associating additional information with a trace, such as the environment in which it was executed, the user who initiated it, or an internal correlation ID. For more information on tags and metadata, see the [Concepts](/langsmith/observability-concepts#tags) page. For information on how to query traces and runs by metadata and tags, see the [Filter traces in the application](/langsmith/filter-traces-in-application) page.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/add-metadata-tags.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

---

## Add nodes

**URL:** llms-txt#add-nodes

workflow.add_node("generate_topic", generate_topic)
workflow.add_node("write_joke", write_joke)

---

## Add OtelSpanProcessor to the tracer provider

**URL:** llms-txt#add-otelspanprocessor-to-the-tracer-provider

tracer_provider.add_span_processor(OtelSpanProcessor())

---

## Add the function to the kernel

**URL:** llms-txt#add-the-function-to-the-kernel

code_analyzer = kernel.add_function(
    function_name="analyzeCode",
    plugin_name="codeAnalysisPlugin",
    prompt_template_config=prompt_template_config,
)

---

## Add the middleware to the app

**URL:** llms-txt#add-the-middleware-to-the-app

**Contents:**
- Configure `langgraph.json`
  - Customize middleware ordering
- Start server
- Deploying
- Next steps

app.add_middleware(CustomHeaderMiddleware)
json  theme={null}
{
  "dependencies": ["."],
  "graphs": {
    "agent": "./src/agent/graph.py:graph"
  },
  "env": ".env",
  "http": {
    "app": "./src/agent/webapp.py:app"
  }
  // Other configuration options like auth, store, etc.
}
json  theme={null}
{
  "dependencies": ["."],
  "graphs": {
    "agent": "./src/agent/graph.py:graph"
  },
  "env": ".env",
  "http": {
    "app": "./src/agent/webapp.py:app",
    "middleware_order": "auth_first"
  },
  "auth": {
    "path": "./auth.py:my_auth"
  }
}
bash  theme={null}
langgraph dev --no-browser
```

Now any request to your server will include the custom header `X-Custom-Header` in its response.

You can deploy this app as-is to cloud or to your self-hosted platform.

Now that you've added custom middleware to your deployment, you can use similar techniques to add [custom routes](/langsmith/custom-routes) or define [custom lifespan events](/langsmith/custom-lifespan) to further customize your server's behavior.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/custom-middleware.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
## Configure `langgraph.json`

Add the following to your `langgraph.json` configuration file. Make sure the path points to the `webapp.py` file you created above.
```

Example 2 (unknown):
```unknown
### Customize middleware ordering

By default, custom middleware runs before authentication logic. To run custom middleware *after* authentication, set `middleware_order` to `auth_first` in your `http` configuration. (This customization is supported starting with API server v0.4.35 and later.)
```

Example 3 (unknown):
```unknown
## Start server

Test the server out locally:
```

---

## Add the nodes

**URL:** llms-txt#add-the-nodes

orchestrator_worker_builder.add_node("orchestrator", orchestrator)
orchestrator_worker_builder.add_node("llm_call", llm_call)
orchestrator_worker_builder.add_node("synthesizer", synthesizer)

---

## Add to conversation history

**URL:** llms-txt#add-to-conversation-history

**Contents:**
  - Tool Message

messages = [
    SystemMessage("You are a helpful assistant"),
    HumanMessage("Can you help me?"),
    ai_msg,  # Insert as if it came from the model
    HumanMessage("Great! What's 2+2?")
]

response = model.invoke(messages)
python  theme={null}
from langchain.chat_models import init_chat_model

model = init_chat_model("gpt-5-nano")

def get_weather(location: str) -> str:
    """Get the weather at a location."""
    ...

model_with_tools = model.bind_tools([get_weather])
response = model_with_tools.invoke("What's the weather in Paris?")

for tool_call in response.tool_calls:
    print(f"Tool: {tool_call['name']}")
    print(f"Args: {tool_call['args']}")
    print(f"ID: {tool_call['id']}")
python  theme={null}
from langchain.chat_models import init_chat_model

model = init_chat_model("gpt-5-nano")

response = model.invoke("Hello!")
response.usage_metadata

{'input_tokens': 8,
 'output_tokens': 304,
 'total_tokens': 312,
 'input_token_details': {'audio': 0, 'cache_read': 0},
 'output_token_details': {'audio': 0, 'reasoning': 256}}
python  theme={null}
chunks = []
full_message = None
for chunk in model.stream("Hi"):
    chunks.append(chunk)
    print(chunk.text)
    full_message = chunk if full_message is None else full_message + chunk
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
<Accordion title="Attributes">
  <ParamField path="text" type="string">
    The text content of the message.
  </ParamField>

  <ParamField path="content" type="string | dict[]">
    The raw content of the message.
  </ParamField>

  <ParamField path="content_blocks" type="ContentBlock[]">
    The standardized [content blocks](#message-content) of the message.
  </ParamField>

  <ParamField path="tool_calls" type="dict[] | None">
    The tool calls made by the model. Empty if no tools are called.
  </ParamField>

  <ParamField path="id" type="string">
    A unique identifier for the message (either automatically generated by LangChain or returned in the provider response)
  </ParamField>

  <ParamField path="usage_metadata" type="dict | None">
    The usage metadata of the message, which can contain token counts when available.
  </ParamField>

  <ParamField path="response_metadata" type="ResponseMetadata | None">
    The response metadata of the message.
  </ParamField>
</Accordion>

#### Tool calls

When models make [tool calls](/oss/python/langchain/models#tool-calling), they're included in the [`AIMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessage):
```

Example 2 (unknown):
```unknown
Other structured data, such as reasoning or citations, can also appear in message [content](/oss/python/langchain/messages#message-content).

#### Token usage

An [`AIMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessage) can hold token counts and other usage metadata in its [`usage_metadata`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.UsageMetadata) field:
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
See [`UsageMetadata`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.UsageMetadata) for details.

#### Streaming and chunks

During streaming, you'll receive [`AIMessageChunk`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessageChunk) objects that can be combined into a full message object:
```

---

## After a model makes a tool call

**URL:** llms-txt#after-a-model-makes-a-tool-call

ai_message = AIMessage(
    content=[],
    tool_calls=[{
        "name": "get_weather",
        "args": {"location": "San Francisco"},
        "id": "call_123"
    }]
)

---

## After model hook

**URL:** llms-txt#after-model-hook

@after_model
def log_after_model(state: AgentState, runtime: Runtime[Context]) -> dict | None:  # [!code highlight]
    print(f"Completed request for user: {runtime.context.user_name}")  # [!code highlight]
    return None

agent = create_agent(
    model="gpt-5-nano",
    tools=[...],
    middleware=[dynamic_system_prompt, log_before_model, log_after_model],  # [!code highlight]
    context_schema=Context
)

agent.invoke(
    {"messages": [{"role": "user", "content": "What's my name?"}]},
    context=Context(user_name="John Smith")
)
```

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/runtime.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Agents

**URL:** llms-txt#agents

**Contents:**
- Core components
  - Model
  - Tools
  - System prompt
- Invocation
- Advanced concepts
  - Structured output
  - Memory
  - Streaming
  - Middleware

Source: https://docs.langchain.com/oss/javascript/langchain/agents

Agents combine language models with [tools](/oss/javascript/langchain/tools) to create systems that can reason about tasks, decide which tools to use, and iteratively work towards solutions.

`createAgent()` provides a production-ready agent implementation.

[An LLM Agent runs tools in a loop to achieve a goal](https://simonwillison.net/2025/Sep/18/agents/).
An agent runs until a stop condition is met - i.e., when the model emits a final output or an iteration limit is reached.

<Info>
  `createAgent()` builds a **graph**-based agent runtime using [LangGraph](/oss/javascript/langgraph/overview). A graph consists of nodes (steps) and edges (connections) that define how your agent processes information. The agent moves through this graph, executing nodes like the model node (which calls the model), the tools node (which executes tools), or middleware.

Learn more about the [Graph API](/oss/javascript/langgraph/graph-api).
</Info>

The [model](/oss/javascript/langchain/models) is the reasoning engine of your agent. It can be specified in multiple ways, supporting both static and dynamic model selection.

Static models are configured once when creating the agent and remain unchanged throughout execution. This is the most common and straightforward approach.

To initialize a static model from a <Tooltip tip="A string that follows the format `provider:model` (e.g. openai:gpt-5)" cta="See mappings" href="https://reference.langchain.com/python/langchain/models/#langchain.chat_models.init_chat_model(model_provider)">model identifier string</Tooltip>:

Model identifier strings use the format `provider:model` (e.g. `"openai:gpt-5"`). You may want more control over the model configuration, in which case you can initialize a model instance directly using the provider package:

Model instances give you complete control over configuration. Use them when you need to set specific parameters like `temperature`, `max_tokens`, `timeouts`, or configure API keys, `base_url`, and other provider-specific settings. Refer to the [API reference](/oss/javascript/integrations/providers/) to see available params and methods on your model.

Dynamic models are selected at <Tooltip tip="The execution environment of your agent, containing immutable configuration and contextual data that persists throughout the agent's execution (e.g., user IDs, session details, or application-specific configuration).">runtime</Tooltip> based on the current <Tooltip tip="The data that flows through your agent's execution, including messages, custom fields, and any information that needs to be tracked and potentially modified during processing (e.g., user preferences or tool usage stats).">state</Tooltip> and context. This enables sophisticated routing logic and cost optimization.

To use a dynamic model, create middleware with `wrapModelCall` that modifies the model in the request:

For more details on middleware and advanced patterns, see the [middleware documentation](/oss/javascript/langchain/middleware).

<Tip>
  For model configuration details, see [Models](/oss/javascript/langchain/models). For dynamic model selection patterns, see [Dynamic model in middleware](/oss/javascript/langchain/middleware#dynamic-model).
</Tip>

Tools give agents the ability to take actions. Agents go beyond simple model-only tool binding by facilitating:

* Multiple tool calls in sequence (triggered by a single prompt)
* Parallel tool calls when appropriate
* Dynamic tool selection based on previous results
* Tool retry logic and error handling
* State persistence across tool calls

For more information, see [Tools](/oss/javascript/langchain/tools).

Pass a list of tools to the agent.

If an empty tool list is provided, the agent will consist of a single LLM node without tool-calling capabilities.

#### Tool error handling

To customize how tool errors are handled, use the `wrapToolCall` hook in a custom middleware:

The agent will return a @\[`ToolMessage`] with the custom error message when a tool fails.

#### Tool use in the ReAct loop

Agents follow the ReAct ("Reasoning + Acting") pattern, alternating between brief reasoning steps with targeted tool calls and feeding the resulting observations into subsequent decisions until they can deliver a final answer.

<Accordion title="Example of ReAct loop">
  Prompt: Identify the current most popular wireless headphones and verify availability.

* **Reasoning**: "Popularity is time-sensitive, I need to use the provided search tool."
  * **Acting**: Call `search_products("wireless headphones")`

* **Reasoning**: "I need to confirm availability for the top-ranked item before answering."
  * **Acting**: Call `check_inventory("WH-1000XM5")`

* **Reasoning**: "I have the most popular model and its stock status. I can now answer the user's question."
  * **Acting**: Produce final answer

<Tip>
  To learn more about tools, see [Tools](/oss/javascript/langchain/tools).
</Tip>

You can shape how your agent approaches tasks by providing a prompt. The `systemPrompt` parameter can be provided as a string:

When no @\[`system_prompt`] is provided, the agent will infer its task from the messages directly.

#### Dynamic system prompt

For more advanced use cases where you need to modify the system prompt based on runtime context or agent state, you can use [middleware](/oss/javascript/langchain/middleware).

<Tip>
  For more details on message types and formatting, see [Messages](/oss/javascript/langchain/messages). For comprehensive middleware documentation, see [Middleware](/oss/javascript/langchain/middleware).
</Tip>

You can invoke an agent by passing an update to its [`State`](/oss/javascript/langgraph/graph-api#state). All agents include a [sequence of messages](/oss/javascript/langgraph/use-graph-api#messagesstate) in their state; to invoke the agent, pass a new message:

For streaming steps and / or tokens from the agent, refer to the [streaming](/oss/javascript/langchain/streaming) guide.

Otherwise, the agent follows the LangGraph [Graph API](/oss/javascript/langgraph/use-graph-api) and supports all associated methods.

### Structured output

In some situations, you may want the agent to return an output in a specific format. LangChain provides a simple, universal way to do this with the `responseFormat` parameter.

<Tip>
  To learn about structured output, see [Structured output](/oss/javascript/langchain/structured-output).
</Tip>

Agents maintain conversation history automatically through the message state. You can also configure the agent to use a custom state schema to remember additional information during the conversation.

Information stored in the state can be thought of as the [short-term memory](/oss/javascript/langchain/short-term-memory) of the agent:

<Tip>
  To learn more about memory, see [Memory](/oss/javascript/concepts/memory). For information on implementing long-term memory that persists across sessions, see [Long-term memory](/oss/javascript/langchain/long-term-memory).
</Tip>

We've seen how the agent can be called with `invoke` to get a final response. If the agent executes multiple steps, this may take a while. To show intermediate progress, we can stream back messages as they occur.

<Tip>
  For more details on streaming, see [Streaming](/oss/javascript/langchain/streaming).
</Tip>

[Middleware](/oss/javascript/langchain/middleware) provides powerful extensibility for customizing agent behavior at different stages of execution. You can use middleware to:

* Process state before the model is called (e.g., message trimming, context injection)
* Modify or validate the model's response (e.g., guardrails, content filtering)
* Handle tool execution errors with custom logic
* Implement dynamic model selection based on state or context
* Add custom logging, monitoring, or analytics

Middleware integrates seamlessly into the agent's execution graph, allowing you to intercept and modify data flow at key points without changing the core agent logic.

<Tip>
  For comprehensive middleware documentation including hooks like `beforeModel`, `afterModel`, and `wrapToolCall`, see [Middleware](/oss/javascript/langchain/middleware).
</Tip>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/agents.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
<Info>
  `createAgent()` builds a **graph**-based agent runtime using [LangGraph](/oss/javascript/langgraph/overview). A graph consists of nodes (steps) and edges (connections) that define how your agent processes information. The agent moves through this graph, executing nodes like the model node (which calls the model), the tools node (which executes tools), or middleware.

  Learn more about the [Graph API](/oss/javascript/langgraph/graph-api).
</Info>

## Core components

### Model

The [model](/oss/javascript/langchain/models) is the reasoning engine of your agent. It can be specified in multiple ways, supporting both static and dynamic model selection.

#### Static model

Static models are configured once when creating the agent and remain unchanged throughout execution. This is the most common and straightforward approach.

To initialize a static model from a <Tooltip tip="A string that follows the format `provider:model` (e.g. openai:gpt-5)" cta="See mappings" href="https://reference.langchain.com/python/langchain/models/#langchain.chat_models.init_chat_model(model_provider)">model identifier string</Tooltip>:
```

Example 2 (unknown):
```unknown
Model identifier strings use the format `provider:model` (e.g. `"openai:gpt-5"`). You may want more control over the model configuration, in which case you can initialize a model instance directly using the provider package:
```

Example 3 (unknown):
```unknown
Model instances give you complete control over configuration. Use them when you need to set specific parameters like `temperature`, `max_tokens`, `timeouts`, or configure API keys, `base_url`, and other provider-specific settings. Refer to the [API reference](/oss/javascript/integrations/providers/) to see available params and methods on your model.

#### Dynamic model

Dynamic models are selected at <Tooltip tip="The execution environment of your agent, containing immutable configuration and contextual data that persists throughout the agent's execution (e.g., user IDs, session details, or application-specific configuration).">runtime</Tooltip> based on the current <Tooltip tip="The data that flows through your agent's execution, including messages, custom fields, and any information that needs to be tracked and potentially modified during processing (e.g., user preferences or tool usage stats).">state</Tooltip> and context. This enables sophisticated routing logic and cost optimization.

To use a dynamic model, create middleware with `wrapModelCall` that modifies the model in the request:
```

Example 4 (unknown):
```unknown
For more details on middleware and advanced patterns, see the [middleware documentation](/oss/javascript/langchain/middleware).

<Tip>
  For model configuration details, see [Models](/oss/javascript/langchain/models). For dynamic model selection patterns, see [Dynamic model in middleware](/oss/javascript/langchain/middleware#dynamic-model).
</Tip>

### Tools

Tools give agents the ability to take actions. Agents go beyond simple model-only tool binding by facilitating:

* Multiple tool calls in sequence (triggered by a single prompt)
* Parallel tool calls when appropriate
* Dynamic tool selection based on previous results
* Tool retry logic and error handling
* State persistence across tool calls

For more information, see [Tools](/oss/javascript/langchain/tools).

#### Defining tools

Pass a list of tools to the agent.
```

---

## Agent Builder

**URL:** llms-txt#agent-builder

**Contents:**
- Memory and updates
- Triggers
- Sub-agents
- Human in the loop
  - Enabling interrupts
  - Actions on interrupts

Source: https://docs.langchain.com/langsmith/agent-builder

<Callout icon="wand-magic-sparkles" color="#2563EB" iconType="regular">
  Agent Builder is in private preview. Sign up for the waitlist [today](https://www.langchain.com/langsmith-agent-builder-waitlist).
</Callout>

Agent Builder lets you turn natural-language ideas into production agents. It's powered by [deep-agents](https://github.com/langchain-ai/deepagents), and is not <Tooltip tip="Predetermined code paths that are designed to operate in a certain order.">workflow based</Tooltip>.

## Memory and updates

Agent Builder includes persistent agent memory and supports self-updates. This lets agents adapt over time and refine how they work without manual edits.

* Persistent memory: Agents retain relevant information across runs to inform future decisions.
* What can be updated: Tools (add, remove, or reconfigure), and instructions/system prompts.
* Agents cannot modify their name, description, and/or triggers attached.

Triggers define when your agent should start running. You can connect your agent to external tools or time-based schedules, letting it respond automatically to messages, emails, or recurring events.

The following examples show some of the apps you can use to trigger your agent:

<CardGroup cols={3}>
  <Card title="Slack" icon="slack">
    Activate your agent when messages are received in specific Slack channels.
  </Card>

<Card title="Gmail" icon="envelope">
    Trigger your agent when emails are received.
  </Card>

<Card title="Cron schedules" icon="clock">
    Run your agent on a time-based schedule for recurring tasks.
  </Card>
</CardGroup>

Agent Builder lets you create sub-agents within a main agent. Sub-agents are smaller, specialized agents that handle specific parts of a larger task. They can operate with their own tools, permissions, or goals while coordinating with the main agent.

Using sub-agents makes it easier to build complex systems by dividing work into focused, reusable components. This modular approach helps keep your agents organized, scalable, and easier to maintain.

Below are a few ways sub-agents can be used in your projects:

* Handle distinct parts of a broader workflow (for example, data retrieval, summarization, or formatting).
* Use different tools or context windows for specialized tasks.
* Run independently but report results back to the main agent.

Human-in-the-loop functionality allows you to review and approve agent actions before they execute, giving you control over critical decisions.

### Enabling interrupts

<Steps>
  <Step title="Select a tool">
    When configuring your agent in Agent Builder, select the tool you want to add human oversight to.
  </Step>

<Step title="Enable interrupts">
    Look for the interrupt option when selecting the tool and toggle it on.
  </Step>

<Step title="Agent pauses for approval">
    The agent will pause and wait for human approval before executing that tool.
  </Step>
</Steps>

### Actions on interrupts

When your agent reaches an interrupt point, you can take one of three actions:

<CardGroup cols={3}>
  <Card title="Accept" icon="check">
    Approve the agent's proposed action and allow it to proceed as planned.
  </Card>

<Card title="Edit" icon="pen-to-square">
    Modify the agent's message or parameters before allowing it to continue.
  </Card>

<Card title="Send feedback" icon="comment">
    Provide feedback to the agent.
  </Card>
</CardGroup>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/agent-builder.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Agent Builder setup

**URL:** llms-txt#agent-builder-setup

**Contents:**
- How to add workspace secrets
- Required model key
- Optional tool keys

Source: https://docs.langchain.com/langsmith/agent-builder-setup

Add required workspace secrets for models and tools used by Agent Builder.

This page lists the workspace secrets you need to add before using Agent Builder. Add these in your LangSmith workspace settings under Secrets. Keep values scoped to your workspace and avoid placing credentials in prompts or code.

## How to add workspace secrets

In the [LangSmith UI](https://smith.langchain.com), ensure that your Anthropic API key is set as a [workspace secret](/langsmith/administration-overview#workspace-secrets).

1. Navigate to <Icon icon="gear" /> **Settings** and then move to the **Secrets** tab.
2. Select **Add secret** and enter the `ANTHROPIC_API_KEY` and your API key as the **Value**.
3. Select **Save secret**.

<Note> When adding workspace secrets in the LangSmith UI, make sure the secret keys match the environment variable names expected by your model provider.</Note>

## Required model key

* `ANTHROPIC_API_KEY`: Required for Agent Builder models. The agent graphs load this key from workspace secrets for inference.

## Optional tool keys

Add keys for any tools you enable. These are read from workspace secrets at runtime.

* `EXA_API_KEY`: Required for Exa search tools (general web and LinkedIn profile search).
* `TAVILY_API_KEY`: Required for Tavily web search.
* `TWITTER_API_KEY` and `TWITTER_API_KEY_SECRET`: Required for Twitter/X read operations (app‑only bearer). Posting/media upload is not enabled.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/agent-builder-setup.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Agent building

**URL:** llms-txt#agent-building

from langchain.agents import create_agent

---

## Agent can read /memories/preferences.txt from the first thread

**URL:** llms-txt#agent-can-read-/memories/preferences.txt-from-the-first-thread

**Contents:**
- Use cases
  - User preferences
  - Self-improving instructions
  - Knowledge base

python  theme={null}
agent = create_deep_agent(
    store=InMemoryStore(),
    backend=lambda rt: CompositeBackend(
        default=StateBackend(rt),
        routes={"/memories/": StoreBackend(rt)}
    ),
    system_prompt="""When users tell you their preferences, save them to
    /memories/user_preferences.txt so you remember them in future conversations."""
)
python  theme={null}
agent = create_deep_agent(
    store=InMemoryStore(),
    backend=lambda rt: CompositeBackend(
        default=StateBackend(rt),
        routes={"/memories/": StoreBackend(rt)}
    ),
    system_prompt="""You have a file at /memories/instructions.txt with additional
    instructions and preferences.

Read this file at the start of conversations to understand user preferences.

When users provide feedback like "please always do X" or "I prefer Y",
    update /memories/instructions.txt using the edit_file tool."""
)
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
## Use cases

### User preferences

Store user preferences that persist across sessions:
```

Example 2 (unknown):
```unknown
### Self-improving instructions

An agent can update its own instructions based on feedback:
```

Example 3 (unknown):
```unknown
Over time, the instructions file accumulates user preferences, helping the agent improve.

### Knowledge base

Build up knowledge over multiple conversations:
```

---

## Agent Chat UI

**URL:** llms-txt#agent-chat-ui

**Contents:**
- Agent Chat UI
  - Features
  - Quick start
  - Local development
  - Connect to your agent

Source: https://docs.langchain.com/oss/javascript/langgraph/ui

LangChain provides a powerful prebuilt user interface that work seamlessly with agents created using [`create_agent`](/oss/javascript/langchain/agents). This UI is designed to provide rich, interactive experiences for your agents with minimal setup, whether you're running locally or in a deployed context (such as [LangSmith](/langsmith/)).

[Agent Chat UI](https://github.com/langchain-ai/agent-chat-ui) is a Next.js application that provides a conversational interface for interacting with any LangChain agent. It supports real-time chat, tool visualization, and advanced features like time-travel debugging and state forking.

Agent Chat UI is open source and can be adapted to your application needs.

<Frame>
  <iframe className="w-full aspect-video rounded-xl" src="https://www.youtube.com/embed/lInrwVnZ83o?si=Uw66mPtCERJm0EjU" title="Agent Chat UI" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowFullScreen />
</Frame>

<Accordion title="Tool visualization">
  Studio automatically renders tool calls and results in an intuitive interface.

<Frame>
        <img src="https://mintcdn.com/langchain-5e9cc07a/zA84oCipUuW8ow2z/oss/images/studio_tools.gif?s=64e762e917f092960472b61a862a81cb" alt="Tool visualization in Studio" data-og-width="1280" width="1280" data-og-height="833" height="833" data-path="oss/images/studio_tools.gif" data-optimize="true" data-opv="3" />
  </Frame>
</Accordion>

<Accordion title="Time-travel debugging">
  Navigate through conversation history and fork from any point

<Frame>
        <img src="https://mintcdn.com/langchain-5e9cc07a/TCDks4pdsHdxWmuJ/oss/images/studio_fork.gif?s=0bb5a397d4b2ed3ff8ec62b9d0f92e3e" alt="Time-travel debugging in Studio" data-og-width="1280" width="1280" data-og-height="833" height="833" data-path="oss/images/studio_fork.gif" data-optimize="true" data-opv="3" />
  </Frame>
</Accordion>

<Accordion title="State inspection">
  View and modify agent state at any point during execution

<Frame>
        <img src="https://mintcdn.com/langchain-5e9cc07a/zA84oCipUuW8ow2z/oss/images/studio_state.gif?s=908d69765b0655cb532620c6e0fa96c8" alt="State inspection in Studio" data-og-width="1280" width="1280" data-og-height="833" height="833" data-path="oss/images/studio_state.gif" data-optimize="true" data-opv="3" />
  </Frame>
</Accordion>

<Accordion title="Human-in-the-loop">
  Built-in support for reviewing and responding to agent requests

<Frame>
        <img src="https://mintcdn.com/langchain-5e9cc07a/TCDks4pdsHdxWmuJ/oss/images/studio_hitl.gif?s=ce7ce6378caf4db29ea6062b9aff0220" alt="Human-in-the-Loop in Studio" data-og-width="1280" width="1280" data-og-height="833" height="833" data-path="oss/images/studio_hitl.gif" data-optimize="true" data-opv="3" />
  </Frame>
</Accordion>

<Tip>
  You can use generative UI in the Agent Chat UI. For more information, see [Implement generative user interfaces with LangGraph](/langsmith/generative-ui-react).
</Tip>

The fastest way to get started is using the hosted version:

1. **Visit [Agent Chat UI](https://agentchat.vercel.app)**
2. **Connect your agent** by entering your deployment URL or local server address
3. **Start chatting** - the UI will automatically detect and render tool calls and interrupts

### Local development

For customization or local development, you can run Agent Chat UI locally:

### Connect to your agent

Agent Chat UI can connect to both [local](/oss/javascript/langgraph/studio#setup-local-agent-server) and [deployed agents](/oss/javascript/langgraph/deploy).

After starting Agent Chat UI, you'll need to configure it to connect to your agent:

1. **Graph ID**: Enter your graph name (find this under `graphs` in your `langgraph.json` file)
2. **Deployment URL**: Your Agent server's endpoint (e.g., `http://localhost:2024` for local development, or your deployed agent's URL)
3. **LangSmith API key (optional)**: Add your LangSmith API key (not required if you're using a local Agent server)

Once configured, Agent Chat UI will automatically fetch and display any interrupted threads from your agent.

<Tip>
  Agent Chat UI has out-of-the-box support for rendering tool calls and tool result messages. To customize what messages are shown, see [Hiding Messages in the Chat](https://github.com/langchain-ai/agent-chat-ui?tab=readme-ov-file#hiding-messages-in-the-chat).
</Tip>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/ui.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

---

## Agent definition

**URL:** llms-txt#agent-definition

graph_builder = StateGraph(State)
graph_builder.add_node(intent_classifier)

---

## Agent harness capabilities

**URL:** llms-txt#agent-harness-capabilities

**Contents:**
- File system access
- Large tool result eviction
- Pluggable storage backends
- Task delegation (subagents)
- Conversation history summarization
- Dangling tool call repair
- To-do list tracking
- Human-in-the-Loop
- Prompt caching (Anthropic)

Source: https://docs.langchain.com/oss/javascript/deepagents/harness

We think of `deepagents` as an "agent harness". It is the same core tool calling loop as other agent frameworks, but with built-in tools and capabilities.

This page lists out the components that make up the agent harness.

## File system access

The harness provides six tools for file system operations, making files first-class citizens in the agent's environment:

| Tool         | Description                                                                                   |
| ------------ | --------------------------------------------------------------------------------------------- |
| `ls`         | List files in a directory with metadata (size, modified time)                                 |
| `read_file`  | Read file contents with line numbers, supports offset/limit for large files                   |
| `write_file` | Create new files                                                                              |
| `edit_file`  | Perform exact string replacements in files (with global replace mode)                         |
| `glob`       | Find files matching patterns (e.g., `**/*.py`)                                                |
| `grep`       | Search file contents with multiple output modes (files only, content with context, or counts) |

## Large tool result eviction

The harness automatically dumps large tool results to the file system when they exceed a token threshold, preventing context window saturation.

* Monitors tool call results for size (default threshold: 20,000 tokens)
* When exceeded, writes the result to a file instead
* Replaces the tool result with a concise reference to the file
* Agent can later read the file if needed

## Pluggable storage backends

The harness abstracts file system operations behind a protocol, allowing different storage strategies for different use cases.

**Available backends:**

1. **StateBackend** - Ephemeral in-memory storage
   * Files live in the agent's state (checkpointed with conversation)
   * Persists within a thread but not across threads
   * Useful for temporary working files

2. **FilesystemBackend** - Real filesystem access
   * Read/write from actual disk
   * Supports virtual mode (sandboxed to a root directory)
   * Integrates with system tools (ripgrep for grep)
   * Security features: path validation, size limits, symlink prevention

3. **StoreBackend** - Persistent cross-conversation storage
   * Uses LangGraph's BaseStore for durability
   * Namespaced per assistant\_id
   * Files persist across conversations
   * Useful for long-term memory or knowledge bases

4. **CompositeBackend** - Route different paths to different backends
   * Example: `/` → StateBackend, `/memories/` → StoreBackend
   * Longest-prefix matching for routing
   * Enables hybrid storage strategies

## Task delegation (subagents)

The harness allows the main agent to create ephemeral "subagents" for isolated multi-step tasks.

* **Context isolation** - Subagent's work doesn't clutter main agent's context
* **Parallel execution** - Multiple subagents can run concurrently
* **Specialization** - Subagents can have different tools/configurations
* **Token efficiency** - Large subtask context is compressed into a single result

* Main agent has a `task` tool
* When invoked, creates a fresh agent instance with its own context
* Subagent executes autonomously until completion
* Returns a single final report to the main agent
* Subagents are stateless (can't send multiple messages back)

**Default subagent:**

* "general-purpose" subagent automatically available
* Has filesystem tools by default
* Can be customized with additional tools/middleware

**Custom subagents:**

* Define specialized subagents with specific tools
* Example: code-reviewer, web-researcher, test-runner
* Configure via `subagents` parameter

## Conversation history summarization

The harness automatically compresses old conversation history when token usage becomes excessive.

* Triggers at 170,000 tokens
* Keeps the most recent 6 messages intact
* Older messages are summarized by the model

* Enables very long conversations without hitting context limits
* Preserves recent context while compressing ancient history
* Transparent to the agent (appears as a special system message)

## Dangling tool call repair

The harness fixes message history when tool calls are interrupted or cancelled before receiving results.

* Agent requests tool call: "Please run X"
* Tool call is interrupted (user cancels, error, etc.)
* Agent sees tool\_call in AIMessage but no corresponding ToolMessage
* This creates an invalid message sequence

* Detects AIMessages with tool\_calls that have no results
* Creates synthetic ToolMessage responses indicating the call was cancelled
* Repairs the message history before agent execution

* Prevents agent confusion from incomplete message chains
* Gracefully handles interruptions and errors
* Maintains conversation coherence

## To-do list tracking

The harness provides a `write_todos` tool that agents can use to maintain a structured task list.

* Track multiple tasks with statuses (pending, in\_progress, completed)
* Persisted in agent state
* Helps agent organize complex multi-step work
* Useful for long-running tasks and planning

The harness pauses agent execution at specified tool calls to allow human approval/modification.

* Map tool names to interrupt configurations
* Example: `{"edit_file": True}` - pause before every edit
* Can provide approval messages or modify tool inputs

* Safety gates for destructive operations
* User verification before expensive API calls
* Interactive debugging and guidance

## Prompt caching (Anthropic)

The harness enables Anthropic's prompt caching feature to reduce redundant token processing.

* Caches portions of the prompt that repeat across turns
* Significantly reduces latency and cost for long system prompts
* Automatically skips for non-Anthropic models

* System prompts (especially with filesystem docs) can be 5k+ tokens
* These repeat every turn without caching
* Caching provides \~10x speedup and cost reduction

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/deepagents/harness.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Agent model

**URL:** llms-txt#agent-model

qa_llm = init_chat_model("claude-sonnet-4-5-20250929")

---

## Agent reads /memories/project_notes.txt from previous conversation

**URL:** llms-txt#agent-reads-/memories/project_notes.txt-from-previous-conversation

**Contents:**
  - Research projects
- Store implementations
  - InMemoryStore (development)
  - PostgresStore (production)
- Best practices
  - Use descriptive paths
  - Document the memory structure
  - Prune old data
  - Choose the right storage

python  theme={null}
research_agent = create_deep_agent(
    store=InMemoryStore(),
    backend=lambda rt: CompositeBackend(
        default=StateBackend(rt),
        routes={"/memories/": StoreBackend(rt)}
    ),
    system_prompt="""You are a research assistant.

Save your research progress to /memories/research/:
    - /memories/research/sources.txt - List of sources found
    - /memories/research/notes.txt - Key findings and notes
    - /memories/research/report.md - Final report draft

This allows research to continue across multiple sessions."""
)
python  theme={null}
from langgraph.store.memory import InMemoryStore

store = InMemoryStore()
agent = create_deep_agent(
    store=store,
    backend=lambda rt: CompositeBackend(
        default=StateBackend(rt),
        routes={"/memories/": StoreBackend(rt)}
    )
)
python  theme={null}
from langgraph.store.postgres import PostgresStore
import os

store = PostgresStore(connection_string=os.environ["DATABASE_URL"])
agent = create_deep_agent(
    store=store,
    backend=lambda rt: CompositeBackend(
        default=StateBackend(rt),
        routes={"/memories/": StoreBackend(rt)}
    )
)

/memories/user_preferences.txt
/memories/research/topic_a/sources.txt
/memories/research/topic_a/notes.txt
/memories/project/requirements.md

Your persistent memory structure:
- /memories/preferences.txt: User preferences and settings
- /memories/context/: Long-term context about the user
- /memories/knowledge/: Facts and information learned over time
```

Implement periodic cleanup of outdated persistent files to keep storage manageable.

### Choose the right storage

* **Development**: Use `InMemoryStore` for quick iteration
* **Production**: Use `PostgresStore` or other persistent stores
* **Multi-tenant**: Consider using assistant\_id-based namespacing in your store

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/deepagents/long-term-memory.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
### Research projects

Maintain research state across sessions:
```

Example 2 (unknown):
```unknown
## Store implementations

Any LangGraph `BaseStore` implementation works:

### InMemoryStore (development)

Good for testing and development, but data is lost on restart:
```

Example 3 (unknown):
```unknown
### PostgresStore (production)

For production, use a persistent store:
```

Example 4 (unknown):
```unknown
## Best practices

### Use descriptive paths

Organize persistent files with clear paths:
```

---

## Agent Server API reference for LangSmith Deployment

**URL:** llms-txt#agent-server-api-reference-for-langsmith-deployment

**Contents:**
- Authentication

Source: https://docs.langchain.com/langsmith/server-api-ref

The Agent Server API reference is available within each [deployment](/langsmith/deployments) at the `/docs` endpoint (e.g. `http://localhost:8124/docs`).

<Card title="API Reference" href="https://langchain-ai.github.io/langgraph/cloud/reference/api/api_ref.html" icon="book">
  View the full Agent Server API reference documentation
</Card>

For deployments to LangSmith, authentication is required. Pass the `X-Api-Key` header with each request to the Agent Server. The value of the header should be set to a valid LangSmith API key for the organization where the Agent Server is deployed.

Example `curl` command:

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/server-api-ref.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Agent Server

**URL:** llms-txt#agent-server

**Contents:**
- Application structure
- Parts of a deployment
  - Graphs
  - Persistence and task queue
- Learn more

Source: https://docs.langchain.com/langsmith/agent-server

LangSmith Deployment's **Agent Server** offers an API for creating and managing agent-based applications. It is built on the concept of [assistants](/langsmith/assistants), which are agents configured for specific tasks, and includes built-in [persistence](/oss/python/langgraph/persistence#memory-store) and a **task queue**. This versatile API supports a wide range of agentic application use cases, from background processing to real-time interactions.

Use Agent Server to create and manage [assistants](/langsmith/assistants), [threads](/oss/python/langgraph/persistence#threads), [runs](/langsmith/assistants#execution), [cron jobs](/langsmith/cron-jobs), [webhooks](/langsmith/use-webhooks), and more.

<Tip>
  **API reference**<br />
  For detailed information on the API endpoints and data models, refer to the [API reference docs](https://langchain-ai.github.io/langgraph/cloud/reference/api/api_ref.html).
</Tip>

To use the Enterprise version of the Agent Server, you must acquire a license key that you will need to specify when running the Docker image. To acquire a license key, [contact our sales team](https://www.langchain.com/contact-sales).

You can run the Enterprise version of the Agent Server on the following LangSmith [platform](/langsmith/platform-setup) options:

* [Cloud](/langsmith/cloud)
* [Hybrid](/langsmith/hybrid)
* [Self-hosted](/langsmith/self-hosted)

## Application structure

To deploy an Agent Server application, you need to specify the graph(s) you want to deploy, as well as any relevant configuration settings, such as dependencies and environment variables.

Read the [application structure](/langsmith/application-structure) guide to learn how to structure your LangGraph application for deployment.

## Parts of a deployment

When you deploy Agent Server, you are deploying one or more [graphs](#graphs), a database for [persistence](/oss/python/langgraph/persistence), and a task queue.

When you deploy a graph with Agent Server, you are deploying a "blueprint" for an [Assistant](/langsmith/assistants).

An [Assistant](/langsmith/assistants) is a graph paired with specific configuration settings. You can create multiple assistants per graph, each with unique settings to accommodate different use cases
that can be served by the same graph.

Upon deployment, Agent Server will automatically create a default assistant for each graph using the graph's default configuration settings.

<Note>
  We often think of a graph as implementing an [agent](/oss/python/langgraph/workflows-agents), but a graph does not necessarily need to implement an agent. For example, a graph could implement a simple
  chatbot that only supports back-and-forth conversation, without the ability to influence any application control flow. In reality, as applications get more complex, a graph will often implement a more complex flow that may use [multiple agents](/oss/python/langchain/multi-agent) working in tandem.
</Note>

### Persistence and task queue

Agent Server leverages a database for [persistence](/oss/python/langgraph/persistence) and a task queue.

[PostgreSQL](https://www.postgresql.org/) is supported as a database for Agent Server and [Redis](https://redis.io/) as the task queue.

If you're deploying using [LangSmith cloud](/langsmith/cloud), these components are managed for you. If you're deploying Agent Server on your [own infrastructure](/langsmith/self-hosted), you'll need to set up and manage these components yourself.

For more information on how these components are set up and managed, review the [hosting options](/langsmith/platform-setup) guide.

* [Application Structure](/langsmith/application-structure) guide explains how to structure your application for deployment.
* The [API Reference](https://langchain-ai.github.io/langgraph/cloud/reference/api/api_ref.html) provides detailed information on the API endpoints and data models.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/agent-server.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Agent Server changelog

**URL:** llms-txt#agent-server-changelog

**Contents:**
- v0.5.4
- v0.5.2
- v0.5.1
- v0.5.0
- v0.4.47
- v0.4.46
- v0.4.45
- v0.4.44
- v0.4.43
- v0.4.40

Source: https://docs.langchain.com/langsmith/agent-server-changelog

[Agent Server](/langsmith/agent-server) is an API platform for creating and managing agent-based applications. It provides built-in persistence, a task queue, and supports deploying, configuring, and running assistants (agentic workflows) at scale. This changelog documents all notable updates, features, and fixes to Agent Server releases.

<a id="2025-11-05" />

* Implemented `ormsgpack` with `OPT_REPLACE_SURROGATES` and updated for compatibility with the latest FastAPI release affecting custom authentication dependencies.

<a id="2025-11-03" />

* Added retry logic for PostgreSQL connections during startup to enhance deployment reliability and improved error logging for easier debugging.

<a id="2025-11-03" />

* Resolved an issue where persistence was not functioning correctly with LangChain.js's createAgent feature.
* Optimized assistants CRUD performance by improving database connection pooling and gRPC client reuse, reducing latency for large payloads.

<a id="2025-10-31" />

* Updated dependency requirements to support the latest security patch, removed JSON fallback for serialization, and adjusted deserialization behavior for enhanced security.

<a id="2025-10-29" />

* Validated and auto-corrected environment configuration types using TypeAdapter.
* Added support for LangChain.js and LangGraph.js version 1.x, ensuring compatibility.
* Updated hono library from version 4.9.7 to 4.10.3, addressing a CORS middleware security issue and enhancing JWT audience validation.
* Introduced a modular benchmark framework, adding support for assistants and streams, with improvements to the existing ramp benchmark methodology.
* Introduced a gRPC API for core threads CRUD operations, with updated Python and TypeScript clients.
* Updated `hono` package from version 4.9.7 to 4.10.2, including security improvements for JWT audience validation.
* Updated `hono` dependency from version 4.9.7 to 4.10.3 to fix a security issue and improve CORS middleware handling.
* Introduced basic CRUD operations for threads, including create, get, patch, delete, search, count, and copy, with support for Go, gRPC server, and Python and TypeScript clients.

<a id="2025-10-21" />

* Added an option to enable message streaming from subgraph events, giving users more control over event notifications.

<a id="2025-10-21" />

* Implemented support for authorization on custom routes, controlled by the `enable_custom_route_auth` flag.
* Set default tracing to off for improved performance and simplified debugging.

<a id="2025-10-18" />

* Used Redis key prefix for license-related keys to prevent conflicts with existing setups.

<a id="2025-10-16" />

* Implemented a health check for Redis connections to prevent them from idling out.

<a id="2025-10-15" />

* Prevented duplicate messages in resumable run and thread streams by addressing a race condition and adding tests to ensure consistent behavior.
* Ensured that runs don't start until the pubsub subscription is confirmed to prevent message drops on startup.
* Renamed platform from langgraph to improve clarity and branding.
* Reset PostgreSQL connections after use to prevent lock holding and improved error reporting for transaction issues.

<a id="2025-10-10" />

* Upgraded `hono` from version 4.7.6 to 4.9.7, addressing a security issue related to the `bodyLimit` middleware.
* Allowed customization of the base authentication URL to enhance flexibility.
* Pinned the 'ty' dependency to a stable version using 'uv' to prevent unexpected linting failures.

<a id="2025-10-08" />

* Replaced `LANGSMITH_API_KEY` with `LANGSMITH_CONTROL_PLANE_API_KEY` to support hybrid deployments requiring license verification.
* Introduced self-hosted log ingestion support, configurable via `SELF_HOSTED_LOGS_ENABLED` and `SELF_HOSTED_LOGS_ENDPOINT` environment variables.

<a id="2025-10-06" />

* Required create permissions for copying threads to ensure proper authorization.

<a id="2025-10-03" />

* Improved error handling and added a delay to the sweep loop for smoother operation during Redis downtime or cancellation errors.
* Updated the queue entrypoint to start the core-api gRPC server when `FF_USE_CORE_API` is enabled.
* Introduced checks for invalid configurations in assistant endpoints to ensure consistency with other endpoints.

<a id="2025-10-02" />

* Resolved a timezone issue in the core API, ensuring accurate time data retrieval.
* Introduced a new `middleware_order` setting to apply authentication middleware before custom middleware, allowing finer control over protected route configurations.
* Logged the Redis URL when errors occur during Redis client creation.
* Improved Go engine/runtime context propagation to ensure consistent execution flow.
* Removed the unnecessary `assistants.put` call from the executor entrypoint to streamline the process.

<a id="2025-10-01" />

* Blocked unauthorized users from updating thread TTL settings to enhance security.

<a id="2025-10-01" />

* Improved error handling for Redis locks by logging `LockNotOwnedError` and extending initial pool migration lock timeout to 60 seconds.
* Updated the BaseMessage schema to align with the latest langchain-core version and synchronized build dependencies for consistent local development.

<a id="2025-09-30" />

* Added a GO persistence layer to the API image, enabling GRPC server operation with PostgreSQL support and enhancing configurability.
* Set the status to error when a timeout occurs to improve error handling.

<a id="2025-09-30" />

* Added support for context when using `stream_mode="events"` and included new tests for this functionality.
* Added support for overriding the server port using `$LANGGRAPH_SERVER_PORT` and removed an unnecessary Dockerfile `ARG` for cleaner configuration.
* Applied authorization filters to all table references in thread delete CTE to enhance security.
* Introduced self-hosted metrics ingestion capability, allowing metrics to be sent to an OTLP collector every minute when the corresponding environment variables are set.
* Ensured that the `set_latest` function properly updates the name and description of the version.

<a id="2025-09-26" />

* Ensured proper cleanup of redis pubsub connections in all scenarios.

<a id="2025-09-25" />

* Added a format parameter to the queue metrics server for enhanced customization.
* Corrected `MOUNT_PREFIX` environment variable usage in CLI for consistency with documentation and to prevent confusion.
* Added a feature to log warnings when messages are dropped due to no subscribers, controllable via a feature flag.
* Added support for Bookworm and Bullseye distributions in Node images.
* Consolidated executor definitions by moving them from the `langgraph-go` repository, improving manageability and updating the checkpointer setup method for server migrations.
* Ensured correct response headers are sent for a2a, improving compatibility and communication.
* Consolidated PostgreSQL checkpoint implementation, added CI testing for the `/core` directory, fixed RemoteStore test errors, and enhanced the Store implementation with transactions.
* Added PostgreSQL migrations to the queue server to prevent errors from graphs being added before migrations are performed.

<a id="2025-09-23" />

* Replaced `coredis` with `redis-py` to improve connection handling and reliability under high traffic loads.

<a id="2025-09-22-v0.4.24" />

* Added functionality to return full message history for A2A calls in accordance with the A2A spec.
* Added a `LANGGRAPH_SERVER_HOST` environment variable to Dockerfiles to support custom host settings for dual stack mode.

<a id="2025-09-22" />

* Use a faster message codec for redis streaming.

<a id="2025-09-19" />

* Ported long-stream handling to the run stream, join, and cancel endpoints for improved stream management.

<a id="2025-09-18" />

* Added A2A streaming functionality and enhanced testing with the A2A SDK.
* Added Prometheus metrics to track language usage in graphs, middleware, and authentication for improved insights.
* Fixed bugs in Open Source Software related to message conversion for chunks.
* Removed await from pubsub subscribes to reduce flakiness in cluster tests and added retries in the shutdown suite to enhance API stability.

<a id="2025-09-11" />

* Optimized Pubsub initialization to prevent overhead and address subscription timing issues, ensuring smoother run execution.

<a id="2025-09-11" />

* Removed warnings from psycopg by addressing function checks introduced in version 3.2.10.

<a id="2025-09-11" />

* Filtered out logs with mount prefix to reduce noise in logging output.

<a id="2025-09-10" />

* Added support for implicit thread creation in a2a to streamline operations.
* Improved error serialization and emission in distributed runtime streams, enabling more comprehensive testing.

<a id="2025-09-09" />

* Monitored queue status in the health endpoint to ensure correct behavior when PostgreSQL fails to initialize.
* Addressed an issue with unequal swept ID lengths to improve log clarity.
* Enhanced streaming outputs by avoiding re-serialization of DR payloads, using msgpack byte inspection for json-like parsing.

<a id="2025-09-04" />

* Ensured metrics are returned even when experiencing database connection issues.
* Optimized update streams to prevent unnecessary data transmission.
* Upgraded `hono` from version 4.9.2 to 4.9.6 in the `storage_postgres/langgraph-api-server` for improved URL path parsing security.
* Added retries and an in-memory cache for LangSmith access calls to improve resilience against single failures.

<a id="2025-09-04" />

* Added support for TTL (time-to-live) in thread updates.

<a id="2025-09-04" />

* In distributed runtime, update serde logic for final checkpoint -> thread setting.

<a id="2025-09-02" />

* Added support for filtering search results by IDs in the search endpoint for more precise queries.
* Included configurable headers for assistant endpoints to enhance request customization.
* Implemented a simple A2A endpoint with support for agent card retrieval, task creation, and task management.

<a id="2025-08-30" />

* Stopped the inclusion of x-api-key to enhance security.

<a id="2025-08-29" />

* Fixed a race condition when joining streams, preventing duplicate start events.

<a id="2025-08-29" />

* Ensured the checkpointer starts and stops correctly before and after the queue to improve shutdown and startup efficiency.
* Resolved an issue where workers were being prematurely cancelled when the queue was cancelled.
* Prevented queue termination by adding a fallback for cases when Redis fails to wake a worker.

<a id="2025-08-28" />

* Set the custom auth thread\_id to None for stateless runs to prevent conflicts.
* Improved Redis signaling in the Go runtime by adding a wakeup worker and Redis lock implementation, and updated sweep logic.

<a id="2025-08-27" />

* Added stream mode to thread stream for improved data processing.
* Added a durability parameter to runs for improved data persistence.

<a id="2025-08-27" />

* Ensured pubsub is initialized before creating a run to prevent errors from missing messages.

<a id="2025-08-25" />

* Emitted attempt messages correctly within the thread stream.
* Reduced cluster conflicts by using only the thread ID for hashing in cluster mapping, prioritizing efficiency with stream\_thread\_cache.
* Introduced a stream endpoint for threads to track all outputs across sequentially executed runs.
* Made the filter query builder in PostgreSQL more robust against malformed expressions and improved validation to prevent potential security risks.

<a id="2025-08-25" />

* Added custom Prometheus metrics for Redis/PG connection pools and switched the queue server to Uvicorn/Starlette for improved monitoring.
* Restored Wolfi image build by correcting shell command formatting and added a Makefile target for testing with nginx.

<a id="2025-08-22" />

* Added timeouts to specific Redis calls to prevent workers from being left active.
* Updated the Golang runtime and added pytest skips for unsupported functionalities, including initial support for passing store to node and message streaming.
* Introduced a reverse proxy setup for serving combined Python and Node.js graphs, with nginx handling server routing, to facilitate a Postgres/Redis backend for the Node.js API server.

<a id="2025-08-21" />

* Added a statement timeout to the pool to prevent long-running queries.

<a id="2025-08-21" />

* Set a default 15-minute statement timeout and implemented monitoring for long-running queries to ensure system efficiency.
* Stop propagating run configurable values to the thread configuration, because this can cause issues on subsequent runs if you are specifying a checkpoint\_id. This is a **slight breaking change** in behavior, since the thread value will no longer automatically reflect the unioned configuration of the most recent run. We believe this behavior is more intuitive, however.
* Enhanced compatibility with older worker versions by handling event data in channel names within ops.py.

<a id="2025-08-20" />

* Fixed an unbound local error and improved logging for thread interruptions or errors, along with type updates.

<a id="2025-08-20" />

* Added enhanced logging to aid in debugging metaview issues.
* Upgraded executor and runtime to the latest version for improved performance and stability.

<a id="2025-08-19" />

* Ensured async coroutines are properly awaited to prevent potential runtime errors.

<a id="2025-08-18" />

* Enhanced search functionality to improve performance by allowing users to select specific columns for query results.

<a id="2025-08-18" />

* Added count endpoints for crons, threads, and assistants to enhance data tracking (#1132).
* Improved SSH functionality for better reliability and stability.
* Updated @langchain/langgraph-api to version 0.0.59 to fix an invalid state schema issue.

<a id="2025-08-15" />

* Added Go language images to enhance project compatibility and functionality.
* Printed internal PIDs for JS workers to facilitate process inspection via SIGUSR1 signal.
* Resolved a `run_pkey` error that occurred when attempting to insert duplicate runs.
* Added `ty run` command and switched to using uuid7 for generating run IDs.
* Implemented the initial Golang runtime to expand language support.

<a id="2025-08-14" />

* Added support for `object agent spec` with descriptions in JS.

<a id="2025-08-13" />

* Added a feature flag (FF\_RICH\_THREADS=false) to disable thread updates on run creation, reducing lock contention and simplifying thread status handling.
* Utilized existing connections for `aput` and `apwrite` operations to improve performance.
* Improved error handling for decoding issues to enhance data processing reliability.
* Excluded headers from logs to improve security while maintaining runtime functionality.
* Fixed an error that prevented mapping slots to a single node.
* Added debug logs to track node execution in JS deployments for improved issue diagnosis.
* Changed the default multitask strategy to enqueue, improving throughput by eliminating the need to fetch inflight runs during new run insertions.
* Optimized database operations for `Runs.next` and `Runs.sweep` to reduce redundant queries and improve efficiency.
* Improved run creation speed by skipping unnecessary inflight runs queries.

<a id="2025-08-11" />

* Stopped passing internal LGP fields to context to prevent breaking type checks.
* Exposed content-location headers to ensure correct resumability behavior in the API.

<a id="2025-08-08" />

* Ensured synchronized updates between `configurable` and `context` in assistants, preventing setup errors and supporting smoother version transitions.

<a id="2025-08-08" />

* Excluded unrequested stream modes from the resumable stream to optimize functionality.

<a id="2025-08-08" />

* Made access logger headers configurable to enhance logging flexibility.
* Debounced the Runs.stats function to reduce the frequency of expensive calls and improve performance.
* Introduced debouncing for sweepers to enhance performance and efficiency (#1147).
* Acquired a lock for TTL sweeping to prevent database spamming during scale-out operations.

<a id="2025-08-06" />

* Updated tracing context replicas to use the new format, ensuring compatibility.

<a id="2025-08-06" />

* Added an entrypoint to the queue replica for improved deployment management.

<a id="2025-08-06" />

* Utilized persisted interrupt status in `join` to ensure correct handling of user's interrupt state after completion.

<a id="2025-08-06" />

* Consolidated events to a single channel to prevent race conditions and optimize startup performance.
* Ensured custom lifespans are invoked on queue workers for proper setup, and added tests.

<a id="2025-08-04" />

* Restored the original streaming behavior of runs, ensuring consistent inclusion of interrupt events based on `stream_mode` settings.
* Optimized `Runs.next` query to reduce average execution time from \~14.43ms to \~2.42ms, improving performance.
* Added support for stream mode "tasks" and "checkpoints", normalized the UI namespace, and upgraded `@langchain/langgraph-api` for enhanced functionality.

<a id="2025-07-31" />

* Added a composite index on threads for faster searches with owner-based authentication and updated the default sort order to `updated_at` for improved query performance.

<a id="2025-07-31" />

* Reduced the default number of history checkpoints from 10 to 1 to optimize performance.

<a id="2025-07-31" />

* Optimized cache re-use to enhance application performance and efficiency.

<a id="2025-07-30" />

* Improved thread search pagination by updating response headers with `X-Pagination-Total` and `X-Pagination-Next` for better navigation.

<a id="2025-07-30" />

* Ensured sync logging methods are awaited and added a linter to prevent future occurrences.
* Fixed an issue where JavaScript tasks were not being populated correctly for JS graphs.

<a id="2025-07-29" />

* Fixed JS graph streaming failure by starting the heartbeat as soon as the connection opens.

<a id="2025-07-29" />

* Added interrupts as default values for join operations while preserving stream behavior.

<a id="2025-07-28" />

* Fixed an issue where config schema was missing when `config_type` was not set, ensuring more reliable configurations.

<a id="2025-07-28" />

* Prepared for LangGraph v0.6 compatibility with new context API support and bug fixes.

<a id="2025-07-27" />

* Implemented caching for authentication processes to enhance performance and efficiency.
* Optimized database performance by merging count and select queries.

<a id="2025-07-27" />

* Made log streams resumable, enhancing reliability and improving user experience when reconnecting.

<a id="2025-07-27" />

* Added a heapdump endpoint to save memory heap information to a file.

<a id="2025-07-25" />

* Used the correct metadata endpoint to resolve issues with data retrieval.

<a id="2025-07-24" />

* Captured interrupt events in the wait method to preserve previous behavior from langgraph 0.5.0.
* Added support for SDK structlog in the JavaScript environment for enhanced logging capabilities.

<a id="2025-07-24" />

* Corrected the metadata endpoint for self-hosted deployments.

<a id="2025-07-22" />

* Improved license check by adding an in-memory cache and handling Redis connection errors more effectively.
* Reloaded assistants to preserve manually created ones while discarding those removed from the configuration file.
* Reverted changes to ensure the UI namespace for gen UI is a valid JavaScript property name.
* Ensured that the UI namespace for generated UI is a valid JavaScript property name, improving API compliance.
* Enhanced error handling to return a 422 status code for unprocessable entity requests.

<a id="2025-07-19" />

* Added context to langgraph nodes to improve log filtering and trace visibility.

<a id="2025-07-19" />

* Improved interoperability with the ckpt ingestion worker on the main loop to prevent task scheduling issues.
* Delayed queue worker startup until after migrations are completed to prevent premature execution.
* Enhanced thread state error handling by adding specific metadata and improved response codes for better clarity when state updates fail during creation.
* Exposed the interrupt ID when retrieving the thread state to improve API transparency.

<a id="2025-07-17" />

* Added a fallback mechanism for configurable header patterns to handle exclude/include settings more effectively.

<a id="2025-07-17" />

* Avoided setting the future if it is already done to prevent redundant operations.
* Resolved compatibility errors in CI by switching from `typing.TypedDict` to `typing_extensions.TypedDict` for Python versions below 3.12.

<a id="2025-07-16" />

* Improved performance by omitting pending sends for langgraph versions 0.5 and above.
* Improved server startup logs to provide clearer warnings when the DD\_API\_KEY environment variable is set.

<a id="2025-07-16" />

* Removed the GIN index for run metadata to improve performance.

<a id="2025-07-16" />

* Enabled copying functionality for blobs and checkpoints, improving data management flexibility.

<a id="2025-07-16" />

* Reduced writes to the `checkpoint_blobs` table by inlining small values (null, numeric, str, etc.). This means we don't need to store extra values for channels that haven't been updated.

<a id="2025-07-16" />

* Improve checkpoint writes via node-local background queueing.

<a id="2025-07-15" />

* Decoupled checkpoint writing from thread/run state by removing foreign keys and updated logger to prevent timeout-related failures.

<a id="2025-07-14" />

* Removed the foreign key constraint for `thread` in the `run` table to simplify database schema.

<a id="2025-07-14" />

* Added more detailed logs for Redis worker signaling to improve debugging.

<a id="2025-07-11" />

* Honored tool descriptions in the `/mcp` endpoint to align with expected functionality.

<a id="2025-07-10" />

* Added support for the `on_disconnect` field to `runs/wait` and included disconnect logs for better debugging.

<a id="2025-07-09" />

* Removed unnecessary status updates to streamline thread handling and updated version to 0.2.84.

<a id="2025-07-09" />

* Reduced the default time-to-live for resumable streams to 2 minutes.
* Enhanced data submission logic to send data to both Beacon and LangSmith instance based on license configuration.
* Enabled submission of self-hosted data to a LangSmith instance when the endpoint is configured.

<a id="2025-07-03" />

* Addressed a race condition in background runs by implementing a lock using join, ensuring reliable execution across CTEs.

<a id="2025-07-03" />

* Optimized run streams by reducing initial wait time to improve responsiveness for older or non-existent runs.

<a id="2025-07-03" />

* Corrected parameter passing in the `logger.ainfo()` API call to resolve a TypeError.

<a id="2025-07-02" />

* Fixed a JsonDecodeError in checkpointing with remote graph by correcting JSON serialization to handle trailing slashes properly.
* Introduced a configuration flag to disable webhooks globally across all routes.

<a id="2025-07-02" />

* Added timeout retries to webhook calls to improve reliability.
* Added HTTP request metrics, including a request count and latency histogram, for enhanced monitoring capabilities.

<a id="2025-07-02" />

* Added HTTP metrics to improve performance monitoring.
* Changed the Redis cache delimiter to reduce conflicts with subgraph message names and updated caching behavior.

<a id="2025-07-01" />

* Updated Redis cache delimiter to prevent conflicts with subgraph messages.

<a id="2025-06-30" />

* Scheduled webhooks in an isolated loop to ensure thread-safe operations and prevent errors with PYTHONASYNCIODEBUG=1.

<a id="2025-06-27" />

* Fixed an infinite frame loop issue and removed the dict\_parser due to structlog's unexpected behavior.
* Throw a 409 error on deadlock occurrence during run cancellations to handle lock conflicts gracefully.

<a id="2025-06-27" />

* Ensured compatibility with future langgraph versions.
* Implemented a 409 response status to handle deadlock issues during cancellation.

<a id="2025-06-26" />

* Improved logging for better clarity and detail regarding log types.

<a id="2025-06-26" />

* Improved error handling to better distinguish and log TimeoutErrors caused by users from internal run timeouts.

<a id="2025-06-26" />

* Added sorting and pagination to the crons API and updated schema definitions for improved accuracy.

<a id="2025-06-26" />

* Fixed a 404 error when creating multiple runs with the same thread\_id using `on_not_exist="create"`.

<a id="2025-06-25" />

* Ensured that only fields from `assistant_versions` are returned when necessary.
* Ensured consistent data types for in-memory and PostgreSQL users, improving internal authentication handling.

<a id="2025-06-24" />

* Added descriptions to version entries for better clarity.

<a id="2025-06-23" />

* Improved user handling for custom authentication in the JS Studio.
* Added Prometheus-format run statistics to the metrics endpoint for better monitoring.
* Added run statistics in Prometheus format to the metrics endpoint.

<a id="2025-06-20" />

* Set a maximum idle time for Redis connections to prevent unnecessary open connections.

<a id="2025-06-20" />

* Enhanced error logging to include traceback details for dictionary operations.
* Added a `/metrics` endpoint to expose queue worker metrics for monitoring.

<a id="2025-06-18" />

* Removed CancelledError from retriable exceptions to allow local interrupts while maintaining retriability for workers.
* Introduced middleware to gracefully shut down the server after completing in-flight requests upon receiving a SIGINT.
* Reduced metadata stored in checkpoint to only include necessary information.
* Improved error handling in join runs to return error details when present.

<a id="2025-06-17" />

* Improved application stability by adding a handler for SIGTERM signals.

<a id="2025-06-17" />

* Improved the handling of cancellations in the queue entrypoint.
* Improved cancellation handling in the queue entry point.

<a id="2025-06-16" />

* Enhanced error message for LuaLock timeout during license validation.
* Fixed the \$contains filter in custom auth by requiring an explicit ::text cast and updated tests accordingly.
* Ensured project and tenant IDs are formatted as UUIDs for consistency.

<a id="2025-06-13" />

* Resolved a timing issue to ensure the queue starts only after the graph is registered.
* Improved performance by setting thread and run status in a single query and enhanced error handling during checkpoint writes.
* Reduced the default background grace period to 3 minutes.

<a id="2025-06-12" />

* Now logging expected graphs when one is omitted to improve traceability.
* Implemented a time-to-live (TTL) feature for resumable streams.
* Improved query efficiency and consistency by adding a unique index and optimizing row locking.

<a id="2025-06-12" />

* Handled `CancelledError` by marking tasks as ready to retry, improving error management in worker processes.
* Added LG API version and request ID to metadata and logs for better tracking.
* Added LG API version and request ID to metadata and logs to improve traceability.
* Improved database performance by creating indexes concurrently.
* Ensured postgres write is committed only after the Redis running marker is set to prevent race conditions.
* Enhanced query efficiency and reliability by adding a unique index on thread\_id/running, optimizing row locks, and ensuring deterministic run selection.
* Resolved a race condition by ensuring Postgres updates only occur after the Redis running marker is set.

<a id="2025-06-07" />

* Introduced a new connection for each operation while preserving transaction characteristics in Threads state `update()` and `bulk()` commands.

<a id="2025-06-05" />

* Enhanced streaming feature by incorporating tracing contexts.
* Removed an unnecessary query from the Crons.search function.
* Resolved connection reuse issue when scheduling next run for multiple cron jobs.
* Removed an unnecessary query in the Crons.search function to improve efficiency.
* Resolved an issue with scheduling the next cron run by improving connection reuse.

<a id="2025-06-04" />

* Enhanced the worker logic to exit the pipeline before continuing when the Redis message limit is reached.
* Introduced a ceiling for Redis message size with an option to skip messages larger than 128 MB for improved performance.
* Ensured the pipeline always closes properly to prevent resource leaks.

<a id="2025-06-04" />

* Improved performance by omitting logs in metadata calls and ensuring output schema compliance in value streaming.
* Ensured the connection is properly closed after use.
* Aligned output format to strictly adhere to the specified schema.
* Stopped sending internal logs in metadata requests to improve privacy.

<a id="2025-06-04" />

* Added timestamps to track the start and end of a request's run.
* Added tracer information to the configuration settings.
* Added support for streaming with tracing contexts.

<a id="2025-06-03" />

* Added locking mechanism to prevent errors in pipelined executions.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/agent-server-changelog.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Agent tools

**URL:** llms-txt#agent-tools

@tool
def lookup_track( ...

@tool
def lookup_album( ...

@tool
def lookup_artist( ...

---

## Agent will pause and wait for approval before executing sensitive tools

**URL:** llms-txt#agent-will-pause-and-wait-for-approval-before-executing-sensitive-tools

**Contents:**
- Custom guardrails
  - Before agent guardrails
  - After agent guardrails
  - Combine multiple guardrails
- Additional resources

result = agent.invoke(
    {"messages": [{"role": "user", "content": "Send an email to the team"}]},
    config=config
)

result = agent.invoke(
    Command(resume={"decisions": [{"type": "approve"}]}),
    config=config  # Same thread ID to resume the paused conversation
)
python title="Class syntax" theme={null}
  from typing import Any

from langchain.agents.middleware import AgentMiddleware, AgentState, hook_config
  from langgraph.runtime import Runtime

class ContentFilterMiddleware(AgentMiddleware):
      """Deterministic guardrail: Block requests containing banned keywords."""

def __init__(self, banned_keywords: list[str]):
          super().__init__()
          self.banned_keywords = [kw.lower() for kw in banned_keywords]

@hook_config(can_jump_to=["end"])
      def before_agent(self, state: AgentState, runtime: Runtime) -> dict[str, Any] | None:
          # Get the first user message
          if not state["messages"]:
              return None

first_message = state["messages"][0]
          if first_message.type != "human":
              return None

content = first_message.content.lower()

# Check for banned keywords
          for keyword in self.banned_keywords:
              if keyword in content:
                  # Block execution before any processing
                  return {
                      "messages": [{
                          "role": "assistant",
                          "content": "I cannot process requests containing inappropriate content. Please rephrase your request."
                      }],
                      "jump_to": "end"
                  }

# Use the custom guardrail
  from langchain.agents import create_agent

agent = create_agent(
      model="gpt-4o",
      tools=[search_tool, calculator_tool],
      middleware=[
          ContentFilterMiddleware(
              banned_keywords=["hack", "exploit", "malware"]
          ),
      ],
  )

# This request will be blocked before any processing
  result = agent.invoke({
      "messages": [{"role": "user", "content": "How do I hack into a database?"}]
  })
  python title="Decorator syntax" theme={null}
  from typing import Any

from langchain.agents.middleware import before_agent, AgentState, hook_config
  from langgraph.runtime import Runtime

banned_keywords = ["hack", "exploit", "malware"]

@before_agent(can_jump_to=["end"])
  def content_filter(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:
      """Deterministic guardrail: Block requests containing banned keywords."""
      # Get the first user message
      if not state["messages"]:
          return None

first_message = state["messages"][0]
      if first_message.type != "human":
          return None

content = first_message.content.lower()

# Check for banned keywords
      for keyword in banned_keywords:
          if keyword in content:
              # Block execution before any processing
              return {
                  "messages": [{
                      "role": "assistant",
                      "content": "I cannot process requests containing inappropriate content. Please rephrase your request."
                  }],
                  "jump_to": "end"
              }

# Use the custom guardrail
  from langchain.agents import create_agent

agent = create_agent(
      model="gpt-4o",
      tools=[search_tool, calculator_tool],
      middleware=[content_filter],
  )

# This request will be blocked before any processing
  result = agent.invoke({
      "messages": [{"role": "user", "content": "How do I hack into a database?"}]
  })
  python title="Class syntax" theme={null}
  from langchain.agents.middleware import AgentMiddleware, AgentState, hook_config
  from langgraph.runtime import Runtime
  from langchain_core.messages import AIMessage
  from langchain.chat_models import init_chat_model
  from typing import Any

class SafetyGuardrailMiddleware(AgentMiddleware):
      """Model-based guardrail: Use an LLM to evaluate response safety."""

def __init__(self):
          super().__init__()
          self.safety_model = init_chat_model("gpt-4o-mini")

@hook_config(can_jump_to=["end"])
      def after_agent(self, state: AgentState, runtime: Runtime) -> dict[str, Any] | None:
          # Get the final AI response
          if not state["messages"]:
              return None

last_message = state["messages"][-1]
          if not isinstance(last_message, AIMessage):
              return None

# Use a model to evaluate safety
          safety_prompt = f"""Evaluate if this response is safe and appropriate.
          Respond with only 'SAFE' or 'UNSAFE'.

Response: {last_message.content}"""

result = self.safety_model.invoke([{"role": "user", "content": safety_prompt}])

if "UNSAFE" in result.content:
              return {
                  "messages": [{
                      "role": "assistant",
                      "content": "I cannot provide that response. Please rephrase your request."
                  }],
                  "jump_to": "end"
              }

# Use the safety guardrail
  from langchain.agents import create_agent

agent = create_agent(
      model="gpt-4o",
      tools=[search_tool, calculator_tool],
      middleware=[SafetyGuardrailMiddleware()],
  )

result = agent.invoke({
      "messages": [{"role": "user", "content": "How do I make explosives?"}]
  })
  python title="Decorator syntax" theme={null}
  from langchain.agents.middleware import after_agent, AgentState, hook_config
  from langgraph.runtime import Runtime
  from langchain_core.messages import AIMessage
  from langchain.chat_models import init_chat_model
  from typing import Any

safety_model = init_chat_model("gpt-4o-mini")

@after_agent(can_jump_to=["end"])
  def safety_guardrail(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:
      """Model-based guardrail: Use an LLM to evaluate response safety."""
      # Get the final AI response
      if not state["messages"]:
          return None

last_message = state["messages"][-1]
      if not isinstance(last_message, AIMessage):
          return None

# Use a model to evaluate safety
      safety_prompt = f"""Evaluate if this response is safe and appropriate.
      Respond with only 'SAFE' or 'UNSAFE'.

Response: {last_message.content}"""

result = safety_model.invoke([{"role": "user", "content": safety_prompt}])

if "UNSAFE" in result.content:
          return {
              "messages": [{
                  "role": "assistant",
                  "content": "I cannot provide that response. Please rephrase your request."
              }],
              "jump_to": "end"
          }

# Use the safety guardrail
  from langchain.agents import create_agent

agent = create_agent(
      model="gpt-4o",
      tools=[search_tool, calculator_tool],
      middleware=[safety_guardrail],
  )

result = agent.invoke({
      "messages": [{"role": "user", "content": "How do I make explosives?"}]
  })
  python  theme={null}
from langchain.agents import create_agent
from langchain.agents.middleware import PIIMiddleware, HumanInTheLoopMiddleware

agent = create_agent(
    model="gpt-4o",
    tools=[search_tool, send_email_tool],
    middleware=[
        # Layer 1: Deterministic input filter (before agent)
        ContentFilterMiddleware(banned_keywords=["hack", "exploit"]),

# Layer 2: PII protection (before and after model)
        PIIMiddleware("email", strategy="redact", apply_to_input=True),
        PIIMiddleware("email", strategy="redact", apply_to_output=True),

# Layer 3: Human approval for sensitive tools
        HumanInTheLoopMiddleware(interrupt_on={"send_email": True}),

# Layer 4: Model-based safety check (after agent)
        SafetyGuardrailMiddleware(),
    ],
)
```

## Additional resources

* [Middleware documentation](/oss/python/langchain/middleware) - Complete guide to custom middleware
* [Middleware API reference](https://reference.langchain.com/python/langchain/middleware/) - Complete guide to custom middleware
* [Human-in-the-loop](/oss/python/langchain/human-in-the-loop) - Add human review for sensitive operations
* [Testing agents](/oss/python/langchain/test) - Strategies for testing safety mechanisms

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/guardrails.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
<Tip>
  See the [human-in-the-loop documentation](/oss/python/langchain/human-in-the-loop) for complete details on implementing approval workflows.
</Tip>

## Custom guardrails

For more sophisticated guardrails, you can create custom middleware that runs before or after the agent executes. This gives you full control over validation logic, content filtering, and safety checks.

### Before agent guardrails

Use "before agent" hooks to validate requests once at the start of each invocation. This is useful for session-level checks like authentication, rate limiting, or blocking inappropriate requests before any processing begins.

<CodeGroup>
```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
</CodeGroup>

### After agent guardrails

Use "after agent" hooks to validate final outputs once before returning to the user. This is useful for model-based safety checks, quality validation, or final compliance scans on the complete agent response.

<CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## ahead of time and use those to disambiguate the user input. E.g. if a user searches for

**URL:** llms-txt#ahead-of-time-and-use-those-to-disambiguate-the-user-input.-e.g.-if-a-user-searches-for

---

## AIMessage(content='bar', ...)

**URL:** llms-txt#aimessage(content='bar',-...)

**Contents:**
  - InMemorySaver Checkpointer

python  theme={null}
from langgraph.checkpoint.memory import InMemorySaver

agent = create_agent(
    model,
    tools=[],
    checkpointer=InMemorySaver()
)

**Examples:**

Example 1 (unknown):
```unknown
### InMemorySaver Checkpointer

To enable persistence during testing, you can use the [`InMemorySaver`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.memory.InMemorySaver) checkpointer. This allows you to simulate multiple turns to test state-dependent behavior:
```

---

## AIMessage(content='', ..., tool_calls=[{'name': 'foo', 'args': {'bar': 'baz'}, 'id': 'call_1', 'type': 'tool_call'}])

**URL:** llms-txt#aimessage(content='',-...,-tool_calls=[{'name':-'foo',-'args':-{'bar':-'baz'},-'id':-'call_1',-'type':-'tool_call'}])

python  theme={null}
model.invoke("hello, again!")

**Examples:**

Example 1 (unknown):
```unknown
If we invoke the model again, it will return the next item in the iterator:
```

---

## Alerts in LangSmith

**URL:** llms-txt#alerts-in-langsmith

**Contents:**
- Overview
- Configuring an alert
  - Step 1: Navigate To Create Alert
  - Step 2: Select Metric Type
  - Step 2: Define Alert Conditions
  - Step 3: Configure Notification Channel
- Best Practices

Source: https://docs.langchain.com/langsmith/alerts

<Note>
  **Self-hosted Version Requirement**

Access to alerts requires Helm chart version **0.10.3** or later.
</Note>

Effective observability in LLM applications requires proactive detection of failures, performance degradations, and regressions. LangSmith's alerts feature helps identify critical issues such as:

* API rate limit violations from model providers
* Latency increases for your application
* Application changes that affect feedback scores reflecting end-user experience

Alerts in LangSmith are project-scoped, requiring separate configuration for each monitored project.

## Configuring an alert

### Step 1: Navigate To Create Alert

First navigate to the Tracing project that you would like to configure alerts for. Click the Alerts icon on the top right hand corner of the page to view existing alerts for that project and set up a new alert.

### Step 2: Select Metric Type

<div style={{ textAlign: 'center' }}>
    <img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/alert-metric.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=932f55b512d866906160e3ebe9a78ad7" alt="Alert Metrics" data-og-width="597" width="597" data-og-height="134" height="134" data-path="langsmith/images/alert-metric.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/alert-metric.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=9a0140bfcf9df907ccaeffc0abc6d324 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/alert-metric.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=774b40c4cf122330c3b7e7e39bffecde 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/alert-metric.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=599617a29917cffe79547c1a85d110c3 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/alert-metric.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=4e963933afa346141fc2623286f55b48 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/alert-metric.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=fcb38466705fd5d8b94443ec9916a6ee 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/alert-metric.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=d738df80eee5db727e6627c4a0e85ce9 2500w" />
</div>

LangSmith offers threshold-based alerting on three core metrics:

| Metric Type        | Description                         | Use Case                                                                                                                                                |
| ------------------ | ----------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Errored Runs**   | Track runs with an error status     | Monitors for failures in an application.                                                                                                                |
| **Feedback Score** | Measures the average feedback score | Track [feedback from end users](/langsmith/attach-user-feedback) or [online evaluation results](/langsmith/online-evaluations) to alert on regressions. |
| **Latency**        | Measures average run execution time | Tracks the latency of your application to alert on spikes and performance bottlenecks.                                                                  |

Additionally, for **Errored Runs** and **Run Latency**, you can define filters to narrow down the runs that trigger alerts. For example, you might create an error alert filter for all `llm` runs tagged with `support_agent` that encounter a `RateLimitExceeded` error.

<div style={{ textAlign: 'center' }}>
    <img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/alerts-filter.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=b2dd48ba21e857c8a99a26a0d896f950" alt="Alert Metrics" data-og-width="407" width="407" data-og-height="273" height="273" data-path="langsmith/images/alerts-filter.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/alerts-filter.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=d776aa4bb261605c45f4691b95822ad1 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/alerts-filter.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=1cace263d141b044c73a8615c4c9cd15 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/alerts-filter.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=a77dfdb2a2e5a119d11675fc01a857ce 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/alerts-filter.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=d582ea675732440f5b4bae57ae35b766 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/alerts-filter.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=8780c7b52bc0a61c938a7c75357cd068 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/alerts-filter.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=6d2d7349e8856d8575bed75ccde61871 2500w" />
</div>

### Step 2: Define Alert Conditions

Alert conditions consist of several components:

* **Aggregation Method**: Average, Percentage, or Count
* **Comparison Operator**: `>=`, `<=`, or exceeds threshold
* **Threshold Value**: Numerical value triggering the alert
* **Aggregation Window**: Time period for metric calculation (currently choose between 5 or 15 minutes)
* **Feedback Key** (Feedback Score alerts only): Specific feedback metric to monitor

<div style={{ textAlign: 'center' }}>
    <img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/define-conditions.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=d92406d84dec4f1b827b82a989df30b9" alt="Alert Condition Configuration" data-og-width="597" width="597" data-og-height="112" height="112" data-path="langsmith/images/define-conditions.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/define-conditions.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=3311a45f1a32527a54c71d4966fdac3b 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/define-conditions.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=6ed12bea3c447c20bfff16e4e58d27e6 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/define-conditions.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=78955506ecd68ba0bac2ea7053837d6e 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/define-conditions.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=4a0bf3da7b34bdd56777a350315b3f6a 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/define-conditions.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=56a4a9e40b9c2a870b999c52dd13dd68 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/define-conditions.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=5207fb3afe3b40873280d9f23e3e0e24 2500w" />
</div>

**Example:** The configuration shown above would generate an alert when more than 5% of runs within the past 5 minutes result in errors.

You can preview alert behavior over a historical time window to understand how many datapoints—and which ones—would have triggered an alert at a chosen threshold (indicated in red). For example, setting an average latency threshold of 60 seconds for a project lets you visualize potential alerts, as shown in the image below.

<div style={{ textAlign: 'center' }}>
    <img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/alert-preview.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=d7f26bce1113c50bec8f5853c6448415" alt="Alert Metrics" data-og-width="863" width="863" data-og-height="545" height="545" data-path="langsmith/images/alert-preview.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/alert-preview.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=a508e02a73579624ae120276664e0e6a 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/alert-preview.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=4f7c5616752dfea80a346be50532f442 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/alert-preview.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=dd7d2d27fdb2335640d5ac43b6747baf 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/alert-preview.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=abbaea739f003fcbe97ee00e55e68927 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/alert-preview.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=55672ba9518816caf74921bc26694ffa 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/alert-preview.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=f67b99c4b5d709b1756d5b674a20dba1 2500w" />
</div>

### Step 3: Configure Notification Channel

LangSmith supports the following notification channels:

1. [PagerDuty Integration](/langsmith/alerts-pagerduty)
2. [Webhook Notifications](/langsmith/alerts-webhook)

Select the appropriate channel to ensure notifications reach the responsible team members.

* Adjust sensitivity based on application criticality
* Start with broader thresholds and refine based on observed patterns
* Ensure alert routing reaches appropriate on-call personnel

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/alerts.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Alice can still create threads

**URL:** llms-txt#alice-can-still-create-threads

**Contents:**
- Next steps

alice_thread = await alice.threads.create()
print(f"✅ Alice created thread: {alice_thread['thread_id']}")
bash  theme={null}
✅ Alice created thread: dcea5cd8-eb70-4a01-a4b6-643b14e8f754
✅ Bob correctly denied access: Client error '404 Not Found' for url 'http://localhost:2024/threads/dcea5cd8-eb70-4a01-a4b6-643b14e8f754'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404
✅ Bob created his own thread: 400f8d41-e946-429f-8f93-4fe395bc3eed
✅ Alice sees 1 thread
✅ Bob sees 1 thread
✅ Alice correctly denied access:
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500
✅ Alice correctly denied access to searching assistants:
```

Congratulations! You've built a chatbot where each user has their own private conversations. While this system uses simple token-based authentication, these authorization patterns will work with implementing any real authentication system. In the next tutorial, you'll replace your test users with real user accounts using OAuth2.

Now that you can control access to resources, you might want to:

1. Move on to [Connect an authentication provider](/langsmith/add-auth-server) to add real user accounts.
2. Read more about [authorization patterns](/langsmith/auth#authorization).
3. Check out the [API reference](/langsmith/langgraph-python-sdk#langgraph_sdk.auth.Auth) for details about the interfaces and methods used in this tutorial.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/resource-auth.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
Output:
```

---

## Alice creates an assistant

**URL:** llms-txt#alice-creates-an-assistant

alice_assistant = await alice.assistants.create()
print(f"✅ Alice created assistant: {alice_assistant['assistant_id']}")

---

## Alice creates a thread and chats

**URL:** llms-txt#alice-creates-a-thread-and-chats

alice_thread = await alice.threads.create()
print(f"✅ Alice created thread: {alice_thread['thread_id']}")

await alice.runs.create(
    thread_id=alice_thread["thread_id"],
    assistant_id="agent",
    input={"messages": [{"role": "user", "content": "Hi, this is Alice's private chat"}]}
)

---

## All integrations

**URL:** llms-txt#all-integrations

**Contents:**
- Top providers
- Chat Models
- LLMs
- Text Embedding Models
- Vector Stores
- Document loaders
  - File Loaders
  - Web Loaders
- Document Transformers
- Document Compressors

Source: https://docs.langchain.com/oss/javascript/integrations/providers/all_providers

Browse the complete collection of integrations available for JavaScript/TypeScript. LangChain.js offers hundreds of integrations across providers, tools, vector stores, document loaders, and more.

<Columns cols={3}>
  <Card title="Anthropic" href="/oss/javascript/integrations/providers/anthropic" icon="anthropic">
    Integrate with Anthropic's Claude models for advanced reasoning and conversation.
  </Card>

<Card title="AWS" href="/oss/javascript/integrations/providers/aws" icon="aws">
    Access AWS services and foundation models through comprehensive integrations.
  </Card>

<Card title="Google" href="/oss/javascript/integrations/providers/google" icon="google">
    Integrate with Google's AI services including Gemini and Vertex AI.
  </Card>

<Card title="Microsoft" href="/oss/javascript/integrations/providers/microsoft" icon="microsoft">
    Connect to Microsoft Azure services and AI platforms.
  </Card>

<Card title="OpenAI" href="/oss/javascript/integrations/providers/openai" icon="openai">
    Build with GPT models and OpenAI's comprehensive AI platform.
  </Card>
</Columns>

<Columns cols={3}>
  <Card title="Alibaba Tongyi" href="/oss/javascript/integrations/chat/alibaba_tongyi">
    Alibaba's Tongyi language model for Chinese and multilingual applications.
  </Card>

<Card title="Anthropic" href="/oss/javascript/integrations/chat/anthropic" icon="anthropic">
    Claude models for advanced conversational AI and reasoning.
  </Card>

<Card title="Arcjet" href="/oss/javascript/integrations/chat/arcjet">
    Security-focused AI chat integration with built-in protections.
  </Card>

<Card title="Azure OpenAI" href="/oss/javascript/integrations/chat/azure" icon="microsoft">
    OpenAI models through Microsoft Azure's enterprise platform.
  </Card>

<Card title="Baidu Qianfan" href="/oss/javascript/integrations/chat/baidu_qianfan">
    Baidu's Qianfan platform for Chinese language AI models.
  </Card>

<Card title="Baidu Wenxin" href="/oss/javascript/integrations/chat/baidu_wenxin">
    Baidu's Wenxin (ERNIE) models for natural language processing.
  </Card>

<Card title="Amazon Bedrock" href="/oss/javascript/integrations/chat/bedrock" icon="aws">
    Access foundation models through Amazon Bedrock's managed service.
  </Card>

<Card title="Bedrock Converse" href="/oss/javascript/integrations/chat/bedrock_converse" icon="aws">
    Unified Bedrock Converse API for multiple foundation models.
  </Card>

<Card title="Cerebras" href="/oss/javascript/integrations/chat/cerebras">
    Ultra-fast inference with Cerebras Systems' AI processors.
  </Card>

<Card title="Cloudflare Workers AI" href="/oss/javascript/integrations/chat/cloudflare_workersai">
    Run AI models on Cloudflare's edge computing platform.
  </Card>

<Card title="Cohere" href="/oss/javascript/integrations/chat/cohere" icon="cohere">
    Cohere's language models for text generation and understanding.
  </Card>

<Card title="Deep Infra" href="/oss/javascript/integrations/chat/deep_infra">
    Access open-source models through Deep Infra's cloud platform.
  </Card>

<Card title="DeepSeek" href="/oss/javascript/integrations/chat/deepseek">
    DeepSeek's advanced reasoning and coding models.
  </Card>

<Card title="Fake LLM" href="/oss/javascript/integrations/chat/fake">
    Mock chat model for testing and development purposes.
  </Card>

<Card title="Fireworks" href="/oss/javascript/integrations/chat/fireworks" icon="fireworks">
    High-performance inference for open-source models.
  </Card>

<Card title="Friendli" href="/oss/javascript/integrations/chat/friendli">
    Optimized inference engine for efficient model serving.
  </Card>

<Card title="Google Generative AI" href="/oss/javascript/integrations/chat/google_generative_ai" icon="google">
    Google's Gemini models and generative AI capabilities.
  </Card>

<Card title="Google Vertex AI" href="/oss/javascript/integrations/chat/google_vertex_ai" icon="google">
    Enterprise AI platform with Google Cloud's Vertex AI.
  </Card>

<Card title="Groq" href="/oss/javascript/integrations/chat/groq" icon="groq">
    Ultra-fast inference with Groq's specialized hardware.
  </Card>

<Card title="IBM" href="/oss/javascript/integrations/chat/ibm">
    IBM Watson AI models and enterprise solutions.
  </Card>

<Card title="Llama.cpp" href="/oss/javascript/integrations/chat/llama_cpp">
    Run local Llama models with llama.cpp backend.
  </Card>

<Card title="Minimax" href="/oss/javascript/integrations/chat/minimax">
    Minimax's conversational AI models and services.
  </Card>

<Card title="Mistral" href="/oss/javascript/integrations/chat/mistral" icon="mistral">
    Mistral's efficient and powerful language models.
  </Card>

<Card title="Moonshot" href="/oss/javascript/integrations/chat/moonshot">
    Moonshot's AI models for various language tasks.
  </Card>

<Card title="Neural Internet Bittensor" href="/oss/javascript/integrations/chat/ni_bittensor">
    Decentralized AI network through Bittensor protocol.
  </Card>

<Card title="Novita" href="/oss/javascript/integrations/chat/novita">
    Novita's AI models and cloud computing platform.
  </Card>

<Card title="Ollama" href="/oss/javascript/integrations/chat/ollama" icon="ollama">
    Run local models with Ollama's lightweight inference engine.
  </Card>

<Card title="Ollama Functions" href="/oss/javascript/integrations/chat/ollama_functions" icon="ollama">
    Function calling capabilities with Ollama models.
  </Card>

<Card title="OpenAI" href="/oss/javascript/integrations/chat/openai" icon="openai">
    GPT models and OpenAI's comprehensive chat capabilities.
  </Card>

<Card title="Perplexity" href="/oss/javascript/integrations/chat/perplexity">
    Perplexity's search-augmented language models.
  </Card>

<Card title="PremAI" href="/oss/javascript/integrations/chat/premai">
    PremAI's platform for AI model deployment and management.
  </Card>

<Card title="PromptLayer OpenAI" href="/oss/javascript/integrations/chat/prompt_layer_openai">
    OpenAI integration with PromptLayer's observability features.
  </Card>

<Card title="Tencent Hunyuan" href="/oss/javascript/integrations/chat/tencent_hunyuan">
    Tencent's Hunyuan models for Chinese language processing.
  </Card>

<Card title="Together AI" href="/oss/javascript/integrations/chat/togetherai" icon="together">
    Open-source models through Together AI's cloud platform.
  </Card>

<Card title="WebLLM" href="/oss/javascript/integrations/chat/web_llm">
    Run language models directly in web browsers.
  </Card>

<Card title="xAI" href="/oss/javascript/integrations/chat/xai">
    xAI's Grok models for conversational AI.
  </Card>

<Card title="Yandex" href="/oss/javascript/integrations/chat/yandex">
    Yandex's AI models and language processing services.
  </Card>

<Card title="ZhipuAI" href="/oss/javascript/integrations/chat/zhipuai">
    ZhipuAI's ChatGLM and other Chinese language models.
  </Card>
</Columns>

<Columns cols={3}>
  <Card title="AI21" href="/oss/javascript/integrations/llms/ai21">
    AI21 Labs' Jurassic models for text generation.
  </Card>

<Card title="Aleph Alpha" href="/oss/javascript/integrations/llms/aleph_alpha">
    European AI company's multilingual language models.
  </Card>

<Card title="Arcjet" href="/oss/javascript/integrations/llms/arcjet">
    Security-focused LLM integration with built-in protections.
  </Card>

<Card title="AWS SageMaker" href="/oss/javascript/integrations/llms/aws_sagemaker" icon="aws">
    Deploy models on Amazon SageMaker's ML platform.
  </Card>

<Card title="Azure OpenAI" href="/oss/javascript/integrations/llms/azure" icon="microsoft">
    OpenAI models through Microsoft Azure's enterprise platform.
  </Card>

<Card title="Amazon Bedrock" href="/oss/javascript/integrations/llms/bedrock" icon="aws">
    Foundation models through Amazon Bedrock service.
  </Card>

<Card title="Chrome AI" href="/oss/javascript/integrations/llms/chrome_ai">
    Browser-based AI using Chrome's built-in capabilities.
  </Card>

<Card title="Cloudflare Workers AI" href="/oss/javascript/integrations/llms/cloudflare_workersai">
    AI models on Cloudflare's edge computing platform.
  </Card>

<Card title="Cohere" href="/oss/javascript/integrations/llms/cohere" icon="cohere">
    Cohere's language models for various NLP tasks.
  </Card>

<Card title="Deep Infra" href="/oss/javascript/integrations/llms/deep_infra">
    Open-source models through Deep Infra's infrastructure.
  </Card>

<Card title="Fireworks" href="/oss/javascript/integrations/llms/fireworks" icon="fireworks">
    Fast inference for open-source language models.
  </Card>

<Card title="Friendli" href="/oss/javascript/integrations/llms/friendli">
    Optimized serving for efficient model inference.
  </Card>

<Card title="Google Vertex AI" href="/oss/javascript/integrations/llms/google_vertex_ai" icon="google">
    Google Cloud's enterprise AI and ML platform.
  </Card>

<Card title="Gradient AI" href="/oss/javascript/integrations/llms/gradient_ai">
    Private AI model training and deployment platform.
  </Card>

<Card title="Hugging Face" href="/oss/javascript/integrations/llms/huggingface_inference">
    Access thousands of models via Hugging Face Inference API.
  </Card>

<Card title="IBM" href="/oss/javascript/integrations/llms/ibm">
    IBM Watson AI and language model services.
  </Card>

<Card title="JigsawStack" href="/oss/javascript/integrations/llms/jigsawstack">
    JigsawStack's AI infrastructure and model services.
  </Card>

<Card title="LayerUp Security" href="/oss/javascript/integrations/llms/layerup_security">
    Security-enhanced LLM integration with monitoring.
  </Card>

<Card title="Llama.cpp" href="/oss/javascript/integrations/llms/llama_cpp">
    Run Llama models locally with C++ implementation.
  </Card>

<Card title="Mistral" href="/oss/javascript/integrations/llms/mistral" icon="mistral">
    Mistral's open-source and commercial language models.
  </Card>

<Card title="Neural Internet Bittensor" href="/oss/javascript/integrations/llms/ni_bittensor">
    Decentralized AI through Bittensor's peer-to-peer network.
  </Card>

<Card title="Ollama" href="/oss/javascript/integrations/llms/ollama" icon="ollama">
    Local model serving with Ollama's simple interface.
  </Card>

<Card title="OpenAI" href="/oss/javascript/integrations/llms/openai" icon="openai">
    GPT models and OpenAI's language model APIs.
  </Card>

<Card title="PromptLayer OpenAI" href="/oss/javascript/integrations/llms/prompt_layer_openai">
    OpenAI with PromptLayer's logging and observability.
  </Card>

<Card title="Raycast" href="/oss/javascript/integrations/llms/raycast">
    AI integration for Raycast productivity tool.
  </Card>

<Card title="Replicate" href="/oss/javascript/integrations/llms/replicate">
    Run open-source models through Replicate's cloud platform.
  </Card>

<Card title="Together AI" href="/oss/javascript/integrations/llms/together" icon="together">
    Fast inference for open-source models on Together's platform.
  </Card>

<Card title="Writer" href="/oss/javascript/integrations/llms/writer">
    Writer's enterprise AI platform for content generation.
  </Card>

<Card title="Yandex" href="/oss/javascript/integrations/llms/yandex">
    Yandex's language models and AI services.
  </Card>
</Columns>

## Text Embedding Models

<Columns cols={3}>
  <Card title="Alibaba Tongyi" href="/oss/javascript/integrations/text_embedding/alibaba_tongyi">
    Alibaba's embedding models for multilingual text representation.
  </Card>

<Card title="Azure OpenAI" href="/oss/javascript/integrations/text_embedding/azure_openai" icon="microsoft">
    OpenAI embeddings through Microsoft Azure platform.
  </Card>

<Card title="Baidu Qianfan" href="/oss/javascript/integrations/text_embedding/baidu_qianfan">
    Baidu's text embedding models for Chinese content.
  </Card>

<Card title="Amazon Bedrock" href="/oss/javascript/integrations/text_embedding/bedrock" icon="aws">
    Foundation model embeddings through Amazon Bedrock.
  </Card>

<Card title="ByteDance Doubao" href="/oss/javascript/integrations/text_embedding/bytedance_doubao">
    ByteDance's embedding models for content understanding.
  </Card>

<Card title="Cloudflare AI" href="/oss/javascript/integrations/text_embedding/cloudflare_ai">
    Text embeddings on Cloudflare's edge AI platform.
  </Card>

<Card title="Cohere" href="/oss/javascript/integrations/text_embedding/cohere" icon="cohere">
    Cohere's multilingual embedding models.
  </Card>

<Card title="DeepInfra" href="/oss/javascript/integrations/text_embedding/deepinfra">
    Open-source embedding models via DeepInfra.
  </Card>

<Card title="Fireworks" href="/oss/javascript/integrations/text_embedding/fireworks" icon="fireworks">
    Fast embedding inference through Fireworks platform.
  </Card>

<Card title="Google Generative AI" href="/oss/javascript/integrations/text_embedding/google_generativeai" icon="google">
    Google's embedding models for text representation.
  </Card>

<Card title="Google Vertex AI" href="/oss/javascript/integrations/text_embedding/google_vertex_ai" icon="google">
    Enterprise embedding models through Vertex AI.
  </Card>

<Card title="Gradient AI" href="/oss/javascript/integrations/text_embedding/gradient_ai">
    Private embedding models with Gradient AI platform.
  </Card>

<Card title="Hugging Face" href="/oss/javascript/integrations/text_embedding/hugging_face_inference">
    Thousands of embedding models via Hugging Face.
  </Card>

<Card title="IBM" href="/oss/javascript/integrations/text_embedding/ibm">
    IBM Watson embedding models and AI services.
  </Card>

<Card title="Jina" href="/oss/javascript/integrations/text_embedding/jina">
    Jina's neural search and embedding models.
  </Card>

<Card title="Llama.cpp" href="/oss/javascript/integrations/text_embedding/llama_cpp">
    Local embedding generation with llama.cpp.
  </Card>

<Card title="Minimax" href="/oss/javascript/integrations/text_embedding/minimax">
    Minimax's text embedding and representation models.
  </Card>

<Card title="Mistral" href="/oss/javascript/integrations/text_embedding/mistralai" icon="mistral">
    Mistral's efficient embedding models.
  </Card>

<Card title="MixedBread AI" href="/oss/javascript/integrations/text_embedding/mixedbread_ai">
    High-quality multilingual embedding models.
  </Card>

<Card title="Nomic" href="/oss/javascript/integrations/text_embedding/nomic">
    Nomic's open-source embedding models.
  </Card>

<Card title="Ollama" href="/oss/javascript/integrations/text_embedding/ollama" icon="ollama">
    Local embedding models through Ollama.
  </Card>

<Card title="OpenAI" href="/oss/javascript/integrations/text_embedding/openai" icon="openai">
    OpenAI's text-embedding models for semantic search.
  </Card>

<Card title="Pinecone" href="/oss/javascript/integrations/text_embedding/pinecone">
    Pinecone's embedding models and vector database.
  </Card>

<Card title="PremAI" href="/oss/javascript/integrations/text_embedding/premai">
    PremAI's embedding models and AI platform.
  </Card>

<Card title="Tencent Hunyuan" href="/oss/javascript/integrations/text_embedding/tencent_hunyuan">
    Tencent's embedding models for Chinese text.
  </Card>

<Card title="TensorFlow" href="/oss/javascript/integrations/text_embedding/tensorflow">
    TensorFlow-based embedding models and inference.
  </Card>

<Card title="Together AI" href="/oss/javascript/integrations/text_embedding/togetherai" icon="together">
    Open-source embedding models on Together platform.
  </Card>

<Card title="Transformers" href="/oss/javascript/integrations/text_embedding/transformers">
    Local transformer-based embedding models.
  </Card>

<Card title="Voyage AI" href="/oss/javascript/integrations/text_embedding/voyageai">
    Voyage AI's domain-specific embedding models.
  </Card>

<Card title="ZhipuAI" href="/oss/javascript/integrations/text_embedding/zhipuai">
    ZhipuAI's Chinese language embedding models.
  </Card>
</Columns>

<Columns cols={3}>
  <Card title="AnalyticDB" href="/oss/javascript/integrations/vectorstores/analyticdb">
    Alibaba Cloud's AnalyticDB for vector storage and search.
  </Card>

<Card title="AstraDB" href="/oss/javascript/integrations/vectorstores/astradb">
    DataStax Astra DB vector database for scalable storage.
  </Card>

<Card title="Azion EdgeSQL" href="/oss/javascript/integrations/vectorstores/azion-edgesql">
    Edge-based vector storage with Azion's EdgeSQL.
  </Card>

<Card title="Azure AI Search" href="/oss/javascript/integrations/vectorstores/azure_aisearch" icon="microsoft">
    Microsoft Azure's AI-powered search and vector storage.
  </Card>

<Card title="Azure Cosmos DB MongoDB" href="/oss/javascript/integrations/vectorstores/azure_cosmosdb_mongodb" icon="microsoft">
    Vector search in Azure Cosmos DB with MongoDB API.
  </Card>

<Card title="Azure Cosmos DB NoSQL" href="/oss/javascript/integrations/vectorstores/azure_cosmosdb_nosql" icon="microsoft">
    Vector storage in Azure Cosmos DB NoSQL API.
  </Card>

<Card title="Cassandra" href="/oss/javascript/integrations/vectorstores/cassandra">
    Apache Cassandra vector search capabilities.
  </Card>

<Card title="Chroma" href="/oss/javascript/integrations/vectorstores/chroma">
    Open-source embedding database for AI applications.
  </Card>

<Card title="ClickHouse" href="/oss/javascript/integrations/vectorstores/clickhouse">
    Fast columnar database with vector search support.
  </Card>

<Card title="CloseVector" href="/oss/javascript/integrations/vectorstores/closevector">
    High-performance vector database for similarity search.
  </Card>

<Card title="Cloudflare Vectorize" href="/oss/javascript/integrations/vectorstores/cloudflare_vectorize">
    Serverless vector database on Cloudflare's edge.
  </Card>

<Card title="Convex" href="/oss/javascript/integrations/vectorstores/convex">
    Full-stack platform with integrated vector storage.
  </Card>

<Card title="Couchbase Query" href="/oss/javascript/integrations/vectorstores/couchbase_query">
    Recommended vector search method in Couchbase NoSQL database via query service.
  </Card>

<Card title="Couchbase Search" href="/oss/javascript/integrations/vectorstores/couchbase_search">
    Alternative vector search method in Couchbase NoSQL database via search service.
  </Card>

<Card title="Elasticsearch" href="/oss/javascript/integrations/vectorstores/elasticsearch">
    Distributed search engine with vector search support.
  </Card>

<Card title="Faiss" href="/oss/javascript/integrations/vectorstores/faiss">
    Facebook's library for efficient similarity search.
  </Card>

<Card title="Google Cloud SQL PostgreSQL" href="/oss/javascript/integrations/vectorstores/google_cloudsql_pg" icon="google">
    PostgreSQL with vector extensions on Google Cloud.
  </Card>

<Card title="Google Vertex AI" href="/oss/javascript/integrations/vectorstores/googlevertexai" icon="google">
    Vector search through Google Vertex AI platform.
  </Card>

<Card title="SAP HANA Vector" href="/oss/javascript/integrations/vectorstores/hanavector">
    Enterprise vector database with SAP HANA.
  </Card>

<Card title="Hnswlib" href="/oss/javascript/integrations/vectorstores/hnswlib">
    Fast approximate nearest neighbor search library.
  </Card>

<Card title="LanceDB" href="/oss/javascript/integrations/vectorstores/lancedb">
    Developer-friendly embedded vector database.
  </Card>

<Card title="LibSQL" href="/oss/javascript/integrations/vectorstores/libsql">
    SQLite-compatible database with vector extensions.
  </Card>

<Card title="MariaDB" href="/oss/javascript/integrations/vectorstores/mariadb">
    Open-source database with vector search capabilities.
  </Card>

<Card title="Memory Vector Store" href="/oss/javascript/integrations/vectorstores/memory">
    In-memory vector storage for development and testing.
  </Card>

<Card title="Milvus" href="/oss/javascript/integrations/vectorstores/milvus">
    Open-source vector database for AI applications.
  </Card>

<Card title="Momento Vector Index" href="/oss/javascript/integrations/vectorstores/momento_vector_index">
    Serverless vector indexing with Momento's platform.
  </Card>

<Card title="MongoDB Atlas" href="/oss/javascript/integrations/vectorstores/mongodb_atlas">
    Vector search in MongoDB Atlas cloud database.
  </Card>

<Card title="MyScale" href="/oss/javascript/integrations/vectorstores/myscale">
    SQL-compatible vector database for analytics.
  </Card>

<Card title="Neo4j Vector" href="/oss/javascript/integrations/vectorstores/neo4jvector">
    Graph database with integrated vector search.
  </Card>

<Card title="Neon" href="/oss/javascript/integrations/vectorstores/neon">
    Serverless PostgreSQL with vector extensions.
  </Card>

<Card title="OpenSearch" href="/oss/javascript/integrations/vectorstores/opensearch">
    Open-source search engine with vector capabilities.
  </Card>

<Card title="PGVector" href="/oss/javascript/integrations/vectorstores/pgvector">
    PostgreSQL extension for vector similarity search.
  </Card>

<Card title="Pinecone" href="/oss/javascript/integrations/vectorstores/pinecone">
    Managed vector database for machine learning applications.
  </Card>

<Card title="Prisma" href="/oss/javascript/integrations/vectorstores/prisma">
    Type-safe database client with vector support.
  </Card>

<Card title="Qdrant" href="/oss/javascript/integrations/vectorstores/qdrant">
    Open-source vector similarity search engine.
  </Card>

<Card title="Redis" href="/oss/javascript/integrations/vectorstores/redis">
    In-memory database with vector search capabilities.
  </Card>

<Card title="Rockset" href="/oss/javascript/integrations/vectorstores/rockset">
    Real-time analytics database with vector search.
  </Card>

<Card title="SingleStore" href="/oss/javascript/integrations/vectorstores/singlestore">
    Distributed database with built-in vector functions.
  </Card>

<Card title="Supabase" href="/oss/javascript/integrations/vectorstores/supabase">
    Open-source Firebase alternative with vector support.
  </Card>

<Card title="Tigris" href="/oss/javascript/integrations/vectorstores/tigris">
    Developer-focused database with vector search.
  </Card>

<Card title="Turbopuffer" href="/oss/javascript/integrations/vectorstores/turbopuffer">
    High-performance vector database for embeddings.
  </Card>

<Card title="TypeORM" href="/oss/javascript/integrations/vectorstores/typeorm">
    TypeScript ORM with vector database support.
  </Card>

<Card title="Typesense" href="/oss/javascript/integrations/vectorstores/typesense">
    Open-source search engine with vector capabilities.
  </Card>

<Card title="Upstash Vector" href="/oss/javascript/integrations/vectorstores/upstash">
    Serverless vector database with Redis compatibility.
  </Card>

<Card title="USearch" href="/oss/javascript/integrations/vectorstores/usearch">
    Smaller and faster single-file vector search engine.
  </Card>

<Card title="Vectara" href="/oss/javascript/integrations/vectorstores/vectara">
    Neural search platform with built-in understanding.
  </Card>

<Card title="Vercel Postgres" href="/oss/javascript/integrations/vectorstores/vercel_postgres">
    PostgreSQL database with vector extensions on Vercel.
  </Card>

<Card title="Voy" href="/oss/javascript/integrations/vectorstores/voy">
    WebAssembly-based vector database for browsers.
  </Card>

<Card title="Weaviate" href="/oss/javascript/integrations/vectorstores/weaviate">
    Open-source vector database with GraphQL API.
  </Card>

<Card title="Xata" href="/oss/javascript/integrations/vectorstores/xata">
    Serverless database with built-in vector search.
  </Card>

<Card title="Zep Cloud" href="/oss/javascript/integrations/vectorstores/zep_cloud">
    Long-term memory for AI assistants in the cloud.
  </Card>

<Card title="Zep" href="/oss/javascript/integrations/vectorstores/zep">
    Long-term memory for AI assistants and agents.
  </Card>
</Columns>

<Columns cols={3}>
  <Card title="ChatGPT" href="/oss/javascript/integrations/document_loaders/file_loaders/chatgpt">
    Load and parse ChatGPT conversation exports.
  </Card>

<Card title="CSV" href="/oss/javascript/integrations/document_loaders/file_loaders/csv">
    Load data from CSV files with customizable parsing.
  </Card>

<Card title="Directory" href="/oss/javascript/integrations/document_loaders/file_loaders/directory">
    Recursively load documents from filesystem directories.
  </Card>

<Card title="DOCX" href="/oss/javascript/integrations/document_loaders/file_loaders/docx">
    Extract text and metadata from Microsoft Word documents.
  </Card>

<Card title="EPUB" href="/oss/javascript/integrations/document_loaders/file_loaders/epub">
    Load and parse EPUB e-book files.
  </Card>

<Card title="JSON" href="/oss/javascript/integrations/document_loaders/file_loaders/json">
    Load and parse JSON files with flexible structure handling.
  </Card>

<Card title="JSON Lines" href="/oss/javascript/integrations/document_loaders/file_loaders/jsonlines">
    Load newline-delimited JSON files.
  </Card>

<Card title="Multi-File" href="/oss/javascript/integrations/document_loaders/file_loaders/multi_file">
    Load multiple files of different types simultaneously.
  </Card>

<Card title="Notion Markdown" href="/oss/javascript/integrations/document_loaders/file_loaders/notion_markdown">
    Load Notion pages exported as Markdown.
  </Card>

<Card title="OpenAI Whisper Audio" href="/oss/javascript/integrations/document_loaders/file_loaders/openai_whisper_audio">
    Transcribe audio files using OpenAI's Whisper model.
  </Card>

<Card title="PDF" href="/oss/javascript/integrations/document_loaders/file_loaders/pdf">
    Extract text from PDF documents.
  </Card>

<Card title="PPTX" href="/oss/javascript/integrations/document_loaders/file_loaders/pptx">
    Load Microsoft PowerPoint presentations.
  </Card>

<Card title="Subtitles" href="/oss/javascript/integrations/document_loaders/file_loaders/subtitles">
    Load subtitle files (SRT, VTT formats).
  </Card>

<Card title="Text" href="/oss/javascript/integrations/document_loaders/file_loaders/text">
    Load plain text files with encoding detection.
  </Card>

<Card title="Unstructured" href="/oss/javascript/integrations/document_loaders/file_loaders/unstructured">
    Load various file formats using Unstructured.io.
  </Card>
</Columns>

<Columns cols={3}>
  <Card title="Airtable" href="/oss/javascript/integrations/document_loaders/web_loaders/airtable">
    Load records from Airtable bases.
  </Card>

<Card title="Apify Dataset" href="/oss/javascript/integrations/document_loaders/web_loaders/apify_dataset">
    Load data from Apify web scraping datasets.
  </Card>

<Card title="AssemblyAI Audio" href="/oss/javascript/integrations/document_loaders/web_loaders/assemblyai_audio_transcription">
    Transcribe audio using AssemblyAI's API.
  </Card>

<Card title="Azure Blob Storage Container" href="/oss/javascript/integrations/document_loaders/web_loaders/azure_blob_storage_container" icon="microsoft">
    Load files from Azure Blob Storage containers.
  </Card>

<Card title="Azure Blob Storage File" href="/oss/javascript/integrations/document_loaders/web_loaders/azure_blob_storage_file" icon="microsoft">
    Load individual files from Azure Blob Storage.
  </Card>

<Card title="Browserbase" href="/oss/javascript/integrations/document_loaders/web_loaders/browserbase">
    Load web content using Browserbase's cloud browsers.
  </Card>

<Card title="College Confidential" href="/oss/javascript/integrations/document_loaders/web_loaders/college_confidential">
    Scrape College Confidential forum content.
  </Card>

<Card title="Confluence" href="/oss/javascript/integrations/document_loaders/web_loaders/confluence">
    Load pages from Atlassian Confluence.
  </Card>

<Card title="Couchbase" href="/oss/javascript/integrations/document_loaders/web_loaders/couchbase">
    Load documents from Couchbase databases.
  </Card>

<Card title="Figma" href="/oss/javascript/integrations/document_loaders/web_loaders/figma">
    Load Figma design files and comments.
  </Card>

<Card title="Firecrawl" href="/oss/javascript/integrations/document_loaders/web_loaders/firecrawl">
    Crawl websites using Firecrawl's web scraping API.
  </Card>

<Card title="GitBook" href="/oss/javascript/integrations/document_loaders/web_loaders/gitbook">
    Load content from GitBook documentation sites.
  </Card>

<Card title="GitHub" href="/oss/javascript/integrations/document_loaders/web_loaders/github">
    Load files and repositories from GitHub.
  </Card>

<Card title="Google Cloud Storage" href="/oss/javascript/integrations/document_loaders/web_loaders/google_cloud_storage" icon="google">
    Load files from Google Cloud Storage buckets.
  </Card>

<Card title="Google Cloud SQL PostgreSQL" href="/oss/javascript/integrations/document_loaders/web_loaders/google_cloudsql_pg" icon="google">
    Load data from Google Cloud SQL PostgreSQL databases.
  </Card>

<Card title="Hacker News" href="/oss/javascript/integrations/document_loaders/web_loaders/hn">
    Load posts and comments from Hacker News.
  </Card>

<Card title="IMSDb" href="/oss/javascript/integrations/document_loaders/web_loaders/imsdb">
    Load movie scripts from the Internet Movie Script Database.
  </Card>

<Card title="Jira" href="/oss/javascript/integrations/document_loaders/web_loaders/jira">
    Load issues and projects from Atlassian Jira.
  </Card>

<Card title="LangSmith" href="/oss/javascript/integrations/document_loaders/web_loaders/langsmith">
    Load runs and datasets from LangSmith.
  </Card>

<Card title="Notion API" href="/oss/javascript/integrations/document_loaders/web_loaders/notionapi">
    Load pages and databases from Notion.
  </Card>

<Card title="PDF (Web)" href="/oss/javascript/integrations/document_loaders/web_loaders/pdf">
    Load PDF files from web URLs.
  </Card>

<Card title="Recursive URL" href="/oss/javascript/integrations/document_loaders/web_loaders/recursive_url_loader">
    Recursively crawl and load web pages.
  </Card>

<Card title="S3" href="/oss/javascript/integrations/document_loaders/web_loaders/s3" icon="aws">
    Load files from Amazon S3 buckets.
  </Card>

<Card title="SearchAPI" href="/oss/javascript/integrations/document_loaders/web_loaders/searchapi">
    Load search results using SearchAPI.
  </Card>

<Card title="SerpAPI" href="/oss/javascript/integrations/document_loaders/web_loaders/serpapi">
    Load search results using SerpAPI.
  </Card>

<Card title="Sitemap" href="/oss/javascript/integrations/document_loaders/web_loaders/sitemap">
    Load URLs from website sitemaps.
  </Card>

<Card title="Sonix Audio" href="/oss/javascript/integrations/document_loaders/web_loaders/sonix_audio_transcription">
    Transcribe audio using Sonix's transcription API.
  </Card>

<Card title="Sort.xyz Blockchain" href="/oss/javascript/integrations/document_loaders/web_loaders/sort_xyz_blockchain">
    Load blockchain data from Sort.xyz.
  </Card>

<Card title="Spider" href="/oss/javascript/integrations/document_loaders/web_loaders/spider">
    Fast web crawling using Spider API.
  </Card>

<Card title="Taskade" href="/oss/javascript/integrations/document_loaders/web_loaders/taskade">
    Load projects and tasks from Taskade.
  </Card>

<Card title="Web Cheerio" href="/oss/javascript/integrations/document_loaders/web_loaders/web_cheerio">
    Scrape web pages using Cheerio for server-side parsing.
  </Card>

<Card title="Web Playwright" href="/oss/javascript/integrations/document_loaders/web_loaders/web_playwright">
    Load dynamic web content using Playwright browser automation.
  </Card>

<Card title="Web Puppeteer" href="/oss/javascript/integrations/document_loaders/web_loaders/web_puppeteer">
    Scrape JavaScript-heavy websites using Puppeteer.
  </Card>

<Card title="YouTube" href="/oss/javascript/integrations/document_loaders/web_loaders/youtube">
    Load YouTube video transcripts and metadata.
  </Card>
</Columns>

## Document Transformers

<Columns cols={3}>
  <Card title="HTML to Text" href="/oss/javascript/integrations/document_transformers/html-to-text">
    Convert HTML content to clean, readable text.
  </Card>

<Card title="Mozilla Readability" href="/oss/javascript/integrations/document_transformers/mozilla_readability">
    Extract main content from web pages using Mozilla's Readability.
  </Card>

<Card title="OpenAI Metadata Tagger" href="/oss/javascript/integrations/document_transformers/openai_metadata_tagger" icon="openai">
    Generate metadata tags for documents using OpenAI.
  </Card>
</Columns>

## Document Compressors

<Columns cols={3}>
  <Card title="Cohere Rerank" href="/oss/javascript/integrations/document_compressors/cohere_rerank" icon="cohere">
    Rerank documents using Cohere's reranking models.
  </Card>

<Card title="IBM" href="/oss/javascript/integrations/document_compressors/ibm">
    Document compression using IBM Watson AI services.
  </Card>

<Card title="MixedBread AI" href="/oss/javascript/integrations/document_compressors/mixedbread_ai">
    Rerank and compress documents using MixedBread AI.
  </Card>
</Columns>

<Columns cols={3}>
  <Card title="AI Plugin Tool" href="/oss/javascript/integrations/tools/aiplugin-tool">
    Execute OpenAI ChatGPT plugins as tools.
  </Card>

<Card title="Azure Dynamic Sessions" href="/oss/javascript/integrations/tools/azure_dynamic_sessions" icon="microsoft">
    Secure code execution in Azure Dynamic Sessions.
  </Card>

<Card title="Connery" href="/oss/javascript/integrations/tools/connery">
    Modular AI actions and integrations with Connery.
  </Card>

<Card title="Connery Toolkit" href="/oss/javascript/integrations/tools/connery_toolkit">
    Access Connery's toolkit of pre-built actions.
  </Card>

<Card title="DALL-E" href="/oss/javascript/integrations/tools/dalle" icon="openai">
    Generate images using OpenAI's DALL-E models.
  </Card>

<Card title="Decodo" href="/oss/javascript/integrations/tools/decodo">
    Code execution and analysis with Decodo.
  </Card>

<Card title="Discord" href="/oss/javascript/integrations/tools/discord_tool">
    Interact with Discord servers and channels.
  </Card>

<Card title="DuckDuckGo Search" href="/oss/javascript/integrations/tools/duckduckgo_search">
    Privacy-focused web search with DuckDuckGo.
  </Card>

<Card title="Exa Search" href="/oss/javascript/integrations/tools/exa_search">
    AI-powered search engine for better results.
  </Card>

<Card title="Gmail" href="/oss/javascript/integrations/tools/gmail" icon="google">
    Read and send emails through Gmail API.
  </Card>

<Card title="Goat" href="/oss/javascript/integrations/tools/goat">
    Simple tool execution framework.
  </Card>

<Card title="Google Calendar" href="/oss/javascript/integrations/tools/google_calendar" icon="google">
    Manage events and schedules in Google Calendar.
  </Card>

<Card title="Google Places" href="/oss/javascript/integrations/tools/google_places" icon="google">
    Search for places using Google Places API.
  </Card>

<Card title="Google Routes" href="/oss/javascript/integrations/tools/google_routes" icon="google">
    Get directions and routing information.
  </Card>

<Card title="Google Scholar" href="/oss/javascript/integrations/tools/google_scholar" icon="google">
    Search academic papers and citations.
  </Card>

<Card title="Google Trends" href="/oss/javascript/integrations/tools/google_trends" icon="google">
    Analyze search trends and popularity data.
  </Card>

<Card title="IBM" href="/oss/javascript/integrations/tools/ibm">
    Access IBM Watson AI tools and services.
  </Card>

<Card title="JigsawStack" href="/oss/javascript/integrations/tools/jigsawstack">
    AI infrastructure tools from JigsawStack.
  </Card>

<Card title="JSON Tool" href="/oss/javascript/integrations/tools/json">
    Parse and manipulate JSON data structures.
  </Card>

<Card title="Lambda Agent" href="/oss/javascript/integrations/tools/lambda_agent" icon="aws">
    Execute code in AWS Lambda functions.
  </Card>

<Card title="MCP Toolbox" href="/oss/javascript/integrations/tools/mcp_toolbox">
    Model Context Protocol tools and utilities.
  </Card>

<Card title="OpenAPI" href="/oss/javascript/integrations/tools/openapi">
    Generate tools from OpenAPI specifications.
  </Card>

<Card title="Python Interpreter" href="/oss/javascript/integrations/tools/pyinterpreter">
    Execute Python code in a sandboxed environment.
  </Card>

<Card title="SearchAPI" href="/oss/javascript/integrations/tools/searchapi">
    Web search capabilities through SearchAPI.
  </Card>

<Card title="SearXNG" href="/oss/javascript/integrations/tools/searxng">
    Privacy-respecting metasearch engine.
  </Card>

<Card title="SerpAPI" href="/oss/javascript/integrations/tools/serpapi">
    Google Search results through SerpAPI.
  </Card>

<Card title="Step Functions Agent" href="/oss/javascript/integrations/tools/sfn_agent" icon="aws">
    Execute AWS Step Functions workflows.
  </Card>

<Card title="SQL" href="/oss/javascript/integrations/tools/sql">
    Query databases using natural language.
  </Card>

<Card title="StackExchange" href="/oss/javascript/integrations/tools/stackexchange">
    Search Stack Overflow and other SE sites.
  </Card>

<Card title="Stagehand" href="/oss/javascript/integrations/tools/stagehand">
    Browser automation for web interactions.
  </Card>

<Card title="Tavily Crawl" href="/oss/javascript/integrations/tools/tavily_crawl">
    Web crawling capabilities with Tavily.
  </Card>

<Card title="Tavily Extract" href="/oss/javascript/integrations/tools/tavily_extract">
    Extract structured data from web pages.
  </Card>

<Card title="Tavily Map" href="/oss/javascript/integrations/tools/tavily_map">
    Map and visualize web crawling results.
  </Card>

<Card title="Tavily Search" href="/oss/javascript/integrations/tools/tavily_search">
    AI-optimized search for retrieval applications.
  </Card>

<Card title="Tavily Search Community" href="/oss/javascript/integrations/tools/tavily_search_community">
    Community-powered search through Tavily.
  </Card>

<Card title="Vector Store" href="/oss/javascript/integrations/tools/vectorstore">
    Query vector databases as tools.
  </Card>

<Card title="Web Browser" href="/oss/javascript/integrations/tools/webbrowser">
    Automated web browsing and interaction.
  </Card>

<Card title="Wikipedia" href="/oss/javascript/integrations/tools/wikipedia">
    Search and retrieve Wikipedia articles.
  </Card>

<Card title="Wolfram Alpha" href="/oss/javascript/integrations/tools/wolframalpha">
    Computational knowledge through Wolfram Alpha.
  </Card>

<Card title="Zapier Agent" href="/oss/javascript/integrations/tools/zapier_agent">
    Automate workflows using Zapier integrations.
  </Card>
</Columns>

<Columns cols={3}>
  <Card title="ArXiv" href="/oss/javascript/integrations/retrievers/arxiv-retriever">
    Search and retrieve academic papers from ArXiv.
  </Card>

<Card title="Azion EdgeSQL" href="/oss/javascript/integrations/retrievers/azion-edgesql">
    Edge-based document retrieval with Azion.
  </Card>

<Card title="Bedrock Knowledge Bases" href="/oss/javascript/integrations/retrievers/bedrock-knowledge-bases" icon="aws">
    Retrieve from Amazon Bedrock Knowledge Bases.
  </Card>

<Card title="BM25" href="/oss/javascript/integrations/retrievers/bm25">
    BM25 algorithm for keyword-based retrieval.
  </Card>

<Card title="Chaindesk" href="/oss/javascript/integrations/retrievers/chaindesk-retriever">
    Document retrieval using Chaindesk platform.
  </Card>

<Card title="ChatGPT Retriever Plugin" href="/oss/javascript/integrations/retrievers/chatgpt-retriever-plugin" icon="openai">
    Official ChatGPT retriever plugin integration.
  </Card>

<Card title="Dria" href="/oss/javascript/integrations/retrievers/dria">
    Decentralized knowledge retrieval with Dria.
  </Card>

<Card title="Exa" href="/oss/javascript/integrations/retrievers/exa">
    AI-powered web search and retrieval.
  </Card>

<Card title="HyDE" href="/oss/javascript/integrations/retrievers/hyde">
    Hypothetical Document Embeddings for better retrieval.
  </Card>

<Card title="Kendra" href="/oss/javascript/integrations/retrievers/kendra-retriever" icon="aws">
    Enterprise search with Amazon Kendra.
  </Card>

<Card title="Metal" href="/oss/javascript/integrations/retrievers/metal-retriever">
    Managed vector search with Metal.
  </Card>

<Card title="Supabase Hybrid" href="/oss/javascript/integrations/retrievers/supabase-hybrid">
    Hybrid search combining vector and keyword search.
  </Card>

<Card title="Tavily" href="/oss/javascript/integrations/retrievers/tavily">
    AI-optimized search for RAG applications.
  </Card>

<Card title="Time-Weighted" href="/oss/javascript/integrations/retrievers/time-weighted-retriever">
    Time-aware document retrieval and ranking.
  </Card>

<Card title="Vespa" href="/oss/javascript/integrations/retrievers/vespa-retriever">
    Big data serving engine for vector search.
  </Card>

<Card title="Zep Cloud" href="/oss/javascript/integrations/retrievers/zep-cloud-retriever">
    Cloud-based long-term memory retrieval.
  </Card>

<Card title="Zep" href="/oss/javascript/integrations/retrievers/zep-retriever">
    Long-term memory and context retrieval.
  </Card>
</Columns>

<Columns cols={3}>
  <Card title="Cassandra Storage" href="/oss/javascript/integrations/stores/cassandra_storage">
    Distributed key-value storage using Cassandra.
  </Card>

<Card title="File System" href="/oss/javascript/integrations/stores/file_system">
    Local file system storage for development.
  </Card>

<Card title="In-Memory" href="/oss/javascript/integrations/stores/in_memory">
    Fast in-memory storage for temporary data.
  </Card>

<Card title="IoRedis Storage" href="/oss/javascript/integrations/stores/ioredis_storage">
    Redis-based storage using IoRedis client.
  </Card>

<Card title="Upstash Redis Storage" href="/oss/javascript/integrations/stores/upstash_redis_storage">
    Serverless Redis storage with Upstash.
  </Card>

<Card title="Vercel KV Storage" href="/oss/javascript/integrations/stores/vercel_kv_storage">
    Key-value storage on Vercel's edge network.
  </Card>
</Columns>

<Columns cols={3}>
  <Card title="Azure Cosmos DB NoSQL" href="/oss/javascript/integrations/llm_caching/azure_cosmosdb_nosql" icon="microsoft">
    Cache LLM responses in Azure Cosmos DB.
  </Card>
</Columns>

<Columns cols={3}>
  <Card title="Datadog Tracer" href="/oss/javascript/integrations/callbacks/datadog_tracer">
    Monitor and trace LangChain applications with Datadog.
  </Card>

<Card title="Upstash Rate Limit" href="/oss/javascript/integrations/callbacks/upstash_ratelimit_callback">
    Rate limiting for AI applications using Upstash.
  </Card>
</Columns>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/javascript/integrations/providers/all_providers.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## All integration providers

**URL:** llms-txt#all-integration-providers

**Contents:**
- Providers

Source: https://docs.langchain.com/oss/python/integrations/providers/all_providers

Browse the complete collection of integrations available for Python. LangChain Python offers the most extensive ecosystem with 1000+ integrations across LLMs, chat models, retrievers, vector stores, document loaders, and more.

<Columns cols={3}>
  <Card title="Abso" href="/oss/python/integrations/providers/abso" icon="link">
    Custom AI integration platform for enterprise workflows.
  </Card>

<Card title="Acreom" href="/oss/python/integrations/providers/acreom" icon="link">
    Knowledge management platform with AI-powered organization.
  </Card>

<Card title="ActiveLoop DeepLake" href="/oss/python/integrations/providers/activeloop_deeplake" icon="link">
    Vector database for AI applications with deep learning focus.
  </Card>

<Card title="Ads4GPTs" href="/oss/python/integrations/providers/ads4gpts" icon="link">
    Advertising platform for GPT applications and AI services.
  </Card>

<Card title="AgentQL" href="/oss/python/integrations/providers/agentql" icon="link">
    Web scraping with natural language queries.
  </Card>

<Card title="AI21" href="/oss/python/integrations/providers/ai21" icon="link">
    AI21 Labs' Jurassic models for text generation.
  </Card>

<Card title="AIM Tracking" href="/oss/python/integrations/providers/aim_tracking" icon="link">
    Experiment tracking and management platform.
  </Card>

<Card title="AI/ML API" href="/oss/python/integrations/providers/aimlapi" icon="link">
    Unified API for multiple AI and ML services.
  </Card>

<Card title="AI Network" href="/oss/python/integrations/providers/ainetwork" icon="link">
    Decentralized AI computing network platform.
  </Card>

<Card title="Airbyte" href="/oss/python/integrations/providers/airbyte" icon="link">
    Data integration platform for ETL and ELT pipelines.
  </Card>

<Card title="Airtable" href="/oss/python/integrations/providers/airtable" icon="link">
    Cloud-based spreadsheet and database platform.
  </Card>

<Card title="Alchemy" href="/oss/python/integrations/providers/alchemy" icon="link">
    Blockchain development platform and APIs.
  </Card>

<Card title="Aleph Alpha" href="/oss/python/integrations/providers/aleph_alpha" icon="link">
    European AI company's multilingual language models.
  </Card>

<Card title="Alibaba Cloud" href="/oss/python/integrations/providers/alibaba_cloud" icon="link">
    Alibaba's cloud computing and AI services.
  </Card>

<Card title="AnalyticDB" href="/oss/python/integrations/providers/analyticdb" icon="link">
    Alibaba Cloud's real-time analytics database.
  </Card>

<Card title="Anchor Browser" href="/oss/python/integrations/providers/anchor_browser" icon="link">
    Browser automation and web scraping tools.
  </Card>

<Card title="Annoy" href="/oss/python/integrations/providers/annoy" icon="link">
    Approximate nearest neighbors search library.
  </Card>

<Card title="Anthropic" href="/oss/python/integrations/providers/anthropic" icon="anthropic">
    Claude models for advanced reasoning and conversation.
  </Card>

<Card title="Anyscale" href="/oss/python/integrations/providers/anyscale" icon="link">
    Distributed computing platform for ML workloads.
  </Card>

<Card title="Apache Doris" href="/oss/python/integrations/providers/apache_doris" icon="link">
    Real-time analytical database management system.
  </Card>

<Card title="Apache" href="/oss/python/integrations/providers/apache" icon="link">
    Apache Software Foundation tools and libraries.
  </Card>

<Card title="Apify" href="/oss/python/integrations/providers/apify" icon="link">
    Web scraping and automation platform.
  </Card>

<Card title="Apple" href="/oss/python/integrations/providers/apple" icon="link">
    Apple's machine learning and AI frameworks.
  </Card>

<Card title="ArangoDB" href="/oss/python/integrations/providers/arangodb" icon="link">
    Multi-model database with graph capabilities.
  </Card>

<Card title="Arcee" href="/oss/python/integrations/providers/arcee" icon="link">
    Domain-specific language model training platform.
  </Card>

<Card title="ArcGIS" href="/oss/python/integrations/providers/arcgis" icon="link">
    Geographic information system platform.
  </Card>

<Card title="Argilla" href="/oss/python/integrations/providers/argilla" icon="link">
    Data labeling and annotation platform for NLP.
  </Card>

<Card title="Arize" href="/oss/python/integrations/providers/arize" icon="link">
    ML observability and performance monitoring.
  </Card>

<Card title="Arthur Tracking" href="/oss/python/integrations/providers/arthur_tracking" icon="link">
    AI model monitoring and governance platform.
  </Card>

<Card title="arXiv" href="/oss/python/integrations/providers/arxiv" icon="link">
    Academic paper repository and search platform.
  </Card>

<Card title="Ascend" href="/oss/python/integrations/providers/ascend" icon="link">
    Data engineering and pipeline automation platform.
  </Card>

<Card title="Ask News" href="/oss/python/integrations/providers/asknews" icon="link">
    Real-time news search and analysis API.
  </Card>

<Card title="AssemblyAI" href="/oss/python/integrations/providers/assemblyai" icon="link">
    Speech-to-text and audio intelligence API.
  </Card>

<Card title="AstraDB" href="/oss/python/integrations/providers/astradb" icon="link">
    DataStax Astra DB vector database platform.
  </Card>

<Card title="Atlas" href="/oss/python/integrations/providers/atlas" icon="link">
    Data visualization and exploration platform.
  </Card>

<Card title="AwaDB" href="/oss/python/integrations/providers/awadb" icon="link">
    Vector database for AI and ML applications.
  </Card>

<Card title="AWS" href="/oss/python/integrations/providers/aws" icon="aws">
    Amazon Web Services cloud platform and AI services.
  </Card>

<Card title="AZLyrics" href="/oss/python/integrations/providers/azlyrics" icon="link">
    Song lyrics database and search platform.
  </Card>

<Card title="Azure AI" href="/oss/python/integrations/providers/azure_ai" icon="microsoft">
    Microsoft Azure AI and cognitive services.
  </Card>

<Card title="BAAI" href="/oss/python/integrations/providers/baai" icon="link">
    Beijing Academy of AI research and models.
  </Card>

<Card title="Bagel" href="/oss/python/integrations/providers/bagel" icon="link">
    Vector database and semantic search platform.
  </Card>

<Card title="BagelDB" href="/oss/python/integrations/providers/bageldb" icon="link">
    Multi-modal AI database and storage system.
  </Card>

<Card title="Baichuan" href="/oss/python/integrations/providers/baichuan" icon="link">
    Chinese language model from Baichuan AI.
  </Card>

<Card title="Baidu" href="/oss/python/integrations/providers/baidu" icon="link">
    Baidu's AI services and language models.
  </Card>

<Card title="BananaDev" href="/oss/python/integrations/providers/bananadev" icon="link">
    Serverless GPU infrastructure for ML models.
  </Card>

<Card title="Baseten" href="/oss/python/integrations/providers/baseten" icon="link">
    ML model deployment and serving platform.
  </Card>

<Card title="Beam" href="/oss/python/integrations/providers/beam" icon="link">
    Serverless GPU computing platform.
  </Card>

<Card title="Beautiful Soup" href="/oss/python/integrations/providers/beautiful_soup" icon="link">
    HTML and XML parsing library for web scraping.
  </Card>

<Card title="BibTeX" href="/oss/python/integrations/providers/bibtex" icon="link">
    Bibliography management and citation format.
  </Card>

<Card title="Bilibili" href="/oss/python/integrations/providers/bilibili" icon="link">
    Chinese video sharing platform integration.
  </Card>

<Card title="Bittensor" href="/oss/python/integrations/providers/bittensor" icon="link">
    Decentralized AI network and incentive protocol.
  </Card>

<Card title="Blackboard" href="/oss/python/integrations/providers/blackboard" icon="link">
    Educational technology and learning management.
  </Card>

<Card title="Bodo DataFrames" href="/oss/python/integrations/providers/bodo" icon="link">
    High-performance analytics and data processing.
  </Card>

<Card title="BookendAI" href="/oss/python/integrations/providers/bookendai" icon="link">
    AI-powered reading and research assistant.
  </Card>

<Card title="Box" href="/oss/python/integrations/providers/box" icon="link">
    Cloud content management and collaboration.
  </Card>

<Card title="Brave Search" href="/oss/python/integrations/providers/brave_search" icon="link">
    Privacy-focused search engine API.
  </Card>

<Card title="Breebs" href="/oss/python/integrations/providers/breebs" icon="link">
    AI knowledge management and retrieval platform.
  </Card>

<Card title="Brightdata" href="/oss/python/integrations/providers/brightdata" icon="link">
    Web data platform and proxy services.
  </Card>

<Card title="Browserbase" href="/oss/python/integrations/providers/browserbase" icon="link">
    Headless browser automation platform.
  </Card>

<Card title="Browserless" href="/oss/python/integrations/providers/browserless" icon="link">
    Serverless browser automation service.
  </Card>

<Card title="ByteDance" href="/oss/python/integrations/providers/byte_dance" icon="link">
    ByteDance's AI models and services.
  </Card>

<Card title="Cassandra" href="/oss/python/integrations/providers/cassandra" icon="link">
    Distributed NoSQL database management system.
  </Card>

<Card title="Cerebras" href="/oss/python/integrations/providers/cerebras" icon="link">
    AI compute platform with specialized processors.
  </Card>

<Card title="CerebriumAI" href="/oss/python/integrations/providers/cerebriumai" icon="link">
    Serverless GPU platform for AI applications.
  </Card>

<Card title="Chaindesk" href="/oss/python/integrations/providers/chaindesk" icon="link">
    No-code AI chatbot and automation platform.
  </Card>

<Card title="Chroma" href="/oss/python/integrations/providers/chroma" icon="link">
    Open-source embedding database for AI apps.
  </Card>

<Card title="Clarifai" href="/oss/python/integrations/providers/clarifai" icon="link">
    Computer vision and AI model platform.
  </Card>

<Card title="ClearML Tracking" href="/oss/python/integrations/providers/clearml_tracking" icon="link">
    ML experiment tracking and automation.
  </Card>

<Card title="ClickHouse" href="/oss/python/integrations/providers/clickhouse" icon="link">
    Fast columnar database for analytics.
  </Card>

<Card title="ClickUp" href="/oss/python/integrations/providers/clickup" icon="link">
    Project management and productivity platform.
  </Card>

<Card title="Cloudflare" href="/oss/python/integrations/providers/cloudflare" icon="link">
    Web infrastructure and security services.
  </Card>

<Card title="Clova" href="/oss/python/integrations/providers/clova" icon="link">
    Naver's AI assistant and NLP platform.
  </Card>

<Card title="CnosDB" href="/oss/python/integrations/providers/cnosdb" icon="link">
    Time series database for IoT and analytics.
  </Card>

<Card title="Cognee" href="/oss/python/integrations/providers/cognee" icon="link">
    Memory layer for AI applications and agents.
  </Card>

<Card title="CogniSwitch" href="/oss/python/integrations/providers/cogniswitch" icon="link">
    AI knowledge management and retrieval system.
  </Card>

<Card title="Cohere" href="/oss/python/integrations/providers/cohere" icon="link">
    Language AI platform for enterprise applications.
  </Card>

<Card title="College Confidential" href="/oss/python/integrations/providers/college_confidential" icon="link">
    College admissions and education platform.
  </Card>

<Card title="Comet Tracking" href="/oss/python/integrations/providers/comet_tracking" icon="link">
    ML experiment tracking and model management.
  </Card>

<Card title="Confident" href="/oss/python/integrations/providers/confident" icon="link">
    AI observability and monitoring platform.
  </Card>

<Card title="Confluence" href="/oss/python/integrations/providers/confluence" icon="link">
    Team collaboration and documentation platform.
  </Card>

<Card title="Connery" href="/oss/python/integrations/providers/connery" icon="link">
    Plugin system for AI agents and applications.
  </Card>

<Card title="Context" href="/oss/python/integrations/providers/context" icon="link">
    Context management for AI applications.
  </Card>

<Card title="Contextual" href="/oss/python/integrations/providers/contextual" icon="link">
    Contextual AI and language understanding.
  </Card>

<Card title="Couchbase" href="/oss/python/integrations/providers/couchbase" icon="link">
    NoSQL cloud database platform.
  </Card>

<Card title="Coze" href="/oss/python/integrations/providers/coze" icon="link">
    Conversational AI platform and chatbot builder.
  </Card>

<Card title="CrateDB" href="/oss/python/integrations/providers/cratedb" icon="link">
    Distributed SQL database for machine data.
  </Card>

<Card title="CTransformers" href="/oss/python/integrations/providers/ctransformers" icon="link">
    Python bindings for transformer models in C/C++.
  </Card>

<Card title="CTranslate2" href="/oss/python/integrations/providers/ctranslate2" icon="link">
    Fast inference engine for Transformer models.
  </Card>

<Card title="Cube" href="/oss/python/integrations/providers/cube" icon="link">
    Semantic layer for building data applications.
  </Card>

<Card title="Dappier" href="/oss/python/integrations/providers/dappier" icon="link">
    Real-time AI data platform and API.
  </Card>

<Card title="DashVector" href="/oss/python/integrations/providers/dashvector" icon="link">
    Alibaba Cloud's vector database service.
  </Card>

<Card title="Databricks" href="/oss/python/integrations/providers/databricks" icon="link">
    Unified analytics platform for big data and ML.
  </Card>

<Card title="Datadog" href="/oss/python/integrations/providers/datadog" icon="link">
    Monitoring and analytics platform for applications.
  </Card>

<Card title="Datadog Logs" href="/oss/python/integrations/providers/datadog_logs" icon="link">
    Log management and analysis platform.
  </Card>

<Card title="DataForSEO" href="/oss/python/integrations/providers/dataforseo" icon="link">
    SEO and SERP data API platform.
  </Card>

<Card title="DataHerald" href="/oss/python/integrations/providers/dataherald" icon="link">
    Natural language to SQL query platform.
  </Card>

<Card title="Dedoc" href="/oss/python/integrations/providers/dedoc" icon="link">
    Document analysis and structure detection.
  </Card>

<Card title="DeepInfra" href="/oss/python/integrations/providers/deepinfra" icon="link">
    Serverless inference for deep learning models.
  </Card>

<Card title="DeepLake" href="/oss/python/integrations/providers/deeplake" icon="link">
    Vector database for deep learning applications.
  </Card>

<Card title="DeepSeek" href="/oss/python/integrations/providers/deepseek" icon="link">
    Advanced reasoning and coding AI models.
  </Card>

<Card title="DeepSparse" href="/oss/python/integrations/providers/deepsparse" icon="link">
    Inference runtime for sparse neural networks.
  </Card>

<Card title="Dell" href="/oss/python/integrations/providers/dell" icon="link">
    Dell Technologies AI and computing solutions.
  </Card>

<Card title="Diffbot" href="/oss/python/integrations/providers/diffbot" icon="link">
    Web data extraction and knowledge graph.
  </Card>

<Card title="Dingo" href="/oss/python/integrations/providers/dingo" icon="link">
    Distributed vector database system.
  </Card>

<Card title="Discord" href="/oss/python/integrations/providers/discord" icon="link">
    Communication platform integration and bots.
  </Card>

<Card title="Discord Shikenso" href="/oss/python/integrations/providers/discord-shikenso" icon="link">
    Discord analytics and moderation tools.
  </Card>

<Card title="DocArray" href="/oss/python/integrations/providers/docarray" icon="link">
    Data structure for multimodal AI applications.
  </Card>

<Card title="Docling" href="/oss/python/integrations/providers/docling" icon="link">
    Document processing and AI integration.
  </Card>

<Card title="Doctran" href="/oss/python/integrations/providers/doctran" icon="link">
    Document transformation and processing.
  </Card>

<Card title="Docugami" href="/oss/python/integrations/providers/docugami" icon="link">
    Document AI and semantic processing.
  </Card>

<Card title="Docusaurus" href="/oss/python/integrations/providers/docusaurus" icon="link">
    Documentation website generator and platform.
  </Card>

<Card title="Dria" href="/oss/python/integrations/providers/dria" icon="link">
    Decentralized knowledge retrieval network.
  </Card>

<Card title="Dropbox" href="/oss/python/integrations/providers/dropbox" icon="link">
    Cloud storage and file sharing platform.
  </Card>

<Card title="DuckDB" href="/oss/python/integrations/providers/duckdb" icon="link">
    In-process SQL OLAP database management system.
  </Card>

<Card title="DuckDuckGo Search" href="/oss/python/integrations/providers/duckduckgo_search" icon="link">
    Privacy-focused search engine integration.
  </Card>

<Card title="E2B" href="/oss/python/integrations/providers/e2b" icon="link">
    Cloud development environment platform.
  </Card>

<Card title="EdenAI" href="/oss/python/integrations/providers/edenai" icon="link">
    Unified API for multiple AI services.
  </Card>

<Card title="Elasticsearch" href="/oss/python/integrations/providers/elasticsearch" icon="link">
    Distributed search and analytics engine.
  </Card>

<Card title="ElevenLabs" href="/oss/python/integrations/providers/elevenlabs" icon="link">
    AI voice synthesis and speech platform.
  </Card>

<Card title="EmbedChain" href="/oss/python/integrations/providers/embedchain" icon="link">
    Framework for creating RAG applications.
  </Card>

<Card title="Epsilla" href="/oss/python/integrations/providers/epsilla" icon="link">
    Vector database for AI and ML applications.
  </Card>

<Card title="Etherscan" href="/oss/python/integrations/providers/etherscan" icon="link">
    Ethereum blockchain explorer and analytics.
  </Card>

<Card title="EverlyAI" href="/oss/python/integrations/providers/everlyai" icon="link">
    Serverless AI inference platform.
  </Card>

<Card title="Evernote" href="/oss/python/integrations/providers/evernote" icon="link">
    Note-taking and organization platform.
  </Card>

<Card title="Exa Search" href="/oss/python/integrations/providers/exa_search" icon="link">
    AI-powered search engine for developers.
  </Card>

<Card title="Facebook" href="/oss/python/integrations/providers/facebook" icon="link">
    Meta's social platform integration and APIs.
  </Card>

<Card title="FalkorDB" href="/oss/python/integrations/providers/falkordb" icon="link">
    Graph database with ultra-low latency.
  </Card>

<Card title="Fauna" href="/oss/python/integrations/providers/fauna" icon="link">
    Serverless, globally distributed database.
  </Card>

<Card title="Featherless AI" href="/oss/python/integrations/providers/featherless-ai" icon="link">
    Fast and efficient AI model serving.
  </Card>

<Card title="Fiddler" href="/oss/python/integrations/providers/fiddler" icon="link">
    AI observability and monitoring platform.
  </Card>

<Card title="Figma" href="/oss/python/integrations/providers/figma" icon="link">
    Design collaboration and prototyping platform.
  </Card>

<Card title="FireCrawl" href="/oss/python/integrations/providers/firecrawl" icon="link">
    Web scraping and crawling API service.
  </Card>

<Card title="Fireworks" href="/oss/python/integrations/providers/fireworks" icon="link">
    Fast inference platform for open-source models.
  </Card>

<Card title="Flyte" href="/oss/python/integrations/providers/flyte" icon="link">
    Workflow orchestration for ML and data processing.
  </Card>

<Card title="FMP Data" href="/oss/python/integrations/providers/fmp-data" icon="link">
    Financial market data and analytics API.
  </Card>

<Card title="ForefrontAI" href="/oss/python/integrations/providers/forefrontai" icon="link">
    Fine-tuning platform for language models.
  </Card>

<Card title="Friendli" href="/oss/python/integrations/providers/friendli" icon="link">
    Optimized serving engine for AI models.
  </Card>

<Card title="Galaxia" href="/oss/python/integrations/providers/galaxia" icon="link">
    Prompt-driven engineering assistant.
  </Card>

<Card title="Gel" href="/oss/python/integrations/providers/gel" icon="link">
    Knowledge extraction and NLP platform.
  </Card>

<Card title="GeoPandas" href="/oss/python/integrations/providers/geopandas" icon="link">
    Geographic data analysis with Python.
  </Card>

<Card title="Git" href="/oss/python/integrations/providers/git" icon="link">
    Version control system integration.
  </Card>

<Card title="GitBook" href="/oss/python/integrations/providers/gitbook" icon="link">
    Documentation platform and knowledge base.
  </Card>

<Card title="GitHub" href="/oss/python/integrations/providers/github" icon="link">
    Code hosting and collaboration platform.
  </Card>

<Card title="GitLab" href="/oss/python/integrations/providers/gitlab" icon="link">
    DevOps platform and code repository.
  </Card>

<Card title="GOAT" href="/oss/python/integrations/providers/goat" icon="link">
    Tool use framework for AI agents.
  </Card>

<Card title="Golden" href="/oss/python/integrations/providers/golden" icon="link">
    Knowledge graph and data platform.
  </Card>

<Card title="Google" href="/oss/python/integrations/providers/google" icon="google">
    Google's AI services and cloud platform.
  </Card>

<Card title="Google Serper" href="/oss/python/integrations/providers/google_serper" icon="google">
    Google Search API service.
  </Card>

<Card title="GooseAI" href="/oss/python/integrations/providers/gooseai" icon="link">
    Fully managed NLP-as-a-Service platform.
  </Card>

<Card title="GPT4All" href="/oss/python/integrations/providers/gpt4all" icon="link">
    Open-source LLM ecosystem for local deployment.
  </Card>

<Card title="Gradient" href="/oss/python/integrations/providers/gradient" icon="link">
    AI model training and deployment platform.
  </Card>

<Card title="GradientAI" href="/oss/python/integrations/providers/gradientai" icon="link">
    Private AI model training platform.
  </Card>

<Card title="Graph RAG" href="/oss/python/integrations/providers/graph_rag" icon="link">
    Graph-based retrieval augmented generation.
  </Card>

<Card title="GraphSignal" href="/oss/python/integrations/providers/graphsignal" icon="link">
    AI observability and monitoring platform.
  </Card>

<Card title="GreenNode" href="/oss/python/integrations/providers/greennode" icon="link">
    Sustainable AI computing platform.
  </Card>

<Card title="GROBID" href="/oss/python/integrations/providers/grobid" icon="link">
    Machine learning library for bibliographic data.
  </Card>

<Card title="Groq" href="/oss/python/integrations/providers/groq" icon="link">
    Ultra-fast inference with specialized hardware.
  </Card>

<Card title="Gutenberg" href="/oss/python/integrations/providers/gutenberg" icon="link">
    Project Gutenberg digital library access.
  </Card>

<Card title="Hacker News" href="/oss/python/integrations/providers/hacker_news" icon="link">
    Tech news and discussion platform.
  </Card>

<Card title="Hazy Research" href="/oss/python/integrations/providers/hazy_research" icon="link">
    Machine learning research and tools.
  </Card>

<Card title="Helicone" href="/oss/python/integrations/providers/helicone" icon="link">
    LLM observability and monitoring platform.
  </Card>

<Card title="Hologres" href="/oss/python/integrations/providers/hologres" icon="link">
    Real-time interactive analytics service.
  </Card>

<Card title="HTML2Text" href="/oss/python/integrations/providers/html2text" icon="link">
    HTML to plain text conversion utility.
  </Card>

<Card title="Huawei" href="/oss/python/integrations/providers/huawei" icon="link">
    Huawei Cloud AI services and models.
  </Card>

<Card title="Hugging Face" href="/oss/python/integrations/providers/huggingface" icon="link">
    Open platform for ML models and datasets.
  </Card>

<Card title="HyperBrowser" href="/oss/python/integrations/providers/hyperbrowser" icon="link">
    Web automation and scraping platform.
  </Card>

<Card title="IBM" href="/oss/python/integrations/providers/ibm" icon="link">
    IBM Watson AI and enterprise solutions.
  </Card>

<Card title="IEIT Systems" href="/oss/python/integrations/providers/ieit_systems" icon="link">
    Enterprise AI and system integration.
  </Card>

<Card title="iFixit" href="/oss/python/integrations/providers/ifixit" icon="link">
    Repair guides and technical documentation.
  </Card>

<Card title="iFlytek" href="/oss/python/integrations/providers/iflytek" icon="link">
    Chinese speech and language AI platform.
  </Card>

<Card title="IMSDb" href="/oss/python/integrations/providers/imsdb" icon="link">
    Internet Movie Script Database access.
  </Card>

<Card title="InfinispanVS" href="/oss/python/integrations/providers/infinispanvs" icon="link">
    Distributed cache and data grid platform.
  </Card>

<Card title="Infinity" href="/oss/python/integrations/providers/infinity" icon="link">
    High-performance embedding inference server.
  </Card>

<Card title="Infino" href="/oss/python/integrations/providers/infino" icon="link">
    Observability and monitoring platform.
  </Card>

<Card title="Intel" href="/oss/python/integrations/providers/intel" icon="link">
    Intel's AI optimization tools and libraries.
  </Card>

<Card title="IUGU" href="/oss/python/integrations/providers/iugu" icon="link">
    Brazilian payment processing platform.
  </Card>

<Card title="Jaguar" href="/oss/python/integrations/providers/jaguar" icon="link">
    Vector database and search platform.
  </Card>

<Card title="Javelin AI Gateway" href="/oss/python/integrations/providers/javelin_ai_gateway" icon="link">
    AI model gateway and management platform.
  </Card>

<Card title="Jenkins" href="/oss/python/integrations/providers/jenkins" icon="link">
    Automation server and CI/CD platform.
  </Card>

<Card title="Jina" href="/oss/python/integrations/providers/jina" icon="link">
    Neural search framework and cloud platform.
  </Card>

<Card title="John Snow Labs" href="/oss/python/integrations/providers/johnsnowlabs" icon="link">
    Enterprise NLP and healthcare AI platform.
  </Card>

<Card title="Joplin" href="/oss/python/integrations/providers/joplin" icon="link">
    Open-source note taking and organization.
  </Card>

<Card title="KDB.AI" href="/oss/python/integrations/providers/kdbai" icon="link">
    Time-series vector database platform.
  </Card>

<Card title="Kinetica" href="/oss/python/integrations/providers/kinetica" icon="link">
    Real-time analytics and database platform.
  </Card>

<Card title="KoboldAI" href="/oss/python/integrations/providers/koboldai" icon="link">
    Browser-based AI writing assistant.
  </Card>

<Card title="Konko" href="/oss/python/integrations/providers/konko" icon="link">
    Generative AI platform and model hosting.
  </Card>

<Card title="KoNLPy" href="/oss/python/integrations/providers/konlpy" icon="link">
    Korean natural language processing toolkit.
  </Card>

<Card title="Kuzu" href="/oss/python/integrations/providers/kuzu" icon="link">
    Embedded graph database management system.
  </Card>

<Card title="Label Studio" href="/oss/python/integrations/providers/labelstudio" icon="link">
    Data labeling and annotation platform.
  </Card>

<Card title="LakeFS" href="/oss/python/integrations/providers/lakefs" icon="link">
    Git-like version control for data lakes.
  </Card>

<Card title="LanceDB" href="/oss/python/integrations/providers/lancedb" icon="link">
    Developer-friendly embedded vector database.
  </Card>

<Card title="LangChain Decorators" href="/oss/python/integrations/providers/langchain_decorators" icon="link">
    Syntactic sugar and utilities for LangChain.
  </Card>

<Card title="LangFair" href="/oss/python/integrations/providers/langfair" icon="link">
    Bias testing framework for language models.
  </Card>

<Card title="LangFuse" href="/oss/python/integrations/providers/langfuse" icon="link">
    LLM engineering platform and observability.
  </Card>

<Card title="Lantern" href="/oss/python/integrations/providers/lantern" icon="link">
    PostgreSQL vector database extension.
  </Card>

<Card title="Lindorm" href="/oss/python/integrations/providers/lindorm" icon="link">
    Alibaba Cloud's multi-model database service.
  </Card>

<Card title="LinkUp" href="/oss/python/integrations/providers/linkup" icon="link">
    Real-time job market data and search.
  </Card>

<Card title="LiteLLM" href="/oss/python/integrations/providers/litellm" icon="link">
    Unified interface for 100+ LLM APIs.
  </Card>

<Card title="LlamaIndex" href="/oss/python/integrations/providers/llama_index" icon="link">
    Data framework for LLM applications.
  </Card>

<Card title="LlamaCPP" href="/oss/python/integrations/providers/llamacpp" icon="link">
    Port of Meta's LLaMA model in C/C++.
  </Card>

<Card title="LlamaEdge" href="/oss/python/integrations/providers/llamaedge" icon="link">
    Edge computing platform for LLaMA models.
  </Card>

<Card title="LlamaFile" href="/oss/python/integrations/providers/llamafile" icon="link">
    Single-file executable for running LLMs.
  </Card>

<Card title="LLMonitor" href="/oss/python/integrations/providers/llmonitor" icon="link">
    Observability platform for LLM applications.
  </Card>

<Card title="LocalAI" href="/oss/python/integrations/providers/localai" icon="link">
    Self-hosted OpenAI-compatible API server.
  </Card>

<Card title="Log10" href="/oss/python/integrations/providers/log10" icon="link">
    LLM data management and observability.
  </Card>

<Card title="MariaDB" href="/oss/python/integrations/providers/mariadb" icon="link">
    Open-source relational database management.
  </Card>

<Card title="MaritALK" href="/oss/python/integrations/providers/maritalk" icon="link">
    Brazilian Portuguese language model.
  </Card>

<Card title="Marqo" href="/oss/python/integrations/providers/marqo" icon="link">
    End-to-end vector search engine.
  </Card>

<Card title="MediaWiki Dump" href="/oss/python/integrations/providers/mediawikidump" icon="link">
    Wikipedia and MediaWiki data processing.
  </Card>

<Card title="Meilisearch" href="/oss/python/integrations/providers/meilisearch" icon="link">
    Lightning-fast search engine platform.
  </Card>

<Card title="Memcached" href="/oss/python/integrations/providers/memcached" icon="link">
    Distributed memory caching system.
  </Card>

<Card title="Memgraph" href="/oss/python/integrations/providers/memgraph" icon="link">
    Real-time graph database platform.
  </Card>

<Card title="Metal" href="/oss/python/integrations/providers/metal" icon="link">
    Managed vector search and retrieval.
  </Card>

<Card title="Microsoft" href="/oss/python/integrations/providers/microsoft" icon="microsoft">
    Microsoft Azure AI and enterprise services.
  </Card>

<Card title="Milvus" href="/oss/python/integrations/providers/milvus" icon="link">
    Open-source vector database for AI applications.
  </Card>

<Card title="MindsDB" href="/oss/python/integrations/providers/mindsdb" icon="link">
    AI layer for databases and data platforms.
  </Card>

<Card title="Minimax" href="/oss/python/integrations/providers/minimax" icon="link">
    Chinese AI company's language models.
  </Card>

<Card title="MistralAI" href="/oss/python/integrations/providers/mistralai" icon="link">
    Efficient open-source language models.
  </Card>

<Card title="MLflow" href="/oss/python/integrations/providers/mlflow" icon="link">
    ML lifecycle management platform.
  </Card>

<Card title="MLflow Tracking" href="/oss/python/integrations/providers/mlflow_tracking" icon="link">
    Experiment tracking and model registry.
  </Card>

<Card title="MLX" href="/oss/python/integrations/providers/mlx" icon="link">
    Apple's machine learning framework.
  </Card>

<Card title="Modal" href="/oss/python/integrations/providers/modal" icon="link">
    Serverless cloud computing for data science.
  </Card>

<Card title="ModelScope" href="/oss/python/integrations/providers/modelscope" icon="link">
    Alibaba's open-source model hub.
  </Card>

<Card title="Modern Treasury" href="/oss/python/integrations/providers/modern_treasury" icon="link">
    Payment operations and treasury management.
  </Card>

<Card title="Momento" href="/oss/python/integrations/providers/momento" icon="link">
    Serverless cache and vector index.
  </Card>

<Card title="MongoDB" href="/oss/python/integrations/providers/mongodb" icon="link">
    Document-based NoSQL database platform.
  </Card>

<Card title="MongoDB Atlas" href="/oss/python/integrations/providers/mongodb_atlas" icon="link">
    Cloud-hosted MongoDB with vector search.
  </Card>

<Card title="MotherDuck" href="/oss/python/integrations/providers/motherduck" icon="link">
    Serverless analytics with DuckDB in the cloud.
  </Card>

<Card title="Motorhead" href="/oss/python/integrations/providers/motorhead" icon="link">
    Long-term memory for AI conversations.
  </Card>

<Card title="MyScale" href="/oss/python/integrations/providers/myscale" icon="link">
    SQL-compatible vector database platform.
  </Card>

<Card title="Naver" href="/oss/python/integrations/providers/naver" icon="link">
    Naver's AI services and language models.
  </Card>

<Card title="Nebius" href="/oss/python/integrations/providers/nebius" icon="link">
    AI cloud platform and infrastructure.
  </Card>

<Card title="Neo4j" href="/oss/python/integrations/providers/neo4j" icon="link">
    Native graph database and analytics platform.
  </Card>

<Card title="NetMind" href="/oss/python/integrations/providers/netmind" icon="link">
    Decentralized AI computing network.
  </Card>

<Card title="Nimble" href="/oss/python/integrations/providers/nimble" icon="link">
    Web intelligence and data extraction.
  </Card>

<Card title="NLP Cloud" href="/oss/python/integrations/providers/nlpcloud" icon="link">
    Production-ready NLP API platform.
  </Card>

<Card title="Nomic" href="/oss/python/integrations/providers/nomic" icon="link">
    Open-source embedding models and tools.
  </Card>

<Card title="Notion" href="/oss/python/integrations/providers/notion" icon="link">
    All-in-one workspace and collaboration platform.
  </Card>

<Card title="Nuclia" href="/oss/python/integrations/providers/nuclia" icon="link">
    AI-powered search and understanding platform.
  </Card>

<Card title="NVIDIA" href="/oss/python/integrations/providers/nvidia" icon="link">
    NVIDIA's AI computing platform and models.
  </Card>

<Card title="Obsidian" href="/oss/python/integrations/providers/obsidian" icon="link">
    Connected note-taking and knowledge management.
  </Card>

<Card title="OceanBase" href="/oss/python/integrations/providers/oceanbase" icon="link">
    Distributed relational database system.
  </Card>

<Card title="OCI" href="/oss/python/integrations/providers/oci" icon="link">
    Oracle Cloud Infrastructure AI services.
  </Card>

<Card title="OctoAI" href="/oss/python/integrations/providers/octoai" icon="link">
    Efficient AI compute and model serving.
  </Card>

<Card title="Ollama" href="/oss/python/integrations/providers/ollama" icon="link">
    Run large language models locally.
  </Card>

<Card title="Ontotext GraphDB" href="/oss/python/integrations/providers/ontotext_graphdb" icon="link">
    RDF database and semantic graph platform.
  </Card>

<Card title="OpenAI" href="/oss/python/integrations/providers/openai" icon="openai">
    GPT models and comprehensive AI platform.
  </Card>

<Card title="OpenDataLoader PDF" href="/oss/python/integrations/providers/opendataloader_pdf" icon="link">
    Safe, Open, High-Performance — PDF for AI
  </Card>

<Card title="OpenGradient" href="/oss/python/integrations/providers/opengradient" icon="link">
    AI model training and fine-tuning platform.
  </Card>

<Card title="OpenLLM" href="/oss/python/integrations/providers/openllm" icon="link">
    Operating LLMs in production environment.
  </Card>

<Card title="OpenSearch" href="/oss/python/integrations/providers/opensearch" icon="link">
    Distributed search and analytics suite.
  </Card>

<Card title="OpenWeatherMap" href="/oss/python/integrations/providers/openweathermap" icon="link">
    Weather data and forecasting API.
  </Card>

<Card title="Oracle AI" href="/oss/python/integrations/providers/oracleai" icon="link">
    Oracle's AI and machine learning services.
  </Card>

<Card title="Outline" href="/oss/python/integrations/providers/outline" icon="link">
    Team knowledge base and wiki platform.
  </Card>

<Card title="Outlines" href="/oss/python/integrations/providers/outlines" icon="link">
    Structured generation for language models.
  </Card>

<Card title="Oxylabs" href="/oss/python/integrations/providers/oxylabs" icon="link">
    Web scraping and proxy services.
  </Card>

<Card title="Pandas" href="/oss/python/integrations/providers/pandas" icon="link">
    Data analysis and manipulation library.
  </Card>

<Card title="Perigon" href="/oss/python/integrations/providers/perigon" icon="link">
    Real-time news and media monitoring.
  </Card>

<Card title="Permit" href="/oss/python/integrations/providers/permit" icon="link">
    Authorization and access control platform.
  </Card>

<Card title="Perplexity" href="/oss/python/integrations/providers/perplexity" icon="link">
    AI-powered search and reasoning engine.
  </Card>

<Card title="Petals" href="/oss/python/integrations/providers/petals" icon="link">
    Distributed inference for large language models.
  </Card>

<Card title="PG Embedding" href="/oss/python/integrations/providers/pg_embedding" icon="link">
    PostgreSQL vector embedding extensions.
  </Card>

<Card title="pgvector" href="/oss/python/integrations/providers/pgvector" icon="link">
    Vector similarity search for PostgreSQL.
  </Card>

<Card title="Pinecone" href="/oss/python/integrations/providers/pinecone" icon="link">
    Managed vector database for ML applications.
  </Card>

<Card title="PipelineAI" href="/oss/python/integrations/providers/pipelineai" icon="link">
    ML pipeline and model deployment platform.
  </Card>

<Card title="Pipeshift" href="/oss/python/integrations/providers/pipeshift" icon="link">
    AI-powered content moderation platform.
  </Card>

<Card title="PolarisAIDataInsight" href="/oss/python/integrations/providers/polaris_ai_datainsight" icon="link">
    Document-loaders for various file formats.
  </Card>

<Card title="Portkey" href="/oss/python/integrations/providers/portkey/logging_tracing_portkey" icon="link">
    AI gateway and observability platform.
  </Card>

<Card title="Predibase" href="/oss/python/integrations/providers/predibase" icon="link">
    Fine-tuning platform for large language models.
  </Card>

<Card title="PredictionGuard" href="/oss/python/integrations/providers/predictionguard" icon="link">
    AI model security and compliance platform.
  </Card>

<Card title="PreMAI" href="/oss/python/integrations/providers/premai" icon="link">
    AI platform for model deployment and management.
  </Card>

<Card title="Privy" href="/oss/python/integrations/providers/privy" icon="link">
    Wallets and payments for AI agents.
  </Card>

<Card title="Prolog" href="/oss/python/integrations/providers/prolog" icon="link">
    Logic programming language integration.
  </Card>

<Card title="PromptLayer" href="/oss/python/integrations/providers/promptlayer" icon="link">
    Prompt engineering and observability platform.
  </Card>

<Card title="Psychic" href="/oss/python/integrations/providers/psychic" icon="link">
    Universal API for SaaS integrations.
  </Card>

<Card title="PubMed" href="/oss/python/integrations/providers/pubmed" icon="link">
    Biomedical literature database access.
  </Card>

<Card title="Pull MD" href="/oss/python/integrations/providers/pull-md" icon="link">
    Markdown content extraction and processing.
  </Card>

<Card title="PygmalionAI" href="/oss/python/integrations/providers/pygmalionai" icon="link">
    Conversational AI model platform.
  </Card>

<Card title="PyMuPDF4LLM" href="/oss/python/integrations/providers/pymupdf4llm" icon="link">
    PDF processing optimized for LLM ingestion.
  </Card>

<Card title="Qdrant" href="/oss/python/integrations/providers/qdrant" icon="link">
    Vector similarity search engine.
  </Card>

<Card title="Ragatouille" href="/oss/python/integrations/providers/ragatouille" icon="link">
    RAG toolkit with ColBERT indexing.
  </Card>

<Card title="Rank BM25" href="/oss/python/integrations/providers/rank_bm25" icon="link">
    BM25 ranking algorithm implementation.
  </Card>

<Card title="Ray Serve" href="/oss/python/integrations/providers/ray_serve" icon="link">
    Scalable model serving framework.
  </Card>

<Card title="Rebuff" href="/oss/python/integrations/providers/rebuff" icon="link">
    Prompt injection detection and prevention.
  </Card>

<Card title="Reddit" href="/oss/python/integrations/providers/reddit" icon="link">
    Social media platform integration and APIs.
  </Card>

<Card title="Redis" href="/oss/python/integrations/providers/redis" icon="link">
    In-memory data structure store and cache.
  </Card>

<Card title="Remembrall" href="/oss/python/integrations/providers/remembrall" icon="link">
    AI memory and context management.
  </Card>

<Card title="Replicate" href="/oss/python/integrations/providers/replicate" icon="link">
    Cloud platform for running ML models.
  </Card>

<Card title="Roam" href="/oss/python/integrations/providers/roam" icon="link">
    Research and note-taking platform.
  </Card>

<Card title="Robocorp" href="/oss/python/integrations/providers/robocorp" icon="link">
    Python automation and RPA platform.
  </Card>

<Card title="Rockset" href="/oss/python/integrations/providers/rockset" icon="link">
    Real-time analytics database platform.
  </Card>

<Card title="RunPod" href="/oss/python/integrations/providers/runpod" icon="link">
    GPU cloud platform for AI workloads.
  </Card>

<Card title="Salesforce" href="/oss/python/integrations/providers/salesforce" icon="link">
    CRM platform and business automation.
  </Card>

<Card title="SambaNova" href="/oss/python/integrations/providers/sambanova" icon="link">
    AI platform with specialized hardware.
  </Card>

<Card title="SAP" href="/oss/python/integrations/providers/sap" icon="link">
    Enterprise software and AI solutions.
  </Card>

<Card title="ScrapeGraph" href="/oss/python/integrations/providers/scrapegraph" icon="link">
    AI-powered web scraping framework.
  </Card>

<Card title="Scrapeless" href="/oss/python/integrations/providers/scrapeless" icon="link">
    Web scraping API and proxy service.
  </Card>

<Card title="SearchAPI" href="/oss/python/integrations/providers/searchapi" icon="link">
    Real-time search engine results API.
  </Card>

<Card title="SearX" href="/oss/python/integrations/providers/searx" icon="link">
    Privacy-respecting metasearch engine.
  </Card>

<Card title="SemaDB" href="/oss/python/integrations/providers/semadb" icon="link">
    Vector database for semantic search.
  </Card>

<Card title="SerpAPI" href="/oss/python/integrations/providers/serpapi" icon="link">
    Google Search results scraping API.
  </Card>

<Card title="Shale Protocol" href="/oss/python/integrations/providers/shaleprotocol" icon="link">
    Decentralized AI inference protocol.
  </Card>

<Card title="SingleStore" href="/oss/python/integrations/providers/singlestore" icon="link">
    Distributed database with vector capabilities.
  </Card>

<Card title="scikit-learn" href="/oss/python/integrations/providers/sklearn" icon="link">
    Machine learning library for Python.
  </Card>

<Card title="Slack" href="/oss/python/integrations/providers/slack" icon="link">
    Business communication and collaboration.
  </Card>

<Card title="Snowflake" href="/oss/python/integrations/providers/snowflake" icon="link">
    Cloud data platform and analytics.
  </Card>

<Card title="spaCy" href="/oss/python/integrations/providers/spacy" icon="link">
    Industrial-strength NLP library.
  </Card>

<Card title="Spark" href="/oss/python/integrations/providers/spark" icon="link">
    Unified analytics engine for big data.
  </Card>

<Card title="SparkLLM" href="/oss/python/integrations/providers/sparkllm" icon="link">
    iFlytek's multilingual language model.
  </Card>

<Card title="Spreedly" href="/oss/python/integrations/providers/spreedly" icon="link">
    Payment orchestration platform.
  </Card>

<Card title="SQLite" href="/oss/python/integrations/providers/sqlite" icon="link">
    Embedded relational database engine.
  </Card>

<Card title="StackExchange" href="/oss/python/integrations/providers/stackexchange" icon="link">
    Q\&A platform network integration.
  </Card>

<Card title="StarRocks" href="/oss/python/integrations/providers/starrocks" icon="link">
    High-performance analytical database.
  </Card>

<Card title="StochasticAI" href="/oss/python/integrations/providers/stochasticai" icon="link">
    GPU cloud platform for ML acceleration.
  </Card>

<Card title="Streamlit" href="/oss/python/integrations/providers/streamlit" icon="link">
    Web app framework for data science.
  </Card>

<Card title="Stripe" href="/oss/python/integrations/providers/stripe" icon="link">
    Online payment processing platform.
  </Card>

<Card title="Supabase" href="/oss/python/integrations/providers/supabase" icon="link">
    Open-source Firebase alternative.
  </Card>

<Card title="SurrealDB" href="/oss/python/integrations/providers/surrealdb" icon="link">
    Multi-model database for modern applications.
  </Card>

<Card title="Symbl.ai Nebula" href="/oss/python/integrations/providers/symblai_nebula" icon="link">
    Conversation intelligence platform.
  </Card>

<Card title="Tableau" href="/oss/python/integrations/providers/tableau" icon="link">
    Data visualization and business intelligence.
  </Card>

<Card title="Taiga" href="/oss/python/integrations/providers/taiga" icon="link">
    Project management platform for agile teams.
  </Card>

<Card title="Tair" href="/oss/python/integrations/providers/tair" icon="link">
    Alibaba Cloud's in-memory database.
  </Card>

<Card title="Tavily" href="/oss/python/integrations/providers/tavily" icon="link">
    AI-optimized search API for applications.
  </Card>

<Card title="Telegram" href="/oss/python/integrations/providers/telegram" icon="link">
    Messaging platform and bot integration.
  </Card>

<Card title="Tencent" href="/oss/python/integrations/providers/tencent" icon="link">
    Tencent Cloud AI services and models.
  </Card>

<Card title="TensorFlow Datasets" href="/oss/python/integrations/providers/tensorflow_datasets" icon="link">
    Collection of ready-to-use datasets.
  </Card>

<Card title="TensorLake" href="/oss/python/integrations/providers/tensorlake" icon="link">
    Data infrastructure for ML applications.
  </Card>

<Card title="TiDB" href="/oss/python/integrations/providers/tidb" icon="link">
    Distributed SQL database platform.
  </Card>

<Card title="TigerGraph" href="/oss/python/integrations/providers/tigergraph" icon="link">
    Scalable graph database and analytics.
  </Card>

<Card title="Tigris" href="/oss/python/integrations/providers/tigris" icon="link">
    Globally distributed database platform.
  </Card>

<Card title="Tilores" href="/oss/python/integrations/providers/tilores" icon="link">
    Entity resolution and data matching.
  </Card>

<Card title="Together" href="/oss/python/integrations/providers/together" icon="link">
    Fast inference for open-source models.
  </Card>

<Card title="ToMarkdown" href="/oss/python/integrations/providers/tomarkdown" icon="link">
    HTML to Markdown conversion utility.
  </Card>

<Card title="Toolbox LangChain" href="/oss/python/integrations/providers/toolbox" icon="link">
    Extended toolkit for LangChain applications.
  </Card>

<Card title="Transwarp" href="/oss/python/integrations/providers/transwarp" icon="link">
    Big data platform and analytics suite.
  </Card>

<Card title="Trello" href="/oss/python/integrations/providers/trello" icon="link">
    Visual project management and collaboration.
  </Card>

<Card title="Trubrics" href="/oss/python/integrations/providers/trubrics" icon="link">
    LLM evaluation and analytics platform.
  </Card>

<Card title="TrueFoundry" href="/oss/python/integrations/providers/truefoundry" icon="link">
    ML platform for model deployment.
  </Card>

<Card title="TrueLens" href="/oss/python/integrations/providers/trulens" icon="link">
    Evaluation framework for LLM applications.
  </Card>

<Card title="Twitter" href="/oss/python/integrations/providers/twitter" icon="link">
    Social media platform integration.
  </Card>

<Card title="Typesense" href="/oss/python/integrations/providers/typesense" icon="link">
    Fast and typo-tolerant search engine.
  </Card>

<Card title="UnDatasIO" href="/oss/python/integrations/providers/undatasio" icon="link">
    Data extraction and processing platform.
  </Card>

<Card title="Unstructured" href="/oss/python/integrations/providers/unstructured" icon="link">
    Document processing and data extraction.
  </Card>

<Card title="Upstage" href="/oss/python/integrations/providers/upstage" icon="link">
    Document AI and OCR platform.
  </Card>

<Card title="Upstash" href="/oss/python/integrations/providers/upstash" icon="link">
    Serverless data platform for Redis and Kafka.
  </Card>

<Card title="UpTrain" href="/oss/python/integrations/providers/uptrain" icon="link">
    ML observability and evaluation platform.
  </Card>

<Card title="USearch" href="/oss/python/integrations/providers/usearch" icon="link">
    Single-file vector search engine.
  </Card>

<Card title="Valthera" href="/oss/python/integrations/providers/valthera" icon="link">
    AI platform for healthcare applications.
  </Card>

<Card title="Valyu" href="/oss/python/integrations/providers/valyu" icon="link">
    AI-powered data analysis platform.
  </Card>

<Card title="VDMS" href="/oss/python/integrations/providers/vdms" icon="link">
    Visual data management system.
  </Card>

<Card title="Vearch" href="/oss/python/integrations/providers/vearch" icon="link">
    Distributed vector search engine.
  </Card>

<Card title="Vectara" href="/oss/python/integrations/providers/vectara" icon="link">
    Neural search platform with built-in understanding.
  </Card>

<Card title="Vectorize" href="/oss/python/integrations/providers/vectorize" icon="link">
    Vector database and semantic search.
  </Card>

<Card title="Vespa" href="/oss/python/integrations/providers/vespa" icon="link">
    Big data serving engine for vector search.
  </Card>

<Card title="VLite" href="/oss/python/integrations/providers/vlite" icon="link">
    Simple vector database for embeddings.
  </Card>

<Card title="VoyageAI" href="/oss/python/integrations/providers/voyageai" icon="link">
    Embedding models and semantic search.
  </Card>

<Card title="Weights & Biases" href="/oss/python/integrations/providers/wandb" icon="link">
    ML experiment tracking and collaboration.
  </Card>

<Card title="Weights & Biases Tracking" href="/oss/python/integrations/providers/wandb_tracking" icon="link">
    Experiment tracking and model management.
  </Card>

<Card title="Weights & Biases Tracing" href="/oss/python/integrations/providers/wandb_tracing" icon="link">
    LLM tracing and observability.
  </Card>

<Card title="Weather" href="/oss/python/integrations/providers/weather" icon="link">
    Weather data and forecasting services.
  </Card>

<Card title="Weaviate" href="/oss/python/integrations/providers/weaviate" icon="link">
    Open-source vector database with GraphQL.
  </Card>

<Card title="WhatsApp" href="/oss/python/integrations/providers/whatsapp" icon="link">
    Messaging platform integration and automation.
  </Card>

<Card title="WhyLabs Profiling" href="/oss/python/integrations/providers/whylabs_profiling" icon="link">
    AI observability and data monitoring.
  </Card>

<Card title="Wikipedia" href="/oss/python/integrations/providers/wikipedia" icon="link">
    Wikipedia content access and search.
  </Card>

<Card title="Wolfram Alpha" href="/oss/python/integrations/providers/wolfram_alpha" icon="link">
    Computational knowledge engine.
  </Card>

<Card title="Writer" href="/oss/python/integrations/providers/writer" icon="link">
    Enterprise AI writing platform.
  </Card>

<Card title="XAI" href="/oss/python/integrations/providers/xai" icon="link">
    xAI's Grok models for conversational AI.
  </Card>

<Card title="Xata" href="/oss/python/integrations/providers/xata" icon="link">
    Serverless database with vector search.
  </Card>

<Card title="Xinference" href="/oss/python/integrations/providers/xinference" icon="link">
    Distributed inference framework for LLMs.
  </Card>

<Card title="Yahoo" href="/oss/python/integrations/providers/yahoo" icon="link">
    Yahoo services and data integration.
  </Card>

<Card title="Yandex" href="/oss/python/integrations/providers/yandex" icon="link">
    Yandex AI services and language models.
  </Card>

<Card title="YDB" href="/oss/python/integrations/providers/ydb" icon="link">
    Yandex Database distributed storage system.
  </Card>

<Card title="YeagerAI" href="/oss/python/integrations/providers/yeagerai" icon="link">
    AI agent framework and development platform.
  </Card>

<Card title="Yellowbrick" href="/oss/python/integrations/providers/yellowbrick" icon="link">
    Data warehouse and analytics platform.
  </Card>

<Card title="Yi" href="/oss/python/integrations/providers/yi" icon="link">
    01.AI's bilingual language models.
  </Card>

<Card title="You" href="/oss/python/integrations/providers/you" icon="link">
    You.com search engine and AI platform.
  </Card>

<Card title="YouTube" href="/oss/python/integrations/providers/youtube" icon="link">
    Video platform integration and content access.
  </Card>

<Card title="Zep" href="/oss/python/integrations/providers/zep" icon="link">
    Long-term memory for AI assistants.
  </Card>

<Card title="ZeusDB" href="/oss/python/integrations/providers/zeusdb" icon="link">
    High-performance vector database.
  </Card>

<Card title="ZhipuAI" href="/oss/python/integrations/providers/zhipuai" icon="link">
    ChatGLM and other Chinese language models.
  </Card>

<Card title="Zilliz" href="/oss/python/integrations/providers/zilliz" icon="link">
    Managed Milvus vector database service.
  </Card>

<Card title="Zotero" href="/oss/python/integrations/providers/zotero" icon="link">
    Reference management and research tool.
  </Card>
</Columns>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/providers/all_providers.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Analyze an experiment

**URL:** llms-txt#analyze-an-experiment

**Contents:**
- Analyze a single experiment
  - Open the experiment view
  - View experiment results
  - Group results by metadata
  - Repetitions
  - Compare to another experiment
- Download experiment results as a CSV
- Rename an experiment

Source: https://docs.langchain.com/langsmith/analyze-an-experiment

This page describes some of the essential tasks for working with [*experiments*](/langsmith/evaluation-concepts#experiment) in LangSmith:

* **[Analyze a single experiment](#analyze-a-single-experiment)**: View and interpret experiment results, customize columns, filter data, and compare runs.
* **[Download experiment results as a CSV](#how-to-download-experiment-results-as-a-csv)**: Export your experiment data for external analysis and sharing.
* **[Rename an experiment](#how-to-rename-an-experiment)**: Update experiment names in both the Playground and Experiments view.

## Analyze a single experiment

After running an experiment, you can use LangSmith's experiment view to analyze the results and draw insights about your experiment's performance.

### Open the experiment view

To open the experiment view, select the relevant [*dataset*](/langsmith/evaluation-concepts#datasets) from the **Dataset & Experiments** page and then select the experiment you want to view.

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-experiment.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=74207f0a2422f89fdc75b23f0a88c58f" alt="Open experiment view" data-og-width="1640" width="1640" data-og-height="899" height="899" data-path="langsmith/images/select-experiment.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-experiment.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=fa173f885a87adac0c9ced9b3d553876 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-experiment.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=011a588eeca2032ab40c3612345a0b4f 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-experiment.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=7baa325ce358d73c61dbab0cce54222b 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-experiment.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=2810aba5a4ce3f7f0098f167bff7a78f 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-experiment.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=78d38c66b183cee29fa66959f339954c 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-experiment.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=0bf7c03ef6f6ca6e269053a984f29c3a 2500w" />

### View experiment results

#### Customize columns

By default, the experiment view shows the input, output, and reference output for each [example](/langsmith/evaluation-concepts#examples) in the dataset, feedback scores from evaluations and experiment metrics like cost, token counts, latency and status.

You can customize the columns using the **Display** button to make it easier to interpret experiment results:

* **Break out fields from inputs, outputs, and reference outputs** into their own columns. This is especially helpful if you have long inputs/outputs/reference outputs and want to surface important fields.
* **Hide and reorder columns** to create focused views for analysis.
* **Control decimal precision on feedback scores**. By default, LangSmith surfaces numerical feedback scores with a decimal precision of 2, but you can customize this setting to be up to 6 decimals.
* **Set the Heat Map threshold** to high, middle, and low for numeric feedback scores in your experiment, which affects the threshold at which score chips render as red or green:

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/column-heat-map.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=b0203a449f0f7df70900735ba540d712" alt="Column heatmap configuration" data-og-width="1780" width="1780" data-og-height="1688" height="1688" data-path="langsmith/images/column-heat-map.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/column-heat-map.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=1ac06a00a4d11c8455d3996e3b3cc7ea 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/column-heat-map.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=4c2207a21fea1078e5002d0d96c8c989 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/column-heat-map.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=ea673195e58c5bae676a782cb03bbbaa 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/column-heat-map.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=ac4708a7612a3f7e7f82db28ac3a7b91 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/column-heat-map.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=da649bbe343e2fd5e87d1845d1b19944 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/column-heat-map.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=f06ba0a04ef257e2d69193021938e761 2500w" />

<Tip>
  You can set default configurations for an entire dataset or temporarily save settings just for yourself.
</Tip>

To sort or filter feedback scores, you can use the actions in the column headers.

<img src="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/sort-filter.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=067490743d1229ae233f15e46236ed67" alt="Sort and filter" data-og-width="1633" width="1633" data-og-height="788" height="788" data-path="langsmith/images/sort-filter.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/sort-filter.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=a116c731329b3fc088b57eae1d2f41a4 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/sort-filter.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=d4ab881725f50ca23dc2650aa5376efd 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/sort-filter.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=62027a48b9fd97a335caf3bd7e99d0dc 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/sort-filter.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=137a7384faf8c1958dd309d5cfeba998 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/sort-filter.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=3ea99639ad38e02f997f66f504663b8a 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/sort-filter.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=2372af19c9bd6d45b420e870f7b77402 2500w" />

Depending on the view most useful for your analysis, you can change the formatting of the table by toggling between a compact view, a full, view, and a diff view.

* The **Compact** view shows each run as a one-line row, for ease of comparing scores at a glance.
* The **Full** view shows the full output for each run for digging into the details of individual runs.
* The **Diff** view shows the text difference between the reference output and the output for each run.

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/diff-mode.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=fb916d33cea2f344f3483b42d3670696" alt="Diff view" data-og-width="1638" width="1638" data-og-height="969" height="969" data-path="langsmith/images/diff-mode.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/diff-mode.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=d30d2281df2f18a7c45fcae5eb839ded 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/diff-mode.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=edcf8cc4a3aca2bc3498a7c8f97b31b1 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/diff-mode.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=3d0b55249029b6d72a98fea716d4dae7 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/diff-mode.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=ef89e5bf58911f5362635cfefe8ead27 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/diff-mode.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=337796d8a22712dea4c848ba8bc0e94a 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/diff-mode.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=4b1019a7ec0249a9b58da199421689bc 2500w" />

Hover over any of the output cells, and click on the trace icon to view the trace for that run. This will open up a trace in the side panel.

To view the entire tracing project, click on the **View Project** button in the top right of the header.

<img src="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-trace.png?fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=c94c0d2ecedf248c639c971bf29196e6" alt="View trace" data-og-width="1634" width="1634" data-og-height="835" height="835" data-path="langsmith/images/view-trace.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-trace.png?w=280&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=45822be0900e0aaf9a06b9ddf7a8d91c 280w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-trace.png?w=560&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=c0bc6ecc7cb67144c7899b8345d6ccef 560w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-trace.png?w=840&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=85387bf5a2dd30a8411759069d4b3bbc 840w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-trace.png?w=1100&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=f5edba01b477e0c8a4f7a5e81123ffb7 1100w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-trace.png?w=1650&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=0241b8036e5e409a7d365e39d8d72bf1 1650w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-trace.png?w=2500&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=9a7f0042a8319a0398427665d50347d3 2500w" />

#### View evaluator runs

For evaluator scores, you can view the source run by hovering over the evaluator score cell and clicking on the arrow icon. This will open up a trace in the side panel. If you're running a [LLM-as-a-judge evaluator](/langsmith/llm-as-judge), you can view the prompt used for the evaluator in this run. If your experiment has [repetitions](/langsmith/evaluation-concepts#repetitions), you can click on the aggregate average score to find links to all of the individual runs.

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluator-run.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=fc8df7233285b0f5a4ca9b44c06fcb47" alt="View evaluator runs" data-og-width="1634" width="1634" data-og-height="831" height="831" data-path="langsmith/images/evaluator-run.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluator-run.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=368b84b566407689f0e1b69f7e2d1ec8 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluator-run.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=9f7888c2163589263d5ed0827bf9b55a 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluator-run.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=a9cf41bd478fa7bb86b5594f4a0f163a 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluator-run.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=644229f6557ca6b1b68df6be37b79cf9 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluator-run.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=da59385aa685d9e2c163f18385ee201c 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluator-run.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=039cad7ccdd3c0ed5d5e0e30151ed916 2500w" />

### Group results by metadata

You can add metadata to examples to categorize and organize them. For example, if you're evaluating factual accuracy on a question answering dataset, the metadata might include which subject area each question belongs to. Metadata can be added either [via the UI](/langsmith/manage-datasets-in-application#edit-example-metadata) or [via the SDK](/langsmith/manage-datasets-programmatically#update-single-example).

To analyze results by metadata, use the **Group by** dropdown in the top right corner of the experiment view and select your desired metadata key. This displays average feedback scores, latency, total tokens, and cost for each metadata group.

<Info>
  You will only be able to group by example metadata on experiments created after February 20th, 2025. Any experiments before that date can still be grouped by metadata, but only if the metadata is on the experiment traces themselves.
</Info>

If you've run your experiment with [*repetitions*](/langsmith/evaluation-concepts#repetitions), there will be arrows in the output results column so you can view outputs in the table. To view each run from the repetition, hover over the output cell and click the expanded view.

When you run an experiment with repetitions, LangSmith displays the average for each feedback score in the table. Click on the feedback score to view the feedback scores from individual runs, or to view the standard deviation across repetitions.

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/repetitions.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=60962de04e5533d7718ca60fa9c7dcce" alt="Repetitions" data-og-width="1636" width="1636" data-og-height="959" height="959" data-path="langsmith/images/repetitions.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/repetitions.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=8be83801a53f2544883faf173bc16ef1 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/repetitions.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=7a924559be193efcc2c77dba3fea1231 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/repetitions.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=25cbd580d06bda48419b83401c268c2d 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/repetitions.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=9da3908c81d1c8fd44dde6d3ec7dfe1d 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/repetitions.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=775af0be371e662bea7ba7e29c2f21fd 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/repetitions.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=4d593460688be852a64638f092cba9f3 2500w" />

### Compare to another experiment

In the top right of the experiment view, you can select another experiment to compare to. This will open up a comparison view, where you can see how the two experiments compare. To learn more about the comparison view, see [how to compare experiment results](/langsmith/compare-experiment-results).

## Download experiment results as a CSV

LangSmith lets you download experiment results as a CSV file, which allows you to analyze and share your results.

To download as a CSV, click the download icon at the top of the experiment view. The icon is directly to the left of the [Compact toggle](/langsmith/compare-experiment-results#adjust-the-table-display).

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/download-experiment-results-as-csv.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=f237eb4b252a1018097be113434c22fa" alt="Download CSV" data-og-width="1705" width="1705" data-og-height="1345" height="1345" data-path="langsmith/images/download-experiment-results-as-csv.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/download-experiment-results-as-csv.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=679144e623fdd6a5ff643d66378f5f21 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/download-experiment-results-as-csv.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=ca7f980d6a11cf6ac6f54a2e8799ac08 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/download-experiment-results-as-csv.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=ae945f1172fad11a20e16c15ae859409 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/download-experiment-results-as-csv.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=e64dbaffad47fb30e1d44f032533bdcb 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/download-experiment-results-as-csv.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=72f1815228051375821c98a26edd0452 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/download-experiment-results-as-csv.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=51944ba9dddf5dddaa77b12a8a1c8d8a 2500w" />

## Rename an experiment

<Note>
  Experiment names must be unique per workspace.
</Note>

You can rename an experiment in the LangSmith UI in:

* The [Playground](#renaming-an-experiment-in-the-playground). When running experiments in the Playground, a default name with the format `pg::prompt-name::model::uuid` (eg. `pg::gpt-4o-mini::897ee630`) is automatically assigned.

You can rename an experiment immediately after running it by editing its name in the Playground table header.

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rename-in-playground.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=5b647ff1894376bbb727dabc4d73f039" alt="Edit name in playground" data-og-width="1372" width="1372" data-og-height="200" height="200" data-path="langsmith/images/rename-in-playground.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rename-in-playground.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=9d505597b1d2e180ebfa05d4361d3225 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rename-in-playground.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=d09703c543257203a19434a8a30458e1 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rename-in-playground.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=b3a8f6f2bd9fbfb9ba0c3f6c7477e04c 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rename-in-playground.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=591176ffbeb1f0a16f084f31f6717c6f 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rename-in-playground.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=381a59f8752307204b3e9f82a2fdbd16 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rename-in-playground.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=cba501d5c0d826f6f61b57d01e1d94c9 2500w" />

* The [Experiments view](#renaming-an-experiment-in-the-experiments-view). When viewing results in the experiments view, you can rename an experiment by using the pencil icon beside the experiment name.

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rename-in-experiments-view.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=16afa853361ec265a0c7917d815f3132" alt="Edit name in experiments view" data-og-width="1628" width="1628" data-og-height="224" height="224" data-path="langsmith/images/rename-in-experiments-view.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rename-in-experiments-view.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=d87e21875d807e1553289f096137af3f 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rename-in-experiments-view.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=761aab57b06c1e3644ac633f10565e26 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rename-in-experiments-view.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=0dd7d731b4aae11a4b08644e06ac0eb9 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rename-in-experiments-view.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=333a93f0e1d2ff38f9bdcfcfed33c825 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rename-in-experiments-view.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=48d1b7cacab8e71bb4d0fe79573d6f11 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rename-in-experiments-view.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=8185ba3772c13f40ea57501dd4982bc3 2500w" />

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/analyze-an-experiment.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## and this is also supported

**URL:** llms-txt#and-this-is-also-supported

**Contents:**
- Nodes

{"messages": [{"type": "human", "content": "message"}]}
python  theme={null}
from langchain.messages import AnyMessage
from langgraph.graph.message import add_messages
from typing import Annotated
from typing_extensions import TypedDict

class GraphState(TypedDict):
    messages: Annotated[list[AnyMessage], add_messages]
python  theme={null}
from langgraph.graph import MessagesState

class State(MessagesState):
    documents: list[str]
python  theme={null}
from dataclasses import dataclass
from typing_extensions import TypedDict

from langchain_core.runnables import RunnableConfig
from langgraph.graph import StateGraph
from langgraph.runtime import Runtime

class State(TypedDict):
    input: str
    results: str

@dataclass
class Context:
    user_id: str

builder = StateGraph(State)

def plain_node(state: State):
    return state

def node_with_runtime(state: State, runtime: Runtime[Context]):
    print("In node: ", runtime.context.user_id)
    return {"results": f"Hello, {state['input']}!"}

def node_with_config(state: State, config: RunnableConfig):
    print("In node with thread_id: ", config["configurable"]["thread_id"])
    return {"results": f"Hello, {state['input']}!"}

builder.add_node("plain_node", plain_node)
builder.add_node("node_with_runtime", node_with_runtime)
builder.add_node("node_with_config", node_with_config)
...
python  theme={null}
builder.add_node(my_node)

**Examples:**

Example 1 (unknown):
```unknown
Since the state updates are always deserialized into LangChain `Messages` when using [`add_messages`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.message.add_messages), you should use dot notation to access message attributes, like `state["messages"][-1].content`. Below is an example of a graph that uses [`add_messages`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.message.add_messages) as its reducer function.
```

Example 2 (unknown):
```unknown
#### MessagesState

Since having a list of messages in your state is so common, there exists a prebuilt state called `MessagesState` which makes it easy to use messages. `MessagesState` is defined with a single `messages` key which is a list of `AnyMessage` objects and uses the [`add_messages`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.message.add_messages) reducer. Typically, there is more state to track than just messages, so we see people subclass this state and add more fields, like:
```

Example 3 (unknown):
```unknown
## Nodes

In LangGraph, nodes are Python functions (either synchronous or asynchronous) that accept the following arguments:

1. `state`: The [state](#state) of the graph
2. `config`: A [`RunnableConfig`](https://reference.langchain.com/python/langchain_core/runnables/#langchain_core.runnables.RunnableConfig) object that contains configuration information like `thread_id` and tracing information like `tags`
3. `runtime`: A `Runtime` object that contains [runtime `context`](#runtime-context) and other information like `store` and `stream_writer`

Similar to `NetworkX`, you add these nodes to a graph using the [`add_node`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.StateGraph.add_node) method:
```

Example 4 (unknown):
```unknown
Behind the scenes, functions are converted to [RunnableLambda](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableLambda.html)s, which add batch and async support to your function, along with native tracing and debugging.

If you add a node to a graph without specifying a name, it will be given a default name equivalent to the function name.
```

---

## Annotate traces and runs inline

**URL:** llms-txt#annotate-traces-and-runs-inline

Source: https://docs.langchain.com/langsmith/annotate-traces-inline

LangSmith allows you to manually annotate traces with feedback within the application. This can be useful for adding context to a trace, such as a user's comment or a note about a specific issue.
You can annotate a trace either inline or by sending the trace to an annotation queue, which allows you to closely inspect and log feedbacks to runs one at a time.
Feedback tags are associated with your [workspace](/langsmith/administration-overview#workspaces).

<Note>
  **You can attach user feedback to ANY intermediate run (span) of the trace, not just the root span.**

This is useful for critiquing specific parts of the LLM application, such as the retrieval step or generation step of the RAG pipeline.
</Note>

To annotate a trace inline, click on the `Annotate` in the upper right corner of trace view for any particular run that is part of the trace.

<img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/annotate-trace-inline.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=193363baef8b46e592fa63b299b407af" alt="" data-og-width="1722" width="1722" data-og-height="1035" height="1035" data-path="langsmith/images/annotate-trace-inline.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/annotate-trace-inline.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=3751be3488a8a5a488eb4277b4bc574e 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/annotate-trace-inline.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=e4c94e427fd4a4c14e5fca6b1a4fce14 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/annotate-trace-inline.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=3cc298e64a52af87bb4d46760352c958 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/annotate-trace-inline.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=6f13e3f345430b443c6f45642d6031eb 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/annotate-trace-inline.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=4259e830834b95f09e4b457b1e9a2807 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/annotate-trace-inline.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=f65fa998f3962f7b971c8433c8be36aa 2500w" />

This will open up a pane that allows you to choose from feedback tags associated with your workspace and add a score for particular tags. You can also add a standalone comment. Follow [this guide](./set-up-feedback-criteria) to set up feedback tags for your workspace.
You can also set up new feedback criteria from within the pane itself.

<img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/annotation-sidebar.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=6a16e79d91b435f6c5de94d0d58daa59" alt="" data-og-width="1376" width="1376" data-og-height="758" height="758" data-path="langsmith/images/annotation-sidebar.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/annotation-sidebar.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=6b0ee76da4d19ca7d7b5640865f738a7 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/annotation-sidebar.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=2e1acb5f129b2f865eef0399b0b06217 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/annotation-sidebar.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=ac7a1f2abf6faeb918eee11b702bb450 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/annotation-sidebar.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=1454e680d2c557541ea68c794018ceae 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/annotation-sidebar.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=5f45921db9ec69f138be197f71ab6c22 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/annotation-sidebar.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=c0c75ec7d51de574a3b7b1283ed06907 2500w" />

You can use the labeled keyboard shortcuts to streamline the annotation process.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/annotate-traces-inline.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Anthropic (Claude)

**URL:** llms-txt#anthropic-(claude)

**Contents:**
- Model interfaces

Source: https://docs.langchain.com/oss/python/integrations/providers/anthropic

This page covers all LangChain integrations with [Anthropic](https://www.anthropic.com/), the makers of Claude.

<Columns cols={2}>
  <Card title="ChatAnthropic" href="/oss/python/integrations/chat/anthropic" cta="Get started" icon="message" arrow>
    Anthropic chat models.
  </Card>

<Card title="AnthropicLLM" href="/oss/python/integrations/llms/anthropic" cta="Get started" icon="i-cursor" arrow>
    (Legacy) Anthropic text completion models.
  </Card>
</Columns>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/providers/anthropic.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## An experiment is a collection of runs with a reference to the dataset used

**URL:** llms-txt#an-experiment-is-a-collection-of-runs-with-a-reference-to-the-dataset-used

---

## Application-specific evaluation approaches

**URL:** llms-txt#application-specific-evaluation-approaches

**Contents:**
- Agents
  - Evaluating an agent's final response
  - Evaluating a single step of an agent
  - Evaluating an agent's trajectory
- Retrieval augmented generation (RAG)
  - Dataset
  - Evaluator
  - Applying RAG Evaluation
  - RAG evaluation summary
- Summarization

Source: https://docs.langchain.com/langsmith/evaluation-approaches

Below, we will discuss evaluation of a few popular types of LLM applications.

[LLM-powered autonomous agents](https://lilianweng.github.io/posts/2023-06-23-agent/) combine three components (1) Tool calling, (2) Memory, and (3) Planning. Agents [use tool calling](https://python.langchain.com/v0.1/docs/modules/agents/agent_types/tool_calling/) with planning (e.g., often via prompting) and memory (e.g., often short-term message history) to generate responses. [Tool calling](https://python.langchain.com/v0.1/docs/modules/model_io/chat/function_calling/) allows a model to respond to a given prompt by generating two things: (1) a tool to invoke and (2) the input arguments required.

<img src="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tool-use.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=a1c10f940f40ad89c90de8fae3607c1f" alt="Tool use" data-og-width="1021" width="1021" data-og-height="424" height="424" data-path="langsmith/images/tool-use.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tool-use.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=e80012647614cd82cb468430e62fa9aa 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tool-use.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=67a5febcec316bfe2935ba65506558a2 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tool-use.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=e0b52fb90bdbc41332b09404823fbfca 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tool-use.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=8a4e6b4bda0f788f540ca240012c9e89 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tool-use.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=d403f18524e0904c716d0cba9e928cac 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tool-use.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=69133b8862e6738000f733ff7e34daae 2500w" />

Below is a tool-calling agent in [LangGraph](https://langchain-ai.github.io/langgraph/tutorials/introduction/). The `assistant node` is an LLM that determines whether to invoke a tool based upon the input. The `tool condition` sees if a tool was selected by the `assistant node` and, if so, routes to the `tool node`. The `tool node` executes the tool and returns the output as a tool message to the `assistant node`. This loop continues until as long as the `assistant node` selects a tool. If no tool is selected, then the agent directly returns the LLM response.

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-agent.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=37f3c09958c1e2543f633c59cc89df36" alt="Agent" data-og-width="1259" width="1259" data-og-height="492" height="492" data-path="langsmith/images/langgraph-agent.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-agent.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=59cecf5d27098da369bbf60d6a315437 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-agent.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=81426ee5f37211859c572fe51d837c44 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-agent.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=558ca3297a7697a12989f0bdcfd4a4d7 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-agent.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=39a3a80c152f99f133302fe79f4bf63d 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-agent.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=46164054792e7f730cef4897fd9cbf57 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-agent.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=553d05e97a4d052882b6381e1e8b0362 2500w" />

This sets up three general types of agent evaluations that users are often interested in:

* `Final Response`: Evaluate the agent's final response.
* `Single step`: Evaluate any agent step in isolation (e.g., whether it selects the appropriate tool).
* `Trajectory`: Evaluate whether the agent took the expected path (e.g., of tool calls) to arrive at the final answer.

<img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/agent-eval.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=5fe3c96402623ed8a61118f22a6426b6" alt="Agent-eval" data-og-width="1825" width="1825" data-og-height="915" height="915" data-path="langsmith/images/agent-eval.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/agent-eval.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=780c3ea6fecdbd41fa62017d3ac6042e 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/agent-eval.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=f43c0cd9d9b49ae1f2a7ead5f5e58bcd 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/agent-eval.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=360b12b491fb97fd46d444e440345737 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/agent-eval.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=88fd055136cd48a538d908e3899daa6e 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/agent-eval.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=c3e40f0ba26f26f2688553596487ab57 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/agent-eval.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=7b238d8ce02b6d1ab3d6ff3fb2c62d4e 2500w" />

Below we will cover what these are, the components (inputs, outputs, evaluators) needed for each one, and when you should consider this. Note that you likely will want to do multiple (if not all!) of these types of evaluations - they are not mutually exclusive!

### Evaluating an agent's final response

One way to evaluate an agent is to assess its overall performance on a task. This basically involves treating the agent as a black box and simply evaluating whether or not it gets the job done.

The inputs should be the user input and (optionally) a list of tools. In some cases, tool are hardcoded as part of the agent and they don't need to be passed in. In other cases, the agent is more generic, meaning it does not have a fixed set of tools and tools need to be passed in at run time.

The output should be the agent's final response.

The evaluator varies depending on the task you are asking the agent to do. Many agents perform a relatively complex set of steps and the output a final text response. Similar to RAG, LLM-as-judge evaluators are often effective for evaluation in these cases because they can assess whether the agent got a job done directly from the text response.

However, there are several downsides to this type of evaluation. First, it usually takes a while to run. Second, you are not evaluating anything that happens inside the agent, so it can be hard to debug when failures occur. Third, it can sometimes be hard to define appropriate evaluation metrics.

### Evaluating a single step of an agent

Agents generally perform multiple actions. While it is useful to evaluate them end-to-end, it can also be useful to evaluate these individual actions. This generally involves evaluating a single step of the agent - the LLM call where it decides what to do.

The inputs should be the input to a single step. Depending on what you are testing, this could just be the raw user input (e.g., a prompt and / or a set of tools) or it can also include previously completed steps.

The outputs are just the output of that step, which is usually the LLM response. The LLM response often contains tool calls, indicating what action the agent should take next.

The evaluator for this is usually some binary score for whether the correct tool call was selected, as well as some heuristic for whether the input to the tool was correct. The reference tool can be simply specified as a string.

There are several benefits to this type of evaluation. It allows you to evaluate individual actions, which lets you hone in where your application may be failing. They are also relatively fast to run (because they only involve a single LLM call) and evaluation often uses simple heuristic evaluation of the selected tool relative to the reference tool. One downside is that they don't capture the full agent - only one particular step. Another downside is that dataset creation can be challenging, particular if you want to include past history in the agent input. It is pretty easy to generate a dataset for steps early on in an agent's trajectory (e.g., this may only include the input prompt), but it can be difficult to generate a dataset for steps later on in the trajectory (e.g., including numerous prior agent actions and responses).

### Evaluating an agent's trajectory

Evaluating an agent's trajectory involves evaluating all the steps an agent took.

The inputs are again the inputs to the overall agent (the user input, and optionally a list of tools).

The outputs are a list of tool calls, which can be formulated as an "exact" trajectory (e.g., an expected sequence of tool calls) or simply a set of tool calls that are expected (in any order).

The evaluator here is some function over the steps taken. Assessing the "exact" trajectory can use a single binary score that confirms an exact match for each tool name in the sequence. This is simple, but has some flaws. Sometimes there can be multiple correct paths. This evaluation also does not capture the difference between a trajectory being off by a single step versus being completely wrong.

To address these flaws, evaluation metrics can focused on the number of "incorrect" steps taken, which better accounts for trajectories that are close versus ones that deviate significantly. Evaluation metrics can also focus on whether all of the expected tools are called in any order.

However, none of these approaches evaluate the input to the tools; they only focus on the tools selected. In order to account for this, another evaluation technique is to pass the full agent's trajectory (along with a reference trajectory) as a set of messages (e.g., all LLM responses and tool calls) an LLM-as-judge. This can evaluate the complete behavior of the agent, but it is the most challenging reference to compile (luckily, using a framework like LangGraph can help with this!). Another downside is that evaluation metrics can be somewhat tricky to come up with.

## Retrieval augmented generation (RAG)

Retrieval Augmented Generation (RAG) is a powerful technique that involves retrieving relevant documents based on a user's input and passing them to a language model for processing. RAG enables AI applications to generate more informed and context-aware responses by leveraging external knowledge.

<Info>
  For a comprehensive review of RAG concepts, see our [`RAG From Scratch` series](https://github.com/langchain-ai/rag-from-scratch).
</Info>

When evaluating RAG applications, a key consideration is whether you have (or can easily obtain) reference answers for each input question. Reference answers serve as ground truth for assessing the correctness of the generated responses. However, even in the absence of reference answers, various evaluations can still be performed using reference-free RAG evaluation prompts (examples provided below).

`LLM-as-judge` is a commonly used evaluator for RAG because it's an effective way to evaluate factual accuracy or consistency between texts.

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rag-types.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=1252b1369be04ddb4c480af277443ac2" alt="rag-types.png" data-og-width="1696" width="1696" data-og-height="731" height="731" data-path="langsmith/images/rag-types.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rag-types.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=dda9fa7e589b9d37bd31a5ba63d1f0fb 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rag-types.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=e16fca6e49aeb889cc9d6e04baca684a 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rag-types.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=de442b611295218493a5783820794727 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rag-types.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=ad286f13e4a15ec892f5995a539daf5d 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rag-types.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=64c7b545e34ac5d4bf975ee2f44af8df 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rag-types.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=447cb995acde1f42ffeee2883c61a033 2500w" />

When evaluating RAG applications, you can have evaluators that require reference outputs and those that don't:

1. **Require reference output**: Compare the RAG chain's generated answer or retrievals against a reference answer (or retrievals) to assess its correctness.
2. **Don't require reference output**: Perform self-consistency checks using prompts that don't require a reference answer (represented by orange, green, and red in the above figure).

### Applying RAG Evaluation

When applying RAG evaluation, consider the following approaches:

1. `Offline evaluation`: Use offline evaluation for any prompts that rely on a reference answer. This is most commonly used for RAG answer correctness evaluation, where the reference is a ground truth (correct) answer.

2. `Online evaluation`: Employ online evaluation for any reference-free prompts. This allows you to assess the RAG application's performance in real-time scenarios.

3. `Pairwise evaluation`: Utilize pairwise evaluation to compare answers produced by different RAG chains. This evaluation focuses on user-specified criteria (e.g., answer format or style) rather than correctness, which can be evaluated using self-consistency or a ground truth reference.

### RAG evaluation summary

| Evaluator           | Detail                                            | Needs reference output | LLM-as-judge?                                                                         | Pairwise relevant |
| ------------------- | ------------------------------------------------- | ---------------------- | ------------------------------------------------------------------------------------- | ----------------- |
| Document relevance  | Are documents relevant to the question?           | No                     | Yes - [prompt](https://smith.langchain.com/hub/langchain-ai/rag-document-relevance)   | No                |
| Answer faithfulness | Is the answer grounded in the documents?          | No                     | Yes - [prompt](https://smith.langchain.com/hub/langchain-ai/rag-answer-hallucination) | No                |
| Answer helpfulness  | Does the answer help address the question?        | No                     | Yes - [prompt](https://smith.langchain.com/hub/langchain-ai/rag-answer-helpfulness)   | No                |
| Answer correctness  | Is the answer consistent with a reference answer? | Yes                    | Yes - [prompt](https://smith.langchain.com/hub/langchain-ai/rag-answer-vs-reference)  | No                |
| Pairwise comparison | How do multiple answer versions compare?          | No                     | Yes - [prompt](https://smith.langchain.com/hub/langchain-ai/pairwise-evaluation-rag)  | Yes               |

Summarization is one specific type of free-form writing. The evaluation aim is typically to examine the writing (summary) relative to a set of criteria.

`Developer curated examples` of texts to summarize are commonly used for evaluation (see a dataset example [here](https://smith.langchain.com/public/659b07af-1cab-4e18-b21a-91a69a4c3990/d)). However, `user logs` from a production (summarization) app can be used for online evaluation with any of the `Reference-free` evaluation prompts below.

`LLM-as-judge` is typically used for evaluation of summarization (as well as other types of writing) using `Reference-free` prompts that follow provided criteria to grade a summary. It is less common to provide a particular `Reference` summary, because summarization is a creative task and there are many possible correct answers.

`Online` or `Offline` evaluation are feasible because of the `Reference-free` prompt used. `Pairwise` evaluation is also a powerful way to perform comparisons between different summarization chains (e.g., different summarization prompts or LLMs):

| Use Case         | Detail                                                                     | Needs reference output | LLM-as-judge?                                                                                | Pairwise relevant |
| ---------------- | -------------------------------------------------------------------------- | ---------------------- | -------------------------------------------------------------------------------------------- | ----------------- |
| Factual accuracy | Is the summary accurate relative to the source documents?                  | No                     | Yes - [prompt](https://smith.langchain.com/hub/langchain-ai/summary-accurancy-evaluator)     | Yes               |
| Faithfulness     | Is the summary grounded in the source documents (e.g., no hallucinations)? | No                     | Yes - [prompt](https://smith.langchain.com/hub/langchain-ai/summary-hallucination-evaluator) | Yes               |
| Helpfulness      | Is summary helpful relative to user need?                                  | No                     | Yes - [prompt](https://smith.langchain.com/hub/langchain-ai/summary-helpfulness-evaluator)   | Yes               |

## Classification and tagging

Classification and tagging apply a label to a given input (e.g., for toxicity detection, sentiment analysis, etc). Classification/tagging evaluation typically employs the following components, which we will review in detail below:

A central consideration for classification/tagging evaluation is whether you have a dataset with `reference` labels or not. If not, users frequently want to define an evaluator that uses criteria to apply label (e.g., toxicity, etc) to an input (e.g., text, user-question, etc). However, if ground truth class labels are provided, then the evaluation objective is focused on scoring a classification/tagging chain relative to the ground truth class label (e.g., using metrics such as precision, recall, etc).

If ground truth reference labels are provided, then it's common to simply define a [custom heuristic evaluator](/langsmith/code-evaluator) to compare ground truth labels to the chain output. However, it is increasingly common given the emergence of LLMs simply use `LLM-as-judge` to perform the classification/tagging of an input based upon specified criteria (without a ground truth reference).

`Online` or `Offline` evaluation is feasible when using `LLM-as-judge` with the `Reference-free` prompt used. In particular, this is well suited to `Online` evaluation when a user wants to tag / classify application input (e.g., for toxicity, etc).

| Use Case  | Detail              | Needs reference output | LLM-as-judge? | Pairwise relevant |
| --------- | ------------------- | ---------------------- | ------------- | ----------------- |
| Accuracy  | Standard definition | Yes                    | No            | No                |
| Precision | Standard definition | Yes                    | No            | No                |
| Recall    | Standard definition | Yes                    | No            | No                |

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/evaluation-approaches.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Application structure

**URL:** llms-txt#application-structure

**Contents:**
- Key Concepts
- File structure
- Configuration file
  - Examples
- Dependencies
- Graphs
- Environment variables

Source: https://docs.langchain.com/langsmith/application-structure

To deploy on LangSmith, an application must consist of one or more graphs, a configuration file (`langgraph.json`), a file that specifies dependencies, and an optional `.env` file that specifies environment variables.

This page explains how a LangSmith application is organized and how to provide the configuration details required for deployment.

To deploy using LangSmith, provide the following information:

1. A [configuration file](#configuration-file-concepts) (`langgraph.json`) that specifies the dependencies, graphs, and environment variables to use for the application.
2. The [graphs](#graphs) that implement the logic of the application.
3. A file that specifies [dependencies](#dependencies) required to run the application.
4. [Environment variables](#environment-variables) that are required for the application to run.

<Tip>
  **Framework agnostic**

LangSmith Deployment supports deploying a [LangGraph](/oss/python/langgraph/overview) *graph*. However, the implementation of a *node* of a graph can contain arbitrary Python code. This means any framework can be implemented within a node and deployed on LangSmith Deployment. This lets you keep your core application logic outside LangGraph while still using LangSmith for [deployment](/langsmith/deployments), scaling, and [observability](/langsmith/observability).
</Tip>

The following are examples of directory structures for Python and JavaScript applications:

<Tabs>
  <Tab title="Python (requirements.txt)">
    
  </Tab>

<Tab title="Python (pyproject.toml)">
    
  </Tab>

<Tab title="JS (package.json)">
    
  </Tab>
</Tabs>

<Note>
  The directory structure of an application can vary depending on the programming language and the package manager used.
</Note>

<a id="configuration-file-concepts" />

## Configuration file

The `langgraph.json` file is a JSON file that specifies the dependencies, graphs, environment variables, and other settings required to deploy an application.

For details on all supported keys in the JSON file, refer to the [LangGraph configuration file reference](/langsmith/cli#configuration-file).

<Tip>
  The [LangGraph CLI](/langsmith/cli) defaults to using the configuration file `langgraph.json` in the current directory.
</Tip>

<Tabs>
  <Tab title="Python">
    * The dependencies involve a custom local package and the `langchain_openai` package.
    * A single graph will be loaded from the file `./your_package/your_file.py` with the variable `variable`.
    * The environment variables are loaded from the `.env` file.

<Tab title="JavaScript">
    * The dependencies will be loaded from a dependency file in the local directory (e.g., `package.json`).
    * A single graph will be loaded from the file `./your_package/your_file.js` with the function `agent`.
    * The environment variable `OPENAI_API_KEY` is set inline.

An application may depend on other Python packages or JavaScript libraries (depending on the programming language in which the application is written).

You will generally need to specify the following information for dependencies to be set up correctly:

1. A file in the directory that specifies the dependencies (e.g., `requirements.txt`, `pyproject.toml`, or `package.json`).
2. A `dependencies` key in the [configuration file](#configuration-file-concepts) that specifies the dependencies required to run the application.
3. Any additional binaries or system libraries can be specified using `dockerfile_lines` key in the [LangGraph configuration file](#configuration-file-concepts).

Use the `graphs` key in the [configuration file](#configuration-file-concepts) to specify which graphs will be available in the deployed application.

You can specify one or more graphs in the configuration file. Each graph is identified by a unique name and a path to either (1) a compiled graph or (2) a function that defines a graph.

## Environment variables

If you're working with a deployed LangGraph application [locally](/langsmith/local-server), you can configure environment variables in the `env` key of the [configuration file](#configuration-file-concepts).

For a production deployment, you will typically want to configure the environment variables in the deployment environment.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/application-structure.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
</Tab>

  <Tab title="Python (pyproject.toml)">
```

Example 2 (unknown):
```unknown
</Tab>

  <Tab title="JS (package.json)">
```

Example 3 (unknown):
```unknown
</Tab>
</Tabs>

<Note>
  The directory structure of an application can vary depending on the programming language and the package manager used.
</Note>

<a id="configuration-file-concepts" />

## Configuration file

The `langgraph.json` file is a JSON file that specifies the dependencies, graphs, environment variables, and other settings required to deploy an application.

For details on all supported keys in the JSON file, refer to the [LangGraph configuration file reference](/langsmith/cli#configuration-file).

<Tip>
  The [LangGraph CLI](/langsmith/cli) defaults to using the configuration file `langgraph.json` in the current directory.
</Tip>

### Examples

<Tabs>
  <Tab title="Python">
    * The dependencies involve a custom local package and the `langchain_openai` package.
    * A single graph will be loaded from the file `./your_package/your_file.py` with the variable `variable`.
    * The environment variables are loaded from the `.env` file.
```

Example 4 (unknown):
```unknown
</Tab>

  <Tab title="JavaScript">
    * The dependencies will be loaded from a dependency file in the local directory (e.g., `package.json`).
    * A single graph will be loaded from the file `./your_package/your_file.js` with the function `agent`.
    * The environment variable `OPENAI_API_KEY` is set inline.
```

---

## App development in LangSmith Deployment

**URL:** llms-txt#app-development-in-langsmith-deployment

Source: https://docs.langchain.com/langsmith/app-development

**LangSmith Deployment** builds on the open-source [LangGraph](/oss/python/langgraph/overview) framework for developing stateful, multi-agent applications.
LangGraph provides the core abstractions and execution model, while LangSmith adds managed infrastructure, observability, deployment options, assistants, and concurrency controls—supporting the full lifecycle from development to production.

<Callout icon="cubes" color="#4F46E5" iconType="regular">
  LangSmith Deployment is framework-agnostic: you can deploy agents built with LangGraph or [other frameworks](/langsmith/autogen-integration). To get started with LangGraph itself, refer to the [LangGraph quickstart](/oss/python/langgraph/quickstart).
</Callout>

<CardGroup>
  <Card title="Assistants" cta="Explore assistants" href="/langsmith/assistants" icon="user-gear">
    Manage agent configurations, connect to threads, and build interactive assistants.
  </Card>

<Card title="Runs" cta="Learn about runs" href="/langsmith/background-run" icon="play">
    Execute background jobs, stateless runs, cron jobs, and manage configurable headers.
  </Card>

<Card title="Core capabilities" cta="See core features" href="/langsmith/streaming" icon="gear">
    Streaming, human-in-the-loop, webhooks, and concurrency controls like double-texting.
  </Card>

<Card title="Tutorials" cta="View tutorials" href="/langsmith/autogen-integration" icon="graduation-cap">
    Step-by-step examples: AutoGen integration, streaming UI, and generative UI in React.
  </Card>
</CardGroup>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/app-development.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## artist vectorstore for "prince" we'll get back the value "Prince", which we can then

**URL:** llms-txt#artist-vectorstore-for-"prince"-we'll-get-back-the-value-"prince",-which-we-can-then

---

## Assistants

**URL:** llms-txt#assistants

**Contents:**
- Configuration
- Versioning
- Execution
- Video guide

Source: https://docs.langchain.com/langsmith/assistants

**Assistants** allow you to manage configurations (like prompts, LLM selection, tools) separately from your graph's core logic, enabling rapid changes that don't alter the graph architecture. It is a way to create multiple specialized versions of the same graph architecture, each optimized for different use cases through configuration variations rather than structural changes.

For example, imagine a general-purpose writing agent built on a common graph architecture. While the structure remains the same, different writing styles—such as blog posts and tweets—require tailored configurations to optimize performance. To support these variations, you can create multiple assistants (e.g., one for blogs and another for tweets) that share the underlying graph but differ in model selection and system prompt.

<img src="https://mintcdn.com/langchain-5e9cc07a/IMK8wJkjSpMCGODD/langsmith/images/assistants.png?fit=max&auto=format&n=IMK8wJkjSpMCGODD&q=85&s=05402316c8fe86fead077ec774e873f0" alt="assistant versions" data-og-width="1824" width="1824" data-og-height="692" height="692" data-path="langsmith/images/assistants.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/IMK8wJkjSpMCGODD/langsmith/images/assistants.png?w=280&fit=max&auto=format&n=IMK8wJkjSpMCGODD&q=85&s=3ac250197ee8463950b74dc5f6bcd37f 280w, https://mintcdn.com/langchain-5e9cc07a/IMK8wJkjSpMCGODD/langsmith/images/assistants.png?w=560&fit=max&auto=format&n=IMK8wJkjSpMCGODD&q=85&s=d6b01c6ae96bd96b580bf43228610224 560w, https://mintcdn.com/langchain-5e9cc07a/IMK8wJkjSpMCGODD/langsmith/images/assistants.png?w=840&fit=max&auto=format&n=IMK8wJkjSpMCGODD&q=85&s=6125bf9aed49385ec8422e27cb377dad 840w, https://mintcdn.com/langchain-5e9cc07a/IMK8wJkjSpMCGODD/langsmith/images/assistants.png?w=1100&fit=max&auto=format&n=IMK8wJkjSpMCGODD&q=85&s=c54cde5d8a052ceac26d67131407aa73 1100w, https://mintcdn.com/langchain-5e9cc07a/IMK8wJkjSpMCGODD/langsmith/images/assistants.png?w=1650&fit=max&auto=format&n=IMK8wJkjSpMCGODD&q=85&s=780c08f1695bc2e5ba0b6261febb1954 1650w, https://mintcdn.com/langchain-5e9cc07a/IMK8wJkjSpMCGODD/langsmith/images/assistants.png?w=2500&fit=max&auto=format&n=IMK8wJkjSpMCGODD&q=85&s=ed8fba40ce7c1b3455027df735f9bdba 2500w" />

The LangGraph API provides several endpoints for creating and managing assistants and their versions. See the [API reference](https://langchain-ai.github.io/langgraph/cloud/reference/api/api_ref/#tag/assistants) for more details.

<Info>
  Assistants are a [LangSmith](/langsmith/home) concept. They are not available in the open source LangGraph library.
</Info>

Assistants build on the LangGraph open source concept of [configuration](/oss/python/langgraph/graph-api#runtime-context).

While configuration is available in the open source LangGraph library, assistants are only present in [LangSmith](/langsmith/home). This is due to the fact that assistants are tightly coupled to your deployed graph. Upon deployment, Agent Server will automatically create a default assistant for each graph using the graph's default configuration settings.

In practice, an assistant is just an *instance* of a graph with a specific configuration. Therefore, multiple assistants can reference the same graph but can contain different configurations (e.g. prompts, models, tools). The LangSmith Deployment API provides several endpoints for creating and managing assistants. See the [API reference](https://langchain-ai.github.io/langgraph/cloud/reference/api/api_ref/) and [this how-to](/langsmith/configuration-cloud) for more details on how to create assistants.

Assistants support versioning to track changes over time.
Once you've created an assistant, subsequent edits to that assistant will create new versions. See [this how-to](/langsmith/configuration-cloud#create-a-new-version-for-your-assistant) for more details on how to manage assistant versions.

A **run** is an invocation of an assistant. Each run may have its own input, configuration, and metadata, which may affect execution and output of the underlying graph. A run can optionally be executed on a [thread](/oss/python/langgraph/persistence#threads).

LangSmith API provides several endpoints for creating and managing runs. See the [API reference](https://langchain-ai.github.io/langgraph/cloud/reference/api/api_ref/) for more details.

<iframe className="w-full aspect-video rounded-xl" src="https://www.youtube.com/embed/fMsQX6pwXkE?si=6Q28l0taGOynO7sU" title="YouTube video player" frameBorder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowFullScreen />

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/assistants.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Assistant creation

**URL:** llms-txt#assistant-creation

**Contents:**
  - Filter operations
- Common access patterns
  - Single-owner resources
  - Permission-based access

@auth.on.assistants.create
async def on_assistant_create(
    ctx: Auth.types.AuthContext,
    value: Auth.types.assistants.create.value
):
    if "assistants:create" not in ctx.permissions:
        raise Auth.exceptions.HTTPException(
            status_code=403,
            detail="User lacks the required permissions."
        )
python  theme={null}
@auth.on
async def owner_only(ctx: Auth.types.AuthContext, value: dict):
    metadata = value.setdefault("metadata", {})
    metadata["owner"] = ctx.user.identity
    return {"owner": ctx.user.identity}
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
Notice that we are mixing global and resource-specific handlers in the above example. Since each request is handled by the most specific handler, a request to create a `thread` would match the `on_thread_create` handler but NOT the `reject_unhandled_requests` handler. A request to `update` a thread, however would be handled by the global handler, since we don't have a more specific handler for that resource and action.

<a id="filter-operations" />

### Filter operations

Authorization handlers can return `None`, a boolean, or a filter dictionary.

* `None` and `True` mean "authorize access to all underling resources"
* `False` means "deny access to all underling resources (raises a 403 exception)"
* A metadata filter dictionary will restrict access to resources

A filter dictionary is a dictionary with keys that match the resource metadata. It supports three operators:

* The default value is a shorthand for exact match, or "\$eq", below. For example, `{"owner": user_id}` will include only resources with metadata containing `{"owner": user_id}`
* `$eq`: Exact match (e.g., `{"owner": {"$eq": user_id}}`) - this is equivalent to the shorthand above, `{"owner": user_id}`
* `$contains`: List membership (e.g., `{"allowed_users": {"$contains": user_id}}`) or list containment (e.g., `{"allowed_users": {"$contains": [user_id_1, user_id_2]}}`). The value here must be an element of the list or a subset of the elements of the list, respectively. The metadata in the stored resource must be a list/container type.

A dictionary with multiple keys is treated using a logical `AND` filter. For example, `{"owner": org_id, "allowed_users": {"$contains": user_id}}` will only match resources with metadata whose "owner" is `org_id` and whose "allowed\_users" list contains `user_id`.
See the reference [`Auth`](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.Auth)(Auth) for more information.

## Common access patterns

Here are some typical authorization patterns:

### Single-owner resources

This common pattern lets you scope all threads, assistants, crons, and runs to a single user. It's useful for common single-user use cases like regular chatbot-style apps.
```

Example 2 (unknown):
```unknown
### Permission-based access

This pattern lets you control access based on **permissions**. It's useful if you want certain roles to have broader or more restricted access to resources.
```

---

## Assumes you're in an interactive Python environmentfrom IPython.display import Image, display ...

**URL:** llms-txt#assumes-you're-in-an-interactive-python-environmentfrom-ipython.display-import-image,-display-...

python  theme={null}
from langchain.embeddings import init_embeddings
from langchain.tools import tool
from langchain_core.vectorstores import InMemoryVectorStore
from langchain.agents import create_agent

**Examples:**

Example 1 (unknown):
```unknown
<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/refund-graph.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=a65951850208fd3b03848629bdda8ae0" alt="Refund graph" data-og-width="256" width="256" data-og-height="333" height="333" data-path="langsmith/images/refund-graph.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/refund-graph.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=8817f44b37322ab9a51fd01ee7902181 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/refund-graph.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=753a20158640cbeeeb81498d5c5ae95d 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/refund-graph.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=8d38bcff07b53e1f5648b3dd45cffa66 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/refund-graph.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=50a7f863cf45d9df7b59cc3614fdb4e9 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/refund-graph.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=cfbda86ec83a651bfe8e38235579302d 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/refund-graph.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=56745d2e7603dca7fa233e1fd5818008 2500w" />

#### Lookup agent

For the lookup (i.e. question-answering) agent, we'll use a simple ReACT architecture and give the agent tools for looking up track names, artist names, and album names based on various filters. For example, you can look up albums by a particular artist, artists who released songs with a specific name, etc.
```

---

## Assumes you've installed pydantic

**URL:** llms-txt#assumes-you've-installed-pydantic

from pydantic import BaseModel

---

## Assumes you organize information in store like (user_id, resource_type, resource_id)

**URL:** llms-txt#assumes-you-organize-information-in-store-like-(user_id,-resource_type,-resource_id)

@auth.on.store()
async def authorize_store(ctx: Auth.types.AuthContext, value: dict):
    # The "namespace" field for each store item is a tuple you can think of as the directory of an item.
    namespace: tuple = value["namespace"]
    assert namespace[0] == ctx.user.identity, "Not authorized"
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
Notice that instead of one global handler, you now have specific handlers for:

1. Creating threads
2. Reading threads
3. Accessing assistants

The first three of these match specific **actions** on each resource (see [resource actions](/langsmith/auth#resource-specific-handlers)), while the last one (`@auth.on.assistants`) matches *any* action on the `assistants` resource. For each request, LangGraph will run the most specific handler that matches the resource and action being accessed. This means that the four handlers above will run rather than the broadly scoped "`@auth.on`" handler.

Try adding the following test code to your test file:
```

---

## async_client = wrap_anthropic(anthropic.AsyncAnthropic())

**URL:** llms-txt#async_client-=-wrap_anthropic(anthropic.asyncanthropic())

@traceable(run_type="tool", name="Retrieve Context")
def my_tool(question: str) -> str:
    return "During this morning's meeting, we solved all world conflict."

@traceable(name="Chat Pipeline")
def chat_pipeline(question: str):
    context = my_tool(question)
    messages = [
        { "role": "user", "content": f"Question: {question}\nContext: {context}"}
    ]
    messages = client.messages.create(
      model="claude-sonnet-4-5-20250929",
      messages=messages,
      max_tokens=1024,
      system="You are a helpful assistant. Please respond to the user's request only based on the given context."
    )
    return messages

chat_pipeline("Can you summarize this morning's meetings?")
```

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/trace-anthropic.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Augment the LLM with schema for structured output

**URL:** llms-txt#augment-the-llm-with-schema-for-structured-output

structured_llm = llm.with_structured_output(SearchQuery)

---

## Augment the LLM with tools

**URL:** llms-txt#augment-the-llm-with-tools

tools = [add, multiply, divide]
tools_by_name = {tool.name: tool for tool in tools}
llm_with_tools = llm.bind_tools(tools)
python Graph API theme={null}
  from langgraph.graph import MessagesState
  from langchain.messages import SystemMessage, HumanMessage, ToolMessage

# Nodes
  def llm_call(state: MessagesState):
      """LLM decides whether to call a tool or not"""

return {
          "messages": [
              llm_with_tools.invoke(
                  [
                      SystemMessage(
                          content="You are a helpful assistant tasked with performing arithmetic on a set of inputs."
                      )
                  ]
                  + state["messages"]
              )
          ]
      }

def tool_node(state: dict):
      """Performs the tool call"""

result = []
      for tool_call in state["messages"][-1].tool_calls:
          tool = tools_by_name[tool_call["name"]]
          observation = tool.invoke(tool_call["args"])
          result.append(ToolMessage(content=observation, tool_call_id=tool_call["id"]))
      return {"messages": result}

# Conditional edge function to route to the tool node or end based upon whether the LLM made a tool call
  def should_continue(state: MessagesState) -> Literal["tool_node", END]:
      """Decide if we should continue the loop or stop based upon whether the LLM made a tool call"""

messages = state["messages"]
      last_message = messages[-1]

# If the LLM makes a tool call, then perform an action
      if last_message.tool_calls:
          return "tool_node"

# Otherwise, we stop (reply to the user)
      return END

# Build workflow
  agent_builder = StateGraph(MessagesState)

# Add nodes
  agent_builder.add_node("llm_call", llm_call)
  agent_builder.add_node("tool_node", tool_node)

# Add edges to connect nodes
  agent_builder.add_edge(START, "llm_call")
  agent_builder.add_conditional_edges(
      "llm_call",
      should_continue,
      ["tool_node", END]
  )
  agent_builder.add_edge("tool_node", "llm_call")

# Compile the agent
  agent = agent_builder.compile()

# Show the agent
  display(Image(agent.get_graph(xray=True).draw_mermaid_png()))

# Invoke
  messages = [HumanMessage(content="Add 3 and 4.")]
  messages = agent.invoke({"messages": messages})
  for m in messages["messages"]:
      m.pretty_print()
  python Functional API theme={null}
  from langgraph.graph import add_messages
  from langchain.messages import (
      SystemMessage,
      HumanMessage,
      BaseMessage,
      ToolCall,
  )

@task
  def call_llm(messages: list[BaseMessage]):
      """LLM decides whether to call a tool or not"""
      return llm_with_tools.invoke(
          [
              SystemMessage(
                  content="You are a helpful assistant tasked with performing arithmetic on a set of inputs."
              )
          ]
          + messages
      )

@task
  def call_tool(tool_call: ToolCall):
      """Performs the tool call"""
      tool = tools_by_name[tool_call["name"]]
      return tool.invoke(tool_call)

@entrypoint()
  def agent(messages: list[BaseMessage]):
      llm_response = call_llm(messages).result()

while True:
          if not llm_response.tool_calls:
              break

# Execute tools
          tool_result_futures = [
              call_tool(tool_call) for tool_call in llm_response.tool_calls
          ]
          tool_results = [fut.result() for fut in tool_result_futures]
          messages = add_messages(messages, [llm_response, *tool_results])
          llm_response = call_llm(messages).result()

messages = add_messages(messages, llm_response)
      return messages

# Invoke
  messages = [HumanMessage(content="Add 3 and 4.")]
  for chunk in agent.stream(messages, stream_mode="updates"):
      print(chunk)
      print("\n")
  ```
</CodeGroup>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/workflows-agents.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
<CodeGroup>
```

Example 2 (unknown):
```unknown

```

---

## Authentication & access control

**URL:** llms-txt#authentication-&-access-control

**Contents:**
- Core Concepts
  - Authentication vs authorization
- Default security models
  - LangSmith
  - Self-hosted
- System architecture
- Authentication
  - Agent authentication
  - Agent authentication with MCP
- Authorization

Source: https://docs.langchain.com/langsmith/auth

LangSmith provides a flexible authentication and authorization system that can integrate with most authentication schemes.

### Authentication vs authorization

While often used interchangeably, these terms represent distinct security concepts:

* [**Authentication**](#authentication) ("AuthN") verifies *who* you are. This runs as middleware for every request.
* [**Authorization**](#authorization) ("AuthZ") determines *what you can do*. This validates the user's privileges and roles on a per-resource basis.

In LangSmith, authentication is handled by your [`@auth.authenticate`](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.Auth.authenticate) handler, and authorization is handled by your [`@auth.on`](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.Auth.on) handlers.

## Default security models

LangSmith provides different security defaults:

* Uses LangSmith API keys by default
* Requires valid API key in `x-api-key` header
* Can be customized with your auth handler

<Note>
  **Custom auth**
  Custom auth **is supported** for all plans in LangSmith.
</Note>

* No default authentication
* Complete flexibility to implement your security model
* You control all aspects of authentication and authorization

## System architecture

A typical authentication setup involves three main components:

1. **Authentication Provider** (Identity Provider/IdP)

* A dedicated service that manages user identities and credentials
* Handles user registration, login, password resets, etc.
* Issues tokens (JWT, session tokens, etc.) after successful authentication
* Examples: Auth0, Supabase Auth, Okta, or your own auth server

2. **LangGraph Backend** (Resource Server)

* Your LangGraph application that contains business logic and protected resources
* Validates tokens with the auth provider
* Enforces access control based on user identity and permissions
* Doesn't store user credentials directly

3. **Client Application** (Frontend)

* Web app, mobile app, or API client
* Collects time-sensitive user credentials and sends to auth provider
* Receives tokens from auth provider
* Includes these tokens in requests to LangGraph backend

Here's how these components typically interact:

Your [`@auth.authenticate`](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.Auth.authenticate) handler in LangGraph handles steps 4-6, while your [`@auth.on`](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.Auth.on) handlers implement step 7.

Authentication in LangGraph runs as middleware on every request. Your [`@auth.authenticate`](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.Auth.authenticate) handler receives request information and should:

1. Validate the credentials
2. Return [user info](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.types.MinimalUserDict) containing the user's identity and user information if valid
3. Raise an [HTTP exception](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.exceptions.HTTPException) or AssertionError if invalid

The returned user information is available:

* To your authorization handlers via [`ctx.user`](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.types.AuthContext)
* In your application via `config["configuration"]["langgraph_auth_user"]`

<Accordion title="Supported Parameters">
  The [`@auth.authenticate`](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.Auth.authenticate) handler can accept any of the following parameters by name:

* request (Request): The raw ASGI request object
  * path (str): The request path, e.g., `"/threads/abcd-1234-abcd-1234/runs/abcd-1234-abcd-1234/stream"`
  * method (str): The HTTP method, e.g., `"GET"`
  * path\_params (dict\[str, str]): URL path parameters, e.g., `{"thread_id": "abcd-1234-abcd-1234", "run_id": "abcd-1234-abcd-1234"}`
  * query\_params (dict\[str, str]): URL query parameters, e.g., `{"stream": "true"}`
  * headers (dict\[bytes, bytes]): Request headers
  * authorization (str | None): The Authorization header value (e.g., `"Bearer <token>"`)

In many of our tutorials, we will just show the "authorization" parameter to be concise, but you can opt to accept more information as needed
  to implement your custom authentication scheme.
</Accordion>

### Agent authentication

Custom authentication permits delegated access. The values you return in  `@auth.authenticate` are added to the run context, giving agents user-scoped credentials lets them access resources on the user’s behalf.

After authentication, the platform creates a special configuration object that is passed to your graph and all nodes via the configurable context.
This object contains information about the current user, including any custom fields you return from your [`@auth.authenticate`](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.Auth.authenticate) handler.

To enable an agent to act on behalf of the user, use [custom authentication middleware](/langsmith/custom-auth). This will allow the agent to interact with external systems like MCP servers, external databases, and even other agents on behalf of the user.

For more information, see the [Use custom auth](/langsmith/custom-auth#enable-agent-authentication) guide.

### Agent authentication with MCP

For information on how to authenticate an agent to an MCP server, see the [MCP conceptual guide](/oss/python/langchain/mcp).

After authentication, LangGraph calls your [`@auth.on`](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.Auth) handlers to control access to specific resources (e.g., threads, assistants, crons). These handlers can:

1. Add metadata to be saved during resource creation by mutating the `value["metadata"]` dictionary directly. See the [supported actions table](#supported-actions) for the list of types the value can take for each action.
2. Filter resources by metadata during search/list or read operations by returning a [filter dictionary](#filter-operations).
3. Raise an HTTP exception if access is denied.

If you want to just implement simple user-scoped access control, you can use a single [`@auth.on`](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.Auth) handler for all resources and actions. If you want to have different control depending on the resource and action, you can use [resource-specific handlers](#resource-specific-handlers). See the [Supported Resources](#supported-resources) section for a full list of the resources that support access control.

<a id="resource-specific-handlers" />

### Resource-specific handlers

You can register handlers for specific resources and actions by chaining the resource and action names together with the [`@auth.on`](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.Auth) decorator.
When a request is made, the most specific handler that matches that resource and action is called. Below is an example of how to register handlers for specific resources and actions. For the following setup:

1. Authenticated users are able to create threads, read threads, and create runs on threads
2. Only users with the "assistants:create" permission are allowed to create new assistants
3. All other endpoints (e.g., e.g., delete assistant, crons, store) are disabled for all users.

<Tip>
  **Supported Handlers**
  For a full list of supported resources and actions, see the [Supported Resources](#supported-resources) section below.
</Tip>

```python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
Your [`@auth.authenticate`](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.Auth.authenticate) handler in LangGraph handles steps 4-6, while your [`@auth.on`](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.Auth.on) handlers implement step 7.

## Authentication

Authentication in LangGraph runs as middleware on every request. Your [`@auth.authenticate`](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.Auth.authenticate) handler receives request information and should:

1. Validate the credentials
2. Return [user info](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.types.MinimalUserDict) containing the user's identity and user information if valid
3. Raise an [HTTP exception](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.exceptions.HTTPException) or AssertionError if invalid
```

Example 2 (unknown):
```unknown
The returned user information is available:

* To your authorization handlers via [`ctx.user`](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.types.AuthContext)
* In your application via `config["configuration"]["langgraph_auth_user"]`

<Accordion title="Supported Parameters">
  The [`@auth.authenticate`](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.Auth.authenticate) handler can accept any of the following parameters by name:

  * request (Request): The raw ASGI request object
  * path (str): The request path, e.g., `"/threads/abcd-1234-abcd-1234/runs/abcd-1234-abcd-1234/stream"`
  * method (str): The HTTP method, e.g., `"GET"`
  * path\_params (dict\[str, str]): URL path parameters, e.g., `{"thread_id": "abcd-1234-abcd-1234", "run_id": "abcd-1234-abcd-1234"}`
  * query\_params (dict\[str, str]): URL query parameters, e.g., `{"stream": "true"}`
  * headers (dict\[bytes, bytes]): Request headers
  * authorization (str | None): The Authorization header value (e.g., `"Bearer <token>"`)

  In many of our tutorials, we will just show the "authorization" parameter to be concise, but you can opt to accept more information as needed
  to implement your custom authentication scheme.
</Accordion>

### Agent authentication

Custom authentication permits delegated access. The values you return in  `@auth.authenticate` are added to the run context, giving agents user-scoped credentials lets them access resources on the user’s behalf.
```

Example 3 (unknown):
```unknown
After authentication, the platform creates a special configuration object that is passed to your graph and all nodes via the configurable context.
This object contains information about the current user, including any custom fields you return from your [`@auth.authenticate`](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.Auth.authenticate) handler.

To enable an agent to act on behalf of the user, use [custom authentication middleware](/langsmith/custom-auth). This will allow the agent to interact with external systems like MCP servers, external databases, and even other agents on behalf of the user.

For more information, see the [Use custom auth](/langsmith/custom-auth#enable-agent-authentication) guide.

### Agent authentication with MCP

For information on how to authenticate an agent to an MCP server, see the [MCP conceptual guide](/oss/python/langchain/mcp).

## Authorization

After authentication, LangGraph calls your [`@auth.on`](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.Auth) handlers to control access to specific resources (e.g., threads, assistants, crons). These handlers can:

1. Add metadata to be saved during resource creation by mutating the `value["metadata"]` dictionary directly. See the [supported actions table](#supported-actions) for the list of types the value can take for each action.
2. Filter resources by metadata during search/list or read operations by returning a [filter dictionary](#filter-operations).
3. Raise an HTTP exception if access is denied.

If you want to just implement simple user-scoped access control, you can use a single [`@auth.on`](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.Auth) handler for all resources and actions. If you want to have different control depending on the resource and action, you can use [resource-specific handlers](#resource-specific-handlers). See the [Supported Resources](#supported-resources) section for a full list of the resources that support access control.
```

Example 4 (unknown):
```unknown
<a id="resource-specific-handlers" />

### Resource-specific handlers

You can register handlers for specific resources and actions by chaining the resource and action names together with the [`@auth.on`](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.Auth) decorator.
When a request is made, the most specific handler that matches that resource and action is called. Below is an example of how to register handlers for specific resources and actions. For the following setup:

1. Authenticated users are able to create threads, read threads, and create runs on threads
2. Only users with the "assistants:create" permission are allowed to create new assistants
3. All other endpoints (e.g., e.g., delete assistant, crons, store) are disabled for all users.

<Tip>
  **Supported Handlers**
  For a full list of supported resources and actions, see the [Supported Resources](#supported-resources) section below.
</Tip>
```

---

## Authentication methods

**URL:** llms-txt#authentication-methods

**Contents:**
- Cloud
  - Email/Password
  - Social Providers
  - SAML SSO
- Self-Hosted
  - SSO with OAuth 2.0 and OIDC
  - Email/Password a.k.a. basic auth
  - None

Source: https://docs.langchain.com/langsmith/authentication-methods

LangSmith supports multiple authentication methods for easy sign-up and login.

Users can use an email address and password to sign up and login to LangSmith.

Users can alternatively use their credentials from GitHub or Google.

Enterprise customers can configure [SAML SSO](/langsmith/user-management) and [SCIM](/langsmith/user-management)

Self-hosted customers have more control over how their users can login to LangSmith. For more in-depth coverage of configuration options, see [the self-hosting docs](/langsmith/self-hosted) and [Helm chart](https://github.com/langchain-ai/helm/tree/main/charts/langsmith).

### SSO with OAuth 2.0 and OIDC

Production installations should configure SSO in order to use an external identity provider. This enables users to login through an identity platform like Auth0/Okta. LangSmith supports almost any OIDC-compliant provider. Learn more about configuring SSO in the [SSO configuration guide](/langsmith/self-host-sso)

### Email/Password a.k.a. basic auth

This auth method requires very little configuration as it does not require an external identity provider. It is most appropriate to use for self-hosted trials. Learn more in the [basic auth configuration guide](/langsmith/self-host-basic-auth)

<Warning>
  This authentication mode will be removed after the launch of Basic Auth.
</Warning>

If zero authentication methods are enabled, a self-hosted installation does not require any login/sign-up. This configuration should only be used for verifying installation at the infrastructure level, as the feature set supported in this mode is restricted with only a single organization and workspace.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/authentication-methods.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Automatically run evaluators on experiments

**URL:** llms-txt#automatically-run-evaluators-on-experiments

**Contents:**
- Configuring an evaluator on a dataset
- LLM-as-a-judge evaluators
- Custom code evaluators
- Next steps

Source: https://docs.langchain.com/langsmith/bind-evaluator-to-dataset

LangSmith supports two ways to grade experiments created via the SDK:

* **Programmatically**, by specifying evaluators in your code (see [this guide](/langsmith/evaluate-llm-application) for details)
* By **binding evaluators to a dataset** in the UI. This will automatically run the evaluators on any new experiments created, in addition to any evaluators you've set up via the SDK. This is useful when you're iterating on your application (target function), and have a standard set of evaluators you want to run for all experiments.

## Configuring an evaluator on a dataset

1. Click on the **Datasets and Experiments** tab in the sidebar.
2. Select the dataset you want to configure the evaluator for.
3. Click on the **+ Evaluator** button to add an evaluator to the dataset. This will open a pane you can use to configure the evaluator.

<Note>
  When you configure an evaluator for a dataset, it will only affect the experiment runs that are created after the evaluator is configured. It will not affect the evaluation of experiment runs that were created before the evaluator was configured.
</Note>

## LLM-as-a-judge evaluators

The process for binding evaluators to a dataset is very similar to the process for configuring a LLM-as-a-judge evaluator in the Playground. View instructions for [configuring an LLM-as-a-judge evaluator in the Playground.](/langsmith/llm-as-judge?mode=ui)

## Custom code evaluators

The process for binding a code evaluators to a dataset is very similar to the process for configuring a code evaluator in online evaluation. View instruction for [configuring code evaluators](/langsmith/online-evaluations#configure-a-custom-code-evaluator).

The only difference between configuring a code evaluator in online evaluation and binding a code evaluator to a dataset is that the custom code evaluator can reference outputs that are part of the dataset's `Example`.

For custom code evaluators bound to a dataset, the evaluator function takes in two arguments:

* A `Run` ([reference](/langsmith/run-data-format)). This represents the new run in your experiment. For example, if you ran an experiment via SDK, this would contain the input/output from your chain or model you are testing.
* An `Example` ([reference](/langsmith/example-data-format)). This represents the reference example in your dataset that the chain or model you are testing uses. The `inputs` to the Run and Example should be the same. If your Example has a reference `outputs`, then you can use this to compare to the run's output for scoring.

The code below shows an example of a simple evaluator function that checks that the outputs exactly equal the reference outputs.

* Analyze your experiment results in the [experiments tab](/langsmith/analyze-an-experiment)
* Compare your experiment results in the [comparison view](/langsmith/compare-experiment-results)

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/bind-evaluator-to-dataset.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

---

## autoscaling:

**URL:** llms-txt#autoscaling:

---

## await aevaluate(...)

**URL:** llms-txt#await-aevaluate(...)

**Contents:**
- Related

results = await ls_client.aevaluate(
    researcher_app,
    data=dataset,
    evaluators=[concise],
    # Optional, add concurrency.
    max_concurrency=2,  # Optional, add concurrency.
    experiment_prefix="gpt-4o-mini-baseline"  # Optional, random by default.
)
```

* [Run an evaluation (synchronously)](/langsmith/evaluate-llm-application)
* [Handle model rate limits](/langsmith/rate-limiting)

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/evaluation-async.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## AWS (Amazon)

**URL:** llms-txt#aws-(amazon)

**Contents:**
- Chat models
  - Bedrock Chat
  - Bedrock Converse
- LLMs
  - Bedrock
  - Amazon API Gateway
  - SageMaker Endpoint
- Embedding Models
  - Bedrock
  - SageMaker Endpoint

Source: https://docs.langchain.com/oss/python/integrations/providers/aws

This page covers all LangChain integrations with the [Amazon Web Services (AWS)](https://aws.amazon.com/) platform.

> [Amazon Bedrock](https://aws.amazon.com/bedrock/) is a fully managed service that offers a choice of
> high-performing foundation models (FMs) from leading AI companies like `AI21 Labs`, `Anthropic`, `Cohere`,
> `Meta`, `Stability AI`, and `Amazon` via a single API, along with a broad set of capabilities you need to
> build generative AI applications with security, privacy, and responsible AI. Using `Amazon Bedrock`,
> you can easily experiment with and evaluate top FMs for your use case, privately customize them with
> your data using techniques such as fine-tuning and `Retrieval Augmented Generation` (`RAG`), and build
> agents that execute tasks using your enterprise systems and data sources. Since `Amazon Bedrock` is
> serverless, you don't have to manage any infrastructure, and you can securely integrate and deploy
> generative AI capabilities into your applications using the AWS services you are already familiar with.

See a [usage example](/oss/python/integrations/chat/bedrock).

AWS Bedrock maintains a [Converse API](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_Converse.html)
that provides a unified conversational interface for Bedrock models. This API does not
yet support custom models. You can see a list of all
[models that are supported here](https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference.html).

<Info>
  **We recommend the Converse API for users who do not need to use custom models. It can be accessed using [ChatBedrockConverse](https://python.langchain.com/api_reference/aws/chat_models/langchain_aws.chat_models.bedrock_converse.ChatBedrockConverse.html).**
</Info>

See a [usage example](/oss/python/integrations/chat/bedrock).

See a [usage example](/oss/python/integrations/llms/bedrock).

### Amazon API Gateway

> [Amazon API Gateway](https://aws.amazon.com/api-gateway/) is a fully managed service that makes it easy for
> developers to create, publish, maintain, monitor, and secure APIs at any scale. APIs act as the "front door"
> for applications to access data, business logic, or functionality from your backend services. Using
> `API Gateway`, you can create RESTful APIs and WebSocket APIs that enable real-time two-way communication
> applications. `API Gateway` supports containerized and serverless workloads, as well as web applications.
>
> `API Gateway` handles all the tasks involved in accepting and processing up to hundreds of thousands of
> concurrent API calls, including traffic management, CORS support, authorization and access control,
> throttling, monitoring, and API version management. `API Gateway` has no minimum fees or startup costs.
> You pay for the API calls you receive and the amount of data transferred out and, with the `API Gateway`
> tiered pricing model, you can reduce your cost as your API usage scales.

See a [usage example](/oss/python/integrations/llms/amazon_api_gateway).

### SageMaker Endpoint

> [Amazon SageMaker](https://aws.amazon.com/sagemaker/) is a system that can build, train, and deploy
> machine learning (ML) models with fully managed infrastructure, tools, and workflows.

We use `SageMaker` to host our model and expose it as the `SageMaker Endpoint`.

See a [usage example](/oss/python/integrations/llms/sagemaker).

See a [usage example](/oss/python/integrations/text_embedding/bedrock).

### SageMaker Endpoint

See a [usage example](/oss/python/integrations/text_embedding/sagemaker-endpoint).

### AWS S3 Directory and File

> [Amazon Simple Storage Service (Amazon S3)](https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-folders.html)
> is an object storage service.
> [AWS S3 Directory](https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-folders.html)
> [AWS S3 Buckets](https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingBucket.html)

See a [usage example for S3DirectoryLoader](/oss/python/integrations/document_loaders/aws_s3_directory).

See a [usage example for S3FileLoader](/oss/python/integrations/document_loaders/aws_s3_file).

> [Amazon Textract](https://docs.aws.amazon.com/managedservices/latest/userguide/textract.html) is a machine
> learning (ML) service that automatically extracts text, handwriting, and data from scanned documents.

See a [usage example](/oss/python/integrations/document_loaders/amazon_textract).

> [Amazon Athena](https://aws.amazon.com/athena/) is a serverless, interactive analytics service built
> on open-source frameworks, supporting open-table and file formats.

See a [usage example](/oss/python/integrations/document_loaders/athena).

> The [AWS Glue Data Catalog](https://docs.aws.amazon.com/en_en/glue/latest/dg/catalog-and-crawler.html) is a centralized metadata
> repository that allows you to manage, access, and share metadata about
> your data stored in AWS. It acts as a metadata store for your data assets,
> enabling various AWS services and your applications to query and connect
> to the data they need efficiently.

See a [usage example](/oss/python/integrations/document_loaders/glue_catalog).

### Amazon OpenSearch Service

> [Amazon OpenSearch Service](https://aws.amazon.com/opensearch-service/) performs
> interactive log analytics, real-time application monitoring, website search, and more. `OpenSearch` is
> an open source,
> distributed search and analytics suite derived from `Elasticsearch`. `Amazon OpenSearch Service` offers the
> latest versions of `OpenSearch`, support for many versions of `Elasticsearch`, as well as
> visualization capabilities powered by `OpenSearch Dashboards` and `Kibana`.

We need to install several python libraries.

See a [usage example](/oss/python/integrations/vectorstores/opensearch#using-aos-amazon-opensearch-service).

### Amazon DocumentDB Vector Search

> [Amazon DocumentDB (with MongoDB Compatibility)](https://docs.aws.amazon.com/documentdb/) makes it easy to set up, operate, and scale MongoDB-compatible databases in the cloud.
> With Amazon DocumentDB, you can run the same application code and use the same drivers and tools that you use with MongoDB.
> Vector search for Amazon DocumentDB combines the flexibility and rich querying capability of a JSON-based document database with the power of vector search.

#### Installation and Setup

See [detail configuration instructions](/oss/python/integrations/vectorstores/documentdb).

We need to install the `pymongo` python package.

#### Deploy DocumentDB on AWS

[Amazon DocumentDB (with MongoDB Compatibility)](https://docs.aws.amazon.com/documentdb/) is a fast, reliable, and fully managed database service. Amazon DocumentDB makes it easy to set up, operate, and scale MongoDB-compatible databases in the cloud.

AWS offers services for computing, databases, storage, analytics, and other functionality. For an overview of all AWS services, see [Cloud Computing with Amazon Web Services](https://aws.amazon.com/what-is-aws/).

See a [usage example](/oss/python/integrations/vectorstores/documentdb).

[Amazon MemoryDB](https://aws.amazon.com/memorydb/) is a durable, in-memory database service that delivers ultra-fast performance. MemoryDB is compatible with Redis OSS, a popular open source data store,
enabling you to quickly build applications using the same flexible and friendly Redis OSS APIs, and commands that they already use today.

InMemoryVectorStore class provides a vectorstore to connect with Amazon MemoryDB.

See a [usage example](/oss/python/integrations/vectorstores/memorydb).

> [Amazon Kendra](https://docs.aws.amazon.com/kendra/latest/dg/what-is-kendra.html) is an intelligent search service
> provided by `Amazon Web Services` (`AWS`). It utilizes advanced natural language processing (NLP) and machine
> learning algorithms to enable powerful search capabilities across various data sources within an organization.
> `Kendra` is designed to help users find the information they need quickly and accurately,
> improving productivity and decision-making.

> With `Kendra`, we can search across a wide range of content types, including documents, FAQs, knowledge bases,
> manuals, and websites. It supports multiple languages and can understand complex queries, synonyms, and
> contextual meanings to provide highly relevant search results.

We need to install the `langchain-aws` library.

See a [usage example](/oss/python/integrations/retrievers/amazon_kendra_retriever).

### Amazon Bedrock (Knowledge Bases)

> [Knowledge bases for Amazon Bedrock](https://aws.amazon.com/bedrock/knowledge-bases/) is an
> `Amazon Web Services` (`AWS`) offering which lets you quickly build RAG applications by using your
> private data to customize foundation model response.

We need to install the `langchain-aws` library.

See a [usage example](/oss/python/integrations/retrievers/bedrock).

> [`Amazon AWS Lambda`](https://aws.amazon.com/pm/lambda/) is a serverless computing service provided by
> `Amazon Web Services` (`AWS`). It helps developers to build and run applications and services without
> provisioning or managing servers. This serverless architecture enables you to focus on writing and
> deploying code, while AWS automatically takes care of scaling, patching, and managing the
> infrastructure required to run your applications.

We need to install `boto3` python library.

See a [usage example](/oss/python/integrations/tools/awslambda).

> [Amazon Neptune](https://aws.amazon.com/neptune/)
> is a high-performance graph analytics and serverless database for superior scalability and availability.

For the Cypher and SPARQL integrations below, we need to install the `langchain-aws` library.

### Amazon Neptune with Cypher

See a [usage example](/oss/python/integrations/graphs/amazon_neptune_open_cypher).

### Amazon Neptune with SPARQL

### Bedrock token usage

### SageMaker Tracking

> [Amazon SageMaker](https://aws.amazon.com/sagemaker/) is a fully managed service that is used to quickly
> and easily build, train and deploy machine learning (ML) models.

> [Amazon SageMaker Experiments](https://docs.aws.amazon.com/sagemaker/latest/dg/experiments.html) is a capability
> of `Amazon SageMaker` that lets you organize, track,
> compare and evaluate ML experiments and model versions.

We need to install several python libraries.

See a [usage example](/oss/python/integrations/callbacks/sagemaker_tracking).

### Amazon Comprehend Moderation Chain

> [Amazon Comprehend](https://aws.amazon.com/comprehend/) is a natural-language processing (NLP) service that
> uses machine learning to uncover valuable insights and connections in text.

We need to install the `boto3` and `nltk` libraries.

See a [usage example](https://python.langchain.com/v0.1/docs/guides/productionization/safety/amazon_comprehend_chain/).

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/providers/aws.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
### Bedrock Converse

AWS Bedrock maintains a [Converse API](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_Converse.html)
that provides a unified conversational interface for Bedrock models. This API does not
yet support custom models. You can see a list of all
[models that are supported here](https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference.html).

<Info>
  **We recommend the Converse API for users who do not need to use custom models. It can be accessed using [ChatBedrockConverse](https://python.langchain.com/api_reference/aws/chat_models/langchain_aws.chat_models.bedrock_converse.ChatBedrockConverse.html).**
</Info>

See a [usage example](/oss/python/integrations/chat/bedrock).
```

Example 2 (unknown):
```unknown
## LLMs

### Bedrock

See a [usage example](/oss/python/integrations/llms/bedrock).
```

Example 3 (unknown):
```unknown
### Amazon API Gateway

> [Amazon API Gateway](https://aws.amazon.com/api-gateway/) is a fully managed service that makes it easy for
> developers to create, publish, maintain, monitor, and secure APIs at any scale. APIs act as the "front door"
> for applications to access data, business logic, or functionality from your backend services. Using
> `API Gateway`, you can create RESTful APIs and WebSocket APIs that enable real-time two-way communication
> applications. `API Gateway` supports containerized and serverless workloads, as well as web applications.
>
> `API Gateway` handles all the tasks involved in accepting and processing up to hundreds of thousands of
> concurrent API calls, including traffic management, CORS support, authorization and access control,
> throttling, monitoring, and API version management. `API Gateway` has no minimum fees or startup costs.
> You pay for the API calls you receive and the amount of data transferred out and, with the `API Gateway`
> tiered pricing model, you can reduce your cost as your API usage scales.

See a [usage example](/oss/python/integrations/llms/amazon_api_gateway).
```

Example 4 (unknown):
```unknown
### SageMaker Endpoint

> [Amazon SageMaker](https://aws.amazon.com/sagemaker/) is a system that can build, train, and deploy
> machine learning (ML) models with fully managed infrastructure, tools, and workflows.

We use `SageMaker` to host our model and expose it as the `SageMaker Endpoint`.

See a [usage example](/oss/python/integrations/llms/sagemaker).
```

---

## AzureChatOpenAI

**URL:** llms-txt#azurechatopenai

**Contents:**
- Overview
  - Integration details
  - Model features
- Setup
  - Credentials

Source: https://docs.langchain.com/oss/javascript/integrations/chat/azure

Azure OpenAI is a Microsoft Azure service that provides powerful language models from OpenAI.

This will help you getting started with AzureChatOpenAI [chat models](/oss/javascript/langchain/models). For detailed documentation of all AzureChatOpenAI features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_openai.AzureChatOpenAI.html).

### Integration details

| Class                                                                                         | Package                                                                | Local | Serializable | [PY support](https://python.langchain.com/docs/integrations/chat/azure_chat_openai) |                                             Downloads                                             |                                             Version                                            |
| :-------------------------------------------------------------------------------------------- | :--------------------------------------------------------------------- | :---: | :----------: | :---------------------------------------------------------------------------------: | :-----------------------------------------------------------------------------------------------: | :--------------------------------------------------------------------------------------------: |
| [AzureChatOpenAI](https://api.js.langchain.com/classes/langchain_openai.AzureChatOpenAI.html) | [`@langchain/openai`](https://www.npmjs.com/package/@langchain/openai) |   ❌   |       ✅      |                                          ✅                                          | ![NPM - Downloads](https://img.shields.io/npm/dm/@langchain/openai?style=flat-square\&label=%20&) | ![NPM - Version](https://img.shields.io/npm/v/@langchain/openai?style=flat-square\&label=%20&) |

See the links in the table headers below for guides on how to use specific features.

| [Tool calling](/oss/javascript/langchain/tools) | [Structured output](/oss/javascript/langchain/structured-output) | JSON mode | [Image input](/oss/javascript/langchain/messages#multimodal) | Audio input | Video input | [Token-level streaming](/oss/javascript/langchain/streaming/) | [Token usage](/oss/javascript/langchain/models#token-usage) | [Logprobs](/oss/javascript/langchain/models#log-probabilities) |
| :---------------------------------------------: | :--------------------------------------------------------------: | :-------: | :----------------------------------------------------------: | :---------: | :---------: | :-----------------------------------------------------------: | :---------------------------------------------------------: | :------------------------------------------------------------: |
|                        ✅                        |                                 ✅                                |     ✅     |                               ✅                              |      ❌      |      ❌      |                               ✅                               |                              ✅                              |                                ✅                               |

[Azure OpenAI](https://azure.microsoft.com/products/ai-services/openai-service/) is a cloud service to help you quickly develop generative AI experiences with a diverse set of prebuilt and curated models from OpenAI, Meta and beyond.

LangChain.js supports integration with [Azure OpenAI](https://azure.microsoft.com/products/ai-services/openai-service/) using the new Azure integration in the [OpenAI SDK](https://github.com/openai/openai-node).

You can learn more about Azure OpenAI and its difference with the OpenAI API on [this page](https://learn.microsoft.com/azure/ai-services/openai/overview).

If you don't have an Azure account, you can [create a free account](https://azure.microsoft.com/free/) to get started.

You'll also need to have an Azure OpenAI instance deployed. You can deploy a version on Azure Portal following [this guide](https://learn.microsoft.com/azure/ai-services/openai/how-to/create-resource?pivots=web-portal).

Once you have your instance running, make sure you have the name of your instance and key. You can find the key in the Azure Portal, under the "Keys and Endpoint" section of your instance. Then, if using Node.js, you can set your credentials as environment variables:

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```bash  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:
```

---

## AzureOpenAIEmbeddings

**URL:** llms-txt#azureopenaiembeddings

**Contents:**
- Overview
  - Integration details
- Setup
  - Credentials

Source: https://docs.langchain.com/oss/javascript/integrations/text_embedding/azure_openai

[Azure OpenAI](https://azure.microsoft.com/products/ai-services/openai-service/) is a cloud service to help you quickly develop generative AI experiences with a diverse set of prebuilt and curated models from OpenAI, Meta and beyond.

LangChain.js supports integration with [Azure OpenAI](https://azure.microsoft.com/products/ai-services/openai-service/) using the new Azure integration in the [OpenAI SDK](https://github.com/openai/openai-node).

You can learn more about Azure OpenAI and its difference with the OpenAI API on [this page](https://learn.microsoft.com/azure/ai-services/openai/overview). If you don't have an Azure account, you can [create a free account](https://azure.microsoft.com/free/) to get started.

This will help you get started with AzureOpenAIEmbeddings [embedding models](/oss/javascript/integrations/text_embedding) using LangChain. For detailed documentation on `AzureOpenAIEmbeddings` features and configuration options, please refer to the [API reference](https://api.js.langchain.com/classes/langchain_openai.AzureOpenAIEmbeddings.html).

<Info>
  **Previously, LangChain.js supported integration with Azure OpenAI using the dedicated [Azure OpenAI SDK](https://github.com/Azure/azure-sdk-for-js/tree/main/sdk/openai/openai). This SDK is now deprecated in favor of the new Azure integration in the OpenAI SDK, which allows to access the latest OpenAI models and features the same day they are released, and allows seamless transition between the OpenAI API and Azure OpenAI.**

If you are using Azure OpenAI with the deprecated SDK, see the [migration guide](#migration-from-azure-openai-sdk) to update to the new API.
</Info>

### Integration details

| Class                                                                                                     | Package                                                                         | Local | [Py support](https://python.langchain.com/docs/integrations/text_embedding/azure_openai/) |                                             Downloads                                             |                                             Version                                            |
| :-------------------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------ | :---: | :---------------------------------------------------------------------------------------: | :-----------------------------------------------------------------------------------------------: | :--------------------------------------------------------------------------------------------: |
| [AzureOpenAIEmbeddings](https://api.js.langchain.com/classes/langchain_openai.AzureOpenAIEmbeddings.html) | [@langchain/openai](https://api.js.langchain.com/modules/langchain_openai.html) |   ❌   |                                             ✅                                             | ![NPM - Downloads](https://img.shields.io/npm/dm/@langchain/openai?style=flat-square\&label=%20&) | ![NPM - Version](https://img.shields.io/npm/v/@langchain/openai?style=flat-square\&label=%20&) |

To access Azure OpenAI embedding models you'll need to create an Azure account, get an API key, and install the `@langchain/openai` integration package.

You'll need to have an Azure OpenAI instance deployed. You can deploy a version on Azure Portal following [this guide](https://learn.microsoft.com/azure/ai-services/openai/how-to/create-resource?pivots=web-portal).

Once you have your instance running, make sure you have the name of your instance and key. You can find the key in the Azure Portal, under the "Keys and Endpoint" section of your instance.

If you're using Node.js, you can define the following environment variables to use the service:

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```bash  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:
```

---

## A comparative experiment allows you to provide a preferential ranking on the outputs of two or more experiments

**URL:** llms-txt#a-comparative-experiment-allows-you-to-provide-a-preferential-ranking-on-the-outputs-of-two-or-more-experiments

---

## Backends

**URL:** llms-txt#backends

**Contents:**
- Quickstart
- Built-in backends
  - StateBackend (ephemeral)
  - FilesystemBackend (local disk)
  - StoreBackend (LangGraph Store)
  - CompositeBackend (router)
- Specify a backend
- Route to different backends
- Use a virtual filesystem
- Add policy hooks

Source: https://docs.langchain.com/oss/javascript/deepagents/backends

Choose and configure filesystem backends for deep agents. You can specify routes to different backends, implement virtual filesystems, and enforce policies.

Deep agents expose a filesystem surface to the agent via tools like `ls`, `read_file`, `write_file`, `edit_file`, `glob`, and `grep`. These tools operate through a pluggable backend.

This page explains how to [choose a backend](#specify-a-backend), [route different paths to different backends](#route-to-different-backends), [implement your own virtual filesystem](#use-a-virtual-filesystem) (e.g., S3 or Postgres), [add policy hooks](#add-policy-hooks), and [comply with the backend protocol](#protocol-reference).

Here are a few pre-built filesystem backends that you can quickly use with your deep agent:

| Built-in backend                                                 | Description                                                                                                                                                                                                                                                                                   |
| ---------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [Default](#statebackend-ephemeral)                               | `agent = create_deep_agent()` <br /> Ephemeral in state. The default filesystem backend for an agent is stored in `langgraph` state. Note that this filesystem only persists *for a single thread*.                                                                                           |
| [Local filesystem persistence](#filesystembackend-local-disk)    | `agent = create_deep_agent(backend=FilesystemBackend(root_dir="/Users/nh/Desktop/"))` <br />This gives the deep agent access to your local machine's filesystem. You can specify the root directory that the agent has access to. Note that any provided `root_dir` must be an absolute path. |
| [Durable store (LangGraph store)](#storebackend-langgraph-store) | `agent = create_deep_agent(backend=lambda rt: StoreBackend(rt))` <br />This gives the agent access to long-term storage that is *persisted across threads*. This is great for storing longer term memories or instructions that are applicable to the agent over multiple executions.         |
| [Composite](#compositebackend-router)                            | Ephemeral by default, `/memories/` persisted. The Composite backend is maximally flexible. You can specify different routes in the filesystem to point towards different backends. See Composite routing below for a ready-to-paste example.                                                  |

### StateBackend (ephemeral)

* Stores files in LangGraph agent state for the current thread.
* Persists across multiple agent turns on the same thread via checkpoints.

* A scratch pad for the agent to write intermediate results.
* Automatic eviction of large tool outputs which the agent can then read back in piece by piece.

### FilesystemBackend (local disk)

* Reads/writes real files under a configurable `root_dir`.
* You can optionally set `virtual_mode=True` to sandbox and normalize paths under `root_dir`.
* Uses secure path resolution, prevents unsafe symlink traversal when possible, can use ripgrep for fast `grep`.

* Local projects on your machine
* CI sandboxes
* Mounted persistent volumes

### StoreBackend (LangGraph Store)

* Stores files in a LangGraph `BaseStore` provided by the runtime, enabling cross‑thread durable storage.

* When you already run with a configured LangGraph store (for example, Redis, Postgres, or cloud implementations behind `BaseStore`).
* When you're deploying your agent through LangSmith Deployments (a store is automatically provisioned for your agent).

### CompositeBackend (router)

* Routes file operations to different backends based on path prefix.
* Preserves the original path prefixes in listings and search results.

* When you want to give your agent both ephemeral and cross-thread storage, a CompositeBackend allows you provide both a StateBackend and StoreBackend
* When you have multiple sources of information that you want to provide to your agent as part of a single filesystem.
  * e.g. You have long-term memories stored under /memories/ in one Store and you also have a custom backend that has documentation accessible at /docs/.

* Pass a backend to `create_deep_agent(backend=...)`. The filesystem middleware uses it for all tooling.
* You can pass either:
  * An instance implementing `BackendProtocol` (for example, `FilesystemBackend(root_dir=".")`), or
  * A factory `BackendFactory = Callable[[ToolRuntime], BackendProtocol]` (for backends that need runtime like `StateBackend` or `StoreBackend`).
* If omitted, the default is `lambda rt: StateBackend(rt)`.

## Route to different backends

Route parts of the namespace to different backends. Commonly used to persist `/memories/*` and keep everything else ephemeral.

* `/workspace/plan.md` → StateBackend (ephemeral)
* `/memories/agent.md` → FilesystemBackend under `/deepagents/myagent`
* `ls`, `glob`, `grep` aggregate results and show original path prefixes.

* Longer prefixes win (for example, route `"/memories/projects/"` can override `"/memories/"`).
* For StoreBackend routing, ensure the agent runtime provides a store (`runtime.store`).

## Use a virtual filesystem

Build a custom backend to project a remote or database filesystem (e.g., S3 or Postgres) into the tools namespace.

* Paths are absolute (`/x/y.txt`). Decide how to map them to your storage keys/rows.
* Implement `ls_info` and `glob_info` efficiently (server-side listing where available, otherwise local filter).
* Return user-readable error strings for missing files or invalid regex patterns.
* For external persistence, set `files_update=None` in results; only in-state backends should return a `files_update` dict.

Postgres-style outline:

* Table `files(path text primary key, content text, created_at timestamptz, modified_at timestamptz)`
* Map tool operations onto SQL:
  * `ls_info` uses `WHERE path LIKE $1 || '%'`
  * `glob_info` filter in SQL or fetch then apply glob in Python
  * `grep_raw` can fetch candidate rows by extension or last modified time, then scan lines

Enforce enterprise rules by subclassing or wrapping a backend.

Block writes/edits under selected prefixes (subclass):

Generic wrapper (works with any backend):

## Protocol reference

Backends must implement the `BackendProtocol`.

* `ls_info(path: str) -> list[FileInfo]`
  * Return entries with at least `path`. Include `is_dir`, `size`, `modified_at` when available. Sort by `path` for deterministic output.
* `read(file_path: str, offset: int = 0, limit: int = 2000) -> str`
  * Return numbered content. On missing file, return `"Error: File '/x' not found"`.
* `grep_raw(pattern: str, path: Optional[str] = None, glob: Optional[str] = None) -> list[GrepMatch] | str`
  * Return structured matches. For an invalid regex, return a string like `"Invalid regex pattern: ..."` (do not raise).
* `glob_info(pattern: str, path: str = "/") -> list[FileInfo]`
  * Return matched files as `FileInfo` entries (empty list if none).
* `write(file_path: str, content: str) -> WriteResult`
  * Create-only. On conflict, return `WriteResult(error=...)`. On success, set `path` and for state backends set `files_update={...}`; external backends should use `files_update=None`.
* `edit(file_path: str, old_string: str, new_string: str, replace_all: bool = False) -> EditResult`
  * Enforce uniqueness of `old_string` unless `replace_all=True`. If not found, return error. Include `occurrences` on success.

* `WriteResult(error, path, files_update)`
* `EditResult(error, path, files_update, occurrences)`
* `FileInfo` with fields: `path` (required), optionally `is_dir`, `size`, `modified_at`.
* `GrepMatch` with fields: `path`, `line`, `text`.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/deepagents/backends.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
**How it works:**

* Stores files in LangGraph agent state for the current thread.
* Persists across multiple agent turns on the same thread via checkpoints.

**Best for:**

* A scratch pad for the agent to write intermediate results.
* Automatic eviction of large tool outputs which the agent can then read back in piece by piece.

### FilesystemBackend (local disk)
```

Example 2 (unknown):
```unknown
**How it works:**

* Reads/writes real files under a configurable `root_dir`.
* You can optionally set `virtual_mode=True` to sandbox and normalize paths under `root_dir`.
* Uses secure path resolution, prevents unsafe symlink traversal when possible, can use ripgrep for fast `grep`.

**Best for:**

* Local projects on your machine
* CI sandboxes
* Mounted persistent volumes

### StoreBackend (LangGraph Store)
```

Example 3 (unknown):
```unknown
**How it works:**

* Stores files in a LangGraph `BaseStore` provided by the runtime, enabling cross‑thread durable storage.

**Best for:**

* When you already run with a configured LangGraph store (for example, Redis, Postgres, or cloud implementations behind `BaseStore`).
* When you're deploying your agent through LangSmith Deployments (a store is automatically provisioned for your agent).

### CompositeBackend (router)
```

Example 4 (unknown):
```unknown
**How it works:**

* Routes file operations to different backends based on path prefix.
* Preserves the original path prefixes in listings and search results.

**Best for:**

* When you want to give your agent both ephemeral and cross-thread storage, a CompositeBackend allows you provide both a StateBackend and StoreBackend
* When you have multiple sources of information that you want to provide to your agent as part of a single filesystem.
  * e.g. You have long-term memories stored under /memories/ in one Store and you also have a custom backend that has documentation accessible at /docs/.

## Specify a backend

* Pass a backend to `create_deep_agent(backend=...)`. The filesystem middleware uses it for all tooling.
* You can pass either:
  * An instance implementing `BackendProtocol` (for example, `FilesystemBackend(root_dir=".")`), or
  * A factory `BackendFactory = Callable[[ToolRuntime], BackendProtocol]` (for backends that need runtime like `StateBackend` or `StoreBackend`).
* If omitted, the default is `lambda rt: StateBackend(rt)`.

## Route to different backends

Route parts of the namespace to different backends. Commonly used to persist `/memories/*` and keep everything else ephemeral.
```

---

## baseline_results.to_pandas()

**URL:** llms-txt#baseline_results.to_pandas()

**Contents:**
  - Define and evaluate new system

python  theme={null}
candidate_results = await client.aevaluate(
    agent.with_config(model="gpt-4o"),
    data=dataset_name,
    evaluators=[lt_280_chars, gte_3_emojis, is_grounded],
    experiment_prefix="candidate-gpt-4o",
)

**Examples:**

Example 1 (unknown):
```unknown
### Define and evaluate new system

Now, let's define and evaluate our new system. In this example our new system will be the same as the old system, but will use GPT-4o instead of GPT-3.5. Since we've made our model configurable we can just update the default config passed to our agent:
```

---

## Basic authentication with email and password

**URL:** llms-txt#basic-authentication-with-email-and-password

**Contents:**
- Requirements and features
  - Migrating from None auth
  - Configuration

Source: https://docs.langchain.com/langsmith/self-host-basic-auth

LangSmith supports login via username/password with a few limitations:

* You cannot change an existing installation from basic auth mode to OAuth with PKCE (deprecated) or vice versa - installations must be either one or the other. **A basic auth installation requires a completely fresh installation including a separate PostgreSQL database/schema, unless migrating from an existing `None` type installation (see below).**
* Users must be given their initial auto-generated password once they are invited. This password may be changed later by any Organization Admin.
* You cannot use both basic auth and OAuth with client secret at the same time.

## Requirements and features

* There is a single `Default` organization that is provisioned during initial installation, and creating additional organizations is not supported
* Your initial password (configured below) must be least 12 characters long and have at least one lowercase, uppercase, and symbol
* There are no strict requirements for the secret used for signing JWTs, but we recommend securely generating a string of at least 32 characters. For example: `openssl rand -base64 32`

### Migrating from None auth

**Only supported in versions 0.7 and above.**

Migrating an installation from [None](/langsmith/authentication-methods#none) auth mode replaces the single "default" user with a user with the configured credentials and keeps all existing resources. The single pre-existing workspace ID post-migration remains `00000000-0000-0000-0000-000000000000`, but everything else about the migrated installation is standard for a basic auth installation.

To migrate, simply update your configuration as shown below and run `helm upgrade` (or `docker-compose up`) as usual.

<Note>
  Changing the JWT secret will log out your users
</Note>

Additionally, in docker-compose you will need to run the bootstrap command to create the initial organization and user:

Once configured, you will see a login screen like the one below. You should be able to login with the `initialOrgAdminEmail` and `initialOrgAdminPassword` values, and your user will be auto-provisioned with role `Organization Admin`. See the [admin guide](/langsmith/administration-overview#organization-roles) for more details on organization roles.

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui-basic-auth.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=2f88a4bffacc308d7fbd22cba1f1c9f1" alt="LangSmith UI with basic auth" data-og-width="1424" width="1424" data-og-height="1156" height="1156" data-path="langsmith/images/langsmith-ui-basic-auth.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui-basic-auth.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=1d0297842dd132b61fa295b52e93859e 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui-basic-auth.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=975b75e1f4262b5278e2d56896bec326 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui-basic-auth.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=54a1827e8611f72359a121dd209b5860 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui-basic-auth.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=42618816bdfbd171e32d064fd00652da 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui-basic-auth.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=a73742556b59333c002f9d0f5d314e6e 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui-basic-auth.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=93b93edf04b03f1ab652df38270364c2 2500w" />

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/self-host-basic-auth.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

Additionally, in docker-compose you will need to run the bootstrap command to create the initial organization and user:
```

---

## BedrockChat

**URL:** llms-txt#bedrockchat

**Contents:**
- Overview
  - Integration details
  - Model features
- Setup
  - Credentials

Source: https://docs.langchain.com/oss/javascript/integrations/chat/bedrock

[Amazon Bedrock](https://aws.amazon.com/bedrock/) is a fully managed service that offers a choice of high-performing foundation models (FMs) from leading AI companies like AI21 Labs, Anthropic, Cohere, Meta, Stability AI, and Amazon via a single API, along with a broad set of capabilities you need to build generative AI applications with security, privacy, and responsible AI.

This will help you getting started with Amazon Bedrock [chat models](/oss/javascript/langchain/models). For detailed documentation of all `BedrockChat` features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_community_chat_models_bedrock.BedrockChat.html).

<Tip>
  The newer [`ChatBedrockConverse` chat model is now available via the dedicated `@langchain/aws`](/oss/javascript/integrations/chat/bedrock_converse) integration package. Use [tool calling](/oss/javascript/langchain/tools) with more models with this package.
</Tip>

### Integration details

| Class                                                                                                          | Package                                                          | Local | Serializable | [PY support](https://python.langchain.com/docs/integrations/chat/bedrock/) |                                               Downloads                                              |                                              Version                                              |
| :------------------------------------------------------------------------------------------------------------- | :--------------------------------------------------------------- | :---: | :----------: | :------------------------------------------------------------------------: | :--------------------------------------------------------------------------------------------------: | :-----------------------------------------------------------------------------------------------: |
| [`BedrockChat`](https://api.js.langchain.com/classes/langchain_community_chat_models_bedrock.BedrockChat.html) | [`@langchain/community`](https://npmjs.com/@langchain/community) |   ❌   |       ✅      |                                      ✅                                     | ![NPM - Downloads](https://img.shields.io/npm/dm/@langchain/community?style=flat-square\&label=%20&) | ![NPM - Version](https://img.shields.io/npm/v/@langchain/community?style=flat-square\&label=%20&) |

See the links in the table headers below for guides on how to use specific features.

| [Tool calling](/oss/javascript/langchain/tools) | [Structured output](/oss/javascript/langchain/structured-output) | JSON mode | [Image input](/oss/javascript/langchain/messages#multimodal) | Audio input | Video input | [Token-level streaming](/oss/javascript/langchain/streaming/) | [Token usage](/oss/javascript/langchain/models#token-usage) | [Logprobs](/oss/javascript/langchain/models#log-probabilities) |
| :---------------------------------------------: | :--------------------------------------------------------------: | :-------: | :----------------------------------------------------------: | :---------: | :---------: | :-----------------------------------------------------------: | :---------------------------------------------------------: | :------------------------------------------------------------: |
|                        ✅                        |                                 ✅                                |     ❌     |                               ✅                              |      ❌      |      ❌      |                               ✅                               |                              ✅                              |                                ❌                               |

To access Bedrock models you'll need to create an AWS account, set up the Bedrock API service, get an access key ID and secret key, and install the `@langchain/community` integration package.

Head to the [AWS docs](https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html) to sign up for AWS and setup your credentials. You'll also need to turn on model access for your account, which you can do by [following these instructions](https://docs.aws.amazon.com/bedrock/latest/userguide/model-access.html).

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```bash  theme={null}

---

## BedrockEmbeddings

**URL:** llms-txt#bedrockembeddings

**Contents:**
- Overview
  - Integration details
- Setup
  - Credentials

Source: https://docs.langchain.com/oss/javascript/integrations/text_embedding/bedrock

[Amazon Bedrock](https://aws.amazon.com/bedrock/) is a fully managed service that offers a choice of high-performing foundation models (FMs) from leading AI companies like AI21 Labs, Anthropic, Cohere, Meta, Stability AI, and Amazon via a single API, along with a broad set of capabilities you need to build generative AI applications with security, privacy, and responsible AI.

This will help you get started with Amazon Bedrock [embedding models](/oss/javascript/integrations/text_embedding) using LangChain. For detailed documentation on `Bedrock` features and configuration options, please refer to the [API reference](https://api.js.langchain.com/classes/langchain_aws.BedrockEmbeddings.html).

### Integration details

| Class                                                                                | Package                                                                   | Local | [Py support](https://python.langchain.com/docs/integrations/text_embedding/bedrock/) |                                            Downloads                                           |                                           Version                                           |
| :----------------------------------------------------------------------------------- | :------------------------------------------------------------------------ | :---: | :----------------------------------------------------------------------------------: | :--------------------------------------------------------------------------------------------: | :-----------------------------------------------------------------------------------------: |
| [Bedrock](https://api.js.langchain.com/classes/langchain_aws.BedrockEmbeddings.html) | [@langchain/aws](https://api.js.langchain.com/modules/langchain_aws.html) |   ❌   |                                           ✅                                          | ![NPM - Downloads](https://img.shields.io/npm/dm/@langchain/aws?style=flat-square\&label=%20&) | ![NPM - Version](https://img.shields.io/npm/v/@langchain/aws?style=flat-square\&label=%20&) |

To access Bedrock embedding models you'll need to create an AWS account, get an API key, and install the `@langchain/aws` integration package.

Head to the [AWS docs](https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html) to sign up for AWS and setup your credentials. You'll also need to turn on model access for your account, which you can do by [following these instructions](https://docs.aws.amazon.com/bedrock/latest/userguide/model-access.html).

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```bash  theme={null}

---

## Before model hook

**URL:** llms-txt#before-model-hook

@before_model
def log_before_model(state: AgentState, runtime: Runtime[Context]) -> dict | None:  # [!code highlight]
    print(f"Processing request for user: {runtime.context.user_name}")  # [!code highlight]
    return None

---

## Beta LangSmith Collector-Proxy

**URL:** llms-txt#beta-langsmith-collector-proxy

**Contents:**
- When to Use the Collector-Proxy
- Key Features
- Configuration
  - Project Configuration
  - Authentication
- Deployment (Docker)
- Usage
- Health & Scaling
- Horizontal Scaling
- Fork & Extend

Source: https://docs.langchain.com/langsmith/collector-proxy

<Note>
  This is a beta feature. The API may change in future releases.
</Note>

The LangSmith Collector-Proxy is a lightweight, high-performance proxy server that sits between your application and the LangSmith backend. It batches and compresses trace data before sending it to LangSmith, reducing network overhead and improving performance.

## When to Use the Collector-Proxy

The Collector-Proxy is particularly valuable when:

* You're running multiple instances of your application in parallel and need to efficiently aggregate traces
* You want more efficient tracing than direct OTEL API calls to LangSmith (the collector optimizes batching and compression)
* You're using a language that doesn't have a native LangSmith SDK

* **Efficient Data Transfer** Batches multiple spans into fewer, larger uploads.
* **Compression** Uses zstd to minimize payload size.
* **OTLP Support** Accepts OTLP JSON and Protobuf over HTTP POST.
* **Semantic Translation** Maps GenAI/OpenInference conventions to the LangSmith Run model.
* **Flexible Batching** Flush by span count or time interval.

Configure via environment variables:

| Variable             | Description                       | Default                           |
| -------------------- | --------------------------------- | --------------------------------- |
| `HTTP_PORT`          | Port to run the proxy server      | `4318`                            |
| `LANGSMITH_ENDPOINT` | LangSmith backend URL             | `https://api.smith.langchain.com` |
| `LANGSMITH_API_KEY`  | API key for LangSmith             | **Required** (env var or header)  |
| `LANGSMITH_PROJECT`  | Default tracing project           | Default project if not specified  |
| `BATCH_SIZE`         | Spans per upload batch            | `100`                             |
| `FLUSH_INTERVAL_MS`  | Flush interval in milliseconds    | `1000`                            |
| `MAX_BUFFER_BYTES`   | Max uncompressed buffer size      | `10485760` (10 MB)                |
| `MAX_BODY_BYTES`     | Max incoming request body size    | `209715200` (200 MB)              |
| `MAX_RETRIES`        | Retry attempts for failed uploads | `3`                               |
| `RETRY_BACKOFF_MS`   | Initial backoff in milliseconds   | `100`                             |

### Project Configuration

The Collector-Proxy supports LangSmith project configuration with the following priority:

1. If a project is specified in the request headers (`Langsmith-Project`), that project will be used
2. If no project is specified in headers, it will use the project set in the `LANGSMITH_PROJECT` environment variable
3. If neither is set, it will trace to the `default` project.

The API key can be provided either:

* As an environment variable (`LANGSMITH_API_KEY`)
* In the request headers (`X-API-Key`)

## Deployment (Docker)

You can deploy the Collector-Proxy with Docker:

1. **Build the image**

2. **Run the container**

Point any OTLP-compatible client or the OpenTelemetry Collector exporter at:

* **Liveness**: `GET /live` → 200
* **Readiness**: `GET /ready` → 200

## Horizontal Scaling

To ensure full traces are batched correctly, route spans with the same trace ID to the same instance (e.g., via consistent hashing).

Fork the [Collector-Proxy repo on GitHub](https://github.com/langchain-ai/langsmith-collector-proxy) and implement your own converter:

* Create a custom `GenAiConverter` or modify the existing one in `internal/translator/otel_converter.go`
* Register the custom converter in `internal/translator/translator.go`

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/collector-proxy.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
2. **Run the container**
```

Example 2 (unknown):
```unknown
## Usage

Point any OTLP-compatible client or the OpenTelemetry Collector exporter at:
```

Example 3 (unknown):
```unknown
Send a test trace:
```

---

## Bob creates his own thread

**URL:** llms-txt#bob-creates-his-own-thread

bob_thread = await bob.threads.create()
await bob.runs.create(
    thread_id=bob_thread["thread_id"],
    assistant_id="agent",
    input={"messages": [{"role": "user", "content": "Hi, this is Bob's private chat"}]}
)
print(f"✅ Bob created his own thread: {bob_thread['thread_id']}")

---

## Bob tries to access Alice's thread

**URL:** llms-txt#bob-tries-to-access-alice's-thread

try:
    await bob.threads.get(alice_thread["thread_id"])
    print("❌ Bob shouldn't see Alice's thread!")
except Exception as e:
    print("✅ Bob correctly denied access:", e)

---

## Building our graph

**URL:** llms-txt#building-our-graph

graph_builder = StateGraph(State)

graph_builder.add_node(gather_info)
graph_builder.add_node(refund)
graph_builder.add_node(lookup)

graph_builder.set_entry_point("gather_info")
graph_builder.add_edge("lookup", END)
graph_builder.add_edge("refund", END)

refund_graph = graph_builder.compile()

**Examples:**

Example 1 (unknown):
```unknown
We can visualize our refund graph:
```

---

## Build a custom RAG agent

**URL:** llms-txt#build-a-custom-rag-agent

**Contents:**
- Overview
  - Concepts
- Setup
- 1. Preprocess documents
- 2. Create a retriever tool
- 3. Generate query
- 4. Grade documents
- 5. Rewrite question
- 6. Generate an answer
- 7. Assemble the graph

Source: https://docs.langchain.com/oss/javascript/langgraph/agentic-rag

In this tutorial we will build a [retrieval](/oss/javascript/langchain/retrieval) agent using LangGraph.

LangChain offers built-in [agent](/oss/javascript/langchain/agents) implementations, implemented using [LangGraph](/oss/javascript/langgraph/overview) primitives. If deeper customization is required, agents can be implemented directly in LangGraph. This guide demonstrates an example implementation of a retrieval agent. [Retrieval](/oss/javascript/langchain/retrieval) agents are useful when you want an LLM to make a decision about whether to retrieve context from a vectorstore or respond to the user directly.

By the end of the tutorial we will have done the following:

1. Fetch and preprocess documents that will be used for retrieval.
2. Index those documents for semantic search and create a retriever tool for the agent.
3. Build an agentic RAG system that can decide when to use the retriever tool.

<img src="https://mintcdn.com/langchain-5e9cc07a/I6RpA28iE233vhYX/images/langgraph-hybrid-rag-tutorial.png?fit=max&auto=format&n=I6RpA28iE233vhYX&q=85&s=855348219691485642b22a1419939ea7" alt="Hybrid RAG" data-og-width="1615" width="1615" data-og-height="589" height="589" data-path="images/langgraph-hybrid-rag-tutorial.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/I6RpA28iE233vhYX/images/langgraph-hybrid-rag-tutorial.png?w=280&fit=max&auto=format&n=I6RpA28iE233vhYX&q=85&s=09097cb9a1dc57b16d33f084641ea93f 280w, https://mintcdn.com/langchain-5e9cc07a/I6RpA28iE233vhYX/images/langgraph-hybrid-rag-tutorial.png?w=560&fit=max&auto=format&n=I6RpA28iE233vhYX&q=85&s=d0bf85cfa36ac7e1a905593a4688f2d2 560w, https://mintcdn.com/langchain-5e9cc07a/I6RpA28iE233vhYX/images/langgraph-hybrid-rag-tutorial.png?w=840&fit=max&auto=format&n=I6RpA28iE233vhYX&q=85&s=b7626e6ae3cb94fb90a61e6fad69c8ba 840w, https://mintcdn.com/langchain-5e9cc07a/I6RpA28iE233vhYX/images/langgraph-hybrid-rag-tutorial.png?w=1100&fit=max&auto=format&n=I6RpA28iE233vhYX&q=85&s=2425baddda7209901bdde4425c23292c 1100w, https://mintcdn.com/langchain-5e9cc07a/I6RpA28iE233vhYX/images/langgraph-hybrid-rag-tutorial.png?w=1650&fit=max&auto=format&n=I6RpA28iE233vhYX&q=85&s=4e5f030034237589f651b704d0377a76 1650w, https://mintcdn.com/langchain-5e9cc07a/I6RpA28iE233vhYX/images/langgraph-hybrid-rag-tutorial.png?w=2500&fit=max&auto=format&n=I6RpA28iE233vhYX&q=85&s=3ec3c7c91fd2be4d749b1c267027ac1e 2500w" />

We will cover the following concepts:

* [Retrieval](/oss/javascript/langchain/retrieval) using [document loaders](/oss/javascript/integrations/document_loaders), [text splitters](/oss/javascript/integrations/splitters), [embeddings](/oss/javascript/integrations/text_embedding), and [vector stores](/oss/javascript/integrations/vectorstores)
* The LangGraph [Graph API](/oss/javascript/langgraph/graph-api), including state, nodes, edges, and conditional edges.

Let's download the required packages and set our API keys:

<Tip>
  Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. [LangSmith](https://docs.smith.langchain.com) lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph.
</Tip>

## 1. Preprocess documents

1. Fetch documents to use in our RAG system. We will use three of the most recent pages from [Lilian Weng's excellent blog](https://lilianweng.github.io/). We'll start by fetching the content of the pages using `CheerioWebBaseLoader`:

2. Split the fetched documents into smaller chunks for indexing into our vectorstore:

## 2. Create a retriever tool

Now that we have our split documents, we can index them into a vector store that we'll use for semantic search.

1. Use an in-memory vector store and OpenAI embeddings:

2. Create a retriever tool using LangChain's prebuilt `createRetrieverTool`:

Now we will start building components ([nodes](/oss/javascript/langgraph/graph-api#nodes) and [edges](/oss/javascript/langgraph/graph-api#edges)) for our agentic RAG graph.

1. Build a `generateQueryOrRespond` node. It will call an LLM to generate a response based on the current graph state (list of messages). Given the input messages, it will decide to retrieve using the retriever tool, or respond directly to the user. Note that we're giving the chat model access to the `tools` we created earlier via `.bindTools`:

2. Try it on a random input:

3. Ask a question that requires semantic search:

## 4. Grade documents

1. Add a node — `gradeDocuments` — to determine whether the retrieved documents are relevant to the question. We will use a model with structured output using Zod for document grading. We'll also add a [conditional edge](/oss/javascript/langgraph/graph-api#conditional-edges) — `checkRelevance` — that checks the grading result and returns the name of the node to go to (`generate` or `rewrite`):

2. Run this with irrelevant documents in the tool response:

3. Confirm that the relevant documents are classified as such:

## 5. Rewrite question

1. Build the `rewrite` node. The retriever tool can return potentially irrelevant documents, which indicates a need to improve the original user question. To do so, we will call the `rewrite` node:

## 6. Generate an answer

1. Build `generate` node: if we pass the grader checks, we can generate the final answer based on the original question and the retrieved context:

## 7. Assemble the graph

Now we'll assemble all the nodes and edges into a complete graph:

* Start with a `generateQueryOrRespond` and determine if we need to call the retriever tool
* Route to next step using a conditional edge:
  * If `generateQueryOrRespond` returned `tool_calls`, call the retriever tool to retrieve context
  * Otherwise, respond directly to the user
* Grade retrieved document content for relevance to the question (`gradeDocuments`) and route to next step:
  * If not relevant, rewrite the question using `rewrite` and then call `generateQueryOrRespond` again
  * If relevant, proceed to `generate` and generate final response using the @\[`ToolMessage`] with the retrieved document context

## 8. Run the agentic RAG

Now let's test the complete graph by running it with a question:

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/agentic-rag.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>

<Tip>
  Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. [LangSmith](https://docs.smith.langchain.com) lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph.
</Tip>

## 1. Preprocess documents

1. Fetch documents to use in our RAG system. We will use three of the most recent pages from [Lilian Weng's excellent blog](https://lilianweng.github.io/). We'll start by fetching the content of the pages using `CheerioWebBaseLoader`:
```

---

## Build a custom SQL agent

**URL:** llms-txt#build-a-custom-sql-agent

**Contents:**
  - Concepts
- Setup
  - Installation
  - LangSmith
- 1. Select an LLM
- 2. Configure the database
- 3. Add tools for database interactions
- 4. Define application steps

Source: https://docs.langchain.com/oss/python/langgraph/sql-agent

In this tutorial we will build a custom agent that can answer questions about a SQL database using LangGraph.

LangChain offers built-in [agent](/oss/python/langchain/agents) implementations, implemented using [LangGraph](/oss/python/langgraph/overview) primitives. If deeper customization is required, agents can be implemented directly in LangGraph. This guide demonstrates an example implementation of a SQL agent. You can find a tutorial building a SQL agent using higher-level LangChain abstractions [here](/oss/python/langchain/sql-agent).

<Warning>
  Building Q\&A systems of SQL databases requires executing model-generated SQL queries. There are inherent risks in doing this. Make sure that your database connection permissions are always scoped as narrowly as possible for your agent's needs. This will mitigate, though not eliminate, the risks of building a model-driven system.
</Warning>

The [prebuilt agent](/oss/python/langchain/sql-agent) lets us get started quickly, but we relied on the system prompt to constrain its behavior— for example, we instructed the agent to always start with the "list tables" tool, and to always run a query-checker tool before executing the query.

We can enforce a higher degree of control in LangGraph by customizing the agent. Here, we implement a simple ReAct-agent setup, with dedicated nodes for specific tool-calls. We will use the same \[state] as the pre-built agent.

We will cover the following concepts:

* [Tools](/oss/python/langchain/tools) for reading from SQL databases
* The LangGraph [Graph API](/oss/python/langgraph/graph-api), including state, nodes, edges, and conditional edges.
* [Human-in-the-loop](/oss/python/langgraph/interrupts) processes

<CodeGroup>
  
</CodeGroup>

Set up [LangSmith](https://smith.langchain.com) to inspect what is happening inside your chain or agent. Then set the following environment variables:

Select a model that supports [tool-calling](/oss/python/integrations/providers/overview):

<Tabs>
  <Tab title="OpenAI">
    👉 Read the [OpenAI chat model integration docs](/oss/python/integrations/chat/openai/)

</CodeGroup>
  </Tab>

<Tab title="Anthropic">
    👉 Read the [Anthropic chat model integration docs](/oss/python/integrations/chat/anthropic/)

</CodeGroup>
  </Tab>

<Tab title="Azure">
    👉 Read the [Azure chat model integration docs](/oss/python/integrations/chat/azure_chat_openai/)

</CodeGroup>
  </Tab>

<Tab title="Google Gemini">
    👉 Read the [Google GenAI chat model integration docs](/oss/python/integrations/chat/google_generative_ai/)

</CodeGroup>
  </Tab>

<Tab title="AWS Bedrock">
    👉 Read the [AWS Bedrock chat model integration docs](/oss/python/integrations/chat/bedrock/)

</CodeGroup>
  </Tab>
</Tabs>

The output shown in the examples below used OpenAI.

## 2. Configure the database

You will be creating a [SQLite database](https://www.sqlitetutorial.net/sqlite-sample-database/) for this tutorial. SQLite is a lightweight database that is easy to set up and use. We will be loading the `chinook` database, which is a sample database that represents a digital media store.

For convenience, we have hosted the database (`Chinook.db`) on a public GCS bucket.

We will use a handy SQL database wrapper available in the `langchain_community` package to interact with the database. The wrapper provides a simple interface to execute SQL queries and fetch results:

## 3. Add tools for database interactions

Use the `SQLDatabase` wrapper available in the `langchain_community` package to interact with the database. The wrapper provides a simple interface to execute SQL queries and fetch results:

## 4. Define application steps

We construct dedicated nodes for the following steps:

* Listing DB tables
* Calling the "get schema" tool
* Generating a query
* Checking the query

Putting these steps in dedicated nodes lets us (1) force tool-calls when needed, and (2) customize the prompts associated with each step.

```python  theme={null}
from typing import Literal

from langchain.agents import ToolNode
from langchain.messages import AIMessage
from langchain_core.runnables import RunnableConfig
from langgraph.graph import END, START, MessagesState, StateGraph

get_schema_tool = next(tool for tool in tools if tool.name == "sql_db_schema")
get_schema_node = ToolNode([get_schema_tool], name="get_schema")

run_query_tool = next(tool for tool in tools if tool.name == "sql_db_query")
run_query_node = ToolNode([run_query_tool], name="run_query")

**Examples:**

Example 1 (unknown):
```unknown
</CodeGroup>

### LangSmith

Set up [LangSmith](https://smith.langchain.com) to inspect what is happening inside your chain or agent. Then set the following environment variables:
```

Example 2 (unknown):
```unknown
## 1. Select an LLM

Select a model that supports [tool-calling](/oss/python/integrations/providers/overview):

<Tabs>
  <Tab title="OpenAI">
    👉 Read the [OpenAI chat model integration docs](/oss/python/integrations/chat/openai/)
```

Example 3 (unknown):
```unknown
<CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Build a RAG agent with LangChain

**URL:** llms-txt#build-a-rag-agent-with-langchain

**Contents:**
- Overview
  - Concepts
  - Preview
- Setup
  - Installation
  - LangSmith
  - Components
- 1. Indexing
  - Loading documents
  - Splitting documents

Source: https://docs.langchain.com/oss/javascript/langchain/rag

One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q\&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or [RAG](/oss/javascript/langchain/retrieval/).

This tutorial will show how to build a simple Q\&A application over an unstructured text data source. We will demonstrate:

1. A RAG [agent](#rag-agents) that executes searches with a simple tool. This is a good general-purpose implementation.
2. A two-step RAG [chain](#rag-chains) that uses just a single LLM call per query. This is a fast and effective method for simple queries.

We will cover the following concepts:

* **Indexing**: a pipeline for ingesting data from a source and indexing it. *This usually happens in a separate process.*

* **Retrieval and generation**: the actual RAG process, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.

Once we've indexed our data, we will use an [agent](/oss/javascript/langchain/agents) as our orchestration framework to implement the retrieval and generation steps.

<Note>
  The indexing portion of this tutorial will largely follow the [semantic search tutorial](/oss/javascript/langchain/knowledge-base).

If your data is already available for search (i.e., you have a function to execute a search), or you're comfortable with the content from that tutorial, feel free to skip to the section on [retrieval and generation](#2-retrieval-and-generation)
</Note>

In this guide we'll build an app that answers questions about the website's content. The specific website we will use is the [LLM Powered Autonomous Agents](https://lilianweng.github.io/posts/2023-06-23-agent/) blog post by Lilian Weng, which allows us to ask questions about the contents of the post.

We can create a simple indexing pipeline and RAG chain to do this in \~40 lines of code. See below for the full code snippet:

<Accordion title="Expand for full code snippet">

Check out the [LangSmith trace](https://smith.langchain.com/public/a117a1f8-c96c-4c16-a285-00b85646118e/r).
</Accordion>

This tutorial requires these langchain dependencies:

For more details, see our [Installation guide](/oss/javascript/langchain/install).

Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with [LangSmith](https://smith.langchain.com).

After you sign up at the link above, make sure to set your environment variables to start logging traces:

We will need to select three components from LangChain's suite of integrations.

<Tabs>
  <Tab title="OpenAI">
    👉 Read the [OpenAI chat model integration docs](/oss/javascript/integrations/chat/openai/)

</CodeGroup>
  </Tab>

<Tab title="Anthropic">
    👉 Read the [Anthropic chat model integration docs](/oss/javascript/integrations/chat/anthropic/)

</CodeGroup>
  </Tab>

<Tab title="Azure">
    👉 Read the [Azure chat model integration docs](/oss/javascript/integrations/chat/azure/)

</CodeGroup>
  </Tab>

<Tab title="Google Gemini">
    👉 Read the [Google GenAI chat model integration docs](/oss/javascript/integrations/chat/google_generative_ai/)

</CodeGroup>
  </Tab>

<Tab title="Bedrock Converse">
    👉 Read the [AWS Bedrock chat model integration docs](/oss/javascript/integrations/chat/bedrock_converse/)

</CodeGroup>
  </Tab>
</Tabs>

Select an embeddings model:

<Tabs>
  <Tab title="OpenAI">
    <CodeGroup>

<Tab title="Azure">
    <CodeGroup>

<Tab title="AWS">
    <CodeGroup>

<Tab title="VertexAI">
    <CodeGroup>

<Tab title="MistralAI">
    <CodeGroup>

<Tab title="Cohere">
    <CodeGroup>

Select a vector store:

<Tabs>
  <Tab title="Memory">
    <CodeGroup>

<Tab title="Chroma">
    <CodeGroup>

<Tab title="FAISS">
    <CodeGroup>

<Tab title="MongoDB">
    <CodeGroup>

<Tab title="PGVector">
    <CodeGroup>

<Tab title="Pinecone">
    <CodeGroup>

<Tab title="Qdrant">
    <CodeGroup>

<Note>
  **This section is an abbreviated version of the content in the [semantic search tutorial](/oss/javascript/langchain/knowledge-base).**

If your data is already indexed and available for search (i.e., you have a function to execute a search), or if you're comfortable with [document loaders](/oss/javascript/langchain/retrieval#document_loaders), [embeddings](/oss/javascript/langchain/retrieval#embedding_models), and [vector stores](/oss/javascript/langchain/retrieval#vectorstores), feel free to skip to the next section on [retrieval and generation](/oss/javascript/langchain/rag#2-retrieval-and-generation).
</Note>

Indexing commonly works as follows:

1. **Load**: First we need to load our data. This is done with [Document Loaders](/oss/javascript/langchain/retrieval#document_loaders).
2. **Split**: [Text splitters](/oss/javascript/langchain/retrieval#text_splitters) break large `Documents` into smaller chunks. This is useful both for indexing data and passing it into a model, as large chunks are harder to search over and won't fit in a model's finite context window.
3. **Store**: We need somewhere to store and index our splits, so that they can be searched over later. This is often done using a [VectorStore](/oss/javascript/langchain/retrieval#vectorstores) and [Embeddings](/oss/javascript/langchain/retrieval#embedding_models) model.

<img src="https://mintcdn.com/langchain-5e9cc07a/I6RpA28iE233vhYX/images/rag_indexing.png?fit=max&auto=format&n=I6RpA28iE233vhYX&q=85&s=21403ce0d0c772da84dcc5b75cff4451" alt="index_diagram" data-og-width="2583" width="2583" data-og-height="1299" height="1299" data-path="images/rag_indexing.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/I6RpA28iE233vhYX/images/rag_indexing.png?w=280&fit=max&auto=format&n=I6RpA28iE233vhYX&q=85&s=bf4eb8255b82a809dbbd2bc2a96d2ed7 280w, https://mintcdn.com/langchain-5e9cc07a/I6RpA28iE233vhYX/images/rag_indexing.png?w=560&fit=max&auto=format&n=I6RpA28iE233vhYX&q=85&s=4ebc538b2c4765b609f416025e4dbbda 560w, https://mintcdn.com/langchain-5e9cc07a/I6RpA28iE233vhYX/images/rag_indexing.png?w=840&fit=max&auto=format&n=I6RpA28iE233vhYX&q=85&s=1838328a870c7353c42bf1cc2290a779 840w, https://mintcdn.com/langchain-5e9cc07a/I6RpA28iE233vhYX/images/rag_indexing.png?w=1100&fit=max&auto=format&n=I6RpA28iE233vhYX&q=85&s=675f55e100bab5e2904d27db01775ccc 1100w, https://mintcdn.com/langchain-5e9cc07a/I6RpA28iE233vhYX/images/rag_indexing.png?w=1650&fit=max&auto=format&n=I6RpA28iE233vhYX&q=85&s=4b9e544a7a3ec168651558bce854eb60 1650w, https://mintcdn.com/langchain-5e9cc07a/I6RpA28iE233vhYX/images/rag_indexing.png?w=2500&fit=max&auto=format&n=I6RpA28iE233vhYX&q=85&s=f5aeaaaea103128f374c03b05a317263 2500w" />

### Loading documents

We need to first load the blog post contents. We can use [DocumentLoaders](/oss/javascript/langchain/retrieval#document_loaders) for this, which are objects that load in data from a source and return a list of [Document](https://v03.api.js.langchain.com/classes/_langchain_core.documents.Document.html) objects.

`DocumentLoader`: Object that loads data from a source as list of `Documents`.

* [Integrations](/oss/javascript/integrations/document_loaders/): 160+ integrations to choose from.
* [`BaseLoader`](https://v03.api.js.langchain.com/classes/_langchain_core.document_loaders_base.BaseDocumentLoader.html): API reference for the base interface.

### Splitting documents

Our loaded document is over 42k characters which is too long to fit into the context window of many models. Even for those models that could fit the full post in their context window, models can struggle to find information in very long inputs.

To handle this we'll split the [`Document`](https://v03.api.js.langchain.com/classes/_langchain_core.documents.Document.html) into chunks for embedding and vector storage. This should help us retrieve only the most relevant parts of the blog post at run time.

As in the [semantic search tutorial](/oss/javascript/langchain/knowledge-base), we use a `RecursiveCharacterTextSplitter`, which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases.

### Storing documents

Now we need to index our 66 text chunks so that we can search over them at runtime. Following the [semantic search tutorial](/oss/javascript/langchain/knowledge-base), our approach is to [embed](/oss/javascript/langchain/retrieval#embedding_models/) the contents of each document split and insert these embeddings into a [vector store](/oss/javascript/langchain/retrieval#vectorstores/). Given an input query, we can then use vector search to retrieve relevant documents.

We can embed and store all of our document splits in a single command using the vector store and embeddings model selected at the [start of the tutorial](/oss/javascript/langchain/rag#components).

`Embeddings`: Wrapper around a text embedding model, used for converting text to embeddings.

* [Integrations](/oss/javascript/integrations/text_embedding/): 30+ integrations to choose from.
* [Interface](https://v03.api.js.langchain.com/classes/_langchain_core.embeddings.Embeddings.html): API reference for the base interface.

`VectorStore`: Wrapper around a vector database, used for storing and querying embeddings.

* [Integrations](/oss/javascript/integrations/vectorstores/): 40+ integrations to choose from.
* [Interface](https://python.langchain.com/api_reference/core/vectorstores/langchain_core.vectorstores.base.VectorStore.html): API reference for the base interface.

This completes the **Indexing** portion of the pipeline. At this point we have a query-able vector store containing the chunked contents of our blog post. Given a user question, we should ideally be able to return the snippets of the blog post that answer the question.

## 2. Retrieval and Generation

RAG applications commonly work as follows:

1. **Retrieve**: Given a user input, relevant splits are retrieved from storage using a [Retriever](/oss/javascript/langchain/retrieval#retrievers).
2. **Generate**: A [model](/oss/javascript/langchain/models) produces an answer using a prompt that includes both the question with the retrieved data

<img src="https://mintcdn.com/langchain-5e9cc07a/I6RpA28iE233vhYX/images/rag_retrieval_generation.png?fit=max&auto=format&n=I6RpA28iE233vhYX&q=85&s=994c3585cece93c80873d369960afd44" alt="retrieval_diagram" data-og-width="2532" width="2532" data-og-height="1299" height="1299" data-path="images/rag_retrieval_generation.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/I6RpA28iE233vhYX/images/rag_retrieval_generation.png?w=280&fit=max&auto=format&n=I6RpA28iE233vhYX&q=85&s=3bd28b3662e08c8364b60b74f510751e 280w, https://mintcdn.com/langchain-5e9cc07a/I6RpA28iE233vhYX/images/rag_retrieval_generation.png?w=560&fit=max&auto=format&n=I6RpA28iE233vhYX&q=85&s=43484903ca631a47a54e86191eb5ba22 560w, https://mintcdn.com/langchain-5e9cc07a/I6RpA28iE233vhYX/images/rag_retrieval_generation.png?w=840&fit=max&auto=format&n=I6RpA28iE233vhYX&q=85&s=67fe2302e241fc24238a5df1cf56573d 840w, https://mintcdn.com/langchain-5e9cc07a/I6RpA28iE233vhYX/images/rag_retrieval_generation.png?w=1100&fit=max&auto=format&n=I6RpA28iE233vhYX&q=85&s=d390a6a758e688ec36352d30b22249b0 1100w, https://mintcdn.com/langchain-5e9cc07a/I6RpA28iE233vhYX/images/rag_retrieval_generation.png?w=1650&fit=max&auto=format&n=I6RpA28iE233vhYX&q=85&s=59729377317a0631598b6a4a2a7d8c92 1650w, https://mintcdn.com/langchain-5e9cc07a/I6RpA28iE233vhYX/images/rag_retrieval_generation.png?w=2500&fit=max&auto=format&n=I6RpA28iE233vhYX&q=85&s=c07711c71153c3b2dfd5b0104ad3e324 2500w" />

Now let's write the actual application logic. We want to create a simple application that takes a user question, searches for documents relevant to that question, passes the retrieved documents and initial question to a model, and returns an answer.

1. A RAG [agent](#rag-agents) that executes searches with a simple tool. This is a good general-purpose implementation.
2. A two-step RAG [chain](#rag-chains) that uses just a single LLM call per query. This is a fast and effective method for simple queries.

One formulation of a RAG application is as a simple [agent](/oss/javascript/langchain/agents) with a tool that retrieves information. We can assemble a minimal RAG agent by implementing a [tool](/oss/javascript/langchain/tools) that wraps our vector store:

<Tip>
  Here we specify the `responseFormat` to `content_and_artifact` to confiugre the tool to attach raw documents as [artifacts](/oss/javascript/langchain/messages#param-artifact) to each [ToolMessage](/oss/javascript/langchain/messages#tool-message). This will let us access document metadata in our application, separate from the stringified representation that is sent to the model.
</Tip>

Given our tool, we can construct the agent:

Let's test this out. We construct a question that would typically require an iterative sequence of retrieval steps to answer:

1. Generates a query to search for a standard method for task decomposition;
2. Receiving the answer, generates a second query to search for common extensions of it;
3. Having received all necessary context, answers the question.

We can see the full sequence of steps, along with latency and other metadata, in the [LangSmith trace](https://smith.langchain.com/public/7b42d478-33d2-4631-90a4-7cb731681e88/r).

<Tip>
  You can add a deeper level of control and customization using the [LangGraph](/oss/javascript/langgraph/overview) framework directly— for example, you can add steps to grade document relevance and rewrite search queries. Check out LangGraph's [Agentic RAG tutorial](/oss/javascript/langgraph/agentic-rag) for more advanced formulations.
</Tip>

In the above [agentic RAG](#rag-agents) formulation we allow the LLM to use its discretion in generating a [tool call](/oss/javascript/langchain/models#tool-calling) to help answer user queries. This is a good general-purpose solution, but comes with some trade-offs:

| ✅ Benefits                                                                                                                                                 | ⚠️ Drawbacks                                                                                                                                |
| ---------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------- |
| **Search only when needed** – The LLM can handle greetings, follow-ups, and simple queries without triggering unnecessary searches.                        | **Two inference calls** – When a search is performed, it requires one call to generate the query and another to produce the final response. |
| **Contextual search queries** – By treating search as a tool with a `query` input, the LLM crafts its own queries that incorporate conversational context. | **Reduced control** – The LLM may skip searches when they are actually needed, or issue extra searches when unnecessary.                    |
| **Multiple searches allowed** – The LLM can execute several searches in support of a single user query.                                                    |                                                                                                                                             |

Another common approach is a two-step chain, in which we always run a search (potentially using the raw user query) and incorporate the result as context for a single LLM query. This results in a single inference call per query, buying reduced latency at the expense of flexibility.

In this approach we no longer call the model in a loop, but instead make a single pass.

We can implement this chain by removing tools from the agent and instead incorporating the retrieval step into a custom prompt:

In the [LangSmith trace](https://smith.langchain.com/public/0322904b-bc4c-4433-a568-54c6b31bbef4/r/9ef1c23e-380e-46bf-94b3-d8bb33df440c) we can see the retrieved context incorporated into the model prompt.

This is a fast and effective method for simple queries in constrained settings, when we typically do want to run user queries through semantic search to pull additional context.

<Accordion title="Returning source documents">
  The above [RAG chain](#rag-chains) incorporates retrieved context into a single system message for that run.

As in the [agentic RAG](#rag-agents) formulation, we sometimes want to include raw source documents in the application state to have access to document metadata. We can do this for the two-step chain case by:

1. Adding a key to the state to store the retrieved documents
  2. Adding a new node via a [pre-model hook](/oss/javascript/langchain/agents#pre-model-hook) to populate that key (as well as inject the context).

Now that we've implemented a simple RAG application via @\[`create_agent`], we can easily incorporate new features and go deeper:

* [Stream](/oss/javascript/langchain/streaming) tokens and other information for responsive user experiences
* Add [conversational memory](/oss/javascript/langchain/short-term-memory) to support multi-turn interactions
* Add [long-term memory](/oss/javascript/langchain/long-term-memory) to support memory across conversational threads
* Add [structured responses](/oss/javascript/langchain/structured-output)
* Deploy your application with [LangSmith Deployments](/langsmith/deployments)

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/rag.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
Check out the [LangSmith trace](https://smith.langchain.com/public/a117a1f8-c96c-4c16-a285-00b85646118e/r).
</Accordion>

## Setup

### Installation

This tutorial requires these langchain dependencies:

<CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown

```

---

## Build a semantic search engine with LangChain

**URL:** llms-txt#build-a-semantic-search-engine-with-langchain

**Contents:**
- Overview
  - Concepts
- Setup
  - Installation
  - LangSmith
- 1. Documents and Document Loaders
  - Loading documents
  - Splitting
- 2. Embeddings
- 3. Vector stores

Source: https://docs.langchain.com/oss/javascript/langchain/knowledge-base

This tutorial will familiarize you with LangChain's [document loader](/oss/javascript/langchain/retrieval#document-loaders), [embedding](/oss/javascript/langchain/retrieval#embedding-models), and [vector store](/oss/javascript/langchain/retrieval#vector-store) abstractions. These abstractions are designed to support retrieval of data--  from (vector) databases and other sources -- for integration with LLM workflows. They are important for applications that fetch data to be reasoned over as part of model inference, as in the case of retrieval-augmented generation, or [RAG](/oss/javascript/langchain/retrieval).

Here we will build a search engine over a PDF document. This will allow us to retrieve passages in the PDF that are similar to an input query. The guide also includes a minimal RAG implementation on top of the search engine.

This guide focuses on retrieval of text data. We will cover the following concepts:

* [Documents and document loaders](/oss/javascript/integrations/document_loaders);
* [Text splitters](/oss/javascript/integrations/splitters);
* [Embeddings](/oss/javascript/integrations/text_embedding);
* [Vector stores](/oss/javascript/integrations/vectorstores) and [retrievers](/oss/javascript/integrations/retrievers).

This guide requires `@langchain/community` and `pdf-parse`:

For more details, see our [Installation guide](/oss/javascript/langchain/install).

Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls.
As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent.
The best way to do this is with [LangSmith](https://smith.langchain.com).

After you sign up at the link above, make sure to set your environment variables to start logging traces:

## 1. Documents and Document Loaders

LangChain implements a [Document](https://v03.api.js.langchain.com/classes/_langchain_core.documents.Document.html) abstraction, which is intended to represent a unit of text and associated metadata. It has three attributes:

* `pageContent`: a string representing the content;
* `metadata`: a dict containing arbitrary metadata;
* `id`: (optional) a string identifier for the document.

The `metadata` attribute can capture information about the source of the document, its relationship to other documents, and other information. Note that an individual [`Document`](https://v03.api.js.langchain.com/classes/_langchain_core.documents.Document.html) object often represents a chunk of a larger document.

We can generate sample documents when desired:

However, the LangChain ecosystem implements [document loaders](/oss/javascript/langchain/retrieval#document-loaders) that [integrate with hundreds of common sources](/oss/javascript/integrations/document_loaders/). This makes it easy to incorporate data from these sources into your AI application.

### Loading documents

Let's load a PDF into a sequence of [`Document`](https://v03.api.js.langchain.com/classes/_langchain_core.documents.Document.html) objects. [Here is a sample PDF](https://github.com/langchain-ai/langchain/blob/v0.3/docs/docs/example_data/nke-10k-2023.pdf) -- a 10-k filing for Nike from 2023. We can consult the LangChain documentation for [available PDF document loaders](/oss/javascript/integrations/document_loaders/#pdfs).

`PDFLoader` loads one [`Document`](https://v03.api.js.langchain.com/classes/_langchain_core.documents.Document.html) object per PDF page. For each, we can easily access:

* The string content of the page;
* Metadata containing the file name and page number.

For both information retrieval and downstream question-answering purposes, a page may be too coarse a representation. Our goal in the end will be to retrieve [`Document`](https://v03.api.js.langchain.com/classes/_langchain_core.documents.Document.html) objects that answer an input query, and further splitting our PDF will help ensure that the meanings of relevant portions of the document are not "washed out" by surrounding text.

We can use [text splitters](/oss/javascript/langchain/retrieval#text_splitters) for this purpose. Here we will use a simple text splitter that partitions based on characters. We will split our documents into chunks of 1000 characters
with 200 characters of overlap between chunks. The overlap helps
mitigate the possibility of separating a statement from important
context related to it. We use the
`RecursiveCharacterTextSplitter`,
which will recursively split the document using common separators like
new lines until each chunk is the appropriate size. This is the
recommended text splitter for generic text use cases.

Vector search is a common way to store and search over unstructured data (such as unstructured text). The idea is to store numeric vectors that are associated with the text. Given a query, we can [embed](/oss/javascript/langchain/retrieval#embedding_models) it as a vector of the same dimension and use vector similarity metrics (such as cosine similarity) to identify related text.

LangChain supports embeddings from [dozens of providers](/oss/javascript/integrations/text_embedding/). These models specify how text should be converted into a numeric vector. Let's select a model:

<Tabs>
  <Tab title="OpenAI">
    <CodeGroup>

<Tab title="Azure">
    <CodeGroup>

<Tab title="AWS">
    <CodeGroup>

<Tab title="VertexAI">
    <CodeGroup>

<Tab title="MistralAI">
    <CodeGroup>

<Tab title="Cohere">
    <CodeGroup>

Armed with a model for generating text embeddings, we can next store them in a special data structure that supports efficient similarity search.

LangChain @\[VectorStore] objects contain methods for adding text and [`Document`](https://v03.api.js.langchain.com/classes/_langchain_core.documents.Document.html) objects to the store, and querying them using various similarity metrics. They are often initialized with [embedding](/oss/javascript/langchain/retrieval#embedding_models) models, which determine how text data is translated to numeric vectors.

LangChain includes a suite of [integrations](/oss/javascript/integrations/vectorstores) with different vector store technologies. Some vector stores are hosted by a provider (e.g., various cloud providers) and require specific credentials to use; some (such as [Postgres](/oss/javascript/integrations/vectorstores/pgvector)) run in separate infrastructure that can be run locally or via a third-party; others can run in-memory for lightweight workloads. Let's select a vector store:

<Tabs>
  <Tab title="Memory">
    <CodeGroup>

<Tab title="Chroma">
    <CodeGroup>

<Tab title="FAISS">
    <CodeGroup>

<Tab title="MongoDB">
    <CodeGroup>

<Tab title="PGVector">
    <CodeGroup>

<Tab title="Pinecone">
    <CodeGroup>

<Tab title="Qdrant">
    <CodeGroup>

Having instantiated our vector store, we can now index the documents.

Note that most vector store implementations will allow you to connect to an existing vector store--  e.g., by providing a client, index name, or other information. See the documentation for a specific [integration](/oss/javascript/integrations/vectorstores) for more detail.

Once we've instantiated a @\[`VectorStore`] that contains documents, we can query it. @\[VectorStore] includes methods for querying:

* Synchronously and asynchronously;
* By string query and by vector;
* With and without returning similarity scores;
* By similarity and @\[maximum marginal relevance]\[VectorStore.max\_marginal\_relevance\_search] (to balance similarity with query to diversity in retrieved results).

The methods will generally include a list of [Document](https://v03.api.js.langchain.com/classes/_langchain_core.documents.Document.html) objects in their outputs.

Embeddings typically represent text as a "dense" vector such that texts with similar meanings are geometrically close. This lets us retrieve relevant information just by passing in a question, without knowledge of any specific key-terms used in the document.

Return documents based on similarity to a string query:

Return documents based on similarity to an embedded query:

* @\[API Reference]\[VectorStore]
* [Integration-specific docs](/oss/javascript/integrations/vectorstores)

LangChain @\[`VectorStore`] objects do not subclass @\[Runnable]. LangChain @\[Retrievers] are Runnables, so they implement a standard set of methods (e.g., synchronous and asynchronous `invoke` and `batch` operations). Although we can construct retrievers from vector stores, retrievers can interface with non-vector store sources of data, as well (such as external APIs).

Vectorstores implement an `as_retriever` method that will generate a Retriever, specifically a [VectorStoreRetriever](https://python.langchain.com/api_reference/core/vectorstores/langchain_core.vectorstores.base.VectorStoreRetriever.html). These retrievers include specific `search_type` and `search_kwargs` attributes that identify what methods of the underlying vector store to call, and how to parameterize them. For instance, we can replicate the above with the following:

Retrievers can easily be incorporated into more complex applications, such as [retrieval-augmented generation (RAG)](/oss/javascript/langchain/retrieval) applications that combine a given question with retrieved context into a prompt for a LLM. To learn more about building such an application, check out the [RAG tutorial](/oss/javascript/langchain/rag) tutorial.

You've now seen how to build a semantic search engine over a PDF document.

For more on document loaders:

* [Overview](/oss/javascript/langchain/retrieval#document_loaders)
* [Available integrations](/oss/javascript/integrations/document_loaders/)

For more on embeddings:

* [Overview](/oss/javascript/langchain/retrieval#embedding_models/)
* [Available integrations](/oss/javascript/integrations/text_embedding/)

For more on vector stores:

* [Overview](/oss/javascript/langchain/retrieval#vectorstores/)
* [Available integrations](/oss/javascript/integrations/vectorstores/)

For more on RAG, see:

* [Build a Retrieval Augmented Generation (RAG) App](/oss/javascript/langchain/rag/)

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/knowledge-base.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
</CodeGroup>

For more details, see our [Installation guide](/oss/javascript/langchain/install).

### LangSmith

Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls.
As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent.
The best way to do this is with [LangSmith](https://smith.langchain.com).

After you sign up at the link above, make sure to set your environment variables to start logging traces:
```

Example 4 (unknown):
```unknown
## 1. Documents and Document Loaders

LangChain implements a [Document](https://v03.api.js.langchain.com/classes/_langchain_core.documents.Document.html) abstraction, which is intended to represent a unit of text and associated metadata. It has three attributes:

* `pageContent`: a string representing the content;
* `metadata`: a dict containing arbitrary metadata;
* `id`: (optional) a string identifier for the document.

The `metadata` attribute can capture information about the source of the document, its relationship to other documents, and other information. Note that an individual [`Document`](https://v03.api.js.langchain.com/classes/_langchain_core.documents.Document.html) object often represents a chunk of a larger document.

We can generate sample documents when desired:
```

---

## Build a SQL agent

**URL:** llms-txt#build-a-sql-agent

**Contents:**
- Overview
  - Concepts
- Setup
  - Installation
  - LangSmith
- 1. Select an LLM
- 2. Configure the database
- 3. Add tools for database interactions
- 6. Implement human-in-the-loop review
- 4. Execute SQL queries

Source: https://docs.langchain.com/oss/javascript/langchain/sql-agent

In this tutorial, you will learn how to build an agent that can answer questions about a SQL database using LangChain [agents](/oss/javascript/langchain/agents).

At a high level, the agent will:

<Steps>
  <Step title="Fetch the available tables and schemas from the database" />

<Step title="Decide which tables are relevant to the question" />

<Step title="Fetch the schemas for the relevant tables" />

<Step title="Generate a query based on the question and information from the schemas" />

<Step title="Double-check the query for common mistakes using an LLM" />

<Step title="Execute the query and return the results" />

<Step title="Correct mistakes surfaced by the database engine until the query is successful" />

<Step title="Formulate a response based on the results" />
</Steps>

<Warning>
  Building Q\&A systems of SQL databases requires executing model-generated SQL queries. There are inherent risks in doing this. Make sure that your database connection permissions are always scoped as narrowly as possible for your agent's needs. This will mitigate, though not eliminate, the risks of building a model-driven system.
</Warning>

We will cover the following concepts:

* [Tools](/oss/javascript/langchain/tools) for reading from SQL databases
* LangChain [agents](/oss/javascript/langchain/agents)
* [Human-in-the-loop](/oss/javascript/langchain/human-in-the-loop) processes

Set up [LangSmith](https://smith.langchain.com) to inspect what is happening inside your chain or agent. Then set the following environment variables:

Select a model that supports [tool-calling](/oss/javascript/integrations/providers/overview):

<Tabs>
  <Tab title="OpenAI">
    👉 Read the [OpenAI chat model integration docs](/oss/javascript/integrations/chat/openai/)

</CodeGroup>
  </Tab>

<Tab title="Anthropic">
    👉 Read the [Anthropic chat model integration docs](/oss/javascript/integrations/chat/anthropic/)

</CodeGroup>
  </Tab>

<Tab title="Azure">
    👉 Read the [Azure chat model integration docs](/oss/javascript/integrations/chat/azure/)

</CodeGroup>
  </Tab>

<Tab title="Google Gemini">
    👉 Read the [Google GenAI chat model integration docs](/oss/javascript/integrations/chat/google_generative_ai/)

</CodeGroup>
  </Tab>

<Tab title="Bedrock Converse">
    👉 Read the [AWS Bedrock chat model integration docs](/oss/javascript/integrations/chat/bedrock_converse/)

</CodeGroup>
  </Tab>
</Tabs>

The output shown in the examples below used OpenAI.

## 2. Configure the database

You will be creating a [SQLite database](https://www.sqlitetutorial.net/sqlite-sample-database/) for this tutorial. SQLite is a lightweight database that is easy to set up and use. We will be loading the `chinook` database, which is a sample database that represents a digital media store.

For convenience, we have hosted the database (`Chinook.db`) on a public GCS bucket.

## 3. Add tools for database interactions

Use the `SqlDatabase` wrapper available in the `langchain/sql_db` to interact with the database. The wrapper provides a simple interface to execute SQL queries and fetch results:

## 6. Implement human-in-the-loop review

It can be prudent to check the agent's SQL queries before they are executed for any unintended actions or inefficiencies.

LangChain agents feature support for built-in [human-in-the-loop middleware](/oss/javascript/langchain/human-in-the-loop) to add oversight to agent tool calls. Let's configure the agent to pause for human review on calling the `sql_db_query` tool:

<Note>
  We've added a [checkpointer](/oss/javascript/langchain/short-term-memory) to our agent to allow execution to be paused and resumed. See the [human-in-the-loop guide](/oss/javascript/langchain/human-in-the-loop) for detalis on this as well as available middleware configurations.
</Note>

On running the agent, it will now pause for review before executing the `sql_db_query` tool:

We can resume execution, in this case accepting the query, using [Command](/oss/javascript/langgraph/use-graph-api#combine-control-flow-and-state-updates-with-command):

Refer to the [human-in-the-loop guide](/oss/javascript/langchain/human-in-the-loop) for details.

## 4. Execute SQL queries

Before running the command, do a check to check the LLM generated command in ` _safe_sql`:

Then, use `run` from `SQLDatabase` to execute commands with an `execute_sql` tool:

## 5. Use `createAgent`

Use `createAgent` to build a [ReAct agent](https://arxiv.org/pdf/2210.03629) with minimal code. The agent will interpret the request and generate a SQL command. The tools will check the command for safety and then try to execute the command. If the command has an error, the error message is returned to the model. The model can then examine the original request and the new error message and generate a new command. This can continue until the LLM generates the command successfully or reaches an end count. This pattern of providing a model with feedback - error messages in this case - is very powerful.

Initialize the agent with a descriptive system prompt to customize its behavior:

Now, create an agent with the model, tools, and prompt:

Run the agent on a sample query and observe its behavior:

The agent correctly wrote a query, checked the query, and ran it to inform its final response.

<Note>
  You can inspect all aspects of the above run, including steps taken, tools invoked, what prompts were seen by the LLM, and more in the [LangSmith trace](https://smith.langchain.com/public/653d218b-af67-4854-95ca-6abecb9b2520/r).
</Note>

#### (Optional) Use Studio

[Studio](/langsmith/studio) provides a "client side" loop as well as memory so you can run this as a chat interface and query the database. You can ask questions like "Tell me the scheme of the database" or "Show me the invoices for the 5 top customers". You will see the SQL command that is generated and the resulting output. The details of how to get that started are below.

<Accordion title="Run your agent in Studio">
  In addition to the previously mentioned packages, you will need to:

In directory you will run in, you will need a `langgraph.json` file with the following contents:

For deeper customization, check out [this tutorial](/oss/javascript/langgraph/sql-agent) for implementing a SQL agent directly using LangGraph primitives.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/sql-agent.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
</CodeGroup>

### LangSmith

Set up [LangSmith](https://smith.langchain.com) to inspect what is happening inside your chain or agent. Then set the following environment variables:
```

Example 4 (unknown):
```unknown
## 1. Select an LLM

Select a model that supports [tool-calling](/oss/javascript/integrations/providers/overview):

<Tabs>
  <Tab title="OpenAI">
    👉 Read the [OpenAI chat model integration docs](/oss/javascript/integrations/chat/openai/)

    <CodeGroup>
```

---

## Build a supervisor agent

**URL:** llms-txt#build-a-supervisor-agent

**Contents:**
- Overview
  - Why use a supervisor?
  - Concepts
- Setup
  - Installation
  - LangSmith
  - Components
- 1. Define tools
- 2. Create specialized sub-agents
  - Create a calendar agent

Source: https://docs.langchain.com/oss/javascript/langchain/supervisor

The **supervisor pattern** is a [multi-agent](/oss/javascript/langchain/multi-agent) architecture where a central supervisor agent coordinates specialized worker agents. This approach excels when tasks require different types of expertise. Rather than building one agent that manages tool selection across domains, you create focused specialists coordinated by a supervisor who understands the overall workflow.

In this tutorial, you'll build a personal assistant system that demonstrates these benefits through a realistic workflow. The system will coordinate two specialists with fundamentally different responsibilities:

* A **calendar agent** that handles scheduling, availability checking, and event management.
* An **email agent** that manages communication, drafts messages, and sends notifications.

We will also incorporate [human-in-the-loop review](/oss/javascript/langchain/human-in-the-loop) to allow users to approve, edit, and reject actions (such as outbound emails) as desired.

### Why use a supervisor?

Multi-agent architectures allow you to partition [tools](/oss/javascript/langchain/tools) across workers, each with their own individual prompts or instructions. Consider an agent with direct access to all calendar and email APIs: it must choose from many similar tools, understand exact formats for each API, and handle multiple domains simultaneously. If performance degrades, it may be helpful to separate related tools and associated prompts into logical groups (in part to manage iterative improvements).

We will cover the following concepts:

* [Multi-agent systems](/oss/javascript/langchain/multi-agent)
* [Human-in-the-loop review](/oss/javascript/langchain/human-in-the-loop)

This tutorial requires the `langchain` package:

For more details, see our [Installation guide](/oss/javascript/langchain/install).

Set up [LangSmith](https://smith.langchain.com) to inspect what is happening inside your agent. Then set the following environment variables:

We will need to select a chat model from LangChain's suite of integrations:

<Tabs>
  <Tab title="OpenAI">
    👉 Read the [OpenAI chat model integration docs](/oss/javascript/integrations/chat/openai/)

</CodeGroup>
  </Tab>

<Tab title="Anthropic">
    👉 Read the [Anthropic chat model integration docs](/oss/javascript/integrations/chat/anthropic/)

</CodeGroup>
  </Tab>

<Tab title="Azure">
    👉 Read the [Azure chat model integration docs](/oss/javascript/integrations/chat/azure/)

</CodeGroup>
  </Tab>

<Tab title="Google Gemini">
    👉 Read the [Google GenAI chat model integration docs](/oss/javascript/integrations/chat/google_generative_ai/)

</CodeGroup>
  </Tab>

<Tab title="Bedrock Converse">
    👉 Read the [AWS Bedrock chat model integration docs](/oss/javascript/integrations/chat/bedrock_converse/)

</CodeGroup>
  </Tab>
</Tabs>

Start by defining the tools that require structured inputs. In real applications, these would call actual APIs (Google Calendar, SendGrid, etc.). For this tutorial, you'll use stubs to demonstrate the pattern.

## 2. Create specialized sub-agents

Next, we'll create specialized sub-agents that handle each domain.

### Create a calendar agent

The calendar agent understands natural language scheduling requests and translates them into precise API calls. It handles date parsing, availability checking, and event creation.

Test the calendar agent to see how it handles natural language scheduling:

The agent parses "next Tuesday at 2pm" into ISO format ("2024-01-16T14:00:00"), calculates the end time, calls `create_calendar_event`, and returns a natural language confirmation.

### Create an email agent

The email agent handles message composition and sending. It focuses on extracting recipient information, crafting appropriate subject lines and body text, and managing email communication.

Test the email agent with a natural language request:

The agent infers the recipient from the informal request, crafts a professional subject line and body, calls `send_email`, and returns a confirmation. Each sub-agent has a narrow focus with domain-specific tools and prompts, allowing it to excel at its specific task.

## 3. Wrap sub-agents as tools

Now wrap each sub-agent as a tool that the supervisor can invoke. This is the key architectural step that creates the layered system. The supervisor will see high-level tools like "schedule\_event", not low-level tools like "create\_calendar\_event".

The tool descriptions help the supervisor decide when to use each tool, so make them clear and specific. We return only the sub-agent's final response, as the supervisor doesn't need to see intermediate reasoning or tool calls.

## 4. Create the supervisor agent

Now create the supervisor that orchestrates the sub-agents. The supervisor only sees high-level tools and makes routing decisions at the domain level, not the individual API level.

## 5. Use the supervisor

Now test your complete system with complex requests that require coordination across multiple domains:

### Example 1: Simple single-domain request

The supervisor identifies this as a calendar task, calls `schedule_event`, and the calendar agent handles date parsing and event creation.

<Tip>
  For full transparency into the information flow, including prompts and responses for each chat model call, check out the [LangSmith trace](https://smith.langchain.com/public/91a9a95f-fba9-4e84-aff0-371861ad2f4a/r) for the above run.
</Tip>

### Example 2: Complex multi-domain request

The supervisor recognizes this requires both calendar and email actions, calls `schedule_event` for the meeting, then calls `manage_email` for the reminder. Each sub-agent completes its task, and the supervisor synthesizes both results into a coherent response.

<Tip>
  Refer to the [LangSmith trace](https://smith.langchain.com/public/95cd00a3-d1f9-4dba-9731-7bf733fb6a3c/r) to see the detailed information flow for the above run, including individual chat model prompts and responses.
</Tip>

### Complete working example

Here's everything together in a runnable script:

<Expandable title="View complete code" defaultOpen={false}>
  
</Expandable>

### Understanding the architecture

Your system has three layers. The bottom layer contains rigid API tools that require exact formats. The middle layer contains sub-agents that accept natural language, translate it to structured API calls, and return natural language confirmations. The top layer contains the supervisor that routes to high-level capabilities and synthesizes results.

This separation of concerns provides several benefits: each layer has a focused responsibility, you can add new domains without affecting existing ones, and you can test and iterate on each layer independently.

## 6. Add human-in-the-loop review

It can be prudent to incorporate [human-in-the-loop review](/oss/javascript/langchain/human-in-the-loop) of sensitive actions. LangChain includes [built-in middleware](/oss/javascript/langchain/human-in-the-loop#configuring-interrupts) to review tool calls, in this case the tools invoked by sub-agents.

Let's add human-in-the-loop review to both sub-agents:

* We configure the `create_calendar_event` and `send_email` tools to interrupt, permitting all [response types](/oss/javascript/langchain/human-in-the-loop) (`approve`, `edit`, `reject`)
* We add a [checkpointer](/oss/javascript/langchain/short-term-memory) **only to the top-level agent**. This is required to pause and resume execution.

Let's repeat the query. Note that we gather interrupt events into a list to access downstream:

This time we've interrupted execution. Let's inspect the interrupt events:

We can specify decisions for each interrupt by referring to its ID using a [`Command`](https://langchain-ai.github.io/langgraphjs/reference/classes/langgraph.Command.html). Refer to the [human-in-the-loop guide](/oss/javascript/langchain/human-in-the-loop) for additional details. For demonstration purposes, here we will accept the calendar event, but edit the subject of the outbound email:

The run proceeds with our input.

## 7. Advanced: Control information flow

By default, sub-agents receive only the request string from the supervisor. You might want to pass additional context, such as conversation history or user preferences.

### Pass additional conversational context to sub-agents

This allows sub-agents to see the full conversation context, which can be useful for resolving ambiguities like "schedule it for the same time tomorrow" (referencing a previous conversation).

<Tip>
  You can see the full context received by the sub agent in the [chat model call](https://smith.langchain.com/public/c7d54882-afb8-4039-9c5a-4112d0f458b0/r/6803571e-af78-4c68-904a-ecf55771084d) of the LangSmith trace.
</Tip>

### Control what supervisor receives

You can also customize what information flows back to the supervisor:

**Important:** Make sure sub-agent prompts emphasize that their final message should contain all relevant information. A common failure mode is sub-agents that perform tool calls but don't include the results in their final response.

<Tip>
  For a complete working example that demonstrates the full supervisor pattern with human-in-the-loop review and advanced information flow control, check out [`supervisor_complete.ts`](https://github.com/langchain-ai/langchainjs/blob/main/examples/src/createAgent/supervisor_complete.ts) in the LangChain.js examples.
</Tip>

The supervisor pattern creates layers of abstraction where each layer has a clear responsibility. When designing a supervisor system, start with clear domain boundaries and give each sub-agent focused tools and prompts. Write clear tool descriptions for the supervisor, test each layer independently before integration, and control information flow based on your specific needs.

<Tip>
  **When to use the supervisor pattern**

Use the supervisor pattern when you have multiple distinct domains (calendar, email, CRM, database), each domain has multiple tools or complex logic, you want centralized workflow control, and sub-agents don't need to converse directly with users.

For simpler cases with just a few tools, use a single agent. When agents need to have conversations with users, use [handoffs](/oss/javascript/langchain/multi-agent#handoffs) instead. For peer-to-peer collaboration between agents, consider other multi-agent patterns.
</Tip>

Learn about [handoffs](/oss/javascript/langchain/multi-agent#handoffs) for agent-to-agent conversations, explore [context engineering](/oss/javascript/langchain/context-engineering) to fine-tune information flow, read the [multi-agent overview](/oss/javascript/langchain/multi-agent) to compare different patterns, and use [LangSmith](https://smith.langchain.com) to debug and monitor your multi-agent system.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/supervisor.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
</CodeGroup>

For more details, see our [Installation guide](/oss/javascript/langchain/install).

### LangSmith

Set up [LangSmith](https://smith.langchain.com) to inspect what is happening inside your agent. Then set the following environment variables:

<CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## build based on the provided config

**URL:** llms-txt#build-based-on-the-provided-config

def make_graph(config: RunnableConfig):
    user_id = config.get("configurable", {}).get("user_id")
    # route to different graph state / structure based on the user ID
    if user_id == "1":
        return make_default_graph()
    else:
        return make_alternative_graph()

{
    "dependencies": ["."],
    "graphs": {
        "openai_agent": "./openai_agent.py:make_graph",
    },
    "env": "./.env"
}
```

See more info on LangGraph API configuration file [here](/langsmith/cli#configuration-file)

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/graph-rebuild.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
Finally, you need to specify the path to your graph-making function (`make_graph`) in `langgraph.json`:
```

---

## Build Docker image

**URL:** llms-txt#build-docker-image

langgraph build -t my-agent:latest

---

## Build graph

**URL:** llms-txt#build-graph

graph = StateGraph(dict)
graph.add_node("reasoning", reasoning_node)
graph.add_node("fallback", fallback_node)
graph.add_conditional_edges("reasoning", route_based_on_state)
graph.add_edge("fallback", END)
graph.set_entry_point("reasoning")

app = graph.compile()
python  theme={null}
from langchain_core.runnables import RunnableConfig
from langgraph.graph import StateGraph, END
from langgraph.errors import GraphRecursionError

**Examples:**

Example 1 (unknown):
```unknown
#### Proactive vs reactive approaches

There are two main approaches to handling recursion limits: proactive (monitoring within the graph) and reactive (catching errors externally).
```

---

## Build the graph

**URL:** llms-txt#build-the-graph

builder = StateGraph(MessagesState)
builder.add_node("autogen", call_autogen_agent)
builder.add_edge(START, "autogen")

---

## Build the graph with explicit schemas

**URL:** llms-txt#build-the-graph-with-explicit-schemas

builder = StateGraph(OverallState, input_schema=InputState, output_schema=OutputState)
builder.add_node(answer_node)
builder.add_edge(START, "answer_node")
builder.add_edge("answer_node", END)
graph = builder.compile()

---

## Build the graph with input and output schemas specified

**URL:** llms-txt#build-the-graph-with-input-and-output-schemas-specified

builder = StateGraph(OverallState, input_schema=InputState, output_schema=OutputState)
builder.add_node(answer_node)  # Add the answer node
builder.add_edge(START, "answer_node")  # Define the starting edge
builder.add_edge("answer_node", END)  # Define the ending edge
graph = builder.compile()  # Compile the graph

---

## Build the state graph

**URL:** llms-txt#build-the-state-graph

builder = StateGraph(OverallState)
builder.add_node(node)  # node_1 is the first node
builder.add_edge(START, "node")  # Start the graph with node_1
builder.add_edge("node", END)  # End the graph after node_1
graph = builder.compile()

---

## Build workflow

**URL:** llms-txt#build-workflow

orchestrator_worker_builder = StateGraph(State)

---

## Bulk Exporting Trace Data

**URL:** llms-txt#bulk-exporting-trace-data

**Contents:**
- Destinations
- Exporting Data
  - Destinations - Providing a S3 bucket
  - Preparing the Destination
  - Create an export job
  - Scheduled exports
- Monitoring the Export Job
  - Monitor Export Status
  - List Runs for an Export
  - List All Exports

Source: https://docs.langchain.com/langsmith/data-export

<Info>
  **Plan restrictions apply**

Please note that the Data Export functionality is only supported for [LangSmith Plus or Enterprise tiers](https://www.langchain.com/pricing-langsmith).
</Info>

LangSmith's bulk data export functionality allows you to export your traces into an external destination. This can be useful if you want to analyze the
data offline in a tool such as BigQuery, Snowflake, RedShift, Jupyter Notebooks, etc.

An export can be launched to target a specific LangSmith project and date range. Once a batch export is launched, our system will handle the orchestration and resilience of the export process.
Please note that exporting your data may take some time depending on the size of your data. We also have a limit on how many of your exports can run at the same time.
Bulk exports also have a runtime timeout of 24 hours.

Currently we support exporting to an S3 bucket or S3 API compatible bucket that you provide. The data will be exported in
[Parquet](https://parquet.apache.org/docs/overview/) columnar format. This format will allow you to easily import the data into
other systems. The data export will contain equivalent data fields as the [Run data format](/langsmith/run-data-format).

### Destinations - Providing a S3 bucket

To export LangSmith data, you will need to provide an S3 bucket where the data will be exported to.

The following information is needed for the export:

* **Bucket Name**: The name of the S3 bucket where the data will be exported to.
* **Prefix**: The root prefix within the bucket where the data will be exported to.
* **S3 Region**: The region of the bucket - this is needed for AWS S3 buckets.
* **Endpoint URL**: The endpoint URL for the S3 bucket - this is needed for S3 API compatible buckets.
* **Access Key**: The access key for the S3 bucket.
* **Secret Key**: The secret key for the S3 bucket.

We support any S3 compatible bucket, for non AWS buckets such as GCS or MinIO, you will need to provide the endpoint URL.

### Preparing the Destination

<Note>
  **For self-hosted and EU region deployments**

Update the LangSmith URL appropriately for self-hosted installations or organizations in the EU region in the requests below.
  For the EU region, use `eu.api.smith.langchain.com`.
</Note>

<Note>
  **Permissions required**

Both the `backend` and `queue` services require write access to the destination bucket:

* The `backend` service attempts to write a test file to the destination bucket when the export destination is created.
    It will delete the test file if it has permission to do so (delete access is optional).
  * The `queue` service is responsible for bulk export execution and uploading the files to the bucket.
</Note>

The following example demonstrates how to create a destination using cURL. Replace the placeholder values with your actual configuration details.
Note that credentials will be stored securely in an encrypted form in our system.

Use the returned `id` to reference this destination in subsequent bulk export operations.

**If you receive an error while creating a destination, see [debug destination errors](#debugging-destination-errors) for details on how to debug this.**

#### Credentials configuration

<Note>**Requires LangSmith Helm version >= `0.10.34` (application version >= `0.10.91`)**</Note>

We support the following additional credentials formats besides static `access_key_id` and `secret_access_key`:

* To use [temporary credentials](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp_use-resources.html) that include an AWS session token,
  additionally provide the `credentials.session_token` key when creating the bulk export destination.
* (Self-hosted only): To use environment-based credentials such as with [AWS IAM Roles for Service Accounts](https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html) (IRSA),
  omit the `credentials` key from the request when creating the bulk export destination.
  In this case, the [standard Boto3 credentials locations](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/configuration.html#credentials) will be checked in the order defined by the library.

For AWS S3, you can leave off the `endpoint_url` and supply the region that matches the region of your bucket.

#### Google GCS XML S3 compatible bucket

When using Google's GCS bucket, you need to use the XML S3 compatible API, and supply the `endpoint_url`
which is typically `https://storage.googleapis.com`.
Here is an example of the API request when using the GCS XML API which is compatible with S3:

See [Google documentation](https://cloud.google.com/storage/docs/interoperability#xml_api) for more info

### Create an export job

To export data, you will need to create an export job. This job will specify the destination, the project, the date range, and filter expression of the data to export. The filter expression is used to narrow down the set of runs exported and is optional. Not setting the filter field will export all runs. Refer to our [filter query language](/langsmith/trace-query-syntax#filter-query-language) and [examples](/langsmith/export-traces#use-filter-query-language) to determine the correct filter expression for your export.

You can use the following cURL command to create the job:

<Note>
  The `session_id` is also known as the Tracing Project ID, which can be copied from the individual project view by clicking into the project in the Tracing Projects list.
</Note>

Use the returned `id` to reference this export in subsequent bulk export operations.

### Scheduled exports

<Note>
  Requires LangSmith Helm version >= `0.10.42` (application version >= `0.10.109`)
</Note>

Scheduled exports collect runs periodically and export to the configured destination.
To create a scheduled export, include `interval_hours` and remove `end_time`:

* `interval_hours` must be between 1 hour and 168 hours (1 week) inclusive.
* For spawned exports, the first time range exported is `start_time=(scheduled_export_start_time), end_time=(start_time + interval_hours)`.
  Then `start_time=(previous_export_end_time), end_time=(this_export_start_time + interval_hours)`, and so on.
* `end_time` must be omitted for scheduled exports. `end_time` is still required for non-scheduled exports.
* Scheduled exports can be stopped by [cancelling the export](#stop-an-export).
  * Exports that have been spawned by a scheduled export have the `source_bulk_export_id` attribute filled.
  * If desired, these spawned bulk exports must be canceled separately from the source scheduled bulk export -
    canceling the source bulk export **does not** cancel the spawned bulk exports.
* Spawned exports run at `end_time + 10 minutes` to account for any runs that are submitted with `end_time` in the recent past.

If a scheduled bulk export is created with `start_time=2025-07-16T00:00:00Z` and `interval_hours=6`:

| Export | Start Time           | End Time             | Runs At              |
| ------ | -------------------- | -------------------- | -------------------- |
| 1      | 2025-07-16T00:00:00Z | 2025-07-16T06:00:00Z | 2025-07-16T06:10:00Z |
| 2      | 2025-07-16T06:00:00Z | 2025-07-16T12:00:00Z | 2025-07-16T12:10:00Z |
| 3      | 2025-07-16T12:00:00Z | 2025-07-16T18:00:00Z | 2025-07-16T18:10:00Z |

## Monitoring the Export Job

### Monitor Export Status

To monitor the status of an export job, use the following cURL command:

Replace `{export_id}` with the ID of the export you want to monitor. This command retrieves the current status of the specified export job.

### List Runs for an Export

An export is typically broken up into multiple runs which correspond to a specific date partition to export.
To list all runs associated with a specific export, use the following cURL command:

This command fetches all runs related to the specified export, providing details such as run ID, status, creation time, rows exported, etc.

To retrieve a list of all export jobs, use the following cURL command:

This command returns a list of all export jobs along with their current statuses and creation timestamps.

To stop an existing export, use the following cURL command:

Replace `{export_id}` with the ID of the export you wish to cancel. Note that a job cannot be restarted once it has been cancelled,
you will need to create a new export job instead.

## Partitioning Scheme

Data will be exported into your bucket into the follow Hive partitioned format:

## Importing Data into other systems

Importing data from S3 and Parquet format is commonly supported by the majority of analytical systems. See below for documentation links:

To import your data into BigQuery, see [Loading Data from Parquet](https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-parquet) and also
[Hive Partitioned loads](https://cloud.google.com/bigquery/docs/hive-partitioned-loads-gcs).

You can load data into Snowflake from S3 by following the [Load from Cloud Document](https://docs.snowflake.com/en/user-guide/tutorials/load-from-cloud-tutorial).

You can COPY data from S3 or Parquet into Amazon Redshift by following the [AWS COPY command documentation](https://docs.aws.amazon.com/redshift/latest/dg/r_COPY.html).

You can directly query data in S3 / Parquet format in Clickhouse. As an example, if using GCS, you can query the data as follows:

See [Clickhouse S3 Integration Documentation](https://clickhouse.com/docs/en/engines/table-engines/integrations/s3) for more information.

You can query the data from S3 in-memory with SQL using DuckDB. See [S3 import Documentation](https://duckdb.org/docs/guides/network_cloud_storage/s3_import.html).

### Debugging Destination Errors

The destinations API endpoint will validate that the destination and credentials are valid and that write access is
is present for the bucket.

If you receive an error, and would like to debug this error, you can use the [AWS CLI](https://aws.amazon.com/cli/)
to test the connectivity to the bucket. You should be able to write a file with the CLI using the same
data that you supplied to the destinations API above.

```bash  theme={null}
aws configure

**Examples:**

Example 1 (unknown):
```unknown
Use the returned `id` to reference this destination in subsequent bulk export operations.

**If you receive an error while creating a destination, see [debug destination errors](#debugging-destination-errors) for details on how to debug this.**

#### Credentials configuration

<Note>**Requires LangSmith Helm version >= `0.10.34` (application version >= `0.10.91`)**</Note>

We support the following additional credentials formats besides static `access_key_id` and `secret_access_key`:

* To use [temporary credentials](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp_use-resources.html) that include an AWS session token,
  additionally provide the `credentials.session_token` key when creating the bulk export destination.
* (Self-hosted only): To use environment-based credentials such as with [AWS IAM Roles for Service Accounts](https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html) (IRSA),
  omit the `credentials` key from the request when creating the bulk export destination.
  In this case, the [standard Boto3 credentials locations](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/configuration.html#credentials) will be checked in the order defined by the library.

#### AWS S3 bucket

For AWS S3, you can leave off the `endpoint_url` and supply the region that matches the region of your bucket.
```

Example 2 (unknown):
```unknown
#### Google GCS XML S3 compatible bucket

When using Google's GCS bucket, you need to use the XML S3 compatible API, and supply the `endpoint_url`
which is typically `https://storage.googleapis.com`.
Here is an example of the API request when using the GCS XML API which is compatible with S3:
```

Example 3 (unknown):
```unknown
See [Google documentation](https://cloud.google.com/storage/docs/interoperability#xml_api) for more info

### Create an export job

To export data, you will need to create an export job. This job will specify the destination, the project, the date range, and filter expression of the data to export. The filter expression is used to narrow down the set of runs exported and is optional. Not setting the filter field will export all runs. Refer to our [filter query language](/langsmith/trace-query-syntax#filter-query-language) and [examples](/langsmith/export-traces#use-filter-query-language) to determine the correct filter expression for your export.

You can use the following cURL command to create the job:
```

Example 4 (unknown):
```unknown
<Note>
  The `session_id` is also known as the Tracing Project ID, which can be copied from the individual project view by clicking into the project in the Tracing Projects list.
</Note>

Use the returned `id` to reference this export in subsequent bulk export operations.

### Scheduled exports

<Note>
  Requires LangSmith Helm version >= `0.10.42` (application version >= `0.10.109`)
</Note>

Scheduled exports collect runs periodically and export to the configured destination.
To create a scheduled export, include `interval_hours` and remove `end_time`:
```

---

## By default we provide a StateBackend

**URL:** llms-txt#by-default-we-provide-a-statebackend

agent = create_deep_agent()

---

## cache hit, system prompt is cached

**URL:** llms-txt#cache-hit,-system-prompt-is-cached

**Contents:**
  - Model call limit
  - Tool call limit

agent.invoke({"messages": [HumanMessage("What's my name?")]})
python  theme={null}
from langchain.agents import create_agent
from langchain.agents.middleware import ModelCallLimitMiddleware

agent = create_agent(
    model="gpt-4o",
    tools=[...],
    middleware=[
        ModelCallLimitMiddleware(
            thread_limit=10,  # Max 10 calls per thread (across runs)
            run_limit=5,  # Max 5 calls per run (single invocation)
            exit_behavior="end",  # Or "error" to raise exception
        ),
    ],
)
python  theme={null}
from langchain.agents import create_agent
from langchain.agents.middleware import ToolCallLimitMiddleware

**Examples:**

Example 1 (unknown):
```unknown
<Accordion title="Configuration options">
  <ParamField body="type" type="string" default="ephemeral">
    Cache type. Only `"ephemeral"` is currently supported.
  </ParamField>

  <ParamField body="ttl" type="string" default="5m">
    Time to live for cached content. Valid values: `"5m"` or `"1h"`
  </ParamField>

  <ParamField body="min_messages_to_cache" type="number" default="0">
    Minimum number of messages before caching starts
  </ParamField>

  <ParamField body="unsupported_model_behavior" type="string" default="warn">
    Behavior when using non-Anthropic models. Options: `"ignore"`, `"warn"`, or `"raise"`
  </ParamField>
</Accordion>

### Model call limit

Limit the number of model calls to prevent infinite loops or excessive costs.

<Tip>
  **Perfect for:**

  * Preventing runaway agents from making too many API calls
  * Enforcing cost controls on production deployments
  * Testing agent behavior within specific call budgets
</Tip>
```

Example 2 (unknown):
```unknown
<Accordion title="Configuration options">
  <ParamField body="thread_limit" type="number">
    Maximum model calls across all runs in a thread. Defaults to no limit.
  </ParamField>

  <ParamField body="run_limit" type="number">
    Maximum model calls per single invocation. Defaults to no limit.
  </ParamField>

  <ParamField body="exit_behavior" type="string" default="end">
    Behavior when limit is reached. Options: `"end"` (graceful termination) or `"error"` (raise exception)
  </ParamField>
</Accordion>

### Tool call limit

Control agent execution by limiting the number of tool calls, either globally across all tools or for specific tools.

<Tip>
  **Perfect for:**

  * Preventing excessive calls to expensive external APIs
  * Limiting web searches or database queries
  * Enforcing rate limits on specific tool usage
  * Protecting against runaway agent loops
</Tip>

To limit tool calls globally across all tools or for specific tools, set `tool_name`. For each limit, specify one or both of:

* **Thread limit** (`thread_limit`) - Max calls across all runs in a conversation. Persists across invocations. Requires a checkpointer.
* **Run limit** (`run_limit`) - Max calls per single invocation. Resets each turn.

**Exit behaviors:**

| Behavior                   | Effect                                                     | Best For                                                            |
| -------------------------- | ---------------------------------------------------------- | ------------------------------------------------------------------- |
| **`"continue"`** (default) | Blocks exceeded calls with error messages, agent continues | Most use cases - agent handles limits gracefully                    |
| **`"error"`**              | Raises exception immediately                               | Complex workflows where you want to handle the limit error manually |
| **`"end"`**                | Stops with ToolMessage + AI message                        | Single-tool scenarios (errors if other tools pending)               |
```

---

## cache store

**URL:** llms-txt#cache-store

agent.invoke({"messages": [HumanMessage("Hi, my name is Bob")]})

---

## Callbacks

**URL:** llms-txt#callbacks

Source: https://docs.langchain.com/oss/javascript/integrations/callbacks/index

<Columns cols={3}>
  <Card title="Datadog Tracer" icon="link" href="/oss/javascript/integrations/callbacks/datadog_tracer" arrow="true" cta="View guide" />

<Card title="Upstash Rate Limit" icon="link" href="/oss/javascript/integrations/callbacks/upstash_ratelimit_callback" arrow="true" cta="View guide" />
</Columns>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/javascript/integrations/callbacks/index.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Call the function with traced attachments

**URL:** llms-txt#call-the-function-with-traced-attachments

**Contents:**
  - TypeScript

result = trace_with_attachments(
    val=val,
    text=text,
    image=image_attachment,
    audio=audio_attachment,
    video=video_attachment,
    pdf=pdf_attachment,
    csv=csv_attachment,
)
typescript TypeScript theme={null}
type AttachmentData = Uint8Array | ArrayBuffer;
type Attachments = Record<string, [string, AttachmentData]>;

extractAttachments?: (
    ...args: Parameters<Func>
) => [Attachments | undefined, KVMap];
typescript TypeScript theme={null}
import { traceable } from "langsmith/traceable";

const traceableWithAttachments = traceable(
    (
        val: number,
        text: string,
        attachment: Uint8Array,
        attachment2: ArrayBuffer,
        attachment3: Uint8Array,
        attachment4: ArrayBuffer,
        attachment5: Uint8Array,
    ) =>
        `Processed: ${val}, ${text}, ${attachment.length}, ${attachment2.byteLength}, ${attachment3.length}, ${attachment4.byteLength}, ${attachment5.byteLength}`,
    {
        name: "traceWithAttachments",
        extractAttachments: (
            val: number,
            text: string,
            attachment: Uint8Array,
            attachment2: ArrayBuffer,
            attachment3: Uint8Array,
            attachment4: ArrayBuffer,
            attachment5: Uint8Array,
        ) => [
            {
                "image inputs": ["image/png", attachment],
                "mp3 inputs": ["audio/mpeg", new Uint8Array(attachment2)],
                "video inputs": ["video/mp4", attachment3],
                "pdf inputs": ["application/pdf", new Uint8Array(attachment4)],
                "csv inputs": ["text/csv", new Uint8Array(attachment5)],
            },
            { val, text },
        ],
    }
);

const fs = Deno // or Node.js fs module
const image = await fs.readFile("my_image.png"); // Uint8Array
const mp3Buffer = await fs.readFile("my_mp3.mp3");
const mp3ArrayBuffer = mp3Buffer.buffer; // Convert to ArrayBuffer
const video = await fs.readFile("my_video.mp4"); // Uint8Array
const pdfBuffer = await fs.readFile("my_document.pdf");
const pdfArrayBuffer = pdfBuffer.buffer; // Convert to ArrayBuffer
const csv = await fs.readFile("test-vals.csv"); // Uint8Array

// Define example parameters
const val = 42;
const text = "Hello, world!";

// Call traceableWithAttachments with the files
const result = await traceableWithAttachments(
    val, text, image, mp3ArrayBuffer, video, pdfArrayBuffer, csv
);
```

Here is how the above would look in the LangSmith UI. You can expand each attachment to view its contents.

<img src="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-with-attachments.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=cb21a1c6d8904d1d7b2215652a6127a5" alt="" data-og-width="3012" width="3012" data-og-height="1696" height="1696" data-path="langsmith/images/trace-with-attachments.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-with-attachments.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=4caa7aaa44cd296b2f30ff8d6f6d7199 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-with-attachments.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=01a4392a7dbf3d10184778d2ab3737a1 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-with-attachments.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=e8047eabe350453619c255e88d6fe1d5 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-with-attachments.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=9b0bbb43e10ea30cf466fb65502af907 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-with-attachments.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=0060d9877954b82d34593d8789f7d0a5 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-with-attachments.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=ec15ad89f02769defc2d5f5637d88e11 2500w" />

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/upload-files-with-traces.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
### TypeScript

In the TypeScript SDK, you can add attachments to traces by using `Uint8Array` or `ArrayBuffer` as data types. Each attachment's MIME type is specified within `extractAttachments`:

* `Uint8Array`: Useful for handling binary data directly.
* `ArrayBuffer`: Represents fixed-length binary data, which can be converted to `Uint8Array` as needed.

Wrap your function with `traceable` and include your attachments within the `extractAttachments` option.

In the TypeScript SDK, the `extractAttachments` function is an optional parameter in the `traceable` configuration. When the traceable-wrapped function is invoked, it extracts binary data (e.g., images, audio files) from your inputs and logs them alongside other trace data, specifying their MIME types.

Note that you cannot directly pass in a file path in the TypeScript SDK, as accessing local files is not supported in all runtime environments.
```

Example 2 (unknown):
```unknown

```

---

## Call the graph: here we call it to generate a list of jokes

**URL:** llms-txt#call-the-graph:-here-we-call-it-to-generate-a-list-of-jokes

**Contents:**
- Create and control loops

for step in graph.stream({"topic": "animals"}):
    print(step)

{'generate_topics': {'subjects': ['lions', 'elephants', 'penguins']}}
{'generate_joke': {'jokes': ["Why don't lions like fast food? Because they can't catch it!"]}}
{'generate_joke': {'jokes': ["Why don't elephants use computers? They're afraid of the mouse!"]}}
{'generate_joke': {'jokes': ['Why don't penguins like talking to strangers at parties? Because they find it hard to break the ice.']}}
{'best_joke': {'best_selected_joke': 'penguins'}}
python  theme={null}
builder = StateGraph(State)
builder.add_node(a)
builder.add_node(b)

def route(state: State) -> Literal["b", END]:
    if termination_condition(state):
        return END
    else:
        return "b"

builder.add_edge(START, "a")
builder.add_conditional_edges("a", route)
builder.add_edge("b", "a")
graph = builder.compile()
python  theme={null}
from langgraph.errors import GraphRecursionError

try:
    graph.invoke(inputs, {"recursion_limit": 3})
except GraphRecursionError:
    print("Recursion Error")
python  theme={null}
import operator
from typing import Annotated, Literal
from typing_extensions import TypedDict
from langgraph.graph import StateGraph, START, END

class State(TypedDict):
    # The operator.add reducer fn makes this append-only
    aggregate: Annotated[list, operator.add]

def a(state: State):
    print(f'Node A sees {state["aggregate"]}')
    return {"aggregate": ["A"]}

def b(state: State):
    print(f'Node B sees {state["aggregate"]}')
    return {"aggregate": ["B"]}

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
## Create and control loops

When creating a graph with a loop, we require a mechanism for terminating execution. This is most commonly done by adding a [conditional edge](/oss/python/langgraph/graph-api#conditional-edges) that routes to the [END](/oss/python/langgraph/graph-api#end-node) node once we reach some termination condition.

You can also set the graph recursion limit when invoking or streaming the graph. The recursion limit sets the number of [supersteps](/oss/python/langgraph/graph-api#graphs) that the graph is allowed to execute before it raises an error. Read more about the concept of recursion limits [here](/oss/python/langgraph/graph-api#recursion-limit).

Let's consider a simple graph with a loop to better understand how these mechanisms work.

<Tip>
  To return the last value of your state instead of receiving a recursion limit error, see the [next section](#impose-a-recursion-limit).
</Tip>

When creating a loop, you can include a conditional edge that specifies a termination condition:
```

Example 3 (unknown):
```unknown
To control the recursion limit, specify `"recursionLimit"` in the config. This will raise a `GraphRecursionError`, which you can catch and handle:
```

Example 4 (unknown):
```unknown
Let's define a graph with a simple loop. Note that we use a conditional edge to implement a termination condition.
```

---

## candidate_results.to_pandas()

**URL:** llms-txt#candidate_results.to_pandas()

**Contents:**
- Comparing the results

## Comparing the results

After running both experiments, you can view them in your dataset:

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/dataset-page.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=1c5d4f1cf212e2c38917319c7bbf7f99" alt="" data-og-width="3022" width="3022" data-og-height="1536" height="1536" data-path="langsmith/images/dataset-page.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/dataset-page.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=01ebc5373bb6428b614eeade16aeb606 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/dataset-page.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=b2308ef5ed76bb80f111a89000457424 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/dataset-page.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=f2c76f9b1194a1a56efaa97d88b885f0 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/dataset-page.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=beb69429d2a2e208b75924593a9a10c1 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/dataset-page.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=0fa47df443b9695d1e6863a57ca3a016 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/dataset-page.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=d257f3444d8357fb0b0340d09c16e127 2500w" />

The results reveal an interesting tradeoff between the two models:

1. GPT-4o shows improved performance in following formatting rules, consistently including the requested number of emojis
2. However, GPT-4o is less reliable at staying grounded in the provided search results

To illustrate the grounding issue: in [this example run](https://smith.langchain.com/public/be060e19-0bc0-4798-94f5-c3d35719a5f6/r/07d43e7a-8632-479d-ae28-c7eac6e54da4), GPT-4o included facts about Abū Bakr Muhammad ibn Zakariyyā al-Rāzī's medical contributions that weren't present in the search results. This demonstrates how it's pulling from its internal knowledge rather than strictly using the provided information.

This backtesting exercise revealed that while GPT-4o is generally considered a more capable model, simply upgrading to it wouldn't improve our tweet-writer. To effectively use GPT-4o, we would need to:

* Refine our prompts to more strongly emphasize using only provided information
* Or modify our system architecture to better constrain the model's outputs

This insight demonstrates the value of backtesting - it helped us identify potential issues before deployment.

<img src="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tutorial-comparison-view.png?fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=a8ab311399f3d0e69554a62f939fd475" alt="" data-og-width="3018" width="3018" data-og-height="1532" height="1532" data-path="langsmith/images/tutorial-comparison-view.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tutorial-comparison-view.png?w=280&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=92dca1af013a79d9ce2ee944a17e23a9 280w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tutorial-comparison-view.png?w=560&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=16ac3bc225307b5408a49c225646a99e 560w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tutorial-comparison-view.png?w=840&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=517037b9b2a372dd5d4b2dc4e41eac6a 840w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tutorial-comparison-view.png?w=1100&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=d1b1d90b0838a53d1c693617fad61eb4 1100w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tutorial-comparison-view.png?w=1650&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=ecbe835c06c9451cac57d7cf0a16d0b9 1650w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tutorial-comparison-view.png?w=2500&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=77d1da832c66587699653956fc15ccb6 2500w" />

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/run-backtests-new-agent.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Can equivalently use the 'aevaluate' function directly:

**URL:** llms-txt#can-equivalently-use-the-'aevaluate'-function-directly:

---

## Case studies

**URL:** llms-txt#case-studies

Source: https://docs.langchain.com/oss/javascript/langgraph/case-studies

This list of companies using LangGraph and their success stories is compiled from public sources. If your company uses LangGraph, we'd love for you to share your story and add it to the list. You’re also welcome to contribute updates based on publicly available information from other companies, such as blog posts or press releases.

| Company                                                                                                                                 | Industry                             | Use case                                                      | Reference                                                                                                                                                                                                                                                                                                                                     |
| --------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------ | ------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [AirTop](https://www.airtop.ai/)                                                                                                        | Software & Technology (GenAI Native) | Browser automation for AI agents                              | [Case study, 2024](https://blog.langchain.dev/customers-airtop/)                                                                                                                                                                                                                                                                              |
| [AppFolio](https://www.appfolio.com/)                                                                                                   | Real Estate                          | Copilot for domain-specific task                              | [Case study, 2024](https://blog.langchain.dev/customers-appfolio/)                                                                                                                                                                                                                                                                            |
| [Athena Intelligence](https://www.athenaintel.com/)                                                                                     | Software & Technology (GenAI Native) | Research & summarization                                      | [Case study, 2024](https://blog.langchain.dev/customers-athena-intelligence/)                                                                                                                                                                                                                                                                 |
| [BlackRock](https://www.blackrock.com/)                                                                                                 | Financial Services                   | Copilot for domain-specific task                              | [Interrupt talk, 2025](https://youtu.be/oyqeCHFM5U4?feature=shared)                                                                                                                                                                                                                                                                           |
| [Captide](https://www.captide.co/)                                                                                                      | Software & Technology (GenAI Native) | Data extraction                                               | [Case study, 2025](https://blog.langchain.dev/how-captide-is-redefining-equity-research-with-agentic-workflows-built-on-langgraph-and-langsmith/)                                                                                                                                                                                             |
| [Cisco CX](https://www.cisco.com/site/us/en/services/modern-data-center/index.html?CCID=cc005911\&DTID=eivtotr001480\&OID=srwsas032775) | Software & Technology                | Customer support                                              | [Interrupt Talk, 2025](https://youtu.be/gPhyPRtIMn0?feature=shared)                                                                                                                                                                                                                                                                           |
| [Cisco Outshift](https://outshift.cisco.com/)                                                                                           | Software & Technology                | DevOps                                                        | [Video story, 2025](https://www.youtube.com/watch?v=htcb-vGR_x0); [Case study, 2025](https://blog.langchain.com/cisco-outshift/); [Blog post, 2025](https://outshift.cisco.com/blog/build-react-agent-application-for-devops-tasks-using-rest-apis)                                                                                           |
| [Cisco TAC](https://www.cisco.com/c/en/us/support/index.html)                                                                           | Software & Technology                | Customer support                                              | [Video story, 2025](https://youtu.be/EAj0HBDGqaE?feature=shared)                                                                                                                                                                                                                                                                              |
| [City of Hope](https://www.cityofhope.org/)                                                                                             | Non-profit                           | Copilot for domain-specific task                              | [Video story, 2025](https://youtu.be/9ABwtK2gIZU?feature=shared)                                                                                                                                                                                                                                                                              |
| [C.H. Robinson](https://www.chrobinson.com/en-us/)                                                                                      | Logistics                            | Automation                                                    | [Case study, 2025](https://blog.langchain.dev/customers-chrobinson/)                                                                                                                                                                                                                                                                          |
| [Definely](https://www.definely.com/)                                                                                                   | Legal                                | Copilot for domain-specific task                              | [Case study, 2025](https://blog.langchain.com/customers-definely/)                                                                                                                                                                                                                                                                            |
| [Docent Pro](https://docentpro.com/)                                                                                                    | Travel                               | GenAI embedded product experiences                            | [Case study, 2025](https://blog.langchain.com/customers-docentpro/)                                                                                                                                                                                                                                                                           |
| [Elastic](https://www.elastic.co/)                                                                                                      | Software & Technology                | Copilot for domain-specific task                              | [Blog post, 2025](https://www.elastic.co/blog/elastic-security-generative-ai-features)                                                                                                                                                                                                                                                        |
| [Exa](https://exa.ai/)                                                                                                                  | Software & Technology (GenAI Native) | Search                                                        | [Case study, 2025](https://blog.langchain.com/exa/)                                                                                                                                                                                                                                                                                           |
| [GitLab](https://about.gitlab.com/)                                                                                                     | Software & Technology                | Code generation                                               | [Duo workflow docs](https://handbook.gitlab.com/handbook/engineering/architecture/design-documents/duo_workflow/)                                                                                                                                                                                                                             |
| [Harmonic](https://harmonic.ai/)                                                                                                        | Software & Technology                | Search                                                        | [Case study, 2025](https://blog.langchain.com/customers-harmonic/)                                                                                                                                                                                                                                                                            |
| [Inconvo](https://inconvo.ai/?ref=blog.langchain.dev)                                                                                   | Software & Technology                | Code generation                                               | [Case study, 2025](https://blog.langchain.dev/customers-inconvo/)                                                                                                                                                                                                                                                                             |
| [Infor](https://infor.com/)                                                                                                             | Software & Technology                | GenAI embedded product experiences; customer support; copilot | [Case study, 2025](https://blog.langchain.dev/customers-infor/)                                                                                                                                                                                                                                                                               |
| [J.P. Morgan](https://www.jpmorganchase.com/)                                                                                           | Financial Services                   | Copilot for domain-specific task                              | [Interrupt talk, 2025](https://youtu.be/yMalr0jiOAc?feature=shared)                                                                                                                                                                                                                                                                           |
| [Klarna](https://www.klarna.com/)                                                                                                       | Fintech                              | Copilot for domain-specific task                              | [Case study, 2025](https://blog.langchain.dev/customers-klarna/)                                                                                                                                                                                                                                                                              |
| [Komodo Health](https://www.komodohealth.com/)                                                                                          | Healthcare                           | Copilot for domain-specific task                              | [Blog post](https://www.komodohealth.com/perspectives/new-gen-ai-assistant-empowers-the-enterprise/)                                                                                                                                                                                                                                          |
| [LinkedIn](https://www.linkedin.com/)                                                                                                   | Social Media                         | Code generation; Search & discovery                           | [Interrupt talk, 2025](https://youtu.be/NmblVxyBhi8?feature=shared); [Blog post, 2025](https://www.linkedin.com/blog/engineering/ai/practical-text-to-sql-for-data-analytics); [Blog post, 2024](https://www.linkedin.com/blog/engineering/generative-ai/behind-the-platform-the-journey-to-create-the-linkedin-genai-application-tech-stack) |
| [Minimal](https://gominimal.ai/)                                                                                                        | E-commerce                           | Customer support                                              | [Case study, 2025](https://blog.langchain.dev/how-minimal-built-a-multi-agent-customer-support-system-with-langgraph-langsmith/)                                                                                                                                                                                                              |
| [Modern Treasury](https://www.moderntreasury.com/)                                                                                      | Fintech                              | GenAI embedded product experiences                            | [Video story, 2025](https://youtu.be/AwAiffXqaCU?feature=shared)                                                                                                                                                                                                                                                                              |
| [Monday](https://monday.com/)                                                                                                           | Software & Technology                | GenAI embedded product experiences                            | [Interrupt talk, 2025](https://blog.langchain.dev/how-minimal-built-a-multi-agent-customer-support-system-with-langgraph-langsmith/)                                                                                                                                                                                                          |
| [Morningstar](https://www.morningstar.com/)                                                                                             | Financial Services                   | Research & summarization                                      | [Video story, 2025](https://youtu.be/6LidoFXCJPs?feature=shared)                                                                                                                                                                                                                                                                              |
| [OpenRecovery](https://www.openrecovery.com/)                                                                                           | Healthcare                           | Copilot for domain-specific task                              | [Case study, 2024](https://blog.langchain.dev/customers-openrecovery/)                                                                                                                                                                                                                                                                        |
| [Pigment](https://www.pigment.com/)                                                                                                     | Fintech                              | GenAI embedded product experiences                            | [Video story, 2025](https://youtu.be/5JVSO2KYOmE?feature=shared)                                                                                                                                                                                                                                                                              |
| [Prosper](https://www.prosper.com/)                                                                                                     | Fintech                              | Customer support                                              | [Video story, 2025](https://youtu.be/9RFNOYtkwsc?feature=shared)                                                                                                                                                                                                                                                                              |
| [Qodo](https://www.qodo.ai/)                                                                                                            | Software & Technology (GenAI Native) | Code generation                                               | [Blog post, 2025](https://www.qodo.ai/blog/why-we-chose-langgraph-to-build-our-coding-agent/)                                                                                                                                                                                                                                                 |
| [Rakuten](https://www.rakuten.com/)                                                                                                     | E-commerce / Fintech                 | Copilot for domain-specific task                              | [Video story, 2025](https://youtu.be/gD1LIjCkuA8?feature=shared); [Blog post, 2025](https://rakuten.today/blog/from-ai-hype-to-real-world-tools-rakuten-teams-up-with-langchain.html)                                                                                                                                                         |
| [Replit](https://replit.com/)                                                                                                           | Software & Technology                | Code generation                                               | [Blog post, 2024](https://blog.langchain.dev/customers-replit/); [Breakout agent story, 2024](https://www.langchain.com/breakoutagents/replit); [Fireside chat video, 2024](https://www.youtube.com/watch?v=ViykMqljjxU)                                                                                                                      |
| [Rexera](https://www.rexera.com/)                                                                                                       | Real Estate (GenAI Native)           | Copilot for domain-specific task                              | [Case study, 2024](https://blog.langchain.dev/customers-rexera/)                                                                                                                                                                                                                                                                              |
| [Abu Dhabi Government](https://www.tamm.abudhabi/)                                                                                      | Government                           | Search                                                        | [Case study, 2025](https://blog.langchain.com/customers-abu-dhabi-government/)                                                                                                                                                                                                                                                                |
| [Tradestack](https://www.tradestack.uk/)                                                                                                | Software & Technology (GenAI Native) | Copilot for domain-specific task                              | [Case study, 2024](https://blog.langchain.dev/customers-tradestack/)                                                                                                                                                                                                                                                                          |
| [Uber](https://www.uber.com/)                                                                                                           | Transportation                       | Developer productivity; Code generation                       | [Interrupt talk, 2025](https://youtu.be/Bugs0dVcNI8?feature=shared); [Presentation, 2024](https://dpe.org/sessions/ty-smith-adam-huda/this-year-in-ubers-ai-driven-developer-productivity-revolution/); [Video, 2024](https://www.youtube.com/watch?v=8rkA5vWUE4Y)                                                                            |
| [Unify](https://www.unifygtm.com/)                                                                                                      | Software & Technology (GenAI Native) | Copilot for domain-specific task                              | [Interrupt talk, 2025](https://youtu.be/pKk-LfhujwI?feature=shared); [Blog post, 2024](https://blog.langchain.dev/unify-launches-agents-for-account-qualification-using-langgraph-and-langsmith/)                                                                                                                                             |
| [Vizient](https://www.vizientinc.com/)                                                                                                  | Healthcare                           | Copilot for domain-specific task                              | [Video story, 2025](https://www.youtube.com/watch?v=vrjJ6NuyTWA); [Case study, 2025](https://blog.langchain.dev/p/3d2cd58c-13a5-4df9-bd84-7d54ed0ed82c/)                                                                                                                                                                                      |
| [Vodafone](https://www.vodafone.com/)                                                                                                   | Telecommunications                   | Code generation; internal search                              | [Case study, 2025](https://blog.langchain.dev/customers-vodafone/)                                                                                                                                                                                                                                                                            |
| [WebToon](https://www.webtoons.com/en/)                                                                                                 | Media & Entertainment                | Data extraction                                               | [Case study, 2025](https://blog.langchain.com/customers-webtoon/)                                                                                                                                                                                                                                                                             |
| [11x](https://www.11x.ai/)                                                                                                              | Software & Technology (GenAI Native) | Research & outreach                                           | [Interrupt talk, 2025](https://youtu.be/fegwPmaAPQk?feature=shared)                                                                                                                                                                                                                                                                           |

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/case-studies.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## ChatAnthropic

**URL:** llms-txt#chatanthropic

**Contents:**
- Overview
  - Integration details
  - Model features
- Setup
  - Credentials

Source: https://docs.langchain.com/oss/javascript/integrations/chat/anthropic

[Anthropic](https://www.anthropic.com/) is an AI safety and research company. They are the creator of Claude.

This will help you getting started with Anthropic [chat models](/oss/javascript/langchain/models). For detailed documentation of all `ChatAnthropic` features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_anthropic.ChatAnthropic.html).

### Integration details

| Class                                                                                        | Package                                                                      | Local | Serializable | [PY support](https://python.langchain.com/docs/integrations/chat/anthropic/) |                                               Downloads                                              |                                              Version                                              |
| :------------------------------------------------------------------------------------------- | :--------------------------------------------------------------------------- | :---: | :----------: | :--------------------------------------------------------------------------: | :--------------------------------------------------------------------------------------------------: | :-----------------------------------------------------------------------------------------------: |
| [ChatAnthropic](https://api.js.langchain.com/classes/langchain_anthropic.ChatAnthropic.html) | [`@langchain/anthropic`](https://www.npmjs.com/package/@langchain/anthropic) |   ❌   |       ✅      |                                       ✅                                      | ![NPM - Downloads](https://img.shields.io/npm/dm/@langchain/anthropic?style=flat-square\&label=%20&) | ![NPM - Version](https://img.shields.io/npm/v/@langchain/anthropic?style=flat-square\&label=%20&) |

See the links in the table headers below for guides on how to use specific features.

| [Tool calling](/oss/javascript/langchain/tools) | [Structured output](/oss/javascript/langchain/structured-output) | JSON mode | [Image input](/oss/javascript/langchain/messages#multimodal) | Audio input | Video input | [Token-level streaming](/oss/javascript/langchain/streaming/) | [Token usage](/oss/javascript/langchain/models#token-usage) | [Logprobs](/oss/javascript/langchain/models#log-probabilities) |
| :---------------------------------------------: | :--------------------------------------------------------------: | :-------: | :----------------------------------------------------------: | :---------: | :---------: | :-----------------------------------------------------------: | :---------------------------------------------------------: | :------------------------------------------------------------: |
|                        ✅                        |                                 ✅                                |     ❌     |                               ✅                              |      ❌      |      ❌      |                               ✅                               |                              ✅                              |                                ❌                               |

You'll need to sign up and obtain an [Anthropic API key](https://www.anthropic.com/), and install the `@langchain/anthropic` integration package.

Head to [Anthropic's website](https://www.anthropic.com/) to sign up to Anthropic and generate an API key. Once you've done this set the `ANTHROPIC_API_KEY` environment variable:

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```bash  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:
```

---

## ChatGoogleGenerativeAI

**URL:** llms-txt#chatgooglegenerativeai

**Contents:**
- Overview
  - Integration details
  - Model features
- Setup
  - Credentials

Source: https://docs.langchain.com/oss/javascript/integrations/chat/google_generative_ai

[Google AI](https://ai.google.dev/) offers a number of different chat models, including the powerful Gemini series. For information on the latest models, their features, context windows, etc. head to the [Google AI docs](https://ai.google.dev/gemini-api/docs/models/gemini).

This will help you getting started with `ChatGoogleGenerativeAI` [chat models](/oss/javascript/langchain/models). For detailed documentation of all `ChatGoogleGenerativeAI` features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_google_genai.ChatGoogleGenerativeAI.html).

### Integration details

| Class                                                                                                             | Package                                                                                     | Local | Serializable | [PY support](https://python.langchain.com/docs/integrations/chat/google_generative_ai) |                                                Downloads                                                |                                                Version                                               |
| :---------------------------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------------------ | :---: | :----------: | :------------------------------------------------------------------------------------: | :-----------------------------------------------------------------------------------------------------: | :--------------------------------------------------------------------------------------------------: |
| [ChatGoogleGenerativeAI](https://api.js.langchain.com/classes/langchain_google_genai.ChatGoogleGenerativeAI.html) | [@langchain/google-genai](https://api.js.langchain.com/modules/langchain_google_genai.html) |   ❌   |       ✅      |                                            ✅                                           | ![NPM - Downloads](https://img.shields.io/npm/dm/@langchain/google-genai?style=flat-square\&label=%20&) | ![NPM - Version](https://img.shields.io/npm/v/@langchain/google-genai?style=flat-square\&label=%20&) |

See the links in the table headers below for guides on how to use specific features.

| [Tool calling](/oss/javascript/langchain/tools) | [Structured output](/oss/javascript/langchain/structured-output) | JSON mode | [Image input](/oss/javascript/langchain/messages#multimodal) | Audio input | Video input | [Token-level streaming](/oss/javascript/langchain/streaming/) | [Token usage](/oss/javascript/langchain/models#token-usage) | [Logprobs](/oss/javascript/langchain/models#log-probabilities) |
| :---------------------------------------------: | :--------------------------------------------------------------: | :-------: | :----------------------------------------------------------: | :---------: | :---------: | :-----------------------------------------------------------: | :---------------------------------------------------------: | :------------------------------------------------------------: |
|                        ✅                        |                                 ✅                                |     ❌     |                               ✅                              |      ✅      |      ✅      |                               ✅                               |                              ✅                              |                                ❌                               |

You can access Google's `gemini` and `gemini-vision` models, as well as other
generative models in LangChain through `ChatGoogleGenerativeAI` class in the
`@langchain/google-genai` integration package.

<Tip>
  You can also access Google's `gemini` family of models via the LangChain VertexAI and VertexAI-web integrations. Click [here](/oss/javascript/integrations/chat/google_vertex_ai) to read the docs.
</Tip>

Get an API key here: [https://ai.google.dev/tutorials/setup](https://ai.google.dev/tutorials/setup)

Then set the `GOOGLE_API_KEY` environment variable:

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```bash  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:
```

---

## ChatOpenAI

**URL:** llms-txt#chatopenai

**Contents:**
- Overview
  - Integration details
  - Model features
- Setup
  - Credentials

Source: https://docs.langchain.com/oss/javascript/integrations/chat/openai

[OpenAI](https://en.wikipedia.org/wiki/OpenAI) is an artificial intelligence (AI) research laboratory.

This guide will help you getting started with ChatOpenAI [chat models](/oss/javascript/langchain/models). For detailed documentation of all ChatOpenAI features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_openai.ChatOpenAI.html).

<Note>
  **Chat Completions API compatibility**

`ChatOpenAI` is fully compatible with OpenAI's (legacy) [Chat Completions API](https://platform.openai.com/docs/guides/completions). If you are looking to connect to other model providers that support the Chat Completions API, you can do so – see [instructions](/oss/javascript/integrations/chat#chat-completions-api).
</Note>

<Info>
  **OpenAI models hosted on Azure**

Note that certain OpenAI models can also be accessed via the [Microsoft Azure platform](https://azure.microsoft.com/en-us/products/ai-foundry/models/openai/). To use the Azure OpenAI service use the [`AzureChatOpenAI`](/oss/javascript/integrations/chat/azure_chat_openai) integration.
</Info>

### Integration details

| Class                                                                               | Package                                                                | Local | Serializable | [PY support](https://python.langchain.com/docs/integrations/chat/openai) |                                             Downloads                                             |                                             Version                                            |
| :---------------------------------------------------------------------------------- | :--------------------------------------------------------------------- | :---: | :----------: | :----------------------------------------------------------------------: | :-----------------------------------------------------------------------------------------------: | :--------------------------------------------------------------------------------------------: |
| [ChatOpenAI](https://api.js.langchain.com/classes/langchain_openai.ChatOpenAI.html) | [`@langchain/openai`](https://www.npmjs.com/package/@langchain/openai) |   ❌   |       ✅      |                                     ✅                                    | ![NPM - Downloads](https://img.shields.io/npm/dm/@langchain/openai?style=flat-square\&label=%20&) | ![NPM - Version](https://img.shields.io/npm/v/@langchain/openai?style=flat-square\&label=%20&) |

See the links in the table headers below for guides on how to use specific features.

| [Tool calling](/oss/javascript/langchain/tools) | [Structured output](/oss/javascript/langchain/structured-output) | JSON mode | [Image input](/oss/javascript/langchain/messages#multimodal) | Audio input | Video input | [Token-level streaming](/oss/javascript/langchain/streaming/) | [Token usage](/oss/javascript/langchain/models#token-usage) | [Logprobs](/oss/javascript/langchain/models#log-probabilities) |
| :---------------------------------------------: | :--------------------------------------------------------------: | :-------: | :----------------------------------------------------------: | :---------: | :---------: | :-----------------------------------------------------------: | :---------------------------------------------------------: | :------------------------------------------------------------: |
|                        ✅                        |                                 ✅                                |     ✅     |                               ✅                              |      ❌      |      ❌      |                               ✅                               |                              ✅                              |                                ✅                               |

To access OpenAI chat models you'll need to create an OpenAI account, get an API key, and install the `@langchain/openai` integration package.

Head to [OpenAI's website](https://platform.openai.com/) to sign up for OpenAI and generate an API key. Once you've done this set the `OPENAI_API_KEY` environment variable:

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```bash  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:
```

---

## Chat models

**URL:** llms-txt#chat-models

**Contents:**
- Install and use
- Featured providers
- Chat Completions API
- All chat models

Source: https://docs.langchain.com/oss/javascript/integrations/chat/index

[Chat models](/oss/javascript/langchain/models) are language models that use a sequence of [messages](/oss/javascript/langchain/messages) as inputs and return messages as outputs <Tooltip tip="Models that do not include the prefix 'Chat' in their name or include 'LLM' as a suffix in their name typically refer to older models that do not follow the chat model interface and instead use an interface that takes a string as input and returns a string as output.">(as opposed to plaintext)</Tooltip>.

<Tip>
  See [this section for general instructions on installing LangChain packages](/oss/javascript/langchain/install).
</Tip>

<AccordionGroup>
  <Accordion title="OpenAI">
    Install:

Add environment variables:

Instantiate the model:

<Accordion title="Anthropic">
    Install:

Add environment variables:

Instantiate the model:

<Accordion title="Google Gemini">
    Install:

Add environment variables:

Instantiate the model:

<Accordion title="Google VertexAI">
    Install:

Add environment variables:

Instantiate the model:

<Accordion title="MistralAI">
    Install:

Add environment variables:

Instantiate the model:

<Accordion title="FireworksAI">
    Install:

Add environment variables:

Instantiate the model:

<Accordion title="Groq">
    Install:

Add environment variables:

Instantiate the model:

</Accordion>
</AccordionGroup>

## Featured providers

| Model                                                                                | Stream | JSON mode | [Tool Calling](/oss/javascript/langchain/tools/) | [`withStructuredOutput()`](/oss/javascript/langchain/structured-output#the-.withstructuredoutput-method) | [Multimodal](/oss/javascript/langchain/messages#multimodal) |
| :----------------------------------------------------------------------------------- | :----: | :-------: | :----------------------------------------------: | :------------------------------------------------------------------------------------------------------: | :---------------------------------------------------------: |
| [`BedrockChat`](/oss/javascript/integrations/chat/bedrock/)                          |    ✅   |     ❌     |            🟡 (Bedrock Anthropic only)           |                                        🟡 (Bedrock Anthropic only)                                       |                 🟡 (Bedrock Anthropic only)                 |
| [`ChatBedrockConverse`](/oss/javascript/integrations/chat/bedrock_converse/)         |    ✅   |     ❌     |                         ✅                        |                                                     ✅                                                    |                              ✅                              |
| [`ChatAnthropic`](/oss/javascript/integrations/chat/anthropic/)                      |    ✅   |     ❌     |                         ✅                        |                                                     ✅                                                    |                              ✅                              |
| [`ChatCloudflareWorkersAI`](/oss/javascript/integrations/chat/cloudflare_workersai/) |    ✅   |     ❌     |                         ❌                        |                                                     ❌                                                    |                              ❌                              |
| [`ChatCohere`](/oss/javascript/integrations/chat/cohere/)                            |    ✅   |     ❌     |                         ✅                        |                                                     ✅                                                    |                              ✅                              |
| [`ChatFireworks`](/oss/javascript/integrations/chat/fireworks/)                      |    ✅   |     ✅     |                         ✅                        |                                                     ✅                                                    |                              ✅                              |
| [`ChatGoogleGenerativeAI`](/oss/javascript/integrations/chat/google_generative_ai/)  |    ✅   |     ❌     |                         ✅                        |                                                     ✅                                                    |                              ✅                              |
| [`ChatVertexAI`](/oss/javascript/integrations/chat/google_vertex_ai/)                |    ✅   |     ❌     |                         ✅                        |                                                     ✅                                                    |                              ✅                              |
| [`ChatGroq`](/oss/javascript/integrations/chat/groq/)                                |    ✅   |     ✅     |                         ✅                        |                                                     ✅                                                    |                              ✅                              |
| [`ChatMistralAI`](/oss/javascript/integrations/chat/mistral/)                        |    ✅   |     ✅     |                         ✅                        |                                                     ✅                                                    |                              ✅                              |
| [`ChatOllama`](/oss/javascript/integrations/chat/ollama/)                            |    ✅   |     ✅     |                         ✅                        |                                                     ✅                                                    |                              ✅                              |
| [`ChatOpenAI`](/oss/javascript/integrations/chat/openai/)                            |    ✅   |     ✅     |                         ✅                        |                                                     ✅                                                    |                              ✅                              |
| [`ChatTogetherAI`](/oss/javascript/integrations/chat/togetherai/)                    |    ✅   |     ✅     |                         ✅                        |                                                     ✅                                                    |                              ✅                              |
| [`ChatXAI`](/oss/javascript/integrations/chat/xai/)                                  |    ✅   |     ✅     |                         ✅                        |                                                     ✅                                                    |                              ❌                              |

## Chat Completions API

Certain model providers offer endpoints that are compatible with OpenAI's (legacy) [Chat Completions API](https://platform.openai.com/docs/guides/completions). In such case, you can use [`ChatOpenAI`](/oss/javascript/integrations/chat/openai) with a custom `base_url` to connect to these endpoints.

<Accordion title="Example: OpenRouter">
  To use OpenRouter, you will need to sign up for an account and obtain an [API key](https://openrouter.ai/docs/api-reference/authentication).

Refer to the [OpenRouter documentation](https://openrouter.ai/docs/quickstart) for more details.
</Accordion>

<Columns cols={3}>
  <Card title="Alibaba Tongyi" icon="link" href="/oss/javascript/integrations/chat/alibaba_tongyi" arrow="true" cta="View guide" />

<Card title="Anthropic" icon="link" href="/oss/javascript/integrations/chat/anthropic" arrow="true" cta="View guide" />

<Card title="Arcjet Redact" icon="link" href="/oss/javascript/integrations/chat/arcjet" arrow="true" cta="View guide" />

<Card title="Azure OpenAI" icon="link" href="/oss/javascript/integrations/chat/azure" arrow="true" cta="View guide" />

<Card title="Baidu Qianfan" icon="link" href="/oss/javascript/integrations/chat/baidu_qianfan" arrow="true" cta="View guide" />

<Card title="Amazon Bedrock" icon="link" href="/oss/javascript/integrations/chat/bedrock" arrow="true" cta="View guide" />

<Card title="Amazon Bedrock Converse" icon="link" href="/oss/javascript/integrations/chat/bedrock_converse" arrow="true" cta="View guide" />

<Card title="Cerebras" icon="link" href="/oss/javascript/integrations/chat/cerebras" arrow="true" cta="View guide" />

<Card title="Cloudflare Workers AI" icon="link" href="/oss/javascript/integrations/chat/cloudflare_workersai" arrow="true" cta="View guide" />

<Card title="Cohere" icon="link" href="/oss/javascript/integrations/chat/cohere" arrow="true" cta="View guide" />

<Card title="Deep Infra" icon="link" href="/oss/javascript/integrations/chat/deep_infra" arrow="true" cta="View guide" />

<Card title="DeepSeek" icon="link" href="/oss/javascript/integrations/chat/deepseek" arrow="true" cta="View guide" />

<Card title="Fake LLM" icon="link" href="/oss/javascript/integrations/chat/fake" arrow="true" cta="View guide" />

<Card title="Fireworks" icon="link" href="/oss/javascript/integrations/chat/fireworks" arrow="true" cta="View guide" />

<Card title="Friendli" icon="link" href="/oss/javascript/integrations/chat/friendli" arrow="true" cta="View guide" />

<Card title="Google GenAI" icon="link" href="/oss/javascript/integrations/chat/google_generative_ai" arrow="true" cta="View guide" />

<Card title="Google Vertex AI" icon="link" href="/oss/javascript/integrations/chat/google_vertex_ai" arrow="true" cta="View guide" />

<Card title="Groq" icon="link" href="/oss/javascript/integrations/chat/groq" arrow="true" cta="View guide" />

<Card title="IBM watsonx.ai" icon="link" href="/oss/javascript/integrations/chat/ibm" arrow="true" cta="View guide" />

<Card title="Llama CPP" icon="link" href="/oss/javascript/integrations/chat/llama_cpp" arrow="true" cta="View guide" />

<Card title="Minimax" icon="link" href="/oss/javascript/integrations/chat/minimax" arrow="true" cta="View guide" />

<Card title="MistralAI" icon="link" href="/oss/javascript/integrations/chat/mistral" arrow="true" cta="View guide" />

<Card title="Moonshot" icon="link" href="/oss/javascript/integrations/chat/moonshot" arrow="true" cta="View guide" />

<Card title="Novita AI" icon="link" href="/oss/javascript/integrations/chat/novita" arrow="true" cta="View guide" />

<Card title="Ollama" icon="link" href="/oss/javascript/integrations/chat/ollama" arrow="true" cta="View guide" />

<Card title="OpenAI" icon="link" href="/oss/javascript/integrations/chat/openai" arrow="true" cta="View guide" />

<Card title="Perplexity" icon="link" href="/oss/javascript/integrations/chat/perplexity" arrow="true" cta="View guide" />

<Card title="PremAI" icon="link" href="/oss/javascript/integrations/chat/premai" arrow="true" cta="View guide" />

<Card title="Tencent Hunyuan" icon="link" href="/oss/javascript/integrations/chat/tencent_hunyuan" arrow="true" cta="View guide" />

<Card title="Together" icon="link" href="/oss/javascript/integrations/chat/togetherai" arrow="true" cta="View guide" />

<Card title="WebLLM" icon="link" href="/oss/javascript/integrations/chat/web_llm" arrow="true" cta="View guide" />

<Card title="xAI" icon="link" href="/oss/javascript/integrations/chat/xai" arrow="true" cta="View guide" />

<Card title="YandexGPT" icon="link" href="/oss/javascript/integrations/chat/yandex" arrow="true" cta="View guide" />

<Card title="ZhipuAI" icon="link" href="/oss/javascript/integrations/chat/zhipuai" arrow="true" cta="View guide" />
</Columns>

<Info>
  If you'd like to contribute an integration, see [Contributing integrations](/oss/javascript/contributing#add-a-new-integration).
</Info>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/javascript/integrations/chat/index.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
</CodeGroup>

    Add environment variables:
```

Example 4 (unknown):
```unknown
Instantiate the model:
```

---

## Checkpointer is REQUIRED for human-in-the-loop

**URL:** llms-txt#checkpointer-is-required-for-human-in-the-loop

**Contents:**
- Decision types
- Handle interrupts

checkpointer = MemorySaver()

agent = create_deep_agent(
    model="claude-sonnet-4-5-20250929",
    tools=[delete_file, read_file, send_email],
    interrupt_on={
        "delete_file": True,  # Default: approve, edit, reject
        "read_file": False,   # No interrupts needed
        "send_email": {"allowed_decisions": ["approve", "reject"]},  # No editing
    },
    checkpointer=checkpointer  # Required!
)
python  theme={null}
interrupt_on = {
    # Sensitive operations: allow all options
    "delete_file": {"allowed_decisions": ["approve", "edit", "reject"]},

# Moderate risk: approval or rejection only
    "write_file": {"allowed_decisions": ["approve", "reject"]},

# Must approve (no rejection allowed)
    "critical_operation": {"allowed_decisions": ["approve"]},
}
python  theme={null}
import uuid
from langgraph.types import Command

**Examples:**

Example 1 (unknown):
```unknown
## Decision types

The `allowed_decisions` list controls what actions a human can take when reviewing a tool call:

* **`"approve"`**: Execute the tool with the original arguments as proposed by the agent
* **`"edit"`**: Modify the tool arguments before execution
* **`"reject"`**: Skip executing this tool call entirely

You can customize which decisions are available for each tool:
```

Example 2 (unknown):
```unknown
## Handle interrupts

When an interrupt is triggered, the agent pauses execution and returns control. Check for interrupts in the result and handle them accordingly.
```

---

## Check if execution was interrupted

**URL:** llms-txt#check-if-execution-was-interrupted

if result.get("__interrupt__"):
    # Extract interrupt information
    interrupts = result["__interrupt__"][0].value
    action_requests = interrupts["action_requests"]
    review_configs = interrupts["review_configs"]

# Create a lookup map from tool name to review config
    config_map = {cfg["action_name"]: cfg for cfg in review_configs}

# Display the pending actions to the user
    for action in action_requests:
        review_config = config_map[action["name"]]
        print(f"Tool: {action['name']}")
        print(f"Arguments: {action['args']}")
        print(f"Allowed decisions: {review_config['allowed_decisions']}")

# Get user decisions (one per action_request, in order)
    decisions = [
        {"type": "approve"}  # User approved the deletion
    ]

# Resume execution with decisions
    result = agent.invoke(
        Command(resume={"decisions": decisions}),
        config=config  # Must use the same config!
    )

---

## Check if 'is_concise' returned False.

**URL:** llms-txt#check-if-'is_concise'-returned-false.

failed = [r for r in results if not r["evaluation_results"]["results"][0].score]

---

## Check what was interrupted

**URL:** llms-txt#check-what-was-interrupted

---

## Claude Code Logs are translated to Spans by LangSmith

**URL:** llms-txt#claude-code-logs-are-translated-to-spans-by-langsmith

export OTEL_EXPORTER_OTLP_LOGS_ENDPOINT=https://api.smith.langchain.com/otel/v1/claude_code

---

## client.py

**URL:** llms-txt#client.py

from langsmith.run_helpers import get_current_run_tree, traceable
import httpx

@traceable
async def my_client_function():
    headers = {}
    async with httpx.AsyncClient(base_url="...") as client:
        if run_tree := get_current_run_tree():
            # add langsmith-id to headers
            headers.update(run_tree.to_headers())
        return await client.post("/my-route", headers=headers)
python  theme={null}
from langsmith import traceable
from langsmith.middleware import TracingMiddleware
from fastapi import FastAPI, Request

app = FastAPI()  # Or Flask, Django, or any other framework
app.add_middleware(TracingMiddleware)

@traceable
async def some_function():
    ...

@app.post("/my-route")
async def fake_route(request: Request):
    return await some_function()
python  theme={null}
from starlette.applications import Starlette
from starlette.middleware import Middleware
from langsmith.middleware import TracingMiddleware

routes = ...
middleware = [
    Middleware(TracingMiddleware),
]
app = Starlette(..., middleware=middleware)
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
Then the server (or other service) can continue the trace by handling the headers appropriately. If you are using an asgi app Starlette or FastAPI, you can connect the distributed trace using LangSmith's `TracingMiddleware`.

<Info>
  The `TracingMiddleware` class was added in `langsmith==0.1.133`.
</Info>

Example using FastAPI:
```

Example 2 (unknown):
```unknown
Or in Starlette:
```

Example 3 (unknown):
```unknown
If you are using other server frameworks, you can always "receive" the distributed trace by passing the headers in through `langsmith_extra`:
```

---

## Cloud

**URL:** llms-txt#cloud

**Contents:**
- Get started
- Cloud architecture and scalability
  - Architecture
  - Allowlisting IP addresses

Source: https://docs.langchain.com/langsmith/cloud

<Callout icon="rocket" color="#4F46E5" iconType="regular">
  If you're ready to deploy your app to Cloud, follow the [Cloud deployment quickstart](/langsmith/deployment-quickstart) or the [full setup guide](/langsmith/deploy-to-cloud). This page explains the Cloud managed architecture for reference.
</Callout>

The **Cloud** option is a fully managed model where LangChain hosts and operates all LangSmith infrastructure and services:

* **Fully managed infrastructure**: LangChain handles all infrastructure, updates, scaling, and maintenance.
* **Deploy from GitHub**: Connect your repositories and deploy with a few clicks.
* **Automated CI/CD**: Build process is handled automatically by the platform.
* **LangSmith UI**: Full access to [observability](/langsmith/observability), [evaluation](/langsmith/evaluation), [deployment management](/langsmith/deployments), and [Studio](/langsmith/studio).

|                                               | **Who manages it** | **Where it runs** |
| --------------------------------------------- | ------------------ | ----------------- |
| **LangSmith platform (UI, APIs, datastores)** | LangChain          | LangChain's cloud |
| **Your Agent Servers**                        | LangChain          | LangChain's cloud |
| **CI/CD for your apps**                       | LangChain          | LangChain's cloud |

<img src="https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/langgraph-cloud-architecture.png?fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=3f0316122425895270d0ecd47b12e139" alt="Cloud deployment: LangChain hosts and manages all components including the UI, APIs, and your Agent Servers." data-og-width="1425" width="1425" data-og-height="1063" height="1063" data-path="langsmith/images/langgraph-cloud-architecture.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/langgraph-cloud-architecture.png?w=280&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=b34a10fb40ca5188dddfb6be69696c75 280w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/langgraph-cloud-architecture.png?w=560&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=d310ce86e8421878575cf4d4fa72bf78 560w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/langgraph-cloud-architecture.png?w=840&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=7cc3fcee9c36a8d1e8da4cc4abbde3b9 840w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/langgraph-cloud-architecture.png?w=1100&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=a184c950c8585e2fdb5e75ac7f0f5642 1100w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/langgraph-cloud-architecture.png?w=1650&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=cb778e4a6707b00eaf703886d32569bd 1650w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/langgraph-cloud-architecture.png?w=2500&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=00f9d9df44512dded307613502d03299 2500w" />

To deploy your first application to Cloud, follow the [Cloud deployment quickstart](/langsmith/deployment-quickstart) or refer to the [comprehensive setup guide](/langsmith/deploy-to-cloud).

## Cloud architecture and scalability

<Note>
  This section is only relevant for the cloud-managed LangSmith services available at [https://smith.langchain.com](https://smith.langchain.com) and [https://eu.smith.langchain.com](https://eu.smith.langchain.com).

For information on the self-hosted LangSmith solution, please refer to the [self-hosted documentation](/langsmith/self-hosted).
</Note>

LangSmith is deployed on Google Cloud Platform (GCP) and is designed to be highly scalable. Many customers run production workloads on LangSmith for LLM application observability, evaluation, and agent deployment

The US-based LangSmith service is deployed in the `us-central1` (Iowa) region of GCP.

<Note>
  The [EU-based LangSmith service](https://eu.smith.langchain.com) is now available (as of mid-July 2024) and is deployed in the `europe-west4` (Netherlands) region of GCP. If you are interested in an enterprise plan in this region, [contact our sales team](https://www.langchain.com/contact-sales).
</Note>

#### Regional storage

The resources and services in this table are stored in the location corresponding to the URL where sign-up occurred (either the US or EU). Cloud-managed LangSmith uses [Supabase](https://supabase.com) for authentication/authorization and [ClickHouse Cloud](https://clickhouse.com/cloud) for data warehouse.

|                                                | US                                                                 | EU                                                                       |
| ---------------------------------------------- | ------------------------------------------------------------------ | ------------------------------------------------------------------------ |
| URL                                            | [https://smith.langchain.com](https://smith.langchain.com)         | [https://eu.smith.langchain.com](https://eu.smith.langchain.com)         |
| API URL                                        | [https://api.smith.langchain.com](https://api.smith.langchain.com) | [https://eu.api.smith.langchain.com](https://eu.api.smith.langchain.com) |
| GCP                                            | us-central1 (Iowa)                                                 | europe-west4 (Netherlands)                                               |
| Supabase                                       | AWS us-east-1 (N. Virginia)                                        | AWS eu-central-1 (Germany)                                               |
| ClickHouse Cloud                               | us-central1 (Iowa)                                                 | europe-west4 (Netherlands)                                               |
| [LangSmith deployment](/langsmith/deployments) | us-central1 (Iowa)                                                 | europe-west4 (Netherlands)                                               |

See the [Regions FAQ](/langsmith/regions-faq) for more information.

#### Region-independent storage

Data listed here is stored exclusively in the US:

* Payment and billing information with Stripe and Metronome

LangSmith is composed of the following services, all deployed on Google Kubernetes Engine (GKE):

* LangSmith Frontend: serves the LangSmith UI.
* LangSmith Backend: serves the LangSmith API.
* LangSmith Platform Backend: handles authentication and other high-volume tasks. (Internal service)
* LangSmith Playground: handles forwarding requests to various LLM providers for the Playground feature.
* LangSmith Queue: handles processing of asynchronous tasks. (Internal service)

LangSmith uses the following GCP storage services:

* Google Cloud Storage (GCS) for runs inputs and outputs.
* Google Cloud SQL PostgreSQL for transactional workloads.
* Google Cloud Memorystore for Redis for queuing and caching.
* Clickhouse Cloud on GCP for trace ingestion and analytics. Our services connect to Clickhouse Cloud, which is hosted in the same GCP region, via a private endpoint.

Some additional GCP services we use include:

* Google Cloud Load Balancer for routing traffic to the LangSmith services.
* Google Cloud CDN for caching static assets.
* Google Cloud Armor for security and rate limits. For more information on rate limits we enforce, please refer to [this guide](/langsmith/administration-overview#rate-limits).

<div style={{ textAlign: 'center' }}>
  <img className="block dark:hidden" src="https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-light.png?fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=0790cbdf4fe131c74d1e60bb120834e3" alt="Light mode overview" data-og-width="2210" width="2210" data-og-height="1463" height="1463" data-path="langsmith/images/cloud-arch-light.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-light.png?w=280&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=c04d8a044d221559fe2f7b9121275638 280w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-light.png?w=560&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=a15351b254f11cc149ce237ba8853e91 560w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-light.png?w=840&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=d4a409e73830e588519cb1d0b2a17f3b 840w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-light.png?w=1100&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=6dbeda77b57083efb988e15af38f0a6e 1100w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-light.png?w=1650&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=24aadbe2e79db02d76fd5deaea6564e1 1650w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-light.png?w=2500&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=a126aa1f02d36de0a8e391f0e1059b8e 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-dark.png?fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=767f3bc3dc73ffe1a806f54e0aaa428b" alt="Dark mode overview" data-og-width="2210" width="2210" data-og-height="1463" height="1463" data-path="langsmith/images/cloud-arch-dark.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-dark.png?w=280&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=f7367df5b782c821882605418c50563f 280w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-dark.png?w=560&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=60759ef9e927ba0985e21e38acacae6d 560w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-dark.png?w=840&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=383ac38ba52733548d8d97ffabfe384e 840w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-dark.png?w=1100&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=b045b8e19a9926d4d10ec8ad2d2767c1 1100w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-dark.png?w=1650&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=23778aa891c1b42336b274ab1b2f8bec 1650w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-dark.png?w=2500&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=5a64734b4e9fb5dd4af690edf3fa6248 2500w" />
</div>

### Allowlisting IP addresses

#### Egress from LangChain SaaS

All traffic leaving LangSmith services will be routed through a NAT gateway. All traffic will appear to originate from the following IP addresses:

| US             | EU             |
| -------------- | -------------- |
| 34.59.65.97    | 34.13.192.67   |
| 34.67.51.221   | 34.147.105.64  |
| 34.46.212.37   | 34.90.22.166   |
| 34.132.150.88  | 34.147.36.213  |
| 35.188.222.201 | 34.32.137.113  |
| 34.58.194.127  | 34.91.238.184  |
| 34.59.97.173   | 35.204.101.241 |
| 104.198.162.55 | 35.204.48.32   |

It may be helpful to allowlist these IP addresses if connecting to your own AzureOpenAI service or other endpoints that may be required by the Playground or Online Evaluation.

#### Ingress into LangChain SaaS

The langchain endpoints map to the following static IP addresses:

| US             | EU           |
| -------------- | ------------ |
| 34.8.121.39    | 34.95.92.214 |
| 34.107.251.234 | 34.13.73.122 |

You may need to allowlist these to enable traffic from your private network to LangSmith SaaS endpoints (`api.smith.langchain.com`, `smith.langchain.com`, `beacon.langchain.com`, `eu.api.smith.langchain.com`, `eu.smith.langchain.com`, `eu.beacon.langchain.com`).

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/cloud.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Combine input and output

**URL:** llms-txt#combine-input-and-output

class OverallState(InputState, OutputState):
    pass

---

## Compare traces

**URL:** llms-txt#compare-traces

Source: https://docs.langchain.com/langsmith/compare-traces

To compare traces, click on the **Compare** button in the upper right hand side of any trace view.

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-button.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=794720bc9ef293984be25f19bffc89e7" alt="" data-og-width="2936" width="2936" data-og-height="1860" height="1860" data-path="langsmith/images/compare-button.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-button.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=f64c79f89cf94802ed0676cb4d175be7 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-button.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=111db8eecaadd0458001072616ef95fa 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-button.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=1df1b17af22d13e24a613bea71cb2e98 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-button.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=476560e917219525466629c49284de13 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-button.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=12cf8bf414cd5abd48355693f93d3bcf 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-button.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=7242812503868254fde6688b27e3586f 2500w" />

This will show the trace run table. Select the trace you want to compare against the original trace.

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-trace.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=89c9c16780ac8d129736ee800124625a" alt="" data-og-width="2388" width="2388" data-og-height="1856" height="1856" data-path="langsmith/images/select-trace.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-trace.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=1a7d4eb56fb4c2f8814b0584bce967b1 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-trace.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=72c89c532151d35facb58cb62b825213 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-trace.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=acd522e4aa3f54caaa3ef4036eb654b2 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-trace.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=aac705cc1574309b670d54e96bff666f 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-trace.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=db82f5ea3c9ea13b3919f892db65bd9c 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-trace.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=b926b0a266e8945063a71a7496034e97 2500w" />

The pane will open with both traces selected in a side by side comparison view.

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-trace.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=0f3b4479d399d18c3d6d48fb91681f73" alt="" data-og-width="2930" width="2930" data-og-height="1868" height="1868" data-path="langsmith/images/compare-trace.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-trace.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=05c151e7c9d0733ca6bfee6b6ce5ccc6 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-trace.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=579b34d6aa74caea5e055cba22f6fea8 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-trace.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=9a06b98a43f3a195eed05292cacae2d2 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-trace.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=968d1316e524123352412791515af047 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-trace.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=29065b3b65b98841a47ebe6487a26974 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-trace.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=dfe7e02a6db6693d4ff8490e170014bf 2500w" />

To stop comparing, close the pane or click on **Stop comparing** in the upper right hand side of the pane.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/compare-traces.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Compile

**URL:** llms-txt#compile

**Contents:**
  - 1. Run the graph

checkpointer = InMemorySaver()
graph = workflow.compile(checkpointer=checkpointer)
graph
python  theme={null}
config = {
    "configurable": {
        "thread_id": uuid.uuid4(),
    }
}
state = graph.invoke({}, config)

print(state["topic"])
print()
print(state["joke"])

How about "The Secret Life of Socks in the Dryer"? You know, exploring the mysterious phenomenon of how socks go into the laundry as pairs but come out as singles. Where do they go? Are they starting new lives elsewhere? Is there a sock paradise we don't know about? There's a lot of comedic potential in the everyday mystery that unites us all!

**Examples:**

Example 1 (unknown):
```unknown
### 1. Run the graph
```

Example 2 (unknown):
```unknown
**Output:**
```

---

## Compile the graph with the checkpointer and store

**URL:** llms-txt#compile-the-graph-with-the-checkpointer-and-store

graph = graph.compile(checkpointer=checkpointer, store=in_memory_store)
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
We invoke the graph with a `thread_id`, as before, and also with a `user_id`, which we'll use to namespace our memories to this particular user as we showed above.
```

---

## Compile the workflow

**URL:** llms-txt#compile-the-workflow

orchestrator_worker = orchestrator_worker_builder.compile()

---

## Compile with checkpointer for persistence

**URL:** llms-txt#compile-with-checkpointer-for-persistence

**Contents:**
- 3. Test the graph locally

graph = builder.compile(checkpointer=checkpointer)
python  theme={null}
from IPython.display import display, Image

display(Image(graph.get_graph().draw_mermaid_png()))
python {highlight={2,13}} theme={null}

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
<img src="https://mintcdn.com/langchain-5e9cc07a/IMK8wJkjSpMCGODD/langsmith/images/autogen-output.png?fit=max&auto=format&n=IMK8wJkjSpMCGODD&q=85&s=1165c5d1a5c154b2491d6a5fca30853f" alt="LangGraph chatbot with one step: START routes to autogen, where call_autogen_agent sends the latest user message (with prior context) to the AutoGen agent." data-og-width="180" width="180" data-og-height="134" height="134" data-path="langsmith/images/autogen-output.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/IMK8wJkjSpMCGODD/langsmith/images/autogen-output.png?w=280&fit=max&auto=format&n=IMK8wJkjSpMCGODD&q=85&s=6a6671038776cd1784c968ee2ecf973e 280w, https://mintcdn.com/langchain-5e9cc07a/IMK8wJkjSpMCGODD/langsmith/images/autogen-output.png?w=560&fit=max&auto=format&n=IMK8wJkjSpMCGODD&q=85&s=94c98b5b118ae49006d2f56179e1dc0d 560w, https://mintcdn.com/langchain-5e9cc07a/IMK8wJkjSpMCGODD/langsmith/images/autogen-output.png?w=840&fit=max&auto=format&n=IMK8wJkjSpMCGODD&q=85&s=703b4822dfc1c3395c16dd9e7d0f1462 840w, https://mintcdn.com/langchain-5e9cc07a/IMK8wJkjSpMCGODD/langsmith/images/autogen-output.png?w=1100&fit=max&auto=format&n=IMK8wJkjSpMCGODD&q=85&s=0f6b4e65d2f036d5b28dde44afbb5fd8 1100w, https://mintcdn.com/langchain-5e9cc07a/IMK8wJkjSpMCGODD/langsmith/images/autogen-output.png?w=1650&fit=max&auto=format&n=IMK8wJkjSpMCGODD&q=85&s=17f044c125e4a480d4bd8814c19a0949 1650w, https://mintcdn.com/langchain-5e9cc07a/IMK8wJkjSpMCGODD/langsmith/images/autogen-output.png?w=2500&fit=max&auto=format&n=IMK8wJkjSpMCGODD&q=85&s=841b246f7eabe796de9c4ac0af4816dd 2500w" />

## 3. Test the graph locally

Before deploying to LangSmith, you can test the graph locally:
```

---

## Composite evaluators

**URL:** llms-txt#composite-evaluators

**Contents:**
- Create a composite evaluator using the UI
  - 1. Navigate to the tracing project or dataset
  - 2. Configure the composite evaluator
  - 3. View composite evaluator results
- Create composite feedback with the SDK
  - 1. Configure evaluators on a dataset
  - 2. Create composite feedback

Source: https://docs.langchain.com/langsmith/composite-evaluators

*Composite evaluators* are a way to combine multiple evaluator scores into a single [score](/langsmith/evaluation-concepts#evaluator-outputs). This is useful when you want to evaluate multiple aspects of your application and combine the results into a single result.

## Create a composite evaluator using the UI

You can create composite evaluators on a [tracing project](/langsmith/observability-concepts#projects) (for [online evaluations](/langsmith/evaluation-concepts#online-evaluation)) or a [dataset](/langsmith/evaluation-concepts#datasets) (for [offline evaluations](/langsmith/evaluation-concepts#offline-evaluation)). With composite evaluators in the UI, you can compute a weighted average or weighted sum of multiple evaluator scores, with configurable weights.

<div style={{ textAlign: 'center' }}>
  <img className="block dark:hidden" src="https://mintcdn.com/langchain-5e9cc07a/cRRwi1N4-QohYC73/langsmith/images/create_composite_evaluator-light.png?fit=max&auto=format&n=cRRwi1N4-QohYC73&q=85&s=b3859ada8b576ebeaf5399ff15359b10" alt="LangSmith UI showing an LLM call trace called ChatOpenAI with a system and human input followed by an AI Output." data-og-width="756" width="756" data-og-height="594" height="594" data-path="langsmith/images/create_composite_evaluator-light.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/cRRwi1N4-QohYC73/langsmith/images/create_composite_evaluator-light.png?w=280&fit=max&auto=format&n=cRRwi1N4-QohYC73&q=85&s=9bab5ad812328acdd6ffe858f487262b 280w, https://mintcdn.com/langchain-5e9cc07a/cRRwi1N4-QohYC73/langsmith/images/create_composite_evaluator-light.png?w=560&fit=max&auto=format&n=cRRwi1N4-QohYC73&q=85&s=4637a2dc732f945d98b0214023266180 560w, https://mintcdn.com/langchain-5e9cc07a/cRRwi1N4-QohYC73/langsmith/images/create_composite_evaluator-light.png?w=840&fit=max&auto=format&n=cRRwi1N4-QohYC73&q=85&s=c3e7b24dde21ed45f481b7a513ecc256 840w, https://mintcdn.com/langchain-5e9cc07a/cRRwi1N4-QohYC73/langsmith/images/create_composite_evaluator-light.png?w=1100&fit=max&auto=format&n=cRRwi1N4-QohYC73&q=85&s=1310a99e2a8b37d68d78f794b8ce6606 1100w, https://mintcdn.com/langchain-5e9cc07a/cRRwi1N4-QohYC73/langsmith/images/create_composite_evaluator-light.png?w=1650&fit=max&auto=format&n=cRRwi1N4-QohYC73&q=85&s=6beb89dcc6ec734b2ad012bc46c58821 1650w, https://mintcdn.com/langchain-5e9cc07a/cRRwi1N4-QohYC73/langsmith/images/create_composite_evaluator-light.png?w=2500&fit=max&auto=format&n=cRRwi1N4-QohYC73&q=85&s=ba7fd7ba48a3e46d8701b6f64bb68f66 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/langchain-5e9cc07a/cRRwi1N4-QohYC73/langsmith/images/create_composite_evaluator-dark.png?fit=max&auto=format&n=cRRwi1N4-QohYC73&q=85&s=ac13f4d2d4a5e3b67285284150b7d592" alt="LangSmith UI showing an LLM call trace called ChatOpenAI with a system and human input followed by an AI Output." data-og-width="761" width="761" data-og-height="585" height="585" data-path="langsmith/images/create_composite_evaluator-dark.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/cRRwi1N4-QohYC73/langsmith/images/create_composite_evaluator-dark.png?w=280&fit=max&auto=format&n=cRRwi1N4-QohYC73&q=85&s=bfc19d802f0327a579d90e519441cf9a 280w, https://mintcdn.com/langchain-5e9cc07a/cRRwi1N4-QohYC73/langsmith/images/create_composite_evaluator-dark.png?w=560&fit=max&auto=format&n=cRRwi1N4-QohYC73&q=85&s=23ab26db75e25795c17abf07e487ba5d 560w, https://mintcdn.com/langchain-5e9cc07a/cRRwi1N4-QohYC73/langsmith/images/create_composite_evaluator-dark.png?w=840&fit=max&auto=format&n=cRRwi1N4-QohYC73&q=85&s=7ce9597b62f3e68b2dc1afa5f17f0e8c 840w, https://mintcdn.com/langchain-5e9cc07a/cRRwi1N4-QohYC73/langsmith/images/create_composite_evaluator-dark.png?w=1100&fit=max&auto=format&n=cRRwi1N4-QohYC73&q=85&s=ee7058d60185a820fe23decf003bd2c1 1100w, https://mintcdn.com/langchain-5e9cc07a/cRRwi1N4-QohYC73/langsmith/images/create_composite_evaluator-dark.png?w=1650&fit=max&auto=format&n=cRRwi1N4-QohYC73&q=85&s=cff38ad541c55d6834edfa67f5650818 1650w, https://mintcdn.com/langchain-5e9cc07a/cRRwi1N4-QohYC73/langsmith/images/create_composite_evaluator-dark.png?w=2500&fit=max&auto=format&n=cRRwi1N4-QohYC73&q=85&s=0f85093799a489eff72dae01ed5b6d94 2500w" />
</div>

### 1. Navigate to the tracing project or dataset

To start configuring a composite evaluator, navigate to the **Tracing Projects** or **Dataset & Experiments** tab and select a project or dataset.

* From within a tracing project: **+ New** > **Evaluator** > **Composite score**
* From within a dataset: **+ Evaluator** > **Composite score**

### 2. Configure the composite evaluator

1. Name your evaluator.
2. Select an aggregation method, either **Average** or **Sum**.
   * **Average**: ∑(weight\*score) / ∑(weight).
   * **Sum**: ∑(weight\*score).
3. Add the feedback keys you want to include in the composite score.
4. Add the weights for the feedback keys. By default, the weights are equal for each feedback key. Adjust the weights to increase or decrease the importance of specific feedback keys in the final score.
5. Click **Create** to save the evaluator.

<Tip> If you need to adjust the weights for the composite scores, they can be updated after the evaluator is created. The resulting scores will be updated for all runs that have the evaluator configured. </Tip>

### 3. View composite evaluator results

Composite scores are attached to a run as **feedback**, similarly to feedback from a single evaluator. How you can view them depends on where the evaluation was run:

**On a tracing project**:

* Composite scores appear as feedback on runs.
* [Filter for runs](/langsmith/filter-traces-in-application) with a composite score, or where the composite score meets a certain threshold.
* [Create a chart](/langsmith/dashboards#custom-dashboards) to visualize trends in the composite score over time.

* View the composite scores in the experiments tab. You can also filter and sort experiments based on the average composite score of their runs.
* Click into an experiment to view the composite score for each run.

<Note> If any of the constituent evaluators are not configured on the run, the composite score will not be calculated for that run. </Note>

## Create composite feedback with the SDK

This guide describes setting up an evaluation that uses multiple evaluators and combines their scores with a custom aggregation function.

<Note> Requires langsmith>=0.4.29 </Note>

### 1. Configure evaluators on a dataset

Start by configuring your evaluators. In this example, the application generates a tweet from a blog introduction and uses three evaluators — summary, tone, and formatting — to assess the output.

If you already have your own dataset with evaluators configured, you can skip this step.

<Accordion title="Configure evaluators on a dataset.">
  
</Accordion>

### 2. Create composite feedback

Create composite feedback that aggregates the individual evaluator scores using your custom function. This example uses a weighted average of the individual evaluator scores.

<Accordion title="Create a composite feedback.">
  
</Accordion>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/composite-evaluators.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
</Accordion>

### 2. Create composite feedback

Create composite feedback that aggregates the individual evaluator scores using your custom function. This example uses a weighted average of the individual evaluator scores.

<Accordion title="Create a composite feedback.">
```

---

## Conditional edge function to create llm_call workers that each write a section of the report

**URL:** llms-txt#conditional-edge-function-to-create-llm_call-workers-that-each-write-a-section-of-the-report

def assign_workers(state: State):
    """Assign a worker to each section in the plan"""

# Kick off section writing in parallel via Send() API
    return [Send("llm_call", {"section": s}) for s in state["sections"]]

---

## Configurable headers

**URL:** llms-txt#configurable-headers

**Contents:**
- Using within your graph

Source: https://docs.langchain.com/langsmith/configurable-headers

LangGraph allows runtime configuration to modify agent behavior and permissions dynamically. When using [LangSmith Deployment](/langsmith/deployment-quickstart), you can pass this configuration in the request body (`config`) or specific request headers. This enables adjustments based on user identity or other requests.

For privacy, control which headers are passed to the runtime configuration via the `http.configurable_headers` section in your [`langgraph.json`](/langsmith/application-structure#configuration-file) file.

Here's how to customize the included and excluded headers:

The `include` and `exclude` lists accept exact header names or patterns using `*` to match any number of characters. For your security, no other regex patterns are supported.

## Using within your graph

You can access the included headers in your graph using the `config` argument of any node.

Or by fetching from context (useful in tools and or within other nested functions).

You can even use this to dynamically compile the graph.

```python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
The `include` and `exclude` lists accept exact header names or patterns using `*` to match any number of characters. For your security, no other regex patterns are supported.

## Using within your graph

You can access the included headers in your graph using the `config` argument of any node.
```

Example 2 (unknown):
```unknown
Or by fetching from context (useful in tools and or within other nested functions).
```

Example 3 (unknown):
```unknown
You can even use this to dynamically compile the graph.
```

---

## Configure API URL and key

**URL:** llms-txt#configure-api-url-and-key

---

## Configure custom TLS certificates

**URL:** llms-txt#configure-custom-tls-certificates

**Contents:**
- Mount internal CAs for TLS
- Use custom TLS certificates for model providers

Source: https://docs.langchain.com/langsmith/self-host-custom-tls-certificates

Use this guide to configure TLS in LangSmith. Start by mounting internal certificate authorities (CAs) so your deployment trusts the right roots system‑wide, for database or external service calls. You can then configure [Playground](/langsmith/prompt-engineering-concepts#prompt-playground)-specific mTLS for communicating securely with supported model providers.

* [Mounting internal certificate authorities](#mount-internal-cas-for-tls) (CAs) system-wide to enable TLS for database connections and Playground model calls
* Using Playground-specific TLS settings to provide client certs/keys for mTLS with supported model providers

## Mount internal CAs for TLS

<Note>
  You must use Helm chart version 0.11.9 or later to mount internal CAs using the configuration below.
</Note>

Use this approach to make internal/public CAs trusted system‑wide by LangSmith (Playground model calls and [database/external service connections](/langsmith/self-hosted#storage-services)).

1. Create a file containing all CAs required for TLS with databases and external services. If your deployment is communicating directly to `beacon.langchain.com` without a proxy, make sure to include a public trusted CA. All certs should be concatenated in this file with an empty line in between.
   
2. Create a Kubernetes secret with a key containing the contents of this file.
   
3. If using custom CA for TLS with your databases and other external services, provide the following values to your LangSmith helm chart:
   
4. Make sure to use TLS supported connection strings:
   * <b>Postgres</b>: Add `?sslmode=verify-full&sslrootcert=system` to the end.
   * <b>Redis</b>: Use `rediss://` instead of `redis://` as the prefix.

## Use custom TLS certificates for model providers

<Note>
  This feature is currently only available for the following model providers:

* Azure OpenAI
  * OpenAI
  * Custom (our custom model server). Refer to the [custom model server documentation](/langsmith/custom-endpoint) for more information.

These TLS settings apply to all invocations of the selected model providers (including Online Evaluation). Use them when the provider requires mutual TLS (client cert/key) or when you must override trust with a specific CA for provider calls. They complement the internal CA bundle configured above.
</Note>

You can use custom TLS certificates to connect to model providers in the LangSmith Playground. This is useful if you are using a self-signed certificate, a certificate from a custom certificate authority, or mutual TLS authentication.

To use custom TLS certificates, set the following environment variables. See the [self hosted deployment section](/langsmith/architectural-overview) for more information on how to configure application settings.

* `LANGSMITH_PLAYGROUND_TLS_MODEL_PROVIDERS`: A comma-separated list of model providers that require custom TLS certificates. Note that `azure_openai`, `openai`, and `custom` are currently the only supported model providers, but more providers will be supported in the future.
* \[Optional] `LANGSMITH_PLAYGROUND_TLS_KEY`: The private key in PEM format. This must be a file path (for a mounted volume). This is usually only necessary for mutual TLS authentication.
* \[Optional] `LANGSMITH_PLAYGROUND_TLS_CERT`: The certificate in PEM format. This must be a file path (for a mounted volume). This is usually only necessary for mutual TLS authentication.
* \[Optional] `LANGSMITH_PLAYGROUND_TLS_CA`: The custom certificate authority (CA) certificate in PEM format. This must be a file path (for a mounted volume). Use this to mount CAs only if you're using a helm version below `0.11.9`; otherwise, use the [Mount internal CAs for TLS](./self-host-custom-tls-certificates#mount-internal-cas-for-tls) section above.

Once you have set these environment variables, enter the LangSmith Playground **Settings** page and select the **Provider** that requires custom TLS certificates. Set your model provider configuration as usual, and the custom TLS certificates will be used when connecting to the model provider.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/self-host-custom-tls-certificates.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
-----BEGIN CERTIFICATE-----
   <PUBLIC_CA>
   -----END CERTIFICATE-----

   -----BEGIN CERTIFICATE-----
   <INTERNAL_CA>
   -----END CERTIFICATE-----

   ...
```

Example 2 (unknown):
```unknown
3. If using custom CA for TLS with your databases and other external services, provide the following values to your LangSmith helm chart:
```

---

## Configure LangSmith for scale

**URL:** llms-txt#configure-langsmith-for-scale

**Contents:**
- Summary
- Trace ingestion (write path)
- Trace querying (read path)
- Example LangSmith configurations for scale
  - Low reads, low writes <a name="low-reads-low-writes" />
  - Low reads, high writes <a name="low-reads-high-writes" />

Source: https://docs.langchain.com/langsmith/self-host-scale

A self-hosted LangSmith instance can handle a large number of traces and users. The default configuration for the self-hosted deployment can handle substantial load, and you can configure your deployment to be able to achieve higher scale. This page describes scaling considerations and provides some examples to help configure your self-hosted instance.

For example configurations, refer to [Example LangSmith configurations for scale](#example-langsmith-configurations-for-scale).

The table below provides an overview comparing different LangSmith configurations for various load patterns (reads / writes):

|                                                                                                                                                                  | **[Low / low](#low-reads-low-writes)**              | **[Low / high](#low-reads-high-writes)**            | **[High / low](#high-reads-low-writes)**                                                                                                                                                                                                 | [Medium / medium](#medium-reads-medium-writes)      | [High / high](#high-reads-high-writes)                                                                                                                                                                                                   |
| :--------------------------------------------------------------------------------------------------------------------------------------------------------------- | :-------------------------------------------------- | :-------------------------------------------------- | :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :-------------------------------------------------- | :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| <Tooltip tip="Number of users actively viewing traces on the frontend">Concurrent frontend users</Tooltip>                                                       | 5                                                   | 5                                                   | 50                                                                                                                                                                                                                                       | 20                                                  | 50                                                                                                                                                                                                                                       |
| <Tooltip tip="Number of traces being ingested via SDKs or API endpoints">Traces submitted per second</Tooltip>                                                   | 10                                                  | 1000                                                | 10                                                                                                                                                                                                                                       | 100                                                 | 1000                                                                                                                                                                                                                                     |
| **Frontend replicas**<br />(500m CPU, 1Gi per replica)                                                                                                           | 1 (default)                                         | 4                                                   | 2                                                                                                                                                                                                                                        | 2                                                   | 4                                                                                                                                                                                                                                        |
| **Platform backend replicas**<br />(1 CPU, 2Gi per replica)                                                                                                      | 3 (default)                                         | 20                                                  | 3 (default)                                                                                                                                                                                                                              | 3 (default)                                         | 20                                                                                                                                                                                                                                       |
| **Queue replicas**<br />(1 CPU, 2Gi per replica)                                                                                                                 | 3 (default)                                         | 160                                                 | 6                                                                                                                                                                                                                                        | 10                                                  | 160                                                                                                                                                                                                                                      |
| **Backend replicas**<br />(1 CPU, 2Gi per replica)                                                                                                               | 2 (default)                                         | 5                                                   | 40                                                                                                                                                                                                                                       | 16                                                  | 50                                                                                                                                                                                                                                       |
| **Redis resources**                                                                                                                                              | 8 Gi (default)                                      | 200 Gi external                                     | 8 Gi (default)                                                                                                                                                                                                                           | 13Gi external                                       | 200 Gi external                                                                                                                                                                                                                          |
| **ClickHouse resources**                                                                                                                                         | 4 CPU<br />16 Gi (default)                          | 10 CPU<br />32Gi memory                             | 8 CPU<br />16 Gi per replica                                                                                                                                                                                                             | 16 CPU<br />24Gi memory                             | 14 CPU<br />24 Gi per replica                                                                                                                                                                                                            |
| **ClickHouse setup**                                                                                                                                             | Single instance                                     | Single instance                                     | 3-node <Tooltip tip="Recommended for high read loads to prevent degraded performance. Another option would be [managed clickhouse](/langsmith/self-host-external-clickhouse#langsmith-managed-clickhouse).">replicated cluster</Tooltip> | Single instance                                     | 3-node <Tooltip tip="Recommended for high read loads to prevent degraded performance. Another option would be [managed clickhouse](/langsmith/self-host-external-clickhouse#langsmith-managed-clickhouse).">replicated cluster</Tooltip> |
| <Tooltip tip="We recommend using an external instance and enabling autoexpansion for the disk to handle growing data requirements.">Postgres resources</Tooltip> | 2 CPU<br />8 GB memory<br />10GB storage (external) | 2 CPU<br />8 GB memory<br />10GB storage (external) | 2 CPU<br />8 GB memory<br />10GB storage (external)                                                                                                                                                                                      | 2 CPU<br />8 GB memory<br />10GB storage (external) | 2 CPU<br />8 GB memory<br />10GB storage (external)                                                                                                                                                                                      |
| **Blob storage**                                                                                                                                                 | Disabled                                            | Enabled                                             | Enabled                                                                                                                                                                                                                                  | Enabled                                             | Enabled                                                                                                                                                                                                                                  |

Below we go into more details about the read and write paths as well as provide a `values.yaml` snippet for you to start with for your self-hosted LangSmith instance.

## Trace ingestion (write path)

Common usage that put load on the write path:

* Ingesting traces via the Python or JavaScript LangSmith SDK
* Ingesting traces via the `@traceable` wrapper
* Submitting traces via the `/runs/multipart` endpoint

Services that play a large role in trace ingestion:

* Platform backend service: Receives initial request to ingest traces and places traces on a Redis queue
* Redis cache: Used to queue traces that need to be persisted
* Queue service: Persists traces for querying
* ClickHouse: Persistent storage used for traces

When scaling up the write path (trace ingestion), it is helpful to monitor the four services/resources listed above. Here are some typical changes that can help increase performance of trace ingestion:

* Give ClickHouse more resources (CPU and memory) if it is approaching resource limits.
* Increase the number of platform-backend pods if ingest requests are taking long to respond.
* Increase queue service pod replicas if traces are not being processed from Redis fast enough.
* Use a larger Redis cache if you notice that the current Redis instance is reaching resource limits. This could also be a reason why ingest requests take a long time.

## Trace querying (read path)

Common usage that puts load on the read path:

* Users on the frontend looking at tracing projects or individual traces
* Scripts used to query for trace info
* Hitting either the `/runs/query` or `/runs/<run-id>` api endpoints

Services that play a large role in querying traces:

* Backend service: Receives the request and submits a query to ClickHouse to then respond to the request
* ClickHouse: Persistent storage for traces. This is the main database that is queried when requesting trace info.

When scaling up the read path (trace querying), it is helpful to monitor the two services/resources listed above. Here are some typical changes that can help improve performance of trace querying:

* Increase the number of backend service pods. This would be most impactful if backend service pods are reaching 1 core CPU usage.
* Give ClickHouse more resources (CPU or Memory). ClickHouse can be very resource intensive, but it should lead to better performance.
* Move to a [replicated ClickHouse cluster](/langsmith/self-host-external-clickhouse#ha-replicated-clickhouse-cluster). Adding replicas of ClickHouse helps with read performance, but we recommend staying below 5 replicas (start with 3).

For more precise guidance on how this translates to helm chart values, refer to the examples the following [section](#example-langsmith-configurations-for-scale). If you are unsure why your LangSmith instance cannot handle a certain load pattern, contact the LangChain team.

## Example LangSmith configurations for scale

Below we provide some example LangSmith configurations based on expected read and write loads.

For read load (trace querying):

* Low means roughly 5 users looking at traces at a time (about 10 requests per second)
* Medium means roughly 20 users looking at traces at a time (about 40 requests per second)
* High means roughly 50 users looking at traces at a time (about 100 requests per second)

For write load (trace ingestion):

* Low means up to 10 traces submitted per second
* Medium means up to 100 traces submitted per second
* High means up to 1000 traces submitted per second

<Note>
  The exact optimal configuration depends on your usage and trace payloads. Use the examples below in combination with the information above and your specific usage to update your LangSmith configuration as you see fit. If you have any questions, please reach out to the LangChain team.
</Note>

### Low reads, low writes <a name="low-reads-low-writes" />

The default LangSmith configuration will handle this load. No custom resource configuration is needed here.

### Low reads, high writes <a name="low-reads-high-writes" />

You have a very high scale of trace ingestions, but single digit number of users on the frontend querying traces at any one time.

For this, we recommend a configuration like this:

```yaml  theme={null}
config:
  blobStorage:
    # Please also set the other keys to connect to your blob storage. See configuration section.
    enabled: true
  settings:
    redisRunsExpirySeconds: "3600"

---

## Configure LangSmith OpenTelemetry export (no OTEL env vars or headers needed)

**URL:** llms-txt#configure-langsmith-opentelemetry-export-(no-otel-env-vars-or-headers-needed)

**Contents:**
- Advanced configuration
  - Use OpenTelemetry Collector for fan-out
  - Distributed tracing with LangChain and OpenTelemetry

configure(project_name="adk-otel-demo")

async def main():
    agent = LlmAgent(
        name="travel_assistant",
        model="gemini-2.5-flash-lite",
        instruction="You are a helpful travel assistant.",
    )

session_service = InMemorySessionService()
    runner = Runner(app_name="travel_app", agent=agent, session_service=session_service)

user_id = "user_123"
    session_id = "session_abc"
    await session_service.create_session(app_name="travel_app", user_id=user_id, session_id=session_id)

new_message = types.Content(parts=[types.Part(text="Hi! Recommend a weekend trip to Paris.")], role="user")

for event in runner.run(user_id=user_id, session_id=session_id, new_message=new_message):
        print(event)

if __name__ == "__main__":
    asyncio.run(main())
yaml  theme={null}
   receivers:
     otlp:
       protocols:
         grpc:
           endpoint: 0.0.0.0:4317
         http:
           endpoint: 0.0.0.0:4318

processors:
     batch:

exporters:
     otlphttp/langsmith:
       endpoint: https://api.smith.langchain.com/otel/v1/traces
       headers:
         x-api-key: ${env:LANGSMITH_API_KEY}
         Langsmith-Project: my_project
     otlphttp/other_provider:
       endpoint: https://otel.your-provider.com/v1/traces
       headers:
         api-key: ${env:OTHER_PROVIDER_API_KEY}

service:
     pipelines:
       traces:
         receivers: [otlp]
         processors: [batch]
         exporters: [otlphttp/langsmith, otlphttp/other_provider]
   python  theme={null}
   import os
   from opentelemetry import trace
   from opentelemetry.sdk.trace import TracerProvider
   from opentelemetry.sdk.trace.export import BatchSpanProcessor
   from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter
   from langchain_openai import ChatOpenAI
   from langchain_core.prompts import ChatPromptTemplate

# Point to your local OpenTelemetry Collector
   otlp_exporter = OTLPSpanExporter(
       endpoint="http://localhost:4318/v1/traces"
   )
   provider = TracerProvider()
   processor = BatchSpanProcessor(otlp_exporter)
   provider.add_span_processor(processor)
   trace.set_tracer_provider(provider)

# Set environment variables for LangChain
   os.environ["LANGSMITH_OTEL_ENABLED"] = "true"
   os.environ["LANGSMITH_TRACING"] = "true"

# Create and run a LangChain application
   prompt = ChatPromptTemplate.from_template("Tell me a joke about {topic}")
   model = ChatOpenAI()
   chain = prompt | model
   result = chain.invoke({"topic": "programming"})
   print(result.content)
   python  theme={null}
import os
from opentelemetry import trace
from opentelemetry.propagate import inject, extract
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter
import requests
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

**Examples:**

Example 1 (unknown):
```unknown
<Note>
  You do not need to set OTEL environment variables or exporters. `configure()` wires them for LangSmith automatically; instrumentors (like `GoogleADKInstrumentor`) create the spans.
</Note>

5. View the trace in your LangSmith dashboard ([example](https://smith.langchain.com/public/106f5bed-edca-4357-91a5-80089252c9ed/r)).

## Advanced configuration

### Use OpenTelemetry Collector for fan-out

For more advanced scenarios, you can use the OpenTelemetry Collector to fan out your telemetry data to multiple destinations. This is a more scalable approach than configuring multiple exporters in your application code.

1. [Install the OpenTelemetry Collector](https://opentelemetry.io/docs/collector/getting-started/) for your environment.

2. Create a configuration file (e.g., `otel-collector-config.yaml`) that exports to multiple destinations:
```

Example 2 (unknown):
```unknown
3. Configure your application to send to the collector:
```

Example 3 (unknown):
```unknown
This approach offers several advantages:

* Centralized configuration for all your telemetry destinations
* Reduced overhead in your application code
* Better scalability and resilience
* Ability to add or remove destinations without changing application code

### Distributed tracing with LangChain and OpenTelemetry

Distributed tracing is essential when your LLM application spans multiple services or processes. OpenTelemetry's context propagation capabilities ensure that traces remain connected across service boundaries.

#### Context propagation in distributed tracing

In distributed systems, context propagation passes trace metadata between services so that related spans are linked to the same trace:

* **Trace ID**: A unique identifier for the entire trace
* **Span ID**: A unique identifier for the current span
* **Sampling Decision**: Indicates whether this trace should be sampled

#### Set up distributed tracing with LangChain

To enable distributed tracing across multiple services:
```

---

## Configure LangSmith tracing

**URL:** llms-txt#configure-langsmith-tracing

configure(project_name="multi-framework-app")

---

## Configure OpenTelemetry

**URL:** llms-txt#configure-opentelemetry

tracer_provider = trace.get_tracer_provider()
if not isinstance(tracer_provider, TracerProvider):
    tracer_provider = TracerProvider()
    trace.set_tracer_provider(tracer_provider)

---

## Configure prompt settings

**URL:** llms-txt#configure-prompt-settings

**Contents:**
- Model configurations
  - Create saved configurations
  - Edit configurations
  - Delete configurations
  - Extra parameters
- Tool settings
- Prompt formatting

Source: https://docs.langchain.com/langsmith/managing-model-configurations

The LangSmith [playground](/langsmith/prompt-engineering-concepts#prompt-playground) enables you to control various settings for your prompts. The **Prompt Settings** window contains:

* [Model configuration](#model-configurations)
* [Tool settings](#tool-settings)
* [Prompt formatting](#prompt-formatting)

To access **Prompt Settings**:

1. Navigate to the **Playground** in the left sidebar.
2. Under the **Prompts** heading select the gear <Icon icon="gear" iconType="solid" /> icon next to the model name, which will launch the **Prompt Settings** window.

<div style={{ textAlign: 'center' }}>
     <img className="block dark:hidden" src="https://mintcdn.com/langchain-5e9cc07a/6r3GRtwWCl4ozaHW/langsmith/images/model-config-light.png?fit=max&auto=format&n=6r3GRtwWCl4ozaHW&q=85&s=6c0f7d7012b1e5295fe545149f955e6b" alt="Model Configuration window in the LangSmith UI, settings for Provider, Model, Temperature, Max Output Tokens, Top P, Presence Penalty, Frequency Penalty, Reasoning Effort, etc." data-og-width="886" width="886" data-og-height="689" height="689" data-path="langsmith/images/model-config-light.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/6r3GRtwWCl4ozaHW/langsmith/images/model-config-light.png?w=280&fit=max&auto=format&n=6r3GRtwWCl4ozaHW&q=85&s=4e3b9ad92f6f14f4e0523bef50199318 280w, https://mintcdn.com/langchain-5e9cc07a/6r3GRtwWCl4ozaHW/langsmith/images/model-config-light.png?w=560&fit=max&auto=format&n=6r3GRtwWCl4ozaHW&q=85&s=e538eb740495a8afa8bfc552b13ae294 560w, https://mintcdn.com/langchain-5e9cc07a/6r3GRtwWCl4ozaHW/langsmith/images/model-config-light.png?w=840&fit=max&auto=format&n=6r3GRtwWCl4ozaHW&q=85&s=ebe73264e977153c869fd04d1552d09b 840w, https://mintcdn.com/langchain-5e9cc07a/6r3GRtwWCl4ozaHW/langsmith/images/model-config-light.png?w=1100&fit=max&auto=format&n=6r3GRtwWCl4ozaHW&q=85&s=2eeb01882056046bc73cc019d674af7e 1100w, https://mintcdn.com/langchain-5e9cc07a/6r3GRtwWCl4ozaHW/langsmith/images/model-config-light.png?w=1650&fit=max&auto=format&n=6r3GRtwWCl4ozaHW&q=85&s=8f28fe2fe8054cf0623fb9d17f91966f 1650w, https://mintcdn.com/langchain-5e9cc07a/6r3GRtwWCl4ozaHW/langsmith/images/model-config-light.png?w=2500&fit=max&auto=format&n=6r3GRtwWCl4ozaHW&q=85&s=cf9ad39be3623e73322d123699e73f19 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/langchain-5e9cc07a/ppc8uxWc01j4q7Ia/langsmith/images/model-config-dark.png?fit=max&auto=format&n=ppc8uxWc01j4q7Ia&q=85&s=2e9da272c3fc8f7ac958c6e6d1da85e3" alt="Model Configuration window in the LangSmith UI, settings for Provider, Model, Temperature, Max Output Tokens, Top P, Presence Penalty, Frequency Penalty, Reasoning Effort, etc." data-og-width="881" width="881" data-og-height="732" height="732" data-path="langsmith/images/model-config-dark.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ppc8uxWc01j4q7Ia/langsmith/images/model-config-dark.png?w=280&fit=max&auto=format&n=ppc8uxWc01j4q7Ia&q=85&s=652fb75a4682cfc813743a1260764e59 280w, https://mintcdn.com/langchain-5e9cc07a/ppc8uxWc01j4q7Ia/langsmith/images/model-config-dark.png?w=560&fit=max&auto=format&n=ppc8uxWc01j4q7Ia&q=85&s=02c980a8387f3d69a5870660b1668080 560w, https://mintcdn.com/langchain-5e9cc07a/ppc8uxWc01j4q7Ia/langsmith/images/model-config-dark.png?w=840&fit=max&auto=format&n=ppc8uxWc01j4q7Ia&q=85&s=ee633c06056fa7ad46ea58a179afa169 840w, https://mintcdn.com/langchain-5e9cc07a/ppc8uxWc01j4q7Ia/langsmith/images/model-config-dark.png?w=1100&fit=max&auto=format&n=ppc8uxWc01j4q7Ia&q=85&s=f62a35ed726b5f89c156a40c9ea76f2c 1100w, https://mintcdn.com/langchain-5e9cc07a/ppc8uxWc01j4q7Ia/langsmith/images/model-config-dark.png?w=1650&fit=max&auto=format&n=ppc8uxWc01j4q7Ia&q=85&s=18114575db8e6c7ce928763ddcb88c12 1650w, https://mintcdn.com/langchain-5e9cc07a/ppc8uxWc01j4q7Ia/langsmith/images/model-config-dark.png?w=2500&fit=max&auto=format&n=ppc8uxWc01j4q7Ia&q=85&s=ab24dc4975def52db55c4896ead5b77c 2500w" />
   </div>

## Model configurations

Model configurations define the parameters your prompt runs against. In the LangSmith Playground, you can save and manage these configurations, which allows you to reuse your preferred settings across prompts and sessions. For details on specific settings, refer to your model provider’s documentation (for example, [Anthropic](https://docs.claude.com/en/api/messages), [OpenAI](https://platform.openai.com/docs/api-reference/responses/create)).

### Create saved configurations

1. In the **Model Configurations** tab, adjust the model configuration as needed—you can select a [saved configuration to edit](#edit-configurations).
2. Click the **Save As** button in the top bar.
3. Enter a name and optional description for your configuration and confirm.
4. Now that you've saved the configuration, anyone in your organization's [workspace](/langsmith/administration-overview#workspaces) can access it. All saved configurations are available in the **Model Configuration** dropdown.
5. Once you have created a saved configuration, you can set it as your default, so any new prompt you create will automatically use this configuration. To set a configuration as your default, click the **Set as default** <Icon icon="thumbtack" iconType="solid" /> icon next to the model name in the dropdown.

### Edit configurations

1. To rename a saved configuration, or update the description, select the configuration name or description and make the necessary changes.
2. Update the current configuration's parameters as needed and click the **Save** button at the top.

### Delete configurations

1. Select the configuration you want to remove.
2. Click the trash <Icon icon="trash" iconType="solid" /> icon to delete it.

The **Extra Parameters** field allows you to pass additional model parameters that aren't directly supported in the LangSmith interface. This is particularly useful in two scenarios:

1. When model providers release new parameters that haven't yet been integrated into the LangSmith interface. You can specify these parameters in JSON format to use them right away. For example:

2. When troubleshooting parameter-related errors in the playground, such as:

If you receive an error about unnecessary parameters (which is more common when using [LangChain JS](/oss/python/langchain/overview) for run tracing), you can use this field to remove the extra parameters.

[*Tools*](/langsmith/prompt-engineering-concepts#tools) enable your LLM to perform tasks like searching the web, looking up information, and so on. In the **Tools Settings** tab, you can manage the ways your LLM uses and accesses the tools you have defined in your prompt, including:

* **Parallel Tool Calls**: Calling multiple tools in parallel when appropriate. This allows the model to gather information from different sources simultaneously. (Dependent on model support for parallel execution.)
* **Tool Choice**: Select the tools that the model can access. For more details, refer to [Use tools in a prompt](/langsmith/use-tools).

The **Prompt Format** tab allows you to specify:

* The **Prompt type**. For details on chat and completion prompts, refer to [Prompt engineering](/langsmith/prompt-engineering-concepts#chat-vs-completion) concepts.
* The **Template format**. For details on prompt templating and using variables, refer to [F-string vs. mustache](/langsmith/prompt-engineering-concepts##f-string-vs-mustache).

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/managing-model-configurations.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
2. When troubleshooting parameter-related errors in the playground, such as:
```

---

## Configure Semantic Kernel

**URL:** llms-txt#configure-semantic-kernel

kernel = Kernel()
kernel.add_service(OpenAIChatCompletion())

---

## Configure the OTLP exporter for your custom endpoint

**URL:** llms-txt#configure-the-otlp-exporter-for-your-custom-endpoint

provider = TracerProvider()
otlp_exporter = OTLPSpanExporter(
    # Change to your provider's endpoint
    endpoint="https://otel.your-provider.com/v1/traces",
    # Add any required headers for authentication
    headers={"api-key": "your-api-key"}
)
processor = BatchSpanProcessor(otlp_exporter)
provider.add_span_processor(processor)
trace.set_tracer_provider(provider)

---

## Configure threads

**URL:** llms-txt#configure-threads

**Contents:**
- Group traces into threads
  - Example
- View threads
  - View a thread
  - View feedback
  - Save thread level filter

Source: https://docs.langchain.com/langsmith/threads

Many LLM applications have a chatbot-like interface in which the user and the LLM application engage in a multi-turn conversation. In order to track these conversations, you can use the `Threads` feature in LangSmith.

## Group traces into threads

A `Thread` is a sequence of traces representing a single conversation. Each response is represented as its own trace, but these traces are linked together by being part of the same thread.

To associate traces together, you need to pass in a special `metadata` key where the value is the unique identifier for that thread. The key name should be one of:

* `session_id`
* `thread_id`
* `conversation_id`.

The value can be any string you want, but we recommend using UUIDs, such as `f47ac10b-58cc-4372-a567-0e02b2c3d479`. Check out [this guide](./add-metadata-tags) for instructions on adding metadata to your traces.

This example demonstrates how to log and retrieve conversation history using a structured message format to maintain long-running chats.

After waiting a few seconds, you can make the following calls to continue the conversation. By passing `get_chat_history=True,`/`getChatHistory: true`,
you can continue the conversation from where it left off. This means that the LLM will receive the entire message history and respond to it,
instead of just responding to the latest message.

Keep the conversation going. Since past messages are included, the LLM will remember the conversation.

You can view threads by clicking on the **Threads** tab in any project details page. You will then see a list of all threads, sorted by the most recent activity.

<div style={{ textAlign: 'center' }}>
  <img className="block dark:hidden" src="https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/threads-tab-light.png?fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=45e1c11dce5eaaaf0cf8ae01057647b7" alt="LangSmith UI showing the threads table." data-og-width="1277" width="1277" data-og-height="762" height="762" data-path="langsmith/images/threads-tab-light.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/threads-tab-light.png?w=280&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=cb5d147a58a3a9ecbb1c550a3308e871 280w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/threads-tab-light.png?w=560&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=a7cc6f9c8def1e15cc9c93382b82473d 560w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/threads-tab-light.png?w=840&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=23013f31edf91e66da89e102cb6ea302 840w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/threads-tab-light.png?w=1100&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=1bfbd6daad34b699553a30d8f9663540 1100w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/threads-tab-light.png?w=1650&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=d3fc20ddbe02630ac98a0c336a39caa7 1650w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/threads-tab-light.png?w=2500&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=cfa17b6f006faef0a6f8537eedab6532 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/threads-tab-dark.png?fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=b0ec4964ee49a3ead3a1e8042e406abc" alt="LangSmith UI showing the threads table." data-og-width="1275" width="1275" data-og-height="761" height="761" data-path="langsmith/images/threads-tab-dark.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/threads-tab-dark.png?w=280&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=98750a8129e2e8283871c096a642f8b8 280w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/threads-tab-dark.png?w=560&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=4be143312bdd90ab8318da304c0c1e91 560w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/threads-tab-dark.png?w=840&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=c7ace4e000b6a1c925d32b78983fadca 840w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/threads-tab-dark.png?w=1100&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=9ece1b058745be1c3b2dcfffd9a9af58 1100w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/threads-tab-dark.png?w=1650&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=0ab63a1b906fe2a559cfb5772c95dc47 1650w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/threads-tab-dark.png?w=2500&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=6093b81ee1f92978880e74a9630f451d 2500w" />
</div>

You can then click into a particular thread. This will open the history for a particular thread.

<div style={{ textAlign: 'center' }}>
  <img className="block dark:hidden" src="https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-light.png?fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=f7af4c3904073d5f58f28c656603ca19" alt="LangSmith UI showing the threads table." data-og-width="1273" width="1273" data-og-height="757" height="757" data-path="langsmith/images/thread-overview-light.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-light.png?w=280&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=cd769088ab3ab2dae09982915f23772d 280w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-light.png?w=560&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=70ae6b5a6b8edb83ba3604d4c6e0262e 560w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-light.png?w=840&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=61d89d8077072221373490edac65363c 840w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-light.png?w=1100&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=ad8159fe12f056dbc561c612e3797b97 1100w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-light.png?w=1650&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=3611f7bcc95c45bcb91c093ca36ef348 1650w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-light.png?w=2500&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=1c0426d7e83562e1d76e079959bda186 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-dark.png?fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=f738de4cac932ed2b8657e8f3b706b77" alt="LangSmith UI showing the threads table." data-og-width="1273" width="1273" data-og-height="753" height="753" data-path="langsmith/images/thread-overview-dark.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-dark.png?w=280&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=9bc9dd49c63661dceb981899c5f0332b 280w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-dark.png?w=560&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=01713d47cf762f99be1a1143b01582e8 560w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-dark.png?w=840&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=cfc77e449d0ce27cdfa51b2f7c6ed655 840w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-dark.png?w=1100&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=d84f671a8f2c1207dbb98c72a37d1832 1100w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-dark.png?w=1650&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=d592bd4c8671b3d7f19a49471888a901 1650w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-dark.png?w=2500&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=2405eaef2af227dd5e5efac85fc9e623 2500w" />
</div>

Threads can be viewed in two different ways:

* [Thread overview](/langsmith/threads#thread-overview)
* [Trace view](/langsmith/threads#trace-view)

You can use the buttons at the top of the page to switch between the two views or use the keyboard shortcut `T` to toggle between the two views.

The thread overview page shows you a chatbot-like UI where you can see the inputs and outputs for each turn of the conversation. You can configure which fields in the inputs and outputs are displayed in the overview, or show multiple fields by clicking the **Configure** button.

The JSON path for the inputs and outputs supports negative indexing, so you can use `-1` to access the last element of an array. For example, `inputs.messages[-1].content` will access the last message in the `messages` array.

The trace view here is similar to the trace view when looking at a single run, except that you have easy access to all the runs for each turn in the thread.

When viewing a thread, across the top of the page you will see a section called `Feedback`. This is where you can see the feedback for each of the runs that make up the thread. This feedback is aggregated, so if you evaluate each run of a thread for the same criteria, you will see the average score across all the runs displayed. You can also see [thread level feedback](/langsmith/online-evaluations#configure-multi-turn-online-evaluators) left here.

### Save thread level filter

Similar to saving filters at the project level, you can also save commonly used filters at the thread level. To save filters on the threads table, set a filter using the filters button and then click the **Save filter** button.

You can open up the trace or annotate the trace in a side panel by clicking on `Annotate` and `Open trace`, respectively.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/threads.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

After waiting a few seconds, you can make the following calls to continue the conversation. By passing `get_chat_history=True,`/`getChatHistory: true`,
you can continue the conversation from where it left off. This means that the LLM will receive the entire message history and respond to it,
instead of just responding to the latest message.

<CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>

Keep the conversation going. Since past messages are included, the LLM will remember the conversation.

<CodeGroup>
```

---

## Configure webhook notifications for LangSmith alerts

**URL:** llms-txt#configure-webhook-notifications-for-langsmith-alerts

**Contents:**
- Overview
- Prerequisites
- Integration Configuration
  - Step 1: Prepare Your Receiving Endpoint
  - Step 2: Configure Webhook Parameters
  - Step 3: Test the Webhook
- Troubleshooting
- Security Considerations
- Sending alerts to Slack using a webhook
  - Prerequisites

Source: https://docs.langchain.com/langsmith/alerts-webhook

This guide details the process for setting up webhook notifications for [LangSmith alerts](/langsmith/alerts). Before proceeding, make sure you have followed the steps leading up to the notification step of creating the alert by following [this guide](./alerts). Webhooks enable integration with custom services and third-party platforms by sending HTTP POST requests when alert conditions are triggered. Use webhooks to forward alert data to ticketing systems, chat applications, or custom monitoring solutions.

* An endpoint that can receive HTTP POST requests
* Appropriate authentication credentials for your receiving service (if required)

## Integration Configuration

### Step 1: Prepare Your Receiving Endpoint

Before configuring the webhook in LangSmith, ensure your receiving endpoint:

* Accepts HTTP POST requests
* Can process JSON payloads
* Is accessible from external services
* Has appropriate authentication mechanisms (if required)

Additionally, if on a custom deployment of LangSmith, make sure there are no firewall settings blocking egress traffic from LangSmith services.

### Step 2: Configure Webhook Parameters

<img src="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook-setup.png?fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=fecb6275ad3d576a864d1c6a2771c847" alt="Webhook Setup" data-og-width="754" width="754" data-og-height="523" height="523" data-path="langsmith/images/webhook-setup.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook-setup.png?w=280&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=ef03d3ab887113e73dbdc1097076d103 280w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook-setup.png?w=560&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=a25fcaedcbed92c9c3f2e2bddd8d88bd 560w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook-setup.png?w=840&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=4785471ce1e58f3c48ce19b7be3889c5 840w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook-setup.png?w=1100&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=8c7dd40aeb5635cdf4ddf207d0dfe7c7 1100w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook-setup.png?w=1650&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=aff125529b9db8fbf861999e70bcdb26 1650w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook-setup.png?w=2500&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=e9de7c4f0dcc440d734f4f3d09d2abf4 2500w" />

In the notification section of your alert complete the webhook configuration with the following parameters:

* **URL**: The complete URL of your receiving endpoint
  * Example: `https://api.example.com/incident-webhook`

* **Headers**: JSON Key-value pairs sent with the webhook request

* Common headers include:

* `Authorization`: For authentication tokens
    * `Content-Type`: Usually set to `application/json` (default)
    * `X-Source`: To identify the source as LangSmith

* If no headers, then simply use `{}`

* **Request Body Template**: Customize the JSON payload sent to your endpoint

* Default: LangSmith sends the payload defined and the following additonal key-value pairs appended to the payload:

* `project_name`: Name of the triggered alert
    * `alert_rule_id`: A UUID to identify the LangSmith alert. This can be used as a de-duplication key in the webhook service.
    * `alert_rule_name`: The name of the alert rule.
    * `alert_rule_type`: The type of alert (as of 04/01/2025 all alerts are of type `threshold`).
    * `alert_rule_attribute`: The attribute associated with the alert rule - `error_count`, `feedback_score` or `latency`.
    * `triggered_metric_value`: The value of the metric at the time the threshold was triggered.
    * `triggered_threshold`: The threshold that triggered the alert.
    * `timestamp`: The timestamp that triggered the alert.

### Step 3: Test the Webhook

Click **Send Test Alert** to send the webhook notification to ensure the notification works as intended.

If webhook notifications aren't being delivered:

* Verify the webhook URL is correct and accessible
* Ensure any authentication headers are properly formatted
* Check that your receiving endpoint accepts POST requests
* Examine your endpoint's logs for received but rejected requests
* Verify your custom payload template is valid JSON format

## Security Considerations

* Use HTTPS for your webhook endpoints
* Implement authentication for your webhook endpoint
* Consider adding a shared secret in your headers to verify webhook sources
* Validate incoming webhook requests before processing them

## Sending alerts to Slack using a webhook

Here is an example for configuring LangSmith alerts to send notifications to Slack channels using the [`chat.postMessage`](https://api.slack.com/methods/chat.postMessage) API.

* Access to a Slack workspace
* A LangSmith project to set up alerts
* Permissions to create Slack applications

### Step 1: Create a Slack App

1. Visit the [Slack API Applications page](https://api.slack.com/apps)
2. Click **Create New App**
3. Select **From scratch**
4. Provide an **App Name** (e.g., "LangSmith Alerts")
5. Select the workspace where you want to install the app
6. Click **Create App**

### Step 2: Configure Bot Permissions

1. In the left sidebar of your Slack app configuration, click **OAuth & Permissions**

2. Scroll down to **Bot Token Scopes** under **Scopes** and click **Add an OAuth Scope**

3. Add the following scopes:

* `chat:write` (Send messages as the app)
   * `chat:write.public` (Send messages to channels the app isn't in)
   * `channels:read` (View basic channel information)

### Step 3: Install the App to Your Workspace

1. Scroll up to the top of the **OAuth & Permissions** page
2. Click **Install to Workspace**
3. Review the permissions and click **Allow**
4. Copy the **Bot User OAuth Token** that appears (begins with `xoxb-`)

### Step 4: Configure the Webhook Alert in LangSmith

1. In LangSmith, navigate to your project
2. Select **Alerts → Create Alert**
3. Define your alert metrics and conditions
4. In the notification section, select **Webhook**
5. Configure the webhook with the following settings:

> **Note:** Replace `xoxb-your-token-here` with your actual Bot User OAuth Token

**Request Body Template**

**NOTE:** Fill in the `channel_id`, `alert_name`, `project_name` and `project_url` when creating the alert. You can find your `project_url` in the browser's URL bar. Copy the portion up to but not including any query parameters.

6. Click **Save** to activate the webhook configuration

### Step 5: Test the Integration

1. In the LangSmith alert configuration, click **Test Alert**
2. Check your specified Slack channel for the test notification
3. Verify that the message contains the expected alert information

### (Optional) Step 6: Link to the Alert Preview in the Request Body

After creating an alert, you can optionally link to its preview in the webhook's request body.

<img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/alert-preview-pane.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=286ebb8f90bafbdcacf9a0602aaf749c" alt="Alert Preview Pane" data-og-width="832" width="832" data-og-height="773" height="773" data-path="langsmith/images/alert-preview-pane.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/alert-preview-pane.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=20a409a30bff44a1a8bb1b79a6a2216b 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/alert-preview-pane.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=414bb4719617bd23452273c73327d601 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/alert-preview-pane.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=6bc7bc7aaee65f7f4afac42102047ad2 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/alert-preview-pane.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=491244ac56f6f4bcbb64419b267df0fe 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/alert-preview-pane.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=d47f5ba127c3f61e3cb7498f8b7568fe 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/alert-preview-pane.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=6a70706db839b2d211024116ba19acef 2500w" />

1. Save your alert
2. Find your saved alert in the alerts table and click it
3. Copy the dsiplayed URL
4. Click "Edit Alert"
5. Replace the existing project URL with the copied alert preview URL

## Additional Resources

* [LangSmith Alerts Documentation](/langsmith/alerts)
* [Slack chat.postMessage API Documentation](https://api.slack.com/methods/chat.postMessage)
* [Slack Block Kit Builder](https://app.slack.com/block-kit-builder/)

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/alerts-webhook.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
**Headers**
```

Example 2 (unknown):
```unknown
> **Note:** Replace `xoxb-your-token-here` with your actual Bot User OAuth Token

**Request Body Template**
```

---

## Configure webhook notifications for rules

**URL:** llms-txt#configure-webhook-notifications-for-rules

**Contents:**
- Webhook payload
- Security
  - Webhook custom HTTP headers
  - Webhook Delivery
- Example with Modal
  - Setup
  - Secrets
  - Service

Source: https://docs.langchain.com/langsmith/webhooks

When you add a webhook URL on an automation action, we will make a POST request to your webhook endpoint any time the rules you defined match any new runs.

<img src="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook.png?fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=da310e976aa8824071d65b8fb44b9123" alt="" data-og-width="872" width="872" data-og-height="991" height="991" data-path="langsmith/images/webhook.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook.png?w=280&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=6f30f7cd2de82b0ccb826d257b933f12 280w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook.png?w=560&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=0fce81ff2661e8944ebfb781a07017fe 560w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook.png?w=840&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=5799cf7458a15ac99579ba273d0b875e 840w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook.png?w=1100&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=aa958b944f848f3a64ce068a64bc8433 1100w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook.png?w=1650&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=a3e94463ac9d8fa498accb27124785ab 1650w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook.png?w=2500&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=6a5db18db5cf7097fdba6d6b7162ba6d 2500w" />

The payload we send to your webhook endpoint contains:

* `"rule_id"`: this is the ID of the automation that sent this payload
* `"start_time"` and `"end_time"`: these are the time boundaries where we found matching runs
* `"runs"`: this is an array of runs, where each run is a dictionary. If you need more information about each run we suggest using our SDK in your endpoint to fetch it from our API.
* `"feedback_stats"`: this is a dictionary with the feedback statistics for the runs. An example payload for this field is shown below.

<Note>
  **fetching from S3 URLs**

Depending on how recent your runs are, the `inputs_s3_urls` and `outputs_s3_urls` fields may contain S3 URLs to the actual data instead of the data itself.

The `inputs` and `outputs` can be fetched by the `ROOT.presigned_url` provided in `inputs_s3_urls` and `outputs_s3_urls` respectively.
</Note>

This is an example of the entire payload we send to your webhook endpoint:

We strongly recommend you add a secret query string parameter to the webhook URL, and verify it on any incoming request. This ensures that if someone discovers your webhook URL you can distinguish those calls from authentic webhook notifications.

### Webhook custom HTTP headers

If you'd like to send any specific headers with your webhook, this can be configured per URL. To set this up, click on the `Headers` option next to the URL field and add your headers.

<Note>
  Headers are stored in encrypted format.
</Note>

<img src="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook-headers.png?fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=8d6fde711d74784b803c13aba4b38837" alt="" data-og-width="848" width="848" data-og-height="1004" height="1004" data-path="langsmith/images/webhook-headers.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook-headers.png?w=280&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=1d6c9f67920f0de5bc4b440593b87116 280w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook-headers.png?w=560&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=a62d07be3f38e9c659faadb091f8a23e 560w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook-headers.png?w=840&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=5a6c128eb91f899f9213fd0f8999a11f 840w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook-headers.png?w=1100&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=6a9dd4e7f434f2f2806df60232f34ac3 1100w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook-headers.png?w=1650&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=315d0e900bd0c65dd5264869bd545351 1650w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook-headers.png?w=2500&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=31ce3f7628c5c3ba3916b77d906ff0c5 2500w" />

When delivering events to your webhook endpoint we follow these guidelines

* If we fail to connect to your endpoint, we retry the transport connection up to 2 times, before declaring the delivery failed.
* If your endpoint takes longer than 5 seconds to reply we declare the delivery failed and do not .
* If your endpoint returns a 5xx status code in less than 5 seconds we retry up to 2 times with exponential backoff.
* If your endpoint returns a 4xx status code, we declare the delivery failed and do not retry.
* Anything your endpoint returns in the body will be ignored

## Example with Modal

For an example of how to set this up, we will use [Modal](https://modal.com/). Modal provides autoscaling GPUs for inference and fine-tuning, secure containerization for code agents, and serverless Python web endpoints. We'll focus on the web endpoints here.

First, create a Modal account. Then, locally install the Modal SDK:

To finish setting up your account, run the command:

and follow the instructions

Next, you will need to set up some secrets in Modal.

First, LangSmith will need to authenticate to Modal by passing in a secret.
The easiest way to do this is to pass in a secret in the query parameters.
To validate this secret, we will need to add a secret in *Modal* to validate it.
We will do that by creating a Modal secret.
You can see instructions for secrets [here](https://modal.com/docs/guide/secrets).
For this purpose, let's call our secret `ls-webhook` and have it set an environment variable with the name `LS_WEBHOOK`.

We can also set up a LangSmith secret - luckily there is already an integration template for this!

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/modal-langsmith-secret.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=0c3209b59cb36273d82fb44383efa1d5" alt="LangSmith Modal Template" data-og-width="1229" width="1229" data-og-height="779" height="779" data-path="langsmith/images/modal-langsmith-secret.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/modal-langsmith-secret.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=c2ff70b647c04bb6a45a08de537b4d22 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/modal-langsmith-secret.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=02181b882935f45339d31f48adeed1e9 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/modal-langsmith-secret.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=52119f475cca739369cebb71bfefafae 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/modal-langsmith-secret.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=aa8a1fc73b2e0b7f27c3186732e3bde9 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/modal-langsmith-secret.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=91d36950d22b86f7ad790a61957cbad7 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/modal-langsmith-secret.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=efcdfe7d54ca30079b147e00a0d9e934 2500w" />

After that, you can create a Python file that will serve as your endpoint.
An example is below, with comments explaining what is going on:

```python  theme={null}
from fastapi import HTTPException, status, Request, Query
from modal import Secret, Stub, web_endpoint, Image

stub = Stub("auth-example", image=Image.debian_slim().pip_install("langsmith"))

@stub.function(
    secrets=[Secret.from_name("ls-webhook"), Secret.from_name("my-langsmith-secret")]
)

**Examples:**

Example 1 (unknown):
```unknown
<Note>
  **fetching from S3 URLs**

  Depending on how recent your runs are, the `inputs_s3_urls` and `outputs_s3_urls` fields may contain S3 URLs to the actual data instead of the data itself.

  The `inputs` and `outputs` can be fetched by the `ROOT.presigned_url` provided in `inputs_s3_urls` and `outputs_s3_urls` respectively.
</Note>

This is an example of the entire payload we send to your webhook endpoint:
```

Example 2 (unknown):
```unknown
## Security

We strongly recommend you add a secret query string parameter to the webhook URL, and verify it on any incoming request. This ensures that if someone discovers your webhook URL you can distinguish those calls from authentic webhook notifications.

An example would be
```

Example 3 (unknown):
```unknown
### Webhook custom HTTP headers

If you'd like to send any specific headers with your webhook, this can be configured per URL. To set this up, click on the `Headers` option next to the URL field and add your headers.

<Note>
  Headers are stored in encrypted format.
</Note>

<img src="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook-headers.png?fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=8d6fde711d74784b803c13aba4b38837" alt="" data-og-width="848" width="848" data-og-height="1004" height="1004" data-path="langsmith/images/webhook-headers.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook-headers.png?w=280&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=1d6c9f67920f0de5bc4b440593b87116 280w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook-headers.png?w=560&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=a62d07be3f38e9c659faadb091f8a23e 560w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook-headers.png?w=840&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=5a6c128eb91f899f9213fd0f8999a11f 840w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook-headers.png?w=1100&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=6a9dd4e7f434f2f2806df60232f34ac3 1100w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook-headers.png?w=1650&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=315d0e900bd0c65dd5264869bd545351 1650w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook-headers.png?w=2500&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=31ce3f7628c5c3ba3916b77d906ff0c5 2500w" />

### Webhook Delivery

When delivering events to your webhook endpoint we follow these guidelines

* If we fail to connect to your endpoint, we retry the transport connection up to 2 times, before declaring the delivery failed.
* If your endpoint takes longer than 5 seconds to reply we declare the delivery failed and do not .
* If your endpoint returns a 5xx status code in less than 5 seconds we retry up to 2 times with exponential backoff.
* If your endpoint returns a 4xx status code, we declare the delivery failed and do not retry.
* Anything your endpoint returns in the body will be ignored

## Example with Modal

### Setup

For an example of how to set this up, we will use [Modal](https://modal.com/). Modal provides autoscaling GPUs for inference and fine-tuning, secure containerization for code agents, and serverless Python web endpoints. We'll focus on the web endpoints here.

First, create a Modal account. Then, locally install the Modal SDK:

<CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Configure your agents

**URL:** llms-txt#configure-your-agents

config_list = [
    {
        "model": "gpt-4",
        "api_key": os.getenv("OPENAI_API_KEY"),
    }
]

---

## Configure your collector for LangSmith telemetry

**URL:** llms-txt#configure-your-collector-for-langsmith-telemetry

Source: https://docs.langchain.com/langsmith/langsmith-collector

The various services in a LangSmith deployment emit telemetry data in the form of logs, metrics, and traces. You may already have telemetry collectors set up in your Kubernetes cluster, or would like to deploy one to monitor your application.

This page describes how to configure an [OTel Collector](https://opentelemetry.io/docs/collector/configuration/) to gather telemetry data from LangSmith. Note that all of the concepts discussed below can be translated to other collectors such as [Fluentd](https://www.fluentd.org/) or [FluentBit](https://fluentbit.io/).

<Warning>
  **This section is only applicable for Kubernetes deployments.**
</Warning>

---

## Connect an authentication provider

**URL:** llms-txt#connect-an-authentication-provider

**Contents:**
- Background
- Prerequisites
- 1. Install dependencies
- 2. Set up the authentication provider
- 3. Implement token validation

Source: https://docs.langchain.com/langsmith/add-auth-server

In [the last tutorial](/langsmith/resource-auth), you added resource authorization to give users private conversations. However, you are still using hard-coded tokens for authentication, which is not secure. Now you'll replace those tokens with real user accounts using [OAuth2](/langsmith/deployment-quickstart).

You'll keep the same [`Auth`](/langsmith/langgraph-python-sdk#langgraph_sdk.auth.Auth) object and [resource-level access control](/langsmith/auth#single-owner-resources), but upgrade authentication to use Supabase as your identity provider. While Supabase is used in this tutorial, the concepts apply to any OAuth2 provider. You'll learn how to:

1. Replace test tokens with real JWT tokens
2. Integrate with OAuth2 providers for secure user authentication
3. Handle user sessions and metadata while maintaining our existing authorization logic

OAuth2 involves three main roles:

1. **Authorization server**: The identity provider (e.g., Supabase, Auth0, Google) that handles user authentication and issues tokens
2. **Application backend**: Your LangGraph application. This validates tokens and serves protected resources (conversation data)
3. **Client application**: The web or mobile app where users interact with your service

A standard OAuth2 flow works something like this:

Before you start this tutorial, ensure you have:

* The [bot from the second tutorial](/langsmith/resource-auth) running without errors.
* A [Supabase project](https://supabase.com/dashboard) to use its authentication server.

## 1. Install dependencies

Install the required dependencies. Start in your `custom-auth` directory and ensure you have the `langgraph-cli` installed:

<a id="setup-auth-provider" />

## 2. Set up the authentication provider

Next, fetch the URL of your auth server and the private key for authentication.
Since you're using Supabase for this, you can do this in the Supabase dashboard:

1. In the left sidebar, click on t️⚙ Project Settings" and then click "API"
2. Copy your project URL and add it to your `.env` file

3. Copy your service role secret key and add it to your `.env` file:

4. Copy your "anon public" key and note it down. This will be used later when you set up our client code.

## 3. Implement token validation

In the previous tutorials, you used the [`Auth`](/langsmith/langgraph-python-sdk#langgraph_sdk.auth.Auth) object to [validate hard-coded tokens](/langsmith/set-up-custom-auth) and [add resource ownership](/langsmith/resource-auth).

Now you'll upgrade your authentication to validate real JWT tokens from Supabase. The main changes will all be in the [`@auth.authenticate`](/langsmith/langgraph-python-sdk#langgraph_sdk.auth.Auth.authenticate) decorated function:

* Instead of checking against a hard-coded list of tokens, you'll make an HTTP request to Supabase to validate the token.
* You'll extract real user information (ID, email) from the validated token.
* The existing resource authorization logic remains unchanged.

Update `src/security/auth.py` to implement this:

```python {highlight={8-9,20-30}} title="src/security/auth.py" theme={null}
import os
import httpx
from langgraph_sdk import Auth

**Examples:**

Example 1 (unknown):
```unknown
## Prerequisites

Before you start this tutorial, ensure you have:

* The [bot from the second tutorial](/langsmith/resource-auth) running without errors.
* A [Supabase project](https://supabase.com/dashboard) to use its authentication server.

## 1. Install dependencies

Install the required dependencies. Start in your `custom-auth` directory and ensure you have the `langgraph-cli` installed:

<CodeGroup>
```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
</CodeGroup>

<a id="setup-auth-provider" />

## 2. Set up the authentication provider

Next, fetch the URL of your auth server and the private key for authentication.
Since you're using Supabase for this, you can do this in the Supabase dashboard:

1. In the left sidebar, click on t️⚙ Project Settings" and then click "API"
2. Copy your project URL and add it to your `.env` file
```

Example 4 (unknown):
```unknown
3. Copy your service role secret key and add it to your `.env` file:
```

---

## Connect nodes in a sequence

**URL:** llms-txt#connect-nodes-in-a-sequence

---

## Connect to an external ClickHouse database

**URL:** llms-txt#connect-to-an-external-clickhouse-database

**Contents:**
- Requirements
- HA Replicated Clickhouse Cluster
- LangSmith-managed ClickHouse
- Parameters
- Configuration

Source: https://docs.langchain.com/langsmith/self-host-external-clickhouse

ClickHouse is a high-performance, column-oriented database system. It allows for fast ingestion of data and is optimized for analytical queries.

LangSmith uses ClickHouse as the primary data store for traces and feedback. By default, self-hosted LangSmith will use an internal ClickHouse database that is bundled with the LangSmith instance. This is run as a stateful set in the same Kubernetes cluster as the LangSmith application or as a Docker container on the same host as the LangSmith application.

However, you can configure LangSmith to use an external ClickHouse database for easier management and scaling. By configuring an external ClickHouse database, you can manage backups, scaling, and other operational tasks for your database. While Clickhouse is not yet a native service in Azure, AWS, or Google Cloud, you can run LangSmith with an external ClickHouse database in the following ways:

* [LangSmith-managed ClickHouse](/langsmith/langsmith-managed-clickhouse)

* Provision a [ClickHouse Cloud](https://clickhouse.cloud/) either directly or through a cloud provider marketplace:

* [Azure Marketplace](https://azuremarketplace.microsoft.com/en-us/marketplace/apps/clickhouse.clickhouse_cloud?tab=Overview)
  * [Google Cloud Marketplace](https://console.cloud.google.com/marketplace/product/clickhouse-public/clickhouse-cloud)
  * [AWS Marketplace](https://aws.amazon.com/marketplace/pp/prodview-jettukeanwrfc)

* On a VM in your cloud provider

<Note>
  Using the first two options (LangSmith-managed ClickHouse or ClickHouse Cloud) will provision a Clickhouse service OUTSIDE of your VPC. However, both options support private endpoints, meaning that you can direct traffic to the ClickHouse service without exposing it to the public internet (eg via AWS PrivateLink, or GCP Private Service Connect).

Additionally, sensitive information can be configured to be not stored in Clickhouse. Please reach out to [support@langchain.dev](mailto:support@langchain.dev) for more information.
</Note>

* A provisioned ClickHouse instance that your LangSmith application will have network access to (see above for options).
* A user with admin access to the ClickHouse database. This user will be used to create the necessary tables, indexes, and views.
* We support both standalone ClickHouse and externally managed clustered deployments. For clustered deployments, ensure all nodes are running the same version. Note that clustered setups are not supported with bundled ClickHouse installations.
* We only support ClickHouse versions >= 23.9. Use of ClickHouse versions >= 24.2 requires LangSmith v0.6 or later.
* We rely on a few configuration parameters to be set on your ClickHouse instance. These are detailed below:

<Warning>
  Our system has been tuned to work with the above configuration parameters. Changing these parameters may result in unexpected behavior.
</Warning>

## HA Replicated Clickhouse Cluster

<Warning>
  By default, the setup process above will only work with a single node Clickhouse cluster.
</Warning>

If you would like to use a multi-node Clickhouse cluster for HA, we support this with additional required configuration. This setup can use a Clickhouse cluster with multiple nodes where data replicated via Zookeeper or Clickhouse Keeper. For more information on Clickhouse replication, see [Clickhouse Data Replication Docs](https://clickhouse.com/docs/architecture/replication).

In order to setup LangSmith with a replicated multi-node Clickhouse setup:

* You need to have a Clickhouse cluster that is setup with Keeper or Zookeeper for data replication and the appropriate settings. See [Clickhouse Replication Setup Docs](https://clickhouse.com/docs/architecture/replication).
* You need to set the cluster setting in the [LangSmith Configuration](#configuration) section, specifically the `cluster` settings to match your Clickhouse Cluster name. This will use the `Replicated` table engines when running the Clickhouse migrations.
* If in addition to HA, you would like to load balance among the Clickhouse nodes (to distribute reads or writes), we suggest using a load balancer or DNS load balancing to round robin among your Clickhouse servers.
* **Note**: You will need to enable your `cluster` setting before launching LangSmith for the first time and running the Clickhouse migrations. This is a requirement since the table engine will need to be created as a `Replicated` table engine vs the non replicated engine type.

When running migrations with `cluster` enabled, the migration will create the `Replicated` table engine flavor. This means that data will be replicated among the servers in the cluster. This is a master-master setup where any server can process reads, writes, or merges.

<Note>
  For an example setup of a replicated ClickHouse cluster, refer to the [replicated ClickHouse section](https://github.com/langchain-ai/helm/blob/main/charts/langsmith/examples/replicated-clickhouse/README.md) in the LangSmith Helm chart repo, under examples.
</Note>

## LangSmith-managed ClickHouse

* If using LangSmith-managed ClickHouse, you will need to set up a VPC peering connection between the LangSmith VPC and the ClickHouse VPC. Please reach out to [support@langchain.dev](mailto:support@langchain.dev) for more information.
* You will also need to set up Blob Storage. You can read more about Blob Storage in the [Blob Storage documentation](/langsmith/self-host-blob-storage).

<Note>
  ClickHouse installations managed by LangSmith use a SharedMerge engine, which automatically clusters them and separates compute from storage.
</Note>

For more information, refer to the [managed ClickHouse](/langsmith/langsmith-managed-clickhouse) page.

You will need to provide several parameters to your LangSmith installation to configure an external ClickHouse database. These parameters include:

* Host: The hostname or IP address of the ClickHouse database
* HTTP Port: The port that the ClickHouse database listens on for HTTP connections
* Native Port: The port that the ClickHouse database listens on for [native connections](https://clickhouse.com/docs/en/interfaces/tcp)
* Database: The name of the ClickHouse database that LangSmith should use
* Username: The username to use to connect to the ClickHouse database
* Password: The password to use to connect to the ClickHouse database
* Cluster (Optional): The name of the ClickHouse cluster if using an external Clickhouse cluster. When set, LangSmith will run migrations on the cluster and replicate data across instances.

<Warning>
  Important considerations for clustered deployments:

* Clustered setups must be configured on a fresh schema - existing standalone ClickHouse instances cannot be converted to clustered mode.

* Clustering is only supported with externally managed ClickHouse deployments. It is not compatible with bundled ClickHouse installations as these do not include required ZooKeeper configurations.

* When using a clustered deployment, LangSmith will automatically:

* Run database migrations across all nodes in the cluster

* Configure tables for data replication across the cluster

Note that while data is replicated across nodes, LangSmith does not configure distributed tables or handle query routing - queries will be directed to the specified host. You will need to handle any load balancing or query distribution at the infrastructure level if desired.
</Warning>

With these parameters in hand, you can configure your LangSmith instance to use the provisioned ClickHouse database. You can do this by modifying the `config.yaml` file for your LangSmith Helm Chart installation or the `.env` file for your Docker installation.

Once configured, you should be able to reinstall your LangSmith instance. If everything is configured correctly, your LangSmith instance should now be using your external ClickHouse database.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/self-host-external-clickhouse.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
<Warning>
  Our system has been tuned to work with the above configuration parameters. Changing these parameters may result in unexpected behavior.
</Warning>

## HA Replicated Clickhouse Cluster

<Warning>
  By default, the setup process above will only work with a single node Clickhouse cluster.
</Warning>

If you would like to use a multi-node Clickhouse cluster for HA, we support this with additional required configuration. This setup can use a Clickhouse cluster with multiple nodes where data replicated via Zookeeper or Clickhouse Keeper. For more information on Clickhouse replication, see [Clickhouse Data Replication Docs](https://clickhouse.com/docs/architecture/replication).

In order to setup LangSmith with a replicated multi-node Clickhouse setup:

* You need to have a Clickhouse cluster that is setup with Keeper or Zookeeper for data replication and the appropriate settings. See [Clickhouse Replication Setup Docs](https://clickhouse.com/docs/architecture/replication).
* You need to set the cluster setting in the [LangSmith Configuration](#configuration) section, specifically the `cluster` settings to match your Clickhouse Cluster name. This will use the `Replicated` table engines when running the Clickhouse migrations.
* If in addition to HA, you would like to load balance among the Clickhouse nodes (to distribute reads or writes), we suggest using a load balancer or DNS load balancing to round robin among your Clickhouse servers.
* **Note**: You will need to enable your `cluster` setting before launching LangSmith for the first time and running the Clickhouse migrations. This is a requirement since the table engine will need to be created as a `Replicated` table engine vs the non replicated engine type.

When running migrations with `cluster` enabled, the migration will create the `Replicated` table engine flavor. This means that data will be replicated among the servers in the cluster. This is a master-master setup where any server can process reads, writes, or merges.

<Note>
  For an example setup of a replicated ClickHouse cluster, refer to the [replicated ClickHouse section](https://github.com/langchain-ai/helm/blob/main/charts/langsmith/examples/replicated-clickhouse/README.md) in the LangSmith Helm chart repo, under examples.
</Note>

## LangSmith-managed ClickHouse

* If using LangSmith-managed ClickHouse, you will need to set up a VPC peering connection between the LangSmith VPC and the ClickHouse VPC. Please reach out to [support@langchain.dev](mailto:support@langchain.dev) for more information.
* You will also need to set up Blob Storage. You can read more about Blob Storage in the [Blob Storage documentation](/langsmith/self-host-blob-storage).

<Note>
  ClickHouse installations managed by LangSmith use a SharedMerge engine, which automatically clusters them and separates compute from storage.
</Note>

For more information, refer to the [managed ClickHouse](/langsmith/langsmith-managed-clickhouse) page.

## Parameters

You will need to provide several parameters to your LangSmith installation to configure an external ClickHouse database. These parameters include:

* Host: The hostname or IP address of the ClickHouse database
* HTTP Port: The port that the ClickHouse database listens on for HTTP connections
* Native Port: The port that the ClickHouse database listens on for [native connections](https://clickhouse.com/docs/en/interfaces/tcp)
* Database: The name of the ClickHouse database that LangSmith should use
* Username: The username to use to connect to the ClickHouse database
* Password: The password to use to connect to the ClickHouse database
* Cluster (Optional): The name of the ClickHouse cluster if using an external Clickhouse cluster. When set, LangSmith will run migrations on the cluster and replicate data across instances.

<Warning>
  Important considerations for clustered deployments:

  * Clustered setups must be configured on a fresh schema - existing standalone ClickHouse instances cannot be converted to clustered mode.

  * Clustering is only supported with externally managed ClickHouse deployments. It is not compatible with bundled ClickHouse installations as these do not include required ZooKeeper configurations.

  * When using a clustered deployment, LangSmith will automatically:

  * Run database migrations across all nodes in the cluster

  * Configure tables for data replication across the cluster

  Note that while data is replicated across nodes, LangSmith does not configure distributed tables or handle query routing - queries will be directed to the specified host. You will need to handle any load balancing or query distribution at the infrastructure level if desired.
</Warning>

## Configuration

With these parameters in hand, you can configure your LangSmith instance to use the provisioned ClickHouse database. You can do this by modifying the `config.yaml` file for your LangSmith Helm Chart installation or the `.env` file for your Docker installation.

<CodeGroup>
```

Example 2 (unknown):
```unknown

```

---

## Connect to an external PostgreSQL database

**URL:** llms-txt#connect-to-an-external-postgresql-database

**Contents:**
- Requirements
- Connection String
- Configuration

Source: https://docs.langchain.com/langsmith/self-host-external-postgres

LangSmith uses a PostgreSQL database as the primary data store for transactional workloads and operational data (almost everything besides runs). By default, LangSmith Self-Hosted will use an internal PostgreSQL database. However, you can configure LangSmith to use an external PostgreSQL database (). By configuring an external PostgreSQL database, you can more easily manage backups, scaling, and other operational tasks for your database.

* A provisioned PostgreSQL database that your LangSmith instance will have network access to. We recommend using a managed PostgreSQL service like:

* [Amazon RDS](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_GettingStarted.CreatingConnecting.PostgreSQL.html)
  * [Google Cloud SQL](https://cloud.google.com/curated-resources/cloud-sql#section-1)
  * [Azure Database for PostgreSQL](https://azure.microsoft.com/en-us/products/postgresql#features)

* Note: We only officially support PostgreSQL versions >= 14.

* A user with admin access to the PostgreSQL database. This user will be used to create the necessary tables, indexes, and schemas.

* This user will also need to have the ability to create extensions in the database. We use/will try to install the btree\_gin, btree\_gist, pgcrypto, citext, ltree, and pg\_trgm extensions.

* If using a schema other than public, ensure that you do not have any other schemas with the extensions enabled, or you must include that in your search path.

* Support for pgbouncer and other connection poolers is community-based. Community members have reported that pgbouncer has worked with `pool_mode` = `session` and a suitable setting for `ignore_startup_parameters` (as of writing, `search_path` and `lock_timeout` need to be ignored). Care is needed to avoid polluting connection pools; some level of PostgreSQL expertise is advisable. LangChain Inc currently does not have roadmap plans for formal test coverage or commercial support of pgbouncer or amazon rds proxy or any other poolers, but the community is welcome to discuss and collaborate on support through GitHub issues.

* By default, we recommend an instance with at least 2 vCPUs and 8GB of memory. However, the actual requirements will depend on your workload and the number of users you have. We recommend monitoring your PostgreSQL instance and scaling up as needed.

You will need to provide a connection string to your PostgreSQL database. This connection string should include the following information:

* Host
* Port
* Database
* Username
* Password(Make sure to url encode this if there are any special characters)
* URL params

This will take the form of:

An example connection string might look like:

Without url parameters, the connection string would look like:

With your connection string in hand, you can configure your LangSmith instance to use an external PostgreSQL database. You can do this by modifying the `values` file for your LangSmith Helm Chart installation or the `.env` file for your Docker installation.

Once configured, you should be able to reinstall your LangSmith instance. If everything is configured correctly, your LangSmith instance should now be using your external PostgreSQL database.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/self-host-external-postgres.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
username:password@host:port/database?<url_params>
```

Example 2 (unknown):
```unknown
myuser:mypassword@myhost:5432/mydatabase?sslmode=disable
```

Example 3 (unknown):
```unknown
myuser:mypassword@myhost:5432/mydatabase
```

Example 4 (unknown):
```unknown

```

---

## Connect to an external Redis database

**URL:** llms-txt#connect-to-an-external-redis-database

**Contents:**
- Requirements
- Connection String
- Configuration

Source: https://docs.langchain.com/langsmith/self-host-external-redis

LangSmith uses Redis to back our queuing/caching operations. By default, LangSmith Self-Hosted will use an internal Redis instance. However, you can configure LangSmith to use an external Redis instance. By configuring an external Redis instance, you can more easily manage backups, scaling, and other operational tasks for your Redis instance.

* A provisioned Redis instance that your LangSmith instance will have network access to. We recommend using a managed Redis service like:

* [Amazon ElastiCache](https://aws.amazon.com/elasticache/redis/)
  * [Google Cloud Memorystore](https://cloud.google.com/memorystore)
  * [Azure Cache for Redis](https://azure.microsoft.com/en-us/services/cache/)

* Note: We only officially support Redis versions >= 5.

* We do not support Redis Cluster.

* By default, we recommend an instance with at least 2 vCPUs and 8GB of memory. However, the actual requirements will depend on your tracing workload. We recommend monitoring your Redis instance and scaling up as needed.

<Info>
  Certain tiers of managed Redis services may use Redis Cluster under the hood, but you can point to a single node in the cluster. For example on Azure Cache for Redis, the `Premium` tier and above use Redis Cluster, so you will need to use a lower tier.
</Info>

We use `redis-py` to connect to Redis. This library supports a variety of connection strings. You can find more information on the connection string format [here](https://redis-py.readthedocs.io/en/stable/#redis.StrictRedis.from_url).

You will need to assemble the connection string for your Redis instance. This connection string should include the following information:

* Host
* Database
* Port
* URL params

This will take the form of:

An example connection string might look like:

To use SSL, you can use the `rediss://` prefix. An example connection string with SSL might look like:

With your connection string in hand, you can configure your LangSmith instance to use an external Redis instance. You can do this by modifying the `values` file for your LangSmith Helm Chart installation or the `.env` file for your Docker installation.

Once configured, you should be able to reinstall your LangSmith instance. If everything is configured correctly, your LangSmith instance should now be using your external Redis instance.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/self-host-external-redis.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
"redis://host:port/db?<url_params>"
```

Example 2 (unknown):
```unknown
"redis://langsmith-redis:6379/0"
```

Example 3 (unknown):
```unknown
"rediss://langsmith-redis:6380/0?password=foo"
```

Example 4 (unknown):
```unknown

```

---

## Connect to an OpenAI compliant model provider/proxy

**URL:** llms-txt#connect-to-an-openai-compliant-model-provider/proxy

**Contents:**
- Deploy an OpenAI compliant model
- Use the model in the LangSmith Playground

Source: https://docs.langchain.com/langsmith/custom-openai-compliant-model

The LangSmith playground allows you to use any model that is compliant with the OpenAI API. You can utilize your model by setting the Proxy Provider for  in the playground.

## Deploy an OpenAI compliant model

Many providers offer OpenAI compliant models or proxy services. Some examples of this include:

* [LiteLLM Proxy](https://github.com/BerriAI/litellm?tab=readme-ov-file#quick-start-proxy---cli)
* [Ollama](https://ollama.com/)

You can use these providers to deploy your model and get an API endpoint that is compliant with the OpenAI API.

Take a look at the full [specification](https://platform.openai.com/docs/api-reference/chat) for more information.

## Use the model in the LangSmith Playground

Once you have deployed a model server, you can use it in the LangSmith [Playground](/langsmith/prompt-engineering-concepts#prompt-playground).

To access the **Prompt Settings** menu:

1. Under the **Prompts** heading select the gear <Icon icon="gear" iconType="solid" /> icon next to the model name.
2. In the **Model Configuration** tab, select the model to edit in the dropdown.
3. For the **Provider** dropdown, select **OpenAI Compatible Endpoint**.
4. Add your OpenAI Compatible Endpoint to the **Base URL** input.

<div style={{ textAlign: 'center' }}>
     <img className="block dark:hidden" src="https://mintcdn.com/langchain-5e9cc07a/cemWY9w7h0W8uMbk/langsmith/images/openai-compatible-endpoint.png?fit=max&auto=format&n=cemWY9w7h0W8uMbk&q=85&s=fdbe548e512ed40fb512578d02986b45" alt="Model Configuration window in the LangSmith UI with a model selected and the Provider dropdown with OpenAI Compatible Endpoint selected." data-og-width="897" width="897" data-og-height="572" height="572" data-path="langsmith/images/openai-compatible-endpoint.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/cemWY9w7h0W8uMbk/langsmith/images/openai-compatible-endpoint.png?w=280&fit=max&auto=format&n=cemWY9w7h0W8uMbk&q=85&s=75921e7b30edbac5263ee10178977383 280w, https://mintcdn.com/langchain-5e9cc07a/cemWY9w7h0W8uMbk/langsmith/images/openai-compatible-endpoint.png?w=560&fit=max&auto=format&n=cemWY9w7h0W8uMbk&q=85&s=468265c2ad0a6c1740eb18590dab27a5 560w, https://mintcdn.com/langchain-5e9cc07a/cemWY9w7h0W8uMbk/langsmith/images/openai-compatible-endpoint.png?w=840&fit=max&auto=format&n=cemWY9w7h0W8uMbk&q=85&s=9f9c3f68df77205264eb9221373790f2 840w, https://mintcdn.com/langchain-5e9cc07a/cemWY9w7h0W8uMbk/langsmith/images/openai-compatible-endpoint.png?w=1100&fit=max&auto=format&n=cemWY9w7h0W8uMbk&q=85&s=ccb9820653e1e57b529bc46ac7d20e40 1100w, https://mintcdn.com/langchain-5e9cc07a/cemWY9w7h0W8uMbk/langsmith/images/openai-compatible-endpoint.png?w=1650&fit=max&auto=format&n=cemWY9w7h0W8uMbk&q=85&s=c320b3f4051643b71fba4faa350daf9b 1650w, https://mintcdn.com/langchain-5e9cc07a/cemWY9w7h0W8uMbk/langsmith/images/openai-compatible-endpoint.png?w=2500&fit=max&auto=format&n=cemWY9w7h0W8uMbk&q=85&s=e9d1be7c69021b7fa575a2a466dbfe58 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/langchain-5e9cc07a/cemWY9w7h0W8uMbk/langsmith/images/openai-compatible-endpoint-dark.png?fit=max&auto=format&n=cemWY9w7h0W8uMbk&q=85&s=97459563da21d17228a1bb94a1b9edf3" alt="Model Configuration window in the LangSmith UI with a model selected and the Provider dropdown with OpenAI Compatible Endpoint selected." data-og-width="896" width="896" data-og-height="552" height="552" data-path="langsmith/images/openai-compatible-endpoint-dark.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/cemWY9w7h0W8uMbk/langsmith/images/openai-compatible-endpoint-dark.png?w=280&fit=max&auto=format&n=cemWY9w7h0W8uMbk&q=85&s=c3e8e46813ec673fbc3ac4e4748a4ab6 280w, https://mintcdn.com/langchain-5e9cc07a/cemWY9w7h0W8uMbk/langsmith/images/openai-compatible-endpoint-dark.png?w=560&fit=max&auto=format&n=cemWY9w7h0W8uMbk&q=85&s=381a567b022c71ed6f74abbb7e3cecbd 560w, https://mintcdn.com/langchain-5e9cc07a/cemWY9w7h0W8uMbk/langsmith/images/openai-compatible-endpoint-dark.png?w=840&fit=max&auto=format&n=cemWY9w7h0W8uMbk&q=85&s=7df617e39d5bd098f4e80d523ef85778 840w, https://mintcdn.com/langchain-5e9cc07a/cemWY9w7h0W8uMbk/langsmith/images/openai-compatible-endpoint-dark.png?w=1100&fit=max&auto=format&n=cemWY9w7h0W8uMbk&q=85&s=06408cd348f56fcc409af8273c799a97 1100w, https://mintcdn.com/langchain-5e9cc07a/cemWY9w7h0W8uMbk/langsmith/images/openai-compatible-endpoint-dark.png?w=1650&fit=max&auto=format&n=cemWY9w7h0W8uMbk&q=85&s=6b79b55e2868ed11e9d2c2eb396b004f 1650w, https://mintcdn.com/langchain-5e9cc07a/cemWY9w7h0W8uMbk/langsmith/images/openai-compatible-endpoint-dark.png?w=2500&fit=max&auto=format&n=cemWY9w7h0W8uMbk&q=85&s=4e0439d889aa2175c0802e9bf5db399b 2500w" />
   </div>

If everything is set up correctly, you should see the model's response in the playground. You can also use this functionality to invoke downstream pipelines as well.

For information on how to store your model configuration , refer to [Configure prompt settings](/langsmith/managing-model-configurations).

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/custom-openai-compliant-model.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Connect to a custom model

**URL:** llms-txt#connect-to-a-custom-model

**Contents:**
- Deploy a custom model server
- Adding configurable fields
- Use the model in the LangSmith Playground

Source: https://docs.langchain.com/langsmith/custom-endpoint

The LangSmith playground allows you to use your own custom models. You can deploy a model server that exposes your model's API via , an open source library for serving LangChain applications. Behind the scenes, the playground will interact with your model server to generate responses.

## Deploy a custom model server

For your convenience, we have provided a sample model server that you can use as a reference. You can find the sample model server [here](https://github.com/langchain-ai/langsmith-model-server) We highly recommend using the sample model server as a starting point.

Depending on your model is an instruct-style or chat-style model, you will need to implement either `custom_model.py` or `custom_chat_model.py` respectively.

## Adding configurable fields

It is often useful to configure your model with different parameters. These might include temperature, model\_name, max\_tokens, etc.

To make your model configurable in the LangSmith playground, you need to add configurable fields to your model server. These fields can be used to change model parameters from the playground.

You can add configurable fields by implementing the `with_configurable_fields` function in the `config.py` file. You can

## Use the model in the LangSmith Playground

Once you have deployed a model server, you can use it in the LangSmith Playground. Enter the playground and select either the `ChatCustomModel` or the `CustomModel` provider for chat-style model or instruct-style models.

Enter the `URL`. The playground will automatically detect the available endpoints and configurable fields. You can then invoke the model with the desired parameters.

<img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/playground-custom-model.png?fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=7a2889af5f55cc73661033837a50fad6" alt="ChatCustomModel in Playground" data-og-width="2816" width="2816" data-og-height="1676" height="1676" data-path="langsmith/images/playground-custom-model.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/playground-custom-model.png?w=280&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=c6509706fee0c85205e039f6868a5ead 280w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/playground-custom-model.png?w=560&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=deafe903353d9bec02143ebd578d5599 560w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/playground-custom-model.png?w=840&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=928818d42fc58d83e1b5a04ecaa36630 840w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/playground-custom-model.png?w=1100&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=552046bb4c04947154a2c8fa3457beca 1100w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/playground-custom-model.png?w=1650&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=2735d4eed015cafa0861079133c5220c 1650w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/playground-custom-model.png?w=2500&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=f59ef79d897acce3ae4835ce949d61b6 2500w" />

If everything is set up correctly, you should see the model's response in the playground as well as the configurable fields specified in the `with_configurable_fields`.

See how to store your model configuration for later use [here](/langsmith/managing-model-configurations).

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/custom-endpoint.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## console.log(result);

**URL:** llms-txt#console.log(result);

**Contents:**
  - Error handling

/**
 * {
 *   messages: [
 *     ...
 *     { role: "tool", content: "Returning structured response: {'task': 'update the project timeline', 'assignee': 'Sarah', 'priority': 'high'}", tool_call_id: "call_456", name: "MeetingAction" }
 *   ],
 *   structuredResponse: { task: "update the project timeline", assignee: "Sarah", priority: "high" }
 * }
 */
ts  theme={null}
import * as z from "zod";
import { createAgent, toolStrategy } from "langchain";

const ContactInfo = z.object({
    name: z.string().describe("Person's name"),
    email: z.string().describe("Email address"),
});

const EventDetails = z.object({
    event_name: z.string().describe("Name of the event"),
    date: z.string().describe("Event date"),
});

const agent = createAgent({
    model: "gpt-5",
    tools: [],
    responseFormat: toolStrategy([ContactInfo, EventDetails]),
});

const result = await agent.invoke({
    messages: [
        {
        role: "user",
        content:
            "Extract info: John Doe (john@email.com) is organizing Tech Conference on March 15th",
        },
    ],
});

/**
 * {
 *   messages: [
 *     { role: "user", content: "Extract info: John Doe (john@email.com) is organizing Tech Conference on March 15th" },
 *     { role: "assistant", content: "", tool_calls: [ { name: "ContactInfo", args: { name: "John Doe", email: "john@email.com" }, id: "call_1" }, { name: "EventDetails", args: { event_name: "Tech Conference", date: "March 15th" }, id: "call_2" } ] },
 *     { role: "tool", content: "Error: Model incorrectly returned multiple structured responses (ContactInfo, EventDetails) when only one is expected.\n Please fix your mistakes.", tool_call_id: "call_1", name: "ContactInfo" },
 *     { role: "tool", content: "Error: Model incorrectly returned multiple structured responses (ContactInfo, EventDetails) when only one is expected.\n Please fix your mistakes.", tool_call_id: "call_2", name: "EventDetails" },
 *     { role: "assistant", content: "", tool_calls: [ { name: "ContactInfo", args: { name: "John Doe", email: "john@email.com" }, id: "call_3" } ] },
 *     { role: "tool", content: "Returning structured response: {'name': 'John Doe', 'email': 'john@email.com'}", tool_call_id: "call_3", name: "ContactInfo" }
 *   ],
 *   structuredResponse: { name: "John Doe", email: "john@email.com" }
 * }
 */
ts  theme={null}
import * as z from "zod";
import { createAgent, toolStrategy } from "langchain";

const ProductRating = z.object({
    rating: z.number().min(1).max(5).describe("Rating from 1-5"),
    comment: z.string().describe("Review comment"),
});

const agent = createAgent({
    model: "gpt-5",
    tools: [],
    responseFormat: toolStrategy(ProductRating),
});

const result = await agent.invoke({
    messages: [
        {
        role: "user",
        content: "Parse this: Amazing product, 10/10!",
        },
    ],
});

/**
 * {
 *   messages: [
 *     { role: "user", content: "Parse this: Amazing product, 10/10!" },
 *     { role: "assistant", content: "", tool_calls: [ { name: "ProductRating", args: { rating: 10, comment: "Amazing product" }, id: "call_1" } ] },
 *     { role: "tool", content: "Error: Failed to parse structured output for tool 'ProductRating': 1 validation error for ProductRating\nrating\n  Input should be less than or equal to 5 [type=less_than_equal, input_value=10, input_type=int].\n Please fix your mistakes.", tool_call_id: "call_1", name: "ProductRating" },
 *     { role: "assistant", content: "", tool_calls: [ { name: "ProductRating", args: { rating: 5, comment: "Amazing product" }, id: "call_2" } ] },
 *     { role: "tool", content: "Returning structured response: {'rating': 5, 'comment': 'Amazing product'}", tool_call_id: "call_2", name: "ProductRating" }
 *   ],
 *   structuredResponse: { rating: 5, comment: "Amazing product" }
 * }
 */
ts  theme={null}
const responseFormat = toolStrategy(ProductRating, {
    handleError: "Please provide a valid rating between 1-5 and include a comment."
)

// Error message becomes:
// { role: "tool", content: "Please provide a valid rating between 1-5 and include a comment." }
ts  theme={null}
import { ToolInputParsingException } from "@langchain/core/tools";

const responseFormat = toolStrategy(ProductRating, {
    handleError: (error: ToolStrategyError) => {
        if (error instanceof ToolInputParsingException) {
        return "Please provide a valid rating between 1-5 and include a comment.";
        }
        return error.message;
    }
)

// Only validation errors get retried with default message:
// { role: "tool", content: "Error: Failed to parse structured output for tool 'ProductRating': ...\n Please fix your mistakes." }
ts  theme={null}
const responseFormat = toolStrategy(ProductRating, {
    handleError: (error: ToolStrategyError) => {
        if (error instanceof ToolInputParsingException) {
        return "Please provide a valid rating between 1-5 and include a comment.";
        }
        if (error instanceof CustomUserError) {
        return "This is a custom user error.";
        }
        return error.message;
    }
)
ts  theme={null}
const responseFormat = toolStrategy(ProductRating, {
    handleError: false  // All errors raised
)
```

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/structured-output.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
### Error handling

Models can make mistakes when generating structured output via tool calling. LangChain provides intelligent retry mechanisms to handle these errors automatically.

#### Multiple structured outputs error

When a model incorrectly calls multiple structured output tools, the agent provides error feedback in a @\[`ToolMessage`] and prompts the model to retry:
```

Example 2 (unknown):
```unknown
#### Schema validation error

When structured output doesn't match the expected schema, the agent provides specific error feedback:
```

Example 3 (unknown):
```unknown
#### Error handling strategies

You can customize how errors are handled using the `handleErrors` parameter:

**Custom error message:**
```

Example 4 (unknown):
```unknown
**Handle specific exceptions only:**
```

---

## ContactInfo(name='John Doe', email='john@example.com', phone='(555) 123-4567')

**URL:** llms-txt#contactinfo(name='john-doe',-email='john@example.com',-phone='(555)-123-4567')

**Contents:**
  - Memory

python wrap theme={null}
from langchain.agents.structured_output import ProviderStrategy

agent = create_agent(
    model="gpt-4o",
    response_format=ProviderStrategy(ContactInfo)
)
python  theme={null}
from langchain.agents import AgentState
from langchain.agents.middleware import AgentMiddleware
from typing import Any

class CustomState(AgentState):
    user_preferences: dict

class CustomMiddleware(AgentMiddleware):
    state_schema = CustomState
    tools = [tool1, tool2]

def before_model(self, state: CustomState, runtime) -> dict[str, Any] | None:
        ...

agent = create_agent(
    model,
    tools=tools,
    middleware=[CustomMiddleware()]
)

**Examples:**

Example 1 (unknown):
```unknown
#### ProviderStrategy

`ProviderStrategy` uses the model provider's native structured output generation. This is more reliable but only works with providers that support native structured output (e.g., OpenAI):
```

Example 2 (unknown):
```unknown
<Note>
  As of `langchain 1.0`, simply passing a schema (e.g., `response_format=ContactInfo`) is no longer supported. You must explicitly use `ToolStrategy` or `ProviderStrategy`.
</Note>

<Tip>
  To learn about structured output, see [Structured output](/oss/python/langchain/structured-output).
</Tip>

### Memory

Agents maintain conversation history automatically through the message state. You can also configure the agent to use a custom state schema to remember additional information during the conversation.

Information stored in the state can be thought of as the [short-term memory](/oss/python/langchain/short-term-memory) of the agent:

Custom state schemas must extend [`AgentState`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.AgentState) as a `TypedDict`.

There are two ways to define custom state:

1. Via [middleware](/oss/python/langchain/middleware) (preferred)
2. Via [`state_schema`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.AgentMiddleware.state_schema) on [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent)

<Note>
  Defining custom state via middleware is preferred over defining it via [`state_schema`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.AgentMiddleware.state_schema) on [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent) because it allows you to keep state extensions conceptually scoped to the relevant middleware and tools.

  [`state_schema`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.AgentMiddleware.state_schema) is still supported for backwards compatibility on [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent).
</Note>

#### Defining state via middleware

Use middleware to define custom state when your custom state needs to be accessed by specific middleware hooks and tools attached to said middleware.
```

---

## Context engineering in agents

**URL:** llms-txt#context-engineering-in-agents

**Contents:**
- Overview
  - Why do agents fail?
  - The agent loop
  - What you can control
  - Data sources
  - How it works
- Model Context
  - System Prompt
  - Messages
  - Tools

Source: https://docs.langchain.com/oss/javascript/langchain/context-engineering

The hard part of building agents (or any LLM application) is making them reliable enough. While they may work for a prototype, they often fail in real-world use cases.

### Why do agents fail?

When agents fail, it's usually because the LLM call inside the agent took the wrong action / didn't do what we expected. LLMs fail for one of two reasons:

1. The underlying LLM is not capable enough
2. The "right" context was not passed to the LLM

More often than not - it's actually the second reason that causes agents to not be reliable.

**Context engineering** is providing the right information and tools in the right format so the LLM can accomplish a task. This is the number one job of AI Engineers. This lack of "right" context is the number one blocker for more reliable agents, and LangChain's agent abstractions are uniquely designed to facilitate context engineering.

<Tip>
  New to context engineering? Start with the [conceptual overview](/oss/javascript/concepts/context) to understand the different types of context and when to use them.
</Tip>

A typical agent loop consists of two main steps:

1. **Model call** - calls the LLM with a prompt and available tools, returns either a response or a request to execute tools
2. **Tool execution** - executes the tools that the LLM requested, returns tool results

<div style={{ display: "flex", justifyContent: "center" }}>
  <img src="https://mintcdn.com/langchain-5e9cc07a/Tazq8zGc0yYUYrDl/oss/images/core_agent_loop.png?fit=max&auto=format&n=Tazq8zGc0yYUYrDl&q=85&s=ac72e48317a9ced68fd1be64e89ec063" alt="Core agent loop diagram" className="rounded-lg" data-og-width="300" width="300" data-og-height="268" height="268" data-path="oss/images/core_agent_loop.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Tazq8zGc0yYUYrDl/oss/images/core_agent_loop.png?w=280&fit=max&auto=format&n=Tazq8zGc0yYUYrDl&q=85&s=a4c4b766b6678ef52a6ed556b1a0b032 280w, https://mintcdn.com/langchain-5e9cc07a/Tazq8zGc0yYUYrDl/oss/images/core_agent_loop.png?w=560&fit=max&auto=format&n=Tazq8zGc0yYUYrDl&q=85&s=111869e6e99a52c0eff60a1ef7ddc49c 560w, https://mintcdn.com/langchain-5e9cc07a/Tazq8zGc0yYUYrDl/oss/images/core_agent_loop.png?w=840&fit=max&auto=format&n=Tazq8zGc0yYUYrDl&q=85&s=6c1e21de7b53bd0a29683aca09c6f86e 840w, https://mintcdn.com/langchain-5e9cc07a/Tazq8zGc0yYUYrDl/oss/images/core_agent_loop.png?w=1100&fit=max&auto=format&n=Tazq8zGc0yYUYrDl&q=85&s=88bef556edba9869b759551c610c60f4 1100w, https://mintcdn.com/langchain-5e9cc07a/Tazq8zGc0yYUYrDl/oss/images/core_agent_loop.png?w=1650&fit=max&auto=format&n=Tazq8zGc0yYUYrDl&q=85&s=9b0bdd138e9548eeb5056dc0ed2d4a4b 1650w, https://mintcdn.com/langchain-5e9cc07a/Tazq8zGc0yYUYrDl/oss/images/core_agent_loop.png?w=2500&fit=max&auto=format&n=Tazq8zGc0yYUYrDl&q=85&s=41eb4f053ed5e6b0ba5bad2badf6d755 2500w" />
</div>

This loop continues until the LLM decides to finish.

### What you can control

To build reliable agents, you need to control what happens at each step of the agent loop, as well as what happens between steps.

| Context Type                                  | What You Control                                                                     | Transient or Persistent |
| --------------------------------------------- | ------------------------------------------------------------------------------------ | ----------------------- |
| **[Model Context](#model-context)**           | What goes into model calls (instructions, message history, tools, response format)   | Transient               |
| **[Tool Context](#tool-context)**             | What tools can access and produce (reads/writes to state, store, runtime context)    | Persistent              |
| **[Life-cycle Context](#life-cycle-context)** | What happens between model and tool calls (summarization, guardrails, logging, etc.) | Persistent              |

<CardGroup>
  <Card title="Transient context" icon="bolt" iconType="duotone">
    What the LLM sees for a single call. You can modify messages, tools, or prompts without changing what's saved in state.
  </Card>

<Card title="Persistent context" icon="database" iconType="duotone">
    What gets saved in state across turns. Life-cycle hooks and tool writes modify this permanently.
  </Card>
</CardGroup>

Throughout this process, your agent accesses (reads / writes) different sources of data:

| Data Source         | Also Known As        | Scope               | Examples                                                                   |
| ------------------- | -------------------- | ------------------- | -------------------------------------------------------------------------- |
| **Runtime Context** | Static configuration | Conversation-scoped | User ID, API keys, database connections, permissions, environment settings |
| **State**           | Short-term memory    | Conversation-scoped | Current messages, uploaded files, authentication status, tool results      |
| **Store**           | Long-term memory     | Cross-conversation  | User preferences, extracted insights, memories, historical data            |

LangChain [middleware](/oss/javascript/langchain/middleware) is the mechanism under the hood that makes context engineering practical for developers using LangChain.

Middleware allows you to hook into any step in the agent lifecycle and:

* Update context
* Jump to a different step in the agent lifecycle

Throughout this guide, you'll see frequent use of the middleware API as a means to the context engineering end.

Control what goes into each model call - instructions, available tools, which model to use, and output format. These decisions directly impact reliability and cost.

<CardGroup cols={2}>
  <Card title="System Prompt" icon="message-lines" href="#system-prompt">
    Base instructions from the developer to the LLM.
  </Card>

<Card title="Messages" icon="comments" href="#messages">
    The full list of messages (conversation history) sent to the LLM.
  </Card>

<Card title="Tools" icon="wrench" href="#tools">
    Utilities the agent has access to to take actions.
  </Card>

<Card title="Model" icon="brain-circuit" href="#model">
    The actual model (including configuration) to be called.
  </Card>

<Card title="Response Format" icon="brackets-curly" href="#response-format">
    Schema specification for the model's final response.
  </Card>
</CardGroup>

All of these types of model context can draw from **state** (short-term memory), **store** (long-term memory), or **runtime context** (static configuration).

The system prompt sets the LLM's behavior and capabilities. Different users, contexts, or conversation stages need different instructions. Successful agents draw on memories, preferences, and configuration to provide the right instructions for the current state of the conversation.

<Tabs>
  <Tab title="State">
    Access message count or conversation context from state:

<Tab title="Store">
    Access user preferences from long-term memory:

<Tab title="Runtime Context">
    Access user ID or configuration from Runtime Context:

Messages make up the prompt that is sent to the LLM.
It's critical to manage the content of messages to ensure that the LLM has the right information to respond well.

<Tabs>
  <Tab title="State">
    Inject uploaded file context from State when relevant to current query:

<Tab title="Store">
    Inject user's email writing style from Store to guide drafting:

<Tab title="Runtime Context">
    Inject compliance rules from Runtime Context based on user's jurisdiction:

<Note>
  **Transient vs Persistent Message Updates:**

The examples above use `wrap_model_call` to make **transient** updates - modifying what messages are sent to the model for a single call without changing what's saved in state.

For **persistent** updates that modify state (like the summarization example in [Life-cycle Context](#summarization)), use life-cycle hooks like `before_model` or `after_model` to permanently update the conversation history. See the [middleware documentation](/oss/javascript/langchain/middleware) for more details.
</Note>

Tools let the model interact with databases, APIs, and external systems. How you define and select tools directly impacts whether the model can complete tasks effectively.

Each tool needs a clear name, description, argument names, and argument descriptions. These aren't just metadata—they guide the model's reasoning about when and how to use the tool.

Not every tool is appropriate for every situation. Too many tools may overwhelm the model (overload context) and increase errors; too few limit capabilities. Dynamic tool selection adapts the available toolset based on authentication state, user permissions, feature flags, or conversation stage.

<Tabs>
  <Tab title="State">
    Enable advanced tools only after certain conversation milestones:

<Tab title="Store">
    Filter tools based on user preferences or feature flags in Store:

<Tab title="Runtime Context">
    Filter tools based on user permissions from Runtime Context:

See [Dynamically selecting tools](/oss/javascript/langchain/middleware#dynamically-selecting-tools) for more examples.

Different models have different strengths, costs, and context windows. Select the right model for the task at hand, which
might change during an agent run.

<Tabs>
  <Tab title="State">
    Use different models based on conversation length from State:

<Tab title="Store">
    Use user's preferred model from Store:

<Tab title="Runtime Context">
    Select model based on cost limits or environment from Runtime Context:

See [Dynamic model](/oss/javascript/langchain/agents#dynamic-model) for more examples.

Structured output transforms unstructured text into validated, structured data. When extracting specific fields or returning data for downstream systems, free-form text isn't sufficient.

**How it works:** When you provide a schema as the response format, the model's final response is guaranteed to conform to that schema. The agent runs the model / tool calling loop until the model is done calling tools, then the final response is coerced into the provided format.

#### Defining formats

Schema definitions guide the model. Field names, types, and descriptions specify exactly what format the output should adhere to.

#### Selecting formats

Dynamic response format selection adapts schemas based on user preferences, conversation stage, or role—returning simple formats early and detailed formats as complexity increases.

<Tabs>
  <Tab title="State">
    Configure structured output based on conversation state:

<Tab title="Store">
    Configure output format based on user preferences in Store:

<Tab title="Runtime Context">
    Configure output format based on Runtime Context like user role or environment:

Tools are special in that they both read and write context.

In the most basic case, when a tool executes, it receives the LLM's request parameters and returns a tool message back. The tool does its work and produces a result.

Tools can also fetch important information for the model that allows it to perform and complete tasks.

Most real-world tools need more than just the LLM's parameters. They need user IDs for database queries, API keys for external services, or current session state to make decisions. Tools read from state, store, and runtime context to access this information.

<Tabs>
  <Tab title="State">
    Read from State to check current session information:

<Tab title="Store">
    Read from Store to access persisted user preferences:

<Tab title="Runtime Context">
    Read from Runtime Context for configuration like API keys and user IDs:

Tool results can be used to help an agent complete a given task. Tools can both return results directly to the model
and update the memory of the agent to make important context available to future steps.

<Tabs>
  <Tab title="State">
    Write to State to track session-specific information using Command:

<Tab title="Store">
    Write to Store to persist data across sessions:

See [Tools](/oss/javascript/langchain/tools) for comprehensive examples of accessing state, store, and runtime context in tools.

## Life-cycle Context

Control what happens **between** the core agent steps - intercepting data flow to implement cross-cutting concerns like summarization, guardrails, and logging.

As you've seen in [Model Context](#model-context) and [Tool Context](#tool-context), [middleware](/oss/javascript/langchain/middleware) is the mechanism that makes context engineering practical. Middleware allows you to hook into any step in the agent lifecycle and either:

1. **Update context** - Modify state and store to persist changes, update conversation history, or save insights
2. **Jump in the lifecycle** - Move to different steps in the agent cycle based on context (e.g., skip tool execution if a condition is met, repeat model call with modified context)

<div style={{ display: "flex", justifyContent: "center" }}>
  <img src="https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=eb4404b137edec6f6f0c8ccb8323eaf1" alt="Middleware hooks in the agent loop" className="rounded-lg" data-og-width="500" width="500" data-og-height="560" height="560" data-path="oss/images/middleware_final.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?w=280&fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=483413aa87cf93323b0f47c0dd5528e8 280w, https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?w=560&fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=41b7dd647447978ff776edafe5f42499 560w, https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?w=840&fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=e9b14e264f68345de08ae76f032c52d4 840w, https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?w=1100&fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=ec45e1932d1279b1beee4a4b016b473f 1100w, https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?w=1650&fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=3bca5ebf8aa56632b8a9826f7f112e57 1650w, https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?w=2500&fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=437f141d1266f08a95f030c2804691d9 2500w" />
</div>

### Example: Summarization

One of the most common life-cycle patterns is automatically condensing conversation history when it gets too long. Unlike the transient message trimming shown in [Model Context](#messages), summarization **persistently updates state** - permanently replacing old messages with a summary that's saved for all future turns.

LangChain offers built-in middleware for this:

When the conversation exceeds the token limit, `SummarizationMiddleware` automatically:

1. Summarizes older messages using a separate LLM call
2. Replaces them with a summary message in State (permanently)
3. Keeps recent messages intact for context

The summarized conversation history is permanently updated - future turns will see the summary instead of the original messages.

<Note>
  For a complete list of built-in middleware, available hooks, and how to create custom middleware, see the [Middleware documentation](/oss/javascript/langchain/middleware).
</Note>

1. **Start simple** - Begin with static prompts and tools, add dynamics only when needed
2. **Test incrementally** - Add one context engineering feature at a time
3. **Monitor performance** - Track model calls, token usage, and latency
4. **Use built-in middleware** - Leverage [`SummarizationMiddleware`](/oss/javascript/langchain/middleware#summarization), [`LLMToolSelectorMiddleware`](/oss/javascript/langchain/middleware#llm-tool-selector), etc.
5. **Document your context strategy** - Make it clear what context is being passed and why
6. **Understand transient vs persistent**: Model context changes are transient (per-call), while life-cycle context changes persist to state

* [Context conceptual overview](/oss/javascript/concepts/context) - Understand context types and when to use them
* [Middleware](/oss/javascript/langchain/middleware) - Complete middleware guide
* [Tools](/oss/javascript/langchain/tools) - Tool creation and context access
* [Memory](/oss/javascript/concepts/memory) - Short-term and long-term memory patterns
* [Agents](/oss/javascript/langchain/agents) - Core agent concepts

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/context-engineering.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
</Tab>

  <Tab title="Store">
    Access user preferences from long-term memory:
```

Example 2 (unknown):
```unknown
</Tab>

  <Tab title="Runtime Context">
    Access user ID or configuration from Runtime Context:
```

Example 3 (unknown):
```unknown
</Tab>
</Tabs>

### Messages

Messages make up the prompt that is sent to the LLM.
It's critical to manage the content of messages to ensure that the LLM has the right information to respond well.

<Tabs>
  <Tab title="State">
    Inject uploaded file context from State when relevant to current query:
```

Example 4 (unknown):
```unknown
</Tab>

  <Tab title="Store">
    Inject user's email writing style from Store to guide drafting:
```

---

## Context overview

**URL:** llms-txt#context-overview

**Contents:**
- Config
- Dynamic runtime context
- Dynamic cross-conversation context
- See also

Source: https://docs.langchain.com/oss/javascript/concepts/context

**Context engineering** is the practice of building dynamic systems that provide the right information and tools, in the right format, so that an AI application can accomplish a task. Context can be characterized along two key dimensions:

1. By **mutability**:

* **Static context**: Immutable data that doesn't change during execution (e.g., user metadata, database connections, tools)
* **Dynamic context**: Mutable data that evolves as the application runs (e.g., conversation history, intermediate results, tool call observations)

* **Runtime context**: Data scoped to a single run or invocation
* **Cross-conversation context**: Data that persists across multiple conversations or sessions

<Tip>
  Runtime context refers to local context: data and dependencies your code needs to run. It does **not** refer to:

* The LLM context, which is the data passed into the LLM's prompt.
  * The "context window", which is the maximum number of tokens that can be passed to the LLM.

The runtime context is how you thread data through your agent. Rather than storing things in global state, you can attach values — like a database connection, user session, or configuration — to the context and access them inside tools and middleware. This keeps things stateless, testable, and reusable. For example, you can use user metadata in the runtime context to fetch user preferences and feed them into the context window.
</Tip>

LangGraph provides three ways to manage context, which combines the mutability and lifetime dimensions:

| Context type                                                                                | Description                                   | Mutability | Lifetime           |
| ------------------------------------------------------------------------------------------- | --------------------------------------------- | ---------- | ------------------ |
| [**Config**](#config-static-context)                                                        | data passed at the start of a run             | Static     | Single run         |
| [**Dynamic runtime context (state)**](#dynamic-runtime-context-state)                       | Mutable data that evolves during a single run | Dynamic    | Single run         |
| [**Dynamic cross-conversation context (store)**](#dynamic-cross-conversation-context-store) | Persistent data shared across conversations   | Dynamic    | Cross-conversation |

<a id="static-context" />

Config is for immutable data like user metadata or API keys. Use this when you have values that don't change mid-run.

Specify configuration using a key called **"configurable"** which is reserved for this purpose.

## Dynamic runtime context

**Dynamic runtime context** represents mutable data that can evolve during a single run and is managed through the LangGraph state object. This includes conversation history, intermediate results, and values derived from tools or LLM outputs. In LangGraph, the state object acts as [short-term memory](/oss/javascript/concepts/memory) during a run.

<Tabs>
  <Tab title="In an agent">
    Example shows how to incorporate state into an agent **prompt**.

State can also be accessed by the agent's **tools**, which can read or update the state as needed. See [tool calling guide](/oss/javascript/langchain/tools#short-term-memory) for details.

<Tab title="In a workflow">
    
  </Tab>
</Tabs>

<Tip>
  **Turning on memory**
  Please see the [memory guide](/oss/javascript/langgraph/add-memory) for more details on how to enable memory. This is a powerful feature that allows you to persist the agent's state across multiple invocations. Otherwise, the state is scoped only to a single run.
</Tip>

## Dynamic cross-conversation context

**Dynamic cross-conversation context** represents persistent, mutable data that spans across multiple conversations or sessions and is managed through the LangGraph store. This includes user profiles, preferences, and historical interactions. The LangGraph store acts as [long-term memory](/oss/javascript/concepts/memory#long-term-memory) across multiple runs. This can be used to read or update persistent facts (e.g., user profiles, preferences, prior interactions).

* [Memory conceptual overview](/oss/javascript/concepts/memory)
* [Short-term memory in LangChain](/oss/javascript/langchain/short-term-memory)
* [Long-term memory in LangChain](/oss/javascript/langchain/long-term-memory)
* [Memory in LangGraph](/oss/javascript/langgraph/add-memory)

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/concepts/context.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
<a id="state" />

## Dynamic runtime context

**Dynamic runtime context** represents mutable data that can evolve during a single run and is managed through the LangGraph state object. This includes conversation history, intermediate results, and values derived from tools or LLM outputs. In LangGraph, the state object acts as [short-term memory](/oss/javascript/concepts/memory) during a run.

<Tabs>
  <Tab title="In an agent">
    Example shows how to incorporate state into an agent **prompt**.

    State can also be accessed by the agent's **tools**, which can read or update the state as needed. See [tool calling guide](/oss/javascript/langchain/tools#short-term-memory) for details.
```

Example 2 (unknown):
```unknown
</Tab>

  <Tab title="In a workflow">
```

---

## Continue conversation

**URL:** llms-txt#continue-conversation

**Contents:**
- Message content

messages = [
    HumanMessage("What's the weather in San Francisco?"),
    ai_message,  # Model's tool call
    tool_message,  # Tool execution result
]
response = model.invoke(messages)  # Model processes the result
python  theme={null}
    from langchain.messages import ToolMessage

# Sent to model
    message_content = "It was the best of times, it was the worst of times."

# Artifact available downstream
    artifact = {"document_id": "doc_123", "page": 0}

tool_message = ToolMessage(
        content=message_content,
        tool_call_id="call_123",
        name="search_books",
        artifact=artifact,
    )
    python  theme={null}
from langchain.messages import HumanMessage

**Examples:**

Example 1 (unknown):
```unknown
<Accordion title="Attributes">
  <ParamField path="content" type="string" required>
    The stringified output of the tool call.
  </ParamField>

  <ParamField path="tool_call_id" type="string" required>
    The ID of the tool call that this message is responding to. (this must match the ID of the tool call in the [`AIMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessage))
  </ParamField>

  <ParamField path="name" type="string" required>
    The name of the tool that was called.
  </ParamField>

  <ParamField path="artifact" type="dict">
    Additional data not sent to the model but can be accessed programmatically.
  </ParamField>
</Accordion>

<Note>
  The `artifact` field stores supplementary data that won't be sent to the model but can be accessed programmatically. This is useful for storing raw results, debugging information, or data for downstream processing without cluttering the model's context.

  <Accordion title="Example: Using artifact for retrieval metadata">
    For example, a [retrieval](/oss/python/langchain/retrieval) tool could retrieve a passage from a document for reference by a model. Where message `content` contains text that the model will reference, an `artifact` can contain document identifiers or other metadata that an application can use (e.g., to render a page). See example below:
```

Example 2 (unknown):
```unknown
See the [RAG tutorial](/oss/python/langchain/rag) for an end-to-end example of building retrieval [agents](/oss/python/langchain/agents) with LangChain.
  </Accordion>
</Note>

***

## Message content

You can think of a message's content as the payload of data that gets sent to the model. Messages have a `content` attribute that is loosely-typed, supporting strings and lists of untyped objects (e.g., dictionaries). This allows support for provider-native structures directly in LangChain chat models, such as [multimodal](#multimodal) content and other data.

Separately, LangChain provides dedicated content types for text, reasoning, citations, multi-modal data, server-side tool calls, and other message content. See [content blocks](#standard-content-blocks) below.

LangChain chat models accept message content in the `content` attribute, and can contain:

1. A string
2. A list of content blocks in a provider-native format
3. A list of [LangChain's standard content blocks](#standard-content-blocks)

See below for an example using [multimodal](#multimodal) inputs:
```

---

## Continue execution

**URL:** llms-txt#continue-execution

**Contents:**
  - Review tool calls
- Short-term memory
  - Manage checkpoints
  - Decouple return value from saved value
  - Chatbot example
- Long-term memory
- Workflows
- Integrate with other libraries

for event in graph.stream(Command(resume="baz"), config):
    print(event)
    print("\n")
python  theme={null}
from typing import Union

def review_tool_call(tool_call: ToolCall) -> Union[ToolCall, ToolMessage]:
    """Review a tool call, returning a validated version."""
    human_review = interrupt(
        {
            "question": "Is this correct?",
            "tool_call": tool_call,
        }
    )
    review_action = human_review["action"]
    review_data = human_review.get("data")
    if review_action == "continue":
        return tool_call
    elif review_action == "update":
        updated_tool_call = {**tool_call, **{"args": review_data}}
        return updated_tool_call
    elif review_action == "feedback":
        return ToolMessage(
            content=review_data, name=tool_call["name"], tool_call_id=tool_call["id"]
        )
python  theme={null}
from langgraph.checkpoint.memory import InMemorySaver
from langgraph.graph.message import add_messages
from langgraph.types import Command, interrupt

checkpointer = InMemorySaver()

@entrypoint(checkpointer=checkpointer)
def agent(messages, previous):
    if previous is not None:
        messages = add_messages(previous, messages)

model_response = call_model(messages).result()
    while True:
        if not model_response.tool_calls:
            break

# Review tool calls
        tool_results = []
        tool_calls = []
        for i, tool_call in enumerate(model_response.tool_calls):
            review = review_tool_call(tool_call)
            if isinstance(review, ToolMessage):
                tool_results.append(review)
            else:  # is a validated tool call
                tool_calls.append(review)
                if review != tool_call:
                    model_response.tool_calls[i] = review  # update message

# Execute remaining tool calls
        tool_result_futures = [call_tool(tool_call) for tool_call in tool_calls]
        remaining_tool_results = [fut.result() for fut in tool_result_futures]

# Append to message list
        messages = add_messages(
            messages,
            [model_response, *tool_results, *remaining_tool_results],
        )

# Call model again
        model_response = call_model(messages).result()

# Generate final response
    messages = add_messages(messages, model_response)
    return entrypoint.final(value=model_response, save=messages)
python  theme={null}
config = {
    "configurable": {
        "thread_id": "1",  # [!code highlight]
        # optionally provide an ID for a specific checkpoint,
        # otherwise the latest checkpoint is shown
        # "checkpoint_id": "1f029ca3-1f5b-6704-8004-820c16b69a5a"  # [!code highlight]

}
}
graph.get_state(config)  # [!code highlight]

StateSnapshot(
    values={'messages': [HumanMessage(content="hi! I'm bob"), AIMessage(content='Hi Bob! How are you doing today?), HumanMessage(content="what's my name?"), AIMessage(content='Your name is Bob.')]}, next=(),
    config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1f5b-6704-8004-820c16b69a5a'}},
    metadata={
        'source': 'loop',
        'writes': {'call_model': {'messages': AIMessage(content='Your name is Bob.')}},
        'step': 4,
        'parents': {},
        'thread_id': '1'
    },
    created_at='2025-05-05T16:01:24.680462+00:00',
    parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1790-6b0a-8003-baf965b6a38f'}},
    tasks=(),
    interrupts=()
)
python  theme={null}
config = {
    "configurable": {
        "thread_id": "1"  # [!code highlight]
    }
}
list(graph.get_state_history(config))  # [!code highlight]

[
    StateSnapshot(
        values={'messages': [HumanMessage(content="hi! I'm bob"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?'), HumanMessage(content="what's my name?"), AIMessage(content='Your name is Bob.')]},
        next=(),
        config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1f5b-6704-8004-820c16b69a5a'}},
        metadata={'source': 'loop', 'writes': {'call_model': {'messages': AIMessage(content='Your name is Bob.')}}, 'step': 4, 'parents': {}, 'thread_id': '1'},
        created_at='2025-05-05T16:01:24.680462+00:00',
        parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1790-6b0a-8003-baf965b6a38f'}},
        tasks=(),
        interrupts=()
    ),
    StateSnapshot(
        values={'messages': [HumanMessage(content="hi! I'm bob"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?'), HumanMessage(content="what's my name?")]},
        next=('call_model',),
        config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1790-6b0a-8003-baf965b6a38f'}},
        metadata={'source': 'loop', 'writes': None, 'step': 3, 'parents': {}, 'thread_id': '1'},
        created_at='2025-05-05T16:01:23.863421+00:00',
        parent_config={...}
        tasks=(PregelTask(id='8ab4155e-6b15-b885-9ce5-bed69a2c305c', name='call_model', path=('__pregel_pull', 'call_model'), error=None, interrupts=(), state=None, result={'messages': AIMessage(content='Your name is Bob.')}),),
        interrupts=()
    ),
    StateSnapshot(
        values={'messages': [HumanMessage(content="hi! I'm bob"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')]},
        next=('__start__',),
        config={...},
        metadata={'source': 'input', 'writes': {'__start__': {'messages': [{'role': 'user', 'content': "what's my name?"}]}}, 'step': 2, 'parents': {}, 'thread_id': '1'},
        created_at='2025-05-05T16:01:23.863173+00:00',
        parent_config={...}
        tasks=(PregelTask(id='24ba39d6-6db1-4c9b-f4c5-682aeaf38dcd', name='__start__', path=('__pregel_pull', '__start__'), error=None, interrupts=(), state=None, result={'messages': [{'role': 'user', 'content': "what's my name?"}]}),),
        interrupts=()
    ),
    StateSnapshot(
        values={'messages': [HumanMessage(content="hi! I'm bob"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')]},
        next=(),
        config={...},
        metadata={'source': 'loop', 'writes': {'call_model': {'messages': AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')}}, 'step': 1, 'parents': {}, 'thread_id': '1'},
        created_at='2025-05-05T16:01:23.862295+00:00',
        parent_config={...}
        tasks=(),
        interrupts=()
    ),
    StateSnapshot(
        values={'messages': [HumanMessage(content="hi! I'm bob")]},
        next=('call_model',),
        config={...},
        metadata={'source': 'loop', 'writes': None, 'step': 0, 'parents': {}, 'thread_id': '1'},
        created_at='2025-05-05T16:01:22.278960+00:00',
        parent_config={...}
        tasks=(PregelTask(id='8cbd75e0-3720-b056-04f7-71ac805140a0', name='call_model', path=('__pregel_pull', 'call_model'), error=None, interrupts=(), state=None, result={'messages': AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')}),),
        interrupts=()
    ),
    StateSnapshot(
        values={'messages': []},
        next=('__start__',),
        config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-0870-6ce2-bfff-1f3f14c3e565'}},
        metadata={'source': 'input', 'writes': {'__start__': {'messages': [{'role': 'user', 'content': "hi! I'm bob"}]}}, 'step': -1, 'parents': {}, 'thread_id': '1'},
        created_at='2025-05-05T16:01:22.277497+00:00',
        parent_config=None,
        tasks=(PregelTask(id='d458367b-8265-812c-18e2-33001d199ce6', name='__start__', path=('__pregel_pull', '__start__'), error=None, interrupts=(), state=None, result={'messages': [{'role': 'user', 'content': "hi! I'm bob"}]}),),
        interrupts=()
    )
]
python  theme={null}
from langgraph.func import entrypoint
from langgraph.checkpoint.memory import InMemorySaver

checkpointer = InMemorySaver()

@entrypoint(checkpointer=checkpointer)
def accumulate(n: int, *, previous: int | None) -> entrypoint.final[int, int]:
    previous = previous or 0
    total = previous + n
    # Return the *previous* value to the caller but save the *new* total to the checkpoint.
    return entrypoint.final(value=previous, save=total)

config = {"configurable": {"thread_id": "my-thread"}}

print(accumulate.invoke(1, config=config))  # 0
print(accumulate.invoke(2, config=config))  # 1
print(accumulate.invoke(3, config=config))  # 3
python  theme={null}
from langchain.messages import BaseMessage
from langgraph.graph import add_messages
from langgraph.func import entrypoint, task
from langgraph.checkpoint.memory import InMemorySaver
from langchain_anthropic import ChatAnthropic

model = ChatAnthropic(model="claude-sonnet-4-5-20250929")

@task
def call_model(messages: list[BaseMessage]):
    response = model.invoke(messages)
    return response

checkpointer = InMemorySaver()

@entrypoint(checkpointer=checkpointer)
def workflow(inputs: list[BaseMessage], *, previous: list[BaseMessage]):
    if previous:
        inputs = add_messages(previous, inputs)

response = call_model(inputs).result()
    return entrypoint.final(value=response, save=add_messages(inputs, response))

config = {"configurable": {"thread_id": "1"}}
input_message = {"role": "user", "content": "hi! I'm bob"}
for chunk in workflow.stream([input_message], config, stream_mode="values"):
    chunk.pretty_print()

input_message = {"role": "user", "content": "what's my name?"}
for chunk in workflow.stream([input_message], config, stream_mode="values"):
    chunk.pretty_print()
```

[long-term memory](/oss/python/concepts/memory#long-term-memory) allows storing information across different **thread ids**. This could be useful for learning information about a given user in one conversation and using it in another.

* [Workflows and agent](/oss/python/langgraph/workflows-agents) guide for more examples of how to build workflows using the Functional API.

## Integrate with other libraries

* [Add LangGraph's features to other frameworks using the functional API](/langsmith/autogen-integration): Add LangGraph features like persistence, memory and streaming to other agent frameworks that do not provide them out of the box.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/use-functional-api.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
After resuming, the run proceeds through the remaining step and terminates as expected.

### Review tool calls

To review tool calls before execution, we add a `review_tool_call` function that calls [`interrupt`](/oss/python/langgraph/interrupts#pause-using-interrupt). When this function is called, execution will be paused until we issue a command to resume it.

Given a tool call, our function will [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) for human review. At that point we can either:

* Accept the tool call
* Revise the tool call and continue
* Generate a custom tool message (e.g., instructing the model to re-format its tool call)
```

Example 2 (unknown):
```unknown
We can now update our [entrypoint](/oss/python/langgraph/functional-api#entrypoint) to review the generated tool calls. If a tool call is accepted or revised, we execute in the same way as before. Otherwise, we just append the [`ToolMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.ToolMessage) supplied by the human. The results of prior tasks — in this case the initial model call — are persisted, so that they are not run again following the [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt).
```

Example 3 (unknown):
```unknown
## Short-term memory

Short-term memory allows storing information across different **invocations** of the same **thread id**. See [short-term memory](/oss/python/langgraph/functional-api#short-term-memory) for more details.

### Manage checkpoints

You can view and delete the information stored by the checkpointer.

<a id="checkpoint" />

#### View thread state
```

Example 4 (unknown):
```unknown

```

---

## Contributing

**URL:** llms-txt#contributing

**Contents:**
- Ways to Contribute

Source: https://docs.langchain.com/oss/javascript/contributing/overview

**Welcome! Thank you for your interest in contributing.**

LangChain has helped form the largest developer community in generative AI, and we're always open to new contributors. Whether you're fixing bugs, adding features, improving documentation, or sharing feedback, your involvement helps make LangChain and LangGraph better for everyone.

## Ways to Contribute

<AccordionGroup>
  <Accordion title="Report bugs" icon="bug">
    Found a bug? Please help us fix it by following these steps:

<Steps>
      <Step title="Search">
        Check if the issue already exists in our GitHub Issues for the respective repo:

<Columns cols={2}>
          <Card title="LangChain" icon="link" href="https://github.com/langchain-ai/langchainjs/issues">Issues</Card>
          <Card title="LangGraph" icon="circle-nodes" href="https://github.com/langchain-ai/langgraphjs/issues">Issues</Card>
        </Columns>
      </Step>

<Step title="Create issue">
        If no issue exists, create a new one. When writing, be sure to follow the template provided and to include a [minimal, reproducible, example](https://stackoverflow.com/help/minimal-reproducible-example). Attach any relevant labels to the final issue once created. If a project maintainer is unable to reproduce the issue, it is unlikely to be addressed in a timely manner.
      </Step>

<Step title="Wait">
        A project maintainer will triage the issue and may ask for additional information. Please be patient as we manage a high volume of issues. Do not bump the issue unless you have new information to provide.
      </Step>
    </Steps>

If you are adding an issue, please try to keep it focused on a single topic. If two issues are related, or blocking, please [link them](https://docs.github.com/en/issues/tracking-your-work-with-issues/using-issues/linking-a-pull-request-to-an-issue#linking-a-pull-request-to-an-issue-using-a-keyword) rather than combining them. For example,

<Accordion title="Suggest features" icon="wand-magic-sparkles">
    Have an idea for a new feature or enhancement?

<Steps>
      <Step title="Search">
        Search the issues for the respective repository for existing feature requests:

<Columns cols={2}>
          <Card title="LangChain" icon="link" href="https://github.com/langchain-ai/langchainjs">Issues</Card>
          <Card title="LangGraph" icon="circle-nodes" href="https://github.com/langchain-ai/langgraphjs/labels?q=feature">Issues</Card>
        </Columns>
      </Step>

<Step title="Discuss">
        If no requests exist, start a new discussion under the [relevant category](https://forum.langchain.com/c/help/langchain/13) so that project maintainers and the community can provide feedback.
      </Step>

<Step title="Describe">
        Be sure to describe the use case and why it would be valuable to others. If possible, provide examples or mockups where applicable. Outline test cases that should pass.
      </Step>
    </Steps>
  </Accordion>

<Accordion title="Improve documentation" icon="book">
    Documentation improvements are always welcome! We strive to keep our docs clear and comprehensive, and your perspective can make a big difference.

<Card title="How to propose changes to the documentation" href="/oss/javascript/contributing/documentation" arrow>Guide</Card>
  </Accordion>

<Accordion title="Contribute code" icon="code">
    With a large userbase, it can be hard for our small team to keep up with all the feature requests and bug fixes. If you have the skills and time, we would love your help!

<Card title="How to make your first Pull Request" href="/oss/javascript/contributing/code" arrow>Guide</Card>

If you start working on an issue, please assign it to yourself or ask a maintainer to do so. This helps avoid duplicate work.

If you are looking for something to work on, check out the issues labeled "good first issue" or "help wanted" in our repos:

<Columns cols={2}>
      <Card title="LangChain" icon="link" href="https://github.com/langchain-ai/langchainjs/labels">Labels</Card>
      <Card title="LangGraph" icon="circle-nodes" href="https://github.com/langchain-ai/langgraphjs/labels">Labels</Card>
    </Columns>
  </Accordion>

<Accordion title="Add a new integration" icon="plug-circle-plus">
    <Card title="LangChain" icon="link" href="/oss/javascript/contributing/integrations-langchain" arrow>Guide to adding a new LangChain integration</Card>
  </Accordion>
</AccordionGroup>

Thank you for helping make LangChain better! 🦜❤️

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/contributing/overview.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Contributing integrations

**URL:** llms-txt#contributing-integrations

Source: https://docs.langchain.com/oss/javascript/contributing/integrations-langgraph

<Icon icon="plug-circle-plus" size={32} />

**Integrations are a core component of LangGraph.**

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/contributing/integrations-langgraph.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Contributing to code

**URL:** llms-txt#contributing-to-code

**Contents:**
- Philosophy
- Getting started
  - Quick fix: submit a bugfix
  - Full development setup
  - Development environment
- Repository structure
- Development workflow
  - Testing requirements
  - Code quality standards
- Contribution guidelines

Source: https://docs.langchain.com/oss/javascript/contributing/code

Code contributions are always welcome! Whether you're fixing bugs, adding features, or improving performance, your contributions help deliver a better developer experience for thousands of developers.

<Note>
  Before submitting large **new features or refactors**, please first discuss your ideas in [the forum](https://forum.langchain.com/). This ensures alignment with project goals and prevents duplicate work.

This does not apply to bugfixes or small improvements, which you can contribute directly via pull requests. Be sure to link any relevant issues in your PR description. Use <Tooltip tip="(e.g. `Fixes #123`)">closing keywords</Tooltip> to automatically close issues when the PR is merged.

New integrations should follow the [integration guidelines](/oss/javascript/contributing#add-a-new-integration).
</Note>

Aim to follow these core principles for all code contributions:

<CardGroup cols={2}>
  <Card title="Backwards compatibility" icon="shield" href="#backwards-compatibility" arrow>
    Maintain stable public interfaces and avoid breaking changes
  </Card>

<Card title="Testing first" icon="flask" href="#testing-requirements" arrow>
    Every change must include comprehensive tests to verify correctness and prevent regressions
  </Card>

<Card title="Code quality" icon="star" href="#code-quality-standards" arrow>
    Follow consistent style, documentation, and architecture patterns
  </Card>

<Card title="Security focused" icon="lock" href="#security-guidelines" arrow>
    Prioritize secure coding practices and vulnerability prevention
  </Card>
</CardGroup>

### Quick fix: submit a bugfix

For simple bugfixes, you can get started immediately:

<Steps>
  <Step title="Fork the repository">
    Fork the [LangChain](https://github.com/langchain-ai/langchainjs) or [LangGraph](https://github.com/langchain-ai/langgraphjs) repo to your personal GitHub account
  </Step>

<Step title="Clone and setup">

<Step title="Create a branch">
    
  </Step>

<Step title="Make your changes">
    Fix the bug while following our [code quality standards](#code-quality-standards)
  </Step>

<Step title="Add tests">
    Include [unit tests](#test-writing-guidelines) that fail without your fix. This allows us to verify the bug is resolved and prevents regressions
  </Step>

<Step title="Run build">
    Run the build command to ensure the package still builds properly

<Step title="Run tests">
    Ensure all tests pass locally before submitting your PR

<Step title="Submit a pull request">
    Follow the PR template provided. If applicable, reference the issue you're fixing using a [closing keyword](https://docs.github.com/en/issues/tracking-your-work-with-issues/using-issues/linking-a-pull-request-to-an-issue#linking-a-pull-request-to-an-issue-using-a-keyword) (e.g. `Fixes #123`).
  </Step>
</Steps>

### Full development setup

For ongoing development or larger contributions:

<Steps>
  <Step title="Development environment">
    Set up your environment following our [setup guide](#development-environment) below
  </Step>

<Step title="Repository structure">
    Understand the [repository structure](#repository-structure) and package organization
  </Step>

<Step title="Development workflow">
    Learn our [development workflow](#development-workflow) including testing and linting
  </Step>

<Step title="Contribution guidelines">
    Review our [contribution guidelines](#contribution-guidelines) for features, bugfixes, and integrations
  </Step>
</Steps>

### Development environment

<Warning>
  Our JS/TS projects uses [`pnpm`](https://pnpm.io/) for dependency management. Make sure you have the latest version installed, or run `corepack enable` (on Node 24+) to setup the required pnpm version.
</Warning>

Set up a development environment for the package(s) you're working on.

<Tabs>
  <Tab title="LangChain" icon="link">
    <AccordionGroup>
      <Accordion title="Core abstractions">
        For changes to `langchain-core`:

<Accordion title="Main package">
        For changes to `langchain`:

<Accordion title="Provider packages">
        For changes to [partner integrations](/oss/javascript/integrations/providers/overview):

<Accordion title="Community packages">
        For changes to [community integrations](https://github.com/langchain-ai/langchainjs/tree/main/libs/langchain-community):

</Accordion>
    </AccordionGroup>
  </Tab>

<Tab title="LangGraph" icon="circle-nodes">
    WIP - coming soon! In the meantime, follow instructions for LangChain.
  </Tab>
</Tabs>

## Repository structure

<Tabs>
  <Tab title="LangChain" icon="link">
    LangChain is organized as a monorepo with multiple packages:

<AccordionGroup>
      <Accordion title="Core packages" defaultOpen>
        * **[`langchain`](https://github.com/langchain-ai/langchainjs/tree/main/langchain#readme)** (located in `libs/langchain/`): Main package with chains, agents, and retrieval logic
        * **[`@langchain/core`](https://github.com/langchain-ai/langchainjs/tree/main/langchain-core#readme)** (located in `libs/langchain-core/`): Base interfaces and core abstractions
      </Accordion>

<Accordion title="Partner packages">
        Located in `libs/providers/`, these are independently versioned packages for specific integrations. For example:

* **[`@langchain/openai`](https://github.com/langchain-ai/langchainjs/tree/main/libs/langchain-openai#readme)**: [OpenAI](/oss/javascript/integrations/providers/openai) integrations
        * **[`@langchain/anthropic`](https://github.com/langchain-ai/langchainjs/tree/main/libs/langchain-anthropic#readme)**: [Anthropic](/oss/javascript/integrations/providers/anthropic) integrations
        * **[`@langchain/google-genai`](https://github.com/langchain-ai/langchainjs/tree/main/libs/langchain-google-genai#readme)**: [Google Generative AI](/oss/javascript/integrations/chat/google_generative_ai) integrations
      </Accordion>

<Accordion title="Supporting packages">
        * **[`@langchain/textsplitters`](https://github.com/langchain-ai/langchainjs/tree/main/libs/langchain-textsplitters#readme)**: Text splitting utilities
        * **[`@langchain/standard-tests`](https://github.com/langchain-ai/langchainjs/tree/main/libs/langchain-standard-tests#readme)**: Standard test suites for integrations
        * **[`langchain-community`](https://github.com/langchain-ai/langchainjs/tree/main/libs/langchain-community)**: Community maintained integrations
      </Accordion>
    </AccordionGroup>
  </Tab>

<Tab title="LangGraph" icon="circle-nodes">
    WIP - coming soon! In the meantime, follow instructions for LangChain.
  </Tab>
</Tabs>

## Development workflow

### Testing requirements

<Info>
  Directories are relative to the package you're working in.
</Info>

Every code change must include comprehensive tests.

<Steps>
  <Step title="Unit tests">
    **Location**: `src/tests/FILENAME_BEING_TESTED.test.ts`

* No network calls allowed
    * Test all code paths including edge cases
    * Use mocks for external dependencies

<Step title="Integration tests">
    Integration tests require access to external services/ provider APIs (which can cost money) and therefore are not run by default.

Not every code change will require an integration test, but keep in mind that we'll require/ run integration tests separately as apart of our review process.

**Location**: `src/tests/FILENAME_BEING_TESTED.int.test.ts`

* Test real integrations with external services
    * Use environment variables for API keys
    * Skip gracefully if credentials unavailable

<Step title="Test quality checklist">
    * Tests fail when your code is broken
    * Edge cases and error conditions are tested
    * Proper use of fixtures and mocks
  </Step>
</Steps>

### Code quality standards

Quality requirements:

<Tabs>
  <Tab title="Type hints">
    **Required**: Complete types for all functions

<Tab title="Documentation">
    **Required**: [JSDocs](https://jsdoc.app/about-getting-started) for all exported functions and interfaces

* Document all parameters and return values
    * Include usage examples for complex functions
    * Document raised exceptions
    * Focus on "why" rather than "what"
  </Tab>

<Tab title="Code style">
    **Automated**: Formatting and linting:

* Descriptive variable names
    * Break up complex functions (aim for fewer than 20 lines)
    * Follow existing patterns in the codebase
  </Tab>
</Tabs>

## Contribution guidelines

### Backwards compatibility

<Warning>
  Breaking changes to public APIs are not allowed except for critical security fixes.

See our [versioning policy](/oss/javascript/versioning) for details on major version releases.
</Warning>

Maintain compatibility:

<AccordionGroup>
  <Accordion title="Stable interfaces">
    **Always preserve**:

* Function signatures and parameter names
    * Class interfaces and method names
    * Return value structure and types
    * Import paths for public APIs
  </Accordion>

<Accordion title="Safe changes">
    **Acceptable modifications**:

* Adding new optional parameters/type parameters

* Adding new methods to classes

* Improving performance without changing behavior

* Adding new modules or functions
  </Accordion>

<Accordion title="Before making changes">
    * **Would this break existing user code?**

* Check if your target is public

* Are there existing usage patterns in tests?
  </Accordion>
</AccordionGroup>

For bugfix contributions:

<Steps>
  <Step title="Reproduce the issue">
    Create a minimal test case that demonstrates the bug. Maintainers and other contributors should be able to run this test and see the failure without additional setup or modification
  </Step>

<Step title="Write failing tests">
    Add unit tests that would fail without your fix
  </Step>

<Step title="Implement the fix">
    Make the **minimal change necessary** to resolve the issue
  </Step>

<Step title="Verify the fix">
    Ensure that tests pass and no regressions are introduced
  </Step>

<Step title="Document the change">
    Update docstrings if behavior changes, add comments for complex logic
  </Step>
</Steps>

We aim to keep the bar high for new features. We generally don't accept new core abstractions, changes to infra, changes to dependencies, or new agents/chains from outside contributors without an existing issue that demonstrates an acute need for them.

In general, feature contribution requirements include:

<Steps>
  <Step title="Design discussion">
    Open an issue describing:

* The problem you're solving
    * Proposed API design
    * Expected usage patterns
  </Step>

<Step title="Implementation">
    * Follow existing code patterns
    * Include comprehensive tests and documentation
    * Consider security implications
  </Step>

<Step title="Integration considerations">
    * How does this interact with existing features?
    * Are there performance implications?
    * Does this introduce new dependencies?

We will reject features that are likely to lead to security vulnerabilities or reports.
  </Step>
</Steps>

### Security guidelines

<Warning>
  Security is paramount. Never introduce vulnerabilities or unsafe patterns.
</Warning>

<AccordionGroup>
  <Accordion title="Input validation">
    * Validate and sanitize all user inputs

* Properly escape data in templates and queries

* Never use `eval()`, as this can lead to arbitrary code execution vulnerabilities
  </Accordion>

<Accordion title="Error handling">
    * Use specific exception types
    * Don't expose sensitive information in error messages
    * Implement proper resource cleanup
  </Accordion>

<Accordion title="Dependencies">
    * Avoid adding hard dependencies
    * Keep optional dependencies minimal
    * Review third-party packages for security issues
  </Accordion>
</AccordionGroup>

## Testing and validation

### Running tests locally

Before submitting your PR, ensure you have completed the following steps. Note that the requirements differ slightly between LangChain and LangGraph.

<Tabs>
  <Tab title="LangChain" icon="link">
    <Steps>
      <Step title="Unit tests">

All unit tests must pass
      </Step>

<Step title="Integration tests">

(Run if your changes affect integrations)
      </Step>

<Step title="Formatting">

Code must pass all style checks
      </Step>

<Step title="PR submission">
        Push your branch and open a pull request. Follow the provided form template. Note related issues using a [closing keyword](https://docs.github.com/en/issues/tracking-your-work-with-issues/using-issues/linking-a-pull-request-to-an-issue#linking-a-pull-request-to-an-issue-using-a-keyword). After submitting, wait, and check to ensure the CI checks pass. If any checks fail, address the issues promptly - maintainers may close PRs that do not pass CI within a reasonable timeframe.
      </Step>
    </Steps>
  </Tab>

<Tab title="LangGraph" icon="circle-nodes">
    WIP - coming soon! In the meantime, follow instructions for LangChain.
  </Tab>
</Tabs>

### Test writing guidelines

In order to write effective tests, there's a few good practices to follow:

* Encapsulate the test in a `describe` block that describes the component being tested
* Use natural language to describe the test name
* Be exhaustive with assertions
* Only use snapshots for reasonably sized data objects

<Tabs>
  <Tab title="Unit tests">
    
  </Tab>

<Tab title="Integration tests">
    
  </Tab>

<Tab title="Mock usage">
    
  </Tab>
</Tabs>

Our goal is to have the most accessible developer setup possible. Should you experience any difficulty getting setup, please ask in the [community slack](https://www.langchain.com/join-community) or open a [forum post](https://forum.langchain.com/).

<Check>
  You're now ready to contribute high-quality code to LangChain!
</Check>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/contributing/code.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Create a branch">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Make your changes">
    Fix the bug while following our [code quality standards](#code-quality-standards)
  </Step>

  <Step title="Add tests">
    Include [unit tests](#test-writing-guidelines) that fail without your fix. This allows us to verify the bug is resolved and prevents regressions
  </Step>

  <Step title="Run build">
    Run the build command to ensure the package still builds properly
```

Example 4 (unknown):
```unknown
</Step>

  <Step title="Run tests">
    Ensure all tests pass locally before submitting your PR
```

---

## Contributing to documentation

**URL:** llms-txt#contributing-to-documentation

**Contents:**
- Getting started
  - Quick edit: fix a typo
  - Full development IDE setup
- Documentation types
  - Conceptual guides
  - References
- Writing standard
  - Mintlify components
  - Page structure
  - Co-locate Python and JavaScript/TypeScript content

Source: https://docs.langchain.com/oss/javascript/contributing/documentation

Accessible documentation is a vital part of LangChain. We welcome both documentation for new features/[integrations](/oss/javascript/contributing/publish-langchain#adding-documentation), as well as community improvements to existing docs.

<Note>
  We generally do not merge new tutorials from outside contributors without an acute need. If you feel that a certain topic is missing from docs or is not sufficiently covered, please [open a new issue](https://github.com/langchain-ai/docs/issues).
</Note>

All documentation falls under one of four categories:

<CardGroup cols={2}>
  <Card title="Conceptual guides" icon="lightbulb" href="#conceptual-guides">
    Explanations that provide deeper understanding and insights
  </Card>

<Card title="References" icon="book" href="#references">
    Technical descriptions of APIs and implementation details
  </Card>

<Card title="Tutorials (Learn)" icon="graduation-cap" href="/oss/javascript/learn">
    Lessons that guide users through practical activities to build understanding
  </Card>

<Card title="How-to guides" icon="wrench">
    Task-oriented instructions for users who know what they want to accomplish
  </Card>
</CardGroup>

### Quick edit: fix a typo

For simple changes like fixing typos, you can edit directly on GitHub without setting up a local development environment:

<Info>
  **Prerequisites:**

* A [GitHub](https://github.com/) account
  * Basic familiarity of the [fork-and-pull workflow](https://graphite.dev/guides/understanding-git-fork-pull-request-workflow) for contributing
</Info>

<Steps>
  <Step title="Find the page">
    Navigate to any documentation page, scroll to the bottom of the page, and click the link "Edit the source of this page on GitHub"
  </Step>

<Step title="Fork the repository">
    GitHub will prompt you to fork the repository to your account. Make sure to fork into your <Tooltip tip="If you clone to an organization, maintainers are unable to make edits.">personal account</Tooltip>
  </Step>

<Step title="Make your changes">
    Correct the typo directly in GitHub's web editor
  </Step>

<Step title="Commit your changes">
    Click `Commit changes...` and give your commit a descriptive title like `fix(docs): summary of change`. If applicable, add an [extended description](https://www.gitkraken.com/learn/git/best-practices/git-commit-message#git-commit-message-structure)
  </Step>

<Step title="Create pull request">
    GitHub will redirect you to create a pull request. Give it a title (often the same as the commit) and follow the PR template checklist, if present
  </Step>
</Steps>

<Note>
  Docs PRs are typically reviewed within a few days. Keep an eye on your PR to address any feedback from maintainers. Do not bump the PR unless you have new information to provide - maintainers will address it as their availability permits.
</Note>

### Full development IDE setup

For larger changes or ongoing contributions, it's important to set up a local development environment on your machine. Our documentation build pipeline offers local preview and live reload as you edit, important for ensuring your changes appear as intended before submitting.

Please review the steps to set up your environment outlined in the docs repo [`README.md`](https://github.com/langchain-ai/docs?tab=readme-ov-file#contributing).

## Documentation types

<Note>
  Where applicable, all documentation must have translations in both Python and JavaScript/TypeScript. See [our localization guide](#localization) for details.
</Note>

### Conceptual guides

Conceptual guide cover core concepts abstractly, providing deep understanding.

<AccordionGroup>
  <Accordion title="Characteristics">
    * **Understanding-focused**: Explain why things work as they do
    * **Broad perspective**: Higher and wider view than other types
    * **Design-oriented**: Explain decisions and trade-offs
    * **Context-rich**: Use analogies and comparisons
  </Accordion>

<Accordion title="Tips">
    * Explain design decisions - *"why does concept X exist?"*
    * Use analogies and reference alternatives
    * Avoid blending in too much reference content
    * Link to related tutorials and how-to guides
    * Focus on the **"why"** rather than the "how"
  </Accordion>

<Accordion title="Examples">
    <Columns cols={3}>
      <Card title="Memory" href="/oss/javascript/concepts/memory" icon="brain" />

<Card title="Context" href="/oss/javascript/concepts/context" icon="file-lines" />
    </Columns>
  </Accordion>
</AccordionGroup>

Reference documentation contains detailed, low-level information describing exactly what functionality exists and how to use it.

<Card title="JavaScript/TypeScript reference" href="https://reference.langchain.com/javascript/" icon="js" arrow />

A good reference should:

* Describe what exists (all parameters, options, return values)
* Be comprehensive and structured for easy lookup
* Serve as the authoritative source for technical details

<AccordionGroup>
  <Accordion title="Contributing to references">
    See the contributing guide for [JavaScript/TypeScript reference docs](https://github.com/langchain-ai/docs/blob/main/reference/javascript/README.md).
  </Accordion>

<Accordion title="LangChain reference best practices">
    * **Be consistent**; follow existing patterns for provider-specific documentation
    * Include both basic usage (code snippets) and common edge cases/failure modes
    * Note when features require specific versions
  </Accordion>

<Accordion title="When to create new reference documentation">
    * New integrations or providers need dedicated reference pages
    * Complex configuration options require detailed explanation
    * API changes introduce new parameters or behavior
    * Community frequently asks questions about specific functionality
  </Accordion>
</AccordionGroup>

<Warning>
  Reference documentation has different standards - see the [reference docs contributing guide](https://github.com/langchain-ai/docs/blob/main/reference/javascript/README.md) for details.
</Warning>

### Mintlify components

Use appropriate [Mintlify components](https://mintlify.com/docs/text) to enhance readability:

<Tabs>
  <Tab title="Callouts">
    * `<Note>` for helpful supplementary information
    * `<Warning>` for important cautions and breaking changes
    * `<Tip>` for best practices and advice
    * `<Info>` for neutral contextual information
    * `<Check>` for success confirmations
  </Tab>

<Tab title="Structure">
    * `<Steps>` for sequential procedures
    * `<Tabs>` for platform-specific content
    * `<AccordionGroup>` and `<Accordion>` for progressive disclosure
    * `<CardGroup>` and `<Card>` for highlighting content
  </Tab>

<Tab title="Code">
    * `<CodeGroup>` for multiple language examples
    * Always specify language tags on code blocks
    * Titles for code blocks (e.g. `Success`, `Error Response`)
  </Tab>
</Tabs>

Every documentation page must begin with YAML frontmatter:

### Co-locate Python and JavaScript/TypeScript content

All documentation must be written in both Python and JavaScript/TypeScript when possible. To do so, we use a custom in-line syntax to differentiate between sections that should appear in one or both languages:

<Note>
  We don't want a lack of parity to block contributions. If a feature is only available in one language, it's okay to have documentation only in that language until the other language catches up. In such cases, please include a note indicating that the feature is not yet available in the other language.
</Note>

<Note>
  If you need help translating content between Python and JavaScript/TypeScript, please ask in the [community slack](https://www.langchain.com/join-community) or tag a maintainer in your PR.
</Note>

We are working to add support for other languages, thanks to our LangChain Ambassadors!

### General guidelines

<AccordionGroup>
  <Accordion title="Avoid duplication">
    Multiple pages covering the same material are difficult to maintain and cause confusion. There should be only one canonical page for each concept or feature. Link to other guides instead of re-explaining.
  </Accordion>

<Accordion title="Link frequently">
    Documentation sections don't exist in a vacuum. Link to other sections frequently to allow users to learn about unfamiliar topics. This includes linking to API references and conceptual sections.
  </Accordion>

<Accordion title="Be concise">
    Take a less-is-more approach. If another section with a good explanation exists, link to it rather than re-explain, unless your content presents a new angle.
  </Accordion>
</AccordionGroup>

### Accessibility requirements

Ensure documentation is accessible to all users:

* Structure content for easy scanning with headers and lists
* Use specific, actionable link text instead of "click here"
* Include descriptive alt text for all images and diagrams

### Testing and validation

Before submitting documentation:

<Steps>
  <Step title="Test all code">
    Run all code examples to ensure they work correctly
  </Step>

<Step title="Check formatting">
    
  </Step>

<Step title="Build locally">

Verify the build completes without errors
  </Step>

<Step title="Review links">
    Check that all internal links work correctly
  </Step>
</Steps>

## In-code documentation

### Language and style

<Info>
  Use [Google-style docstrings](https://google.github.io/styleguide/pyguide.html) with complete type hints for all public functions.
</Info>

Follow these standards for all documentation:

* **Voice**: Use second person ("you") for instructions
* **Tense**: Use active voice and present tense
* **Clarity**: Write clear, direct language for technical audiences
* **Consistency**: Use consistent terminology throughout
* **Conciseness**: Keep sentences concise while providing necessary context

<Warning>
  Always test code examples before publishing. Never include real API keys or secrets.
</Warning>

Requirements for code examples:

<Steps>
  <Step title="Completeness">
    Include complete, runnable examples that users can copy and execute without errors
  </Step>

<Step title="Realism">
    Use realistic data instead of placeholder values like "foo" or "example"
  </Step>

<Step title="Error handling">
    Show proper error handling and edge case management
  </Step>

<Step title="Documentation">
    Add explanatory comments for complex logic
  </Step>
</Steps>

Example of a well-documented function:

Our goal is to have the simplest developer setup possible. Should you experience any difficulty getting setup, please ask in the [community slack](https://www.langchain.com/join-community) or open a [forum post](https://forum.langchain.com/).

<Check>
  You now have everything you need to contribute high-quality documentation to LangChain! 🎤🦜
</Check>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/contributing/documentation.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
### Co-locate Python and JavaScript/TypeScript content

All documentation must be written in both Python and JavaScript/TypeScript when possible. To do so, we use a custom in-line syntax to differentiate between sections that should appear in one or both languages:
```

Example 2 (unknown):
```unknown
<Note>
  We don't want a lack of parity to block contributions. If a feature is only available in one language, it's okay to have documentation only in that language until the other language catches up. In such cases, please include a note indicating that the feature is not yet available in the other language.
</Note>

<Note>
  If you need help translating content between Python and JavaScript/TypeScript, please ask in the [community slack](https://www.langchain.com/join-community) or tag a maintainer in your PR.
</Note>

### Localization

We are working to add support for other languages, thanks to our LangChain Ambassadors!

***

## Quality standards

### General guidelines

<AccordionGroup>
  <Accordion title="Avoid duplication">
    Multiple pages covering the same material are difficult to maintain and cause confusion. There should be only one canonical page for each concept or feature. Link to other guides instead of re-explaining.
  </Accordion>

  <Accordion title="Link frequently">
    Documentation sections don't exist in a vacuum. Link to other sections frequently to allow users to learn about unfamiliar topics. This includes linking to API references and conceptual sections.
  </Accordion>

  <Accordion title="Be concise">
    Take a less-is-more approach. If another section with a good explanation exists, link to it rather than re-explain, unless your content presents a new angle.
  </Accordion>
</AccordionGroup>

### Accessibility requirements

Ensure documentation is accessible to all users:

* Structure content for easy scanning with headers and lists
* Use specific, actionable link text instead of "click here"
* Include descriptive alt text for all images and diagrams

### Testing and validation

Before submitting documentation:

<Steps>
  <Step title="Test all code">
    Run all code examples to ensure they work correctly
  </Step>

  <Step title="Check formatting">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Build locally">
```

Example 4 (unknown):
```unknown
Verify the build completes without errors
  </Step>

  <Step title="Review links">
    Check that all internal links work correctly
  </Step>
</Steps>

***

## In-code documentation

### Language and style

<Info>
  Use [Google-style docstrings](https://google.github.io/styleguide/pyguide.html) with complete type hints for all public functions.
</Info>

Follow these standards for all documentation:

* **Voice**: Use second person ("you") for instructions
* **Tense**: Use active voice and present tense
* **Clarity**: Write clear, direct language for technical audiences
* **Consistency**: Use consistent terminology throughout
* **Conciseness**: Keep sentences concise while providing necessary context

### Code examples

<Warning>
  Always test code examples before publishing. Never include real API keys or secrets.
</Warning>

Requirements for code examples:

<Steps>
  <Step title="Completeness">
    Include complete, runnable examples that users can copy and execute without errors
  </Step>

  <Step title="Realism">
    Use realistic data instead of placeholder values like "foo" or "example"
  </Step>

  <Step title="Error handling">
    Show proper error handling and edge case management
  </Step>

  <Step title="Documentation">
    Add explanatory comments for complex logic
  </Step>
</Steps>

Example of a well-documented function:
```

---

## Control plane API reference for LangSmith Deployment

**URL:** llms-txt#control-plane-api-reference-for-langsmith-deployment

**Contents:**
- Host
- Authentication
- Versioning
- Quick Start
- Example Code

Source: https://docs.langchain.com/langsmith/api-ref-control-plane

The control plane API is part of [LangSmith Deployment](/langsmith/deployments). With the control plane API, you can programmatically create, manage, and automate your [Agent Server](/langsmith/agent-server) deployments—for example, as part of a custom CI/CD workflow.

<Card title="API Reference" href="https://api.host.langchain.com/docs" icon="book">
  View the full Control Plane API reference documentation
</Card>

The control plane hosts for Cloud data regions:

| US                               | EU                                  |
| -------------------------------- | ----------------------------------- |
| `https://api.host.langchain.com` | `https://eu.api.host.langchain.com` |

**Note**: Self-hosted deployments of LangSmith will have a custom host for the control plane. The control plane APIs can be accessed at the path `/api-host`. For example, `http(s)://<host>/api-host/v2/deployments`. See [here](../langsmith/self-host-usage#configuring-the-application-you-want-to-use-with-langsmith) for more details.

To authenticate with the control plane API, set the `X-Api-Key` header to a valid LangSmith API key and set the `X-Tenant-Id` header to a valid workspace ID to target.

Example `curl` command:

Each endpoint path is prefixed with a version (e.g. `v1`, `v2`).

1. Call `POST /v2/deployments` to create a new Deployment. The response body contains the Deployment ID (`id`) and the ID of the latest (and first) revision (`latest_revision_id`).
2. Call `GET /v2/deployments/{deployment_id}` to retrieve the Deployment. Set `deployment_id` in the URL to the value of Deployment ID (`id`).
3. Poll for revision `status` until `status` is `DEPLOYED` by calling `GET /v2/deployments/{deployment_id}/revisions/{latest_revision_id}`.
4. Call `PATCH /v2/deployments/{deployment_id}` to update the deployment.

Below is example Python code that demonstrates how to orchestrate the control plane APIs to create a deployment, update the deployment, and delete the deployment.

```python  theme={null}
import os
import time

import requests
from dotenv import load_dotenv

**Examples:**

Example 1 (unknown):
```unknown
## Versioning

Each endpoint path is prefixed with a version (e.g. `v1`, `v2`).

## Quick Start

1. Call `POST /v2/deployments` to create a new Deployment. The response body contains the Deployment ID (`id`) and the ID of the latest (and first) revision (`latest_revision_id`).
2. Call `GET /v2/deployments/{deployment_id}` to retrieve the Deployment. Set `deployment_id` in the URL to the value of Deployment ID (`id`).
3. Poll for revision `status` until `status` is `DEPLOYED` by calling `GET /v2/deployments/{deployment_id}/revisions/{latest_revision_id}`.
4. Call `PATCH /v2/deployments/{deployment_id}` to update the deployment.

## Example Code

Below is example Python code that demonstrates how to orchestrate the control plane APIs to create a deployment, update the deployment, and delete the deployment.
```

---

## Conversation 1: Learn about a project

**URL:** llms-txt#conversation-1:-learn-about-a-project

agent.invoke({
    "messages": [{"role": "user", "content": "We're building a web app with React. Save project notes."}]
})

---

## Conversation 2: Use that knowledge

**URL:** llms-txt#conversation-2:-use-that-knowledge

agent.invoke({
    "messages": [{"role": "user", "content": "What framework are we using?"}]
})

---

## Co-marketing

**URL:** llms-txt#co-marketing

**Contents:**
  - Content we're excited to promote

Source: https://docs.langchain.com/oss/javascript/contributing/comarketing

With over 60 million monthly downloads, LangChain has a large audience of developers building LLM applications. Beyond just listing integrations, we aim to highlight high-quality, educational examples that inspire developers and advance the ecosystem.

<Note>
  While we occasionally share integrations, we prioritize content that provides
  meaningful insights and best practices. Our main social channels are [Twitter](https://x.com/LangChainAI) and
  [LinkedIn](https://www.linkedin.com/company/langchain/), where we highlight the best examples.
</Note>

### Content we're excited to promote

<AccordionGroup>
  <Accordion title="Educational content" icon="graduation-cap">
    Blogs, YouTube videos and other media showcasing educational content. Note that we prefer content that is NOT framed as "here's how to use integration XYZ", but rather "here's how to do ABC", as we find that is more educational and helpful for developers.
  </Accordion>

<Accordion title="End-to-end applications" icon="cube">
    End-to-end applications are great resources for developers looking to build. We prefer to highlight applications that are more complex/agentic in nature, and that use [LangGraph](https://github.com/langchain-ai/langgraphjs) as the orchestration framework. We get particularly excited about anything involving:

* Long-term memory systems
    * Human-in-the-loop interaction patterns
    * Multi-agent architectures
  </Accordion>

<Accordion title="Research" icon="flask">
    We love highlighting novel research! Whether it is research built on top of LangChain or that integrates with it.
  </Accordion>
</AccordionGroup>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/contributing/comarketing.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Create and manage datasets in the UI

**URL:** llms-txt#create-and-manage-datasets-in-the-ui

**Contents:**
- Create a dataset and add examples
  - Manually from a tracing project
  - Automatically from a tracing project
  - From examples in an Annotation Queue
  - From the Prompt Playground
  - Import a dataset from a CSV or JSONL file
  - Create a new dataset from the Datasets & Experiments page
  - Add synthetic examples created by an LLM
- Manage a dataset
  - Create a dataset schema

Source: https://docs.langchain.com/langsmith/manage-datasets-in-application

[*Datasets*](/langsmith/evaluation-concepts#datasets) enable you to perform repeatable evaluations over time using consistent data. Datasets are made up of [*examples*](/langsmith/evaluation-concepts#examples), which store inputs, outputs, and optionally, reference outputs.

This page outlines the various methods for [creating](#create-a-dataset-and-add-examples) and [managing](#manage-a-dataset) datasets in the [LangSmith UI](https://smith.langchain.com).

## Create a dataset and add examples

The following sections explain the different ways you can create a dataset in LangSmith and add examples to it. Depending on your workflow, you can manually curate examples, automatically capture them from tracing, import files, or even generate synthetic data:

* [Manually from a tracing project](#manually-from-a-tracing-project)
* [Automatically from a tracing project](#automatically-from-a-tracing-project)
* [From examples in an Annotation Queue](#from-examples-in-an-annotation-queue)
* [From the Prompt Playground](#from-the-prompt-playground)
* [Import a dataset from a CSV or JSONL file](#import-a-dataset-from-a-csv-or-jsonl-file)
* [Create a new dataset from the dataset page](#create-a-new-dataset-from-the-dataset-page)
* [Add synthetic examples created by an LLM via the Datasets UI](#add-synthetic-examples-created-by-an-llm-via-the-datasets-ui)

### Manually from a tracing project

A common pattern for constructing datasets is to convert notable traces from your application into dataset examples. This approach requires that you have [configured tracing to LangSmith](/langsmith/observability-concepts#tracing-configuration).

<Check>
  A technique to build datasets is to filter the most interesting traces, such as traces that were tagged with poor user feedback, and add them to a dataset. For tips on how to filter traces, refer to [Filter traces](/langsmith/filter-traces-in-application) guide.
</Check>

There are two ways to add data manually from a tracing project to datasets. Navigate to **Tracing Projects** and select a project.

1. Multi-select runs from the runs table. On the **Runs** tab, multi-select runs. At the bottom of the page, click <Icon icon="database" /> **Add to Dataset**.

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiselect-add-to-dataset.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=fe666b6e1888554d0573df770bd9cda2" alt="The Runs table with a run selected and the Add to Dataset button visible at the bottom of the page." data-og-width="2912" width="2912" data-og-height="1464" height="1464" data-path="langsmith/images/multiselect-add-to-dataset.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiselect-add-to-dataset.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=2a5526d48109c7e33cb5b72f19c0ae2d 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiselect-add-to-dataset.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=0434d04eb3c301ea49a369e5def47701 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiselect-add-to-dataset.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=3f0d3184b1dea71c3794ba653637a513 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiselect-add-to-dataset.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=69e347a4a8f26bf7343ba05e88d9fe24 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiselect-add-to-dataset.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=e4187008b9cc8f5936b7d64251d4641b 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiselect-add-to-dataset.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=9419ca3c63abd40cfb899c5cffc36e33 2500w" />

2. On the **Runs** tab, select a run from the table. On the individual run details page, select  **Add to** -> **Dataset** in the top right corner.

<img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-to..dataset.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=fd03b2cf578c3e524223afc5b09d0589" alt="Add to dataset" data-og-width="2898" width="2898" data-og-height="1462" height="1462" data-path="langsmith/images/add-to..dataset.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-to..dataset.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=70baaf45d7471f218c95af5ee77530d8 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-to..dataset.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=dd8fcd49d9b39e3937c3abcb0b2afc31 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-to..dataset.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=9477bdbe20caa5ccdb50ea1e4e18f234 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-to..dataset.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=8c527bf07a2e032583496c9ba4ddf0c5 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-to..dataset.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=1c538e79a32bc8bb0e6458f1bf7a2276 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-to..dataset.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=7a739ceda9692ae6661ce365a9ca46ce 2500w" />

When you select a dataset from the run details page, a modal will pop up letting you know if any [transformations](/langsmith/dataset-transformations) were applied or if schema validation failed. For example, the screenshot below shows a dataset that is using transformations to optimize for collecting LLM runs.

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/confirmation.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=4ac9ca81489294ac40bf4b88a68ba1c9" alt="Confirmation" data-og-width="2898" width="2898" data-og-height="1452" height="1452" data-path="langsmith/images/confirmation.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/confirmation.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=3c09ffaacb03ec55aeddda61a3a58111 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/confirmation.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=9146d5498be48425a6de3caf28db394f 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/confirmation.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=299ac17e36643502a8ddc0a7d4d49780 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/confirmation.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=7607f473d263d7a9d55e1720571d3755 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/confirmation.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=deb6514e2202d020882f4fc810207795 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/confirmation.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=4141f028af5084d808297d936f168121 2500w" />

You can then optionally edit the run before adding it to the dataset.

### Automatically from a tracing project

You can use [run rules](/langsmith/rules) to automatically add traces to a dataset based on certain conditions. For example, you could add all traces that are [tagged](/langsmith/observability-concepts#tags) with a specific use case or have a [low feedback score](/langsmith/observability-concepts#feedback).

### From examples in an Annotation Queue

<Check>
  If you rely on subject matter experts to build meaningful datasets, use [annotation queues](/langsmith/annotation-queues) to provide a streamlined view for reviewers. Human reviewers can optionally modify the inputs/outputs/reference outputs from a trace before it is added to the dataset.
</Check>

Annotation queues can be optionally configured with a default dataset, though you can add runs to any dataset by using the dataset switcher on the bottom of the screen. Once you select the right dataset, click **Add to Dataset** or hit the hot key `D` to add the run to it.

Any modifications you make to the run in your annotation queue will carry over to the dataset, and all metadata associated with the run will also be copied.

<img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-to-dataset-from-aq.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=885c58ab30d94b2371b79730468e0be3" alt="Add to dataset from annotation queue" data-og-width="2290" width="2290" data-og-height="1468" height="1468" data-path="langsmith/images/add-to-dataset-from-aq.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-to-dataset-from-aq.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=90c1279193bf00acf6112980ebd94558 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-to-dataset-from-aq.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=9cb62b05c7d90e5904573d7992a141cc 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-to-dataset-from-aq.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=5c547cc7f74a3b3b34eb7788ad324fcd 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-to-dataset-from-aq.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=6c623a6015dcb818677b6af9d41ad923 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-to-dataset-from-aq.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=47c9a570a9c1e568c0a035aa82adc5d0 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-to-dataset-from-aq.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=c76a247a72ca269c29c99c1da46852e6 2500w" />

Note you can also set up rules to add runs that meet specific criteria to an annotation queue using [automation rules](/langsmith/rules).

### From the Prompt Playground

On the [**Prompt Playground**](/langsmith/observability-concepts#prompt-playground) page, select **Set up Evaluation**, click **+New** if you're starting a new dataset or select from an existing dataset.

<Note>
  Creating datasets inline in the playground is not supported for datasets that have nested keys. In order to add/edit examples with nested keys, you must edit [from the datasets page](/langsmith/manage-datasets-in-application#from-the-datasets-page).
</Note>

To edit the examples:

* Use **+Row** to add a new example to the dataset
* Delete an example using the **⋮** dropdown on the right hand side of the table
* If you're creating a reference-free dataset remove the "Reference Output" column using the **x** button in the column. Note: this action is not reversible.

<img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/playground-dataset.png?fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=583ce80f84902ccb5ccab36a44dddb9b" alt="Create a dataset in the playground" data-og-width="1318" width="1318" data-og-height="981" height="981" data-path="langsmith/images/playground-dataset.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/playground-dataset.png?w=280&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=31fe00c30e17e901334f46845dba1464 280w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/playground-dataset.png?w=560&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=6e63d19dd4be4761d1f6c0d4746395cc 560w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/playground-dataset.png?w=840&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=f625e8480f96e644b4ff2205a2905c9d 840w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/playground-dataset.png?w=1100&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=5126df423f04d152ad638f70f1a7765d 1100w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/playground-dataset.png?w=1650&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=53a13fe730bbe7c2393049dac76266db 1650w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/playground-dataset.png?w=2500&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=998729ce541d92f078382984c76c2501 2500w" />

### Import a dataset from a CSV or JSONL file

On the **Datasets & Experiments** page, click **+New Dataset**, then **Import** an existing dataset from CSV or JSONL file.

### Create a new dataset from the Datasets & Experiments page

1. Navigate to the **Datasets & Experiments** page from the left-hand menu.
2. Click **+ New Dataset**.
3. On the **New Dataset** page, select the **Create from scratch** tab.
4. Add a name and description for the dataset.
5. (Optional) Create a [dataset schema](#create-a-dataset-schema) to validate your dataset.
6. Click **Create**, which will create an empty dataset.
7. To add examples inline, on the dataset's page, go to the **Examples** tab. Click **+ Example**.
8. Define examples in JSON and click **Submit**. For more details on dataset splits, refer to [Create and manage dataset splits](#create-and-manage-dataset-splits).

### Add synthetic examples created by an LLM

If you have existing examples and a [schema](#create-a-dataset-schema) defined on your dataset, when you click **+ Example** there is an option to <Icon icon="sparkles" /> **Add AI-Generated Examples**. This will use an LLM to create [synthetic](/langsmith/evaluation-concepts#synthetic-data) examples.

In **Generate examples**, do the following:

1. Click **API Key** in the top right of the pane to set your OpenAI API key as a [workspace secret](/langsmith/administration-overview#workspaces). If your workspace already has an OpenAI API key set, you can skip this step.

2. Select <Tooltip tip="A few sample input–output pairs that guide the model on how to perform a task.">few-shot examples</Tooltip>: Toggle **Automatic** or **Manual** reference examples. You can select these examples manually from your dataset or use the automatic selection option.

3. Enter the number of synthetic examples you want to generate.

4. Click **Generate**.

<div style={{ textAlign: 'center' }}>
     <img className="block dark:hidden" src="https://mintcdn.com/langchain-5e9cc07a/4E7JL9dL7Pg6moF1/langsmith/images/generate-synthetic-light.png?fit=max&auto=format&n=4E7JL9dL7Pg6moF1&q=85&s=4ec726f80ee38a829ade96caedb61925" alt="The AI-Generated Examples configuration window. Selections for manual and automatic and number of examples to generate." data-og-width="689" width="689" data-og-height="383" height="383" data-path="langsmith/images/generate-synthetic-light.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4E7JL9dL7Pg6moF1/langsmith/images/generate-synthetic-light.png?w=280&fit=max&auto=format&n=4E7JL9dL7Pg6moF1&q=85&s=2a49e15cf0be32a284ab6749941367d4 280w, https://mintcdn.com/langchain-5e9cc07a/4E7JL9dL7Pg6moF1/langsmith/images/generate-synthetic-light.png?w=560&fit=max&auto=format&n=4E7JL9dL7Pg6moF1&q=85&s=c6d6ee9674352b0bc75573cd7f4a5151 560w, https://mintcdn.com/langchain-5e9cc07a/4E7JL9dL7Pg6moF1/langsmith/images/generate-synthetic-light.png?w=840&fit=max&auto=format&n=4E7JL9dL7Pg6moF1&q=85&s=782626122e3c9ebef0cf67c0cd2060cf 840w, https://mintcdn.com/langchain-5e9cc07a/4E7JL9dL7Pg6moF1/langsmith/images/generate-synthetic-light.png?w=1100&fit=max&auto=format&n=4E7JL9dL7Pg6moF1&q=85&s=6e1a7c8b25bc8d15324dfd7c50a8bd5c 1100w, https://mintcdn.com/langchain-5e9cc07a/4E7JL9dL7Pg6moF1/langsmith/images/generate-synthetic-light.png?w=1650&fit=max&auto=format&n=4E7JL9dL7Pg6moF1&q=85&s=bb757aa11517627441a4c8317f7fb313 1650w, https://mintcdn.com/langchain-5e9cc07a/4E7JL9dL7Pg6moF1/langsmith/images/generate-synthetic-light.png?w=2500&fit=max&auto=format&n=4E7JL9dL7Pg6moF1&q=85&s=6bab6b7f6f500ac9044ab3b816484af5 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/langchain-5e9cc07a/4E7JL9dL7Pg6moF1/langsmith/images/generate-synthetic-dark.png?fit=max&auto=format&n=4E7JL9dL7Pg6moF1&q=85&s=6c0ba9da5bf342e702c23406bdfdf18c" alt="The AI-Generated Examples configuration window. Selections for manual and automatic and number of examples to generate." data-og-width="674" width="674" data-og-height="361" height="361" data-path="langsmith/images/generate-synthetic-dark.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4E7JL9dL7Pg6moF1/langsmith/images/generate-synthetic-dark.png?w=280&fit=max&auto=format&n=4E7JL9dL7Pg6moF1&q=85&s=f4f82b37bd0e5b33bb7d113f5d67de38 280w, https://mintcdn.com/langchain-5e9cc07a/4E7JL9dL7Pg6moF1/langsmith/images/generate-synthetic-dark.png?w=560&fit=max&auto=format&n=4E7JL9dL7Pg6moF1&q=85&s=b07f30b95f9c7dd7c3c21919d2a6ee86 560w, https://mintcdn.com/langchain-5e9cc07a/4E7JL9dL7Pg6moF1/langsmith/images/generate-synthetic-dark.png?w=840&fit=max&auto=format&n=4E7JL9dL7Pg6moF1&q=85&s=64a4cbcd28b292e109f2807125516981 840w, https://mintcdn.com/langchain-5e9cc07a/4E7JL9dL7Pg6moF1/langsmith/images/generate-synthetic-dark.png?w=1100&fit=max&auto=format&n=4E7JL9dL7Pg6moF1&q=85&s=fd764de7b0999e27b1b336260f206cd8 1100w, https://mintcdn.com/langchain-5e9cc07a/4E7JL9dL7Pg6moF1/langsmith/images/generate-synthetic-dark.png?w=1650&fit=max&auto=format&n=4E7JL9dL7Pg6moF1&q=85&s=77c5081c2232e183b372a418bffce330 1650w, https://mintcdn.com/langchain-5e9cc07a/4E7JL9dL7Pg6moF1/langsmith/images/generate-synthetic-dark.png?w=2500&fit=max&auto=format&n=4E7JL9dL7Pg6moF1&q=85&s=3be24e6515512dda957b4ce6ba2cd732 2500w" />
   </div>

5. The examples will appear on the **Select generated examples** page. Choose which examples to add to your dataset, with the option to edit them before finalizing. Click **Save Examples**.

6. Each example will be validated against your specified dataset schema and tagged as **synthetic** in the source metadata.

<div style={{ textAlign: 'center' }}>
     <img className="block dark:hidden" src="https://mintcdn.com/langchain-5e9cc07a/mw9POU1xwbwaPxuQ/langsmith/images/select-generated-examples-light.png?fit=max&auto=format&n=mw9POU1xwbwaPxuQ&q=85&s=146c5f6238415bb8d77da15a8a17c839" alt="Select generated examples page with generated examples selected and Save examples button." data-og-width="1781" width="1781" data-og-height="856" height="856" data-path="langsmith/images/select-generated-examples-light.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/mw9POU1xwbwaPxuQ/langsmith/images/select-generated-examples-light.png?w=280&fit=max&auto=format&n=mw9POU1xwbwaPxuQ&q=85&s=1175f7421bb4c5456ebccccddcc2bd56 280w, https://mintcdn.com/langchain-5e9cc07a/mw9POU1xwbwaPxuQ/langsmith/images/select-generated-examples-light.png?w=560&fit=max&auto=format&n=mw9POU1xwbwaPxuQ&q=85&s=d4d85335f1ec8826acce6be4a56d20d2 560w, https://mintcdn.com/langchain-5e9cc07a/mw9POU1xwbwaPxuQ/langsmith/images/select-generated-examples-light.png?w=840&fit=max&auto=format&n=mw9POU1xwbwaPxuQ&q=85&s=167488b8860682ee6b3ceb6dee4e8604 840w, https://mintcdn.com/langchain-5e9cc07a/mw9POU1xwbwaPxuQ/langsmith/images/select-generated-examples-light.png?w=1100&fit=max&auto=format&n=mw9POU1xwbwaPxuQ&q=85&s=468acc35c4f46ce0997f1c552504c3ff 1100w, https://mintcdn.com/langchain-5e9cc07a/mw9POU1xwbwaPxuQ/langsmith/images/select-generated-examples-light.png?w=1650&fit=max&auto=format&n=mw9POU1xwbwaPxuQ&q=85&s=3131cd4260d4cff89da156c774243500 1650w, https://mintcdn.com/langchain-5e9cc07a/mw9POU1xwbwaPxuQ/langsmith/images/select-generated-examples-light.png?w=2500&fit=max&auto=format&n=mw9POU1xwbwaPxuQ&q=85&s=23b27fab3ff759c7f7d48d7057b19409 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/langchain-5e9cc07a/mw9POU1xwbwaPxuQ/langsmith/images/select-generated-examples-dark.png?fit=max&auto=format&n=mw9POU1xwbwaPxuQ&q=85&s=1f1235b31b2d86cf5c7c615c84061e9c" alt="Select generated examples page with generated examples selected and Save examples button." data-og-width="1779" width="1779" data-og-height="838" height="838" data-path="langsmith/images/select-generated-examples-dark.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/mw9POU1xwbwaPxuQ/langsmith/images/select-generated-examples-dark.png?w=280&fit=max&auto=format&n=mw9POU1xwbwaPxuQ&q=85&s=704f33356493a5913bae1c6a744acc2d 280w, https://mintcdn.com/langchain-5e9cc07a/mw9POU1xwbwaPxuQ/langsmith/images/select-generated-examples-dark.png?w=560&fit=max&auto=format&n=mw9POU1xwbwaPxuQ&q=85&s=9222e34f0b8693ecfde55d33378c887a 560w, https://mintcdn.com/langchain-5e9cc07a/mw9POU1xwbwaPxuQ/langsmith/images/select-generated-examples-dark.png?w=840&fit=max&auto=format&n=mw9POU1xwbwaPxuQ&q=85&s=8e96a9e18d8142347fbcd3e6ecbb64c5 840w, https://mintcdn.com/langchain-5e9cc07a/mw9POU1xwbwaPxuQ/langsmith/images/select-generated-examples-dark.png?w=1100&fit=max&auto=format&n=mw9POU1xwbwaPxuQ&q=85&s=3e6c57060fb9bb371fafbe272ed83381 1100w, https://mintcdn.com/langchain-5e9cc07a/mw9POU1xwbwaPxuQ/langsmith/images/select-generated-examples-dark.png?w=1650&fit=max&auto=format&n=mw9POU1xwbwaPxuQ&q=85&s=a81b2b4775f89ce9004a3cc1ea38ae57 1650w, https://mintcdn.com/langchain-5e9cc07a/mw9POU1xwbwaPxuQ/langsmith/images/select-generated-examples-dark.png?w=2500&fit=max&auto=format&n=mw9POU1xwbwaPxuQ&q=85&s=1917622069d093a1f43388bac001a050 2500w" />
   </div>

### Create a dataset schema

LangSmith datasets store arbitrary JSON objects. We recommend (but do not require) that you define a schema for your dataset to ensure that they conform to a specific JSON schema. Dataset schemas are defined with standard [JSON schema](https://json-schema.org/), with the addition of a few [prebuilt types](/langsmith/dataset-json-types) that make it easier to type common primitives like messages and tools.

Certain fields in your schema have a `+ Transformations` option. Transformations are preprocessing steps that, if enabled, update your examples when you add them to the dataset. For example the `convert to OpenAI messages` transformation will convert message-like objects, like LangChain messages, to OpenAI message format.

For the full list of available transformations, see [our reference](/langsmith/dataset-transformations).

<Note>
  If you plan to collect production traces in your dataset from LangChain [ChatModels](https://python.langchain.com/do/langsmith/observability-concepts/chat_models/) or from OpenAI calls using the [LangSmith OpenAI wrapper](/langsmith/annotate-code#wrap-the-openai-client), we offer a prebuilt Chat Model schema that converts messages and tools into industry standard openai formats that can be used downstream with any model for testing. You can also customize the template settings to match your use case.

Please see the [dataset transformations reference](/langsmith/dataset-transformations) for more information.
</Note>

### Create and manage dataset splits

Dataset splits are divisions of your dataset that you can use to segment your data. For example, it is common in machine learning workflows to split datasets into training, validation, and test sets. This can be useful to prevent overfitting - where a model performs well on the training data but poorly on unseen data. In evaluation workflows, it can be useful to do this when you have a dataset with multiple categories that you may want to evaluate separately; or if you are testing a new use case that you may want to include in your dataset in the future, but want to keep separate for now. Note that the same effect can be achieved manually via metadata - but we expect splits to be used for higher level organization of your dataset to split it into separate groups for evaluation, whereas metadata would be used more for storing information on your examples like tags and information about its origin.

In machine learning, it is best practice to keep your splits separate (each example belongs to exactly one split). However, we allow you to select multiple splits for the same example in LangSmith because it can make sense for some evaluation workflows - for example, if an example falls into multiple categories on which you may want to evaluate your application.

In order to create and manage splits in the app, you can select some examples in your dataset and click "Add to Split". From the resulting popup menu, you can select and unselect splits for the selected examples, or create a new split.

<img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-to-split2.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=014aa1fdc735f055c9e66a2a18720d4c" alt="Add to Split" data-og-width="1309" width="1309" data-og-height="915" height="915" data-path="langsmith/images/add-to-split2.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-to-split2.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=0c1ef18d892f91c218d51d47fb313d81 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-to-split2.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=1168e3dd272772dde9cd92d767b832f8 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-to-split2.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=be237dec928908095462f5314bb795fd 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-to-split2.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=6d78b720e3dd1e98bb4880be2f18b4e1 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-to-split2.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=d68fd43c3f0e68fad3cc47f3c80d4f04 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-to-split2.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=95711a7ca8fe56f1a724568454f6fde5 2500w" />

### Edit example metadata

You can add metadata to your examples by clicking on an example and then clicking "Edit" on the top righthand side of the popover. From this page, you can update/delete existing metadata, or add new metadata. You may use this to store information about your examples, such as tags or version info, which you can then [group by](/langsmith/analyze-an-experiment#group-results-by-metadata) when analyzing experiment results or [filter by](/langsmith/manage-datasets-programmatically#list-examples-by-metadata) when you call `list_examples` in the SDK.

<img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-metadata.gif?s=d7235cc2b83913561faccb5083780f17" alt="Add Metadata" data-og-width="1010" width="1010" data-og-height="720" height="720" data-path="langsmith/images/add-metadata.gif" data-optimize="true" data-opv="3" />

You can filter examples by split, metadata key/value or perform full-text search over examples. These filtering options are available to the top left of the examples table.

* **Filter by split**: Select split > Select a split to filter by
* **Filter by metadata**: Filters > Select "Metadata" from the dropdown > Select the metadata key and value to filter on
* **Full-text search**: Filters > Select "Full Text" from the dropdown > Enter your search criteria

You may add multiple filters, and only examples that satisfy all of the filters will be displayed in the table.

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filters-applied.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=2d1f300884d5e886267a137a3cb3e4c7" alt="Filters Applied to Examples" data-og-width="1307" width="1307" data-og-height="370" height="370" data-path="langsmith/images/filters-applied.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filters-applied.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=31bd615f72fbf783850caac0b6f06bb7 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filters-applied.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=4fcef71fff0a7251a559f3d275f527da 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filters-applied.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=591c3a65c909f386e745e6f76107722f 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filters-applied.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=1bc1167f20d8a0b80ce88f3001735a4d 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filters-applied.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=769ab5f104184d1f5841185666c50dbf 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filters-applied.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=7130de5c7f60a1e597ddbffa8575f056 2500w" />

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/manage-datasets-in-application.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Create and run a LangChain application

**URL:** llms-txt#create-and-run-a-langchain-application

**Contents:**
- Supported OpenTelemetry attribute and event mapping
  - Core LangSmith attributes
  - GenAI standard attributes
  - GenAI request parameters
  - GenAI usage metrics
  - TraceLoop attributes
  - OpenInference attributes
  - LLM attributes
  - Prompt template attributes
  - Retriever attributes

prompt = ChatPromptTemplate.from_template("Tell me a joke about {topic}")
model = ChatOpenAI()
chain = prompt | model
result = chain.invoke({"topic": "programming"})
print(result.content)
python  theme={null}
import asyncio
from langsmith.integrations.otel import configure
from google.adk import Runner
from google.adk.agents import LlmAgent
from google.adk.sessions import InMemorySessionService
from google.genai import types

**Examples:**

Example 1 (unknown):
```unknown
<Info>
  Hybrid tracing is available in version **≥ 0.4.1**. To send traces **only** to your OTEL endpoint, set:

  `LANGSMITH_OTEL_ONLY="true"`
  (Recommendation: use **langsmith ≥ 0.4.25**.)
</Info>

## Supported OpenTelemetry attribute and event mapping

When sending traces to LangSmith via OpenTelemetry, the following attributes are mapped to LangSmith fields:

### Core LangSmith attributes

| OpenTelemetry attribute        | LangSmith field  | Notes                                                                        |
| ------------------------------ | ---------------- | ---------------------------------------------------------------------------- |
| `langsmith.trace.name`         | Run name         | Overrides the span name for the run                                          |
| `langsmith.span.kind`          | Run type         | Values: `llm`, `chain`, `tool`, `retriever`, `embedding`, `prompt`, `parser` |
| `langsmith.trace.session_id`   | Session ID       | Session identifier for related traces                                        |
| `langsmith.trace.session_name` | Session name     | Name of the session                                                          |
| `langsmith.span.tags`          | Tags             | Custom tags attached to the span (comma-separated)                           |
| `langsmith.metadata.{key}`     | `metadata.{key}` | Custom metadata with langsmith prefix                                        |

### GenAI standard attributes

| OpenTelemetry attribute                 | LangSmith field               | Notes                                                         |
| --------------------------------------- | ----------------------------- | ------------------------------------------------------------- |
| `gen_ai.system`                         | `metadata.ls_provider`        | The GenAI system (e.g., "openai", "anthropic")                |
| `gen_ai.operation.name`                 | Run type                      | Maps "chat"/"completion" to "llm", "embedding" to "embedding" |
| `gen_ai.prompt`                         | `inputs`                      | The input prompt sent to the model                            |
| `gen_ai.completion`                     | `outputs`                     | The output generated by the model                             |
| `gen_ai.prompt.{n}.role`                | `inputs.messages[n].role`     | Role for the nth input message                                |
| `gen_ai.prompt.{n}.content`             | `inputs.messages[n].content`  | Content for the nth input message                             |
| `gen_ai.prompt.{n}.message.role`        | `inputs.messages[n].role`     | Alternative format for role                                   |
| `gen_ai.prompt.{n}.message.content`     | `inputs.messages[n].content`  | Alternative format for content                                |
| `gen_ai.completion.{n}.role`            | `outputs.messages[n].role`    | Role for the nth output message                               |
| `gen_ai.completion.{n}.content`         | `outputs.messages[n].content` | Content for the nth output message                            |
| `gen_ai.completion.{n}.message.role`    | `outputs.messages[n].role`    | Alternative format for role                                   |
| `gen_ai.completion.{n}.message.content` | `outputs.messages[n].content` | Alternative format for content                                |
| `gen_ai.input.messages`                 | `inputs.messages`             | Array of input messages                                       |
| `gen_ai.output.messages`                | `outputs.messages`            | Array of output messages                                      |
| `gen_ai.tool.name`                      | `invocation_params.tool_name` | Tool name, also sets run type to "tool"                       |

### GenAI request parameters

| OpenTelemetry attribute            | LangSmith field                       | Notes                                   |
| ---------------------------------- | ------------------------------------- | --------------------------------------- |
| `gen_ai.request.model`             | `invocation_params.model`             | The model name used for the request     |
| `gen_ai.response.model`            | `invocation_params.model`             | The model name returned in the response |
| `gen_ai.request.temperature`       | `invocation_params.temperature`       | Temperature setting                     |
| `gen_ai.request.top_p`             | `invocation_params.top_p`             | Top-p sampling setting                  |
| `gen_ai.request.max_tokens`        | `invocation_params.max_tokens`        | Maximum tokens setting                  |
| `gen_ai.request.frequency_penalty` | `invocation_params.frequency_penalty` | Frequency penalty setting               |
| `gen_ai.request.presence_penalty`  | `invocation_params.presence_penalty`  | Presence penalty setting                |
| `gen_ai.request.seed`              | `invocation_params.seed`              | Random seed used for generation         |
| `gen_ai.request.stop_sequences`    | `invocation_params.stop`              | Sequences that stop generation          |
| `gen_ai.request.top_k`             | `invocation_params.top_k`             | Top-k sampling parameter                |
| `gen_ai.request.encoding_formats`  | `invocation_params.encoding_formats`  | Output encoding formats                 |

### GenAI usage metrics

| OpenTelemetry attribute                 | LangSmith field                   | Notes                                     |
| --------------------------------------- | --------------------------------- | ----------------------------------------- |
| `gen_ai.usage.input_tokens`             | `usage_metadata.input_tokens`     | Number of input tokens used               |
| `gen_ai.usage.output_tokens`            | `usage_metadata.output_tokens`    | Number of output tokens used              |
| `gen_ai.usage.total_tokens`             | `usage_metadata.total_tokens`     | Total number of tokens used               |
| `gen_ai.usage.prompt_tokens`            | `usage_metadata.input_tokens`     | Number of input tokens used (deprecated)  |
| `gen_ai.usage.completion_tokens`        | `usage_metadata.output_tokens`    | Number of output tokens used (deprecated) |
| `gen_ai.usage.details.reasoning_tokens` | `usage_metadata.reasoning_tokens` | Number of reasoning tokens used           |

### TraceLoop attributes

| OpenTelemetry attribute                  | LangSmith field  | Notes                                            |
| ---------------------------------------- | ---------------- | ------------------------------------------------ |
| `traceloop.entity.input`                 | `inputs`         | Full input value from TraceLoop                  |
| `traceloop.entity.output`                | `outputs`        | Full output value from TraceLoop                 |
| `traceloop.entity.name`                  | Run name         | Entity name from TraceLoop                       |
| `traceloop.span.kind`                    | Run type         | Maps to LangSmith run types                      |
| `traceloop.llm.request.type`             | Run type         | "embedding" maps to "embedding", others to "llm" |
| `traceloop.association.properties.{key}` | `metadata.{key}` | Custom metadata with traceloop prefix            |

### OpenInference attributes

| OpenTelemetry attribute   | LangSmith field          | Notes                                     |
| ------------------------- | ------------------------ | ----------------------------------------- |
| `input.value`             | `inputs`                 | Full input value, can be string or JSON   |
| `output.value`            | `outputs`                | Full output value, can be string or JSON  |
| `openinference.span.kind` | Run type                 | Maps various kinds to LangSmith run types |
| `llm.system`              | `metadata.ls_provider`   | LLM system provider                       |
| `llm.model_name`          | `metadata.ls_model_name` | Model name from OpenInference             |
| `tool.name`               | Run name                 | Tool name when span kind is "TOOL"        |
| `metadata`                | `metadata.*`             | JSON string of metadata to be merged      |

### LLM attributes

| OpenTelemetry attribute      | LangSmith field                       | Notes                                |
| ---------------------------- | ------------------------------------- | ------------------------------------ |
| `llm.input_messages`         | `inputs.messages`                     | Input messages                       |
| `llm.output_messages`        | `outputs.messages`                    | Output messages                      |
| `llm.token_count.prompt`     | `usage_metadata.input_tokens`         | Prompt token count                   |
| `llm.token_count.completion` | `usage_metadata.output_tokens`        | Completion token count               |
| `llm.token_count.total`      | `usage_metadata.total_tokens`         | Total token count                    |
| `llm.usage.total_tokens`     | `usage_metadata.total_tokens`         | Alternative total token count        |
| `llm.invocation_parameters`  | `invocation_params.*`                 | JSON string of invocation parameters |
| `llm.presence_penalty`       | `invocation_params.presence_penalty`  | Presence penalty                     |
| `llm.frequency_penalty`      | `invocation_params.frequency_penalty` | Frequency penalty                    |
| `llm.request.functions`      | `invocation_params.functions`         | Function definitions                 |

### Prompt template attributes

| OpenTelemetry attribute         | LangSmith field | Notes                                            |
| ------------------------------- | --------------- | ------------------------------------------------ |
| `llm.prompt_template.variables` | Run type        | Sets run type to "prompt", used with input.value |

### Retriever attributes

| OpenTelemetry attribute                     | LangSmith field                     | Notes                                         |
| ------------------------------------------- | ----------------------------------- | --------------------------------------------- |
| `retrieval.documents.{n}.document.content`  | `outputs.documents[n].page_content` | Content of the nth retrieved document         |
| `retrieval.documents.{n}.document.metadata` | `outputs.documents[n].metadata`     | Metadata of the nth retrieved document (JSON) |

### Tool attributes

| OpenTelemetry attribute | LangSmith field                    | Notes                                     |
| ----------------------- | ---------------------------------- | ----------------------------------------- |
| `tools`                 | `invocation_params.tools`          | Array of tool definitions                 |
| `tool_arguments`        | `invocation_params.tool_arguments` | Tool arguments as JSON or key-value pairs |

### Logfire attributes

| OpenTelemetry attribute | LangSmith field    | Notes                                            |
| ----------------------- | ------------------ | ------------------------------------------------ |
| `prompt`                | `inputs`           | Logfire prompt input                             |
| `all_messages_events`   | `outputs`          | Logfire message events output                    |
| `events`                | `inputs`/`outputs` | Logfire events array, splits input/choice events |

### OpenTelemetry event mapping

| Event name                  | LangSmith field      | Notes                                                            |
| --------------------------- | -------------------- | ---------------------------------------------------------------- |
| `gen_ai.content.prompt`     | `inputs`             | Extracts prompt content from event attributes                    |
| `gen_ai.content.completion` | `outputs`            | Extracts completion content from event attributes                |
| `gen_ai.system.message`     | `inputs.messages[]`  | System message in conversation                                   |
| `gen_ai.user.message`       | `inputs.messages[]`  | User message in conversation                                     |
| `gen_ai.assistant.message`  | `outputs.messages[]` | Assistant message in conversation                                |
| `gen_ai.tool.message`       | `outputs.messages[]` | Tool response message                                            |
| `gen_ai.choice`             | `outputs`            | Model choice/response with finish reason                         |
| `exception`                 | `status`, `error`    | Sets status to "error" and extracts exception message/stacktrace |

#### Event attribute extraction

For message events, the following attributes are extracted:

* `content` → message content
* `role` → message role
* `id` → tool\_call\_id (for tool messages)
* `gen_ai.event.content` → full message JSON

For choice events:

* `finish_reason` → choice finish reason
* `message.content` → choice message content
* `message.role` → choice message role
* `tool_calls.{n}.id` → tool call ID
* `tool_calls.{n}.function.name` → tool function name
* `tool_calls.{n}.function.arguments` → tool function arguments
* `tool_calls.{n}.type` → tool call type

For exception events:

* `exception.message` → error message
* `exception.stacktrace` → error stacktrace (appended to message)

## Implementation examples

### Trace using the LangSmith SDK

Use the LangSmith SDK's OpenTelemetry helper to configure export:
```

---

## Create and run the crew

**URL:** llms-txt#create-and-run-the-crew

**Contents:**
- Advanced usage
  - Custom metadata and tags

crew = Crew(
    agents=[market_researcher, data_analyst, content_strategist],
    tasks=[research_task, analysis_task, content_task],
    verbose=True,
    process="sequential"  # Tasks will be executed in order
)

def run_market_research_crew():
    """Run the market research crew and return results."""
    result = crew.kickoff()
    return result

if __name__ == "__main__":
    print("Running CrewAI market research process...")
    output = run_market_research_crew()
    print("\n" + "="*50)
    print("CrewAI Process Output:")
    print("="*50)
    print(output)
python  theme={null}
from opentelemetry import trace

**Examples:**

Example 1 (unknown):
```unknown
## Advanced usage

### Custom metadata and tags

You can add custom metadata to your traces by setting span attributes in your CrewAI application:
```

---

## Create an account and API key

**URL:** llms-txt#create-an-account-and-api-key

**Contents:**
- API keys
- Create an API key
- Delete an API key
- Configure the SDK
- Using API keys outside of the SDK

Source: https://docs.langchain.com/langsmith/create-account-api-key

To get started with LangSmith, you need to create an account. You can sign up for a free account in the [LangSmith UI](https://smith.langchain.com). LangSmith supports sign in with Google, GitHub, and email.

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-account.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=2a18e674c9b6e96dd0d5af16ddeeaf1a" alt="Create account" data-og-width="1768" width="1768" data-og-height="1252" height="1252" data-path="langsmith/images/create-account.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-account.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=45abdaeb33706739a680080f52a5457c 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-account.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=991caa9cc07e229e8a6db0a2ac38fdb9 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-account.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=0ff78a93a7e28cfe9fccbfd5a7d54ec5 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-account.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=04f93900e425e47461c882b25e5298f7 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-account.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=866363d551391983b36edfc481d67404 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-account.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=186e16f729e0a637c0eaab35002aa4ad 2500w" />

LangSmith supports two types of API keys: Service Keys and Personal Access Tokens. Both types of tokens can be used to authenticate requests to the LangSmith API, but they have different use cases.

For more details on Service Keys and Personal Access Tokens, refer to the [Administration overview page](/langsmith/administration-overview).

To log traces and run evaluations with LangSmith, you will need to create an API key to authenticate your requests. API keys can be scoped to a set of [workspaces](/langsmith/administration-overview#workspaces), or the entire [organization](/langsmith/administration-overview#organizations).

To create either type of API key:

1. Navigate to the [Settings page](https://smith.langchain.com/settings) and scroll to the **API Keys** section.
2. For service keys, choose between an organization-scoped and workspace-scoped key. If the key is workspace-scoped, the workspaces must then be specified.

Enterprise users are also able to [assign specific roles](/langsmith/administration-overview#workspace-roles-rbac) to the key, which adjusts its permissions.
3. Set the key's expiration; the key will become unusable after the number of days chosen, or never, if that is selected.
4. Click **Create API Key.**

<Note>
  The API key will be shown only once, so make sure to copy it and store it in a safe place.
</Note>

<img src="https://mintcdn.com/langchain-5e9cc07a/RZqwlMMHZpKJks4w/langsmith/images/create-api-key.png?fit=max&auto=format&n=RZqwlMMHZpKJks4w&q=85&s=e27b419a9c317a78f8a98ff5024e1235" alt="Create API key" data-og-width="1224" width="1224" data-og-height="1137" height="1137" data-path="langsmith/images/create-api-key.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/RZqwlMMHZpKJks4w/langsmith/images/create-api-key.png?w=280&fit=max&auto=format&n=RZqwlMMHZpKJks4w&q=85&s=b0148d07161d0f214a9e6442297e83a3 280w, https://mintcdn.com/langchain-5e9cc07a/RZqwlMMHZpKJks4w/langsmith/images/create-api-key.png?w=560&fit=max&auto=format&n=RZqwlMMHZpKJks4w&q=85&s=305cf6904089edcee1db749552e41b5f 560w, https://mintcdn.com/langchain-5e9cc07a/RZqwlMMHZpKJks4w/langsmith/images/create-api-key.png?w=840&fit=max&auto=format&n=RZqwlMMHZpKJks4w&q=85&s=88ead9f8f16475135f43f32ecdfae35a 840w, https://mintcdn.com/langchain-5e9cc07a/RZqwlMMHZpKJks4w/langsmith/images/create-api-key.png?w=1100&fit=max&auto=format&n=RZqwlMMHZpKJks4w&q=85&s=0803eebefa356974e485217545b9cf13 1100w, https://mintcdn.com/langchain-5e9cc07a/RZqwlMMHZpKJks4w/langsmith/images/create-api-key.png?w=1650&fit=max&auto=format&n=RZqwlMMHZpKJks4w&q=85&s=fe671c360990ca8e4e86f5749a7f59db 1650w, https://mintcdn.com/langchain-5e9cc07a/RZqwlMMHZpKJks4w/langsmith/images/create-api-key.png?w=2500&fit=max&auto=format&n=RZqwlMMHZpKJks4w&q=85&s=02431a31fddd5ee58d30296fed75a238 2500w" />

To delete an API key:

1. Navigate to the [Settings page](https://smith.langchain.com/settings) and scroll to the **API Keys** section.
2. Find the API key you need to delete from the table. Toggle **Personal** or **Service** as needed.
3. Select the trash icon <Icon icon="trash" iconType="solid" /> in the **Actions** column and confirm deletion.

You may set the following environment variables in addition to `LANGSMITH_API_KEY`.

This is only required if using the EU instance.

`LANGSMITH_ENDPOINT=https://eu.api.smith.langchain.com`

This is only required for keys scoped to more than one workspace.

`LANGSMITH_WORKSPACE_ID=<Workspace ID>`

## Using API keys outside of the SDK

See [instructions for managing your organization via API](/langsmith/manage-organization-by-api).

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/create-account-api-key.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Create an AI message manually (e.g., for conversation history)

**URL:** llms-txt#create-an-ai-message-manually-(e.g.,-for-conversation-history)

ai_msg = AIMessage("I'd be happy to help you with that question!")

---

## Create an Ingress for installations (Kubernetes)

**URL:** llms-txt#create-an-ingress-for-installations-(kubernetes)

**Contents:**
- Requirements
- Parameters
- Configuration
  - Option 1: Standard Ingress
  - Option 2: Gateway API
  - Option 3: Istio Gateway

Source: https://docs.langchain.com/langsmith/self-host-ingress

By default, LangSmith will provision a LoadBalancer service for the `langsmith-frontend`. Depending on your cloud provider, this may result in a public IP address being assigned to the service. If you would like to use a custom domain or have more control over the routing of traffic to your LangSmith installation, you can configure an Ingress, Gateway API, or Istio Gateway.

* An existing Kubernetes cluster
* One of the following installed in your Kubernetes cluster:
  * An Ingress Controller (for standard Ingress)
  * Gateway API CRDs and a Gateway resource (for Gateway API)
  * Istio (for Istio Gateway)

You may need to provide certain parameters to your LangSmith installation to configure the Ingress. Additionally, we will want to convert the `langsmith-frontend` service to a ClusterIP service.

* *Hostname (optional)*: The hostname that you would like to use for your LangSmith installation. E.g `"langsmith.example.com"`. If you leave this empty, the ingress will serve all traffic to the LangSmith installation.

* *BasePath (optional)*: If you would like to serve LangSmith under a URL basePath, you can specify it here. For example, adding `"langsmith"` will serve the application at `"example.hostname.com/langsmith"`. This will apply to UI paths as well as API endpoints.

* *IngressClassName (optional)*: The name of the Ingress class that you would like to use. If not set, the default Ingress class will be used.

* *Annotations (optional)*: Additional annotations to add to the Ingress. Certain providers like AWS may use annotations to control things like TLS termination.

For example, you can add the following annotations using the AWS ALB Ingress Controller to attach an ACM certificate to the Ingress:

* *Labels (optional)*: Additional labels to add to the Ingress.

* *TLS (optional)*: If you would like to serve LangSmith over HTTPS, you can add TLS configuration here (many Ingress controllers may have other ways of controlling TLS so this is often not needed). This should be an array of TLS configurations. Each TLS configuration should have the following fields:

* hosts: An array of hosts that the certificate should be valid for. E.g \["langsmith.example.com"]

* secretName: The name of the Kubernetes secret that contains the certificate and private key. This secret should have the following keys:

* tls.crt: The certificate
    * tls.key: The private key

* You can read more about creating a TLS secret [here](https://kubernetes.io/do/langsmith/observability-concepts/services-networking/ingress/#tls).

You can configure your LangSmith instance to use one of three routing options: standard Ingress, Gateway API, or Istio Gateway. Choose the option that best fits your infrastructure.

### Option 1: Standard Ingress

With these parameters in hand, you can configure your LangSmith instance to use an Ingress. You can do this by modifying the `config.yaml` file for your LangSmith Helm Chart installation.

Once configured, you will need to update your LangSmith installation. If everything is configured correctly, your LangSmith instance should now be accessible via the Ingress. You can run the following to check the status of your Ingress:

You should see something like this in the output:

<Warning>
  If you do not have automated DNS setup, you will need to add the IP address to your DNS provider manually.
</Warning>

### Option 2: Gateway API

<Note>
  Gateway API support is available as of LangSmith v0.12.0
</Note>

If your cluster uses the [Kubernetes Gateway API](https://gateway-api.sigs.k8s.io/), you can configure LangSmith to provision HTTPRoute resources. This will create an HTTPRoute for LangSmith and an HTTPRoute for each [agent deployment](/langsmith/deployments).

* *name (required)*: The name of the Gateway resource to reference
* *namespace (required)*: The namespace where the Gateway resource is located
* *hostname (optional)*: The hostname that you would like to use for your LangSmith installation. E.g `"langsmith.example.com"`
* *basePath (optional)*: If you would like to serve LangSmith under a base path, you can specify it here. E.g "example.com/langsmith"
* *sectionName (optional)*: The name of a specific listener section in the Gateway to use
* *annotations (optional)*: Additional annotations to add to the HTTPRoute resources
* *labels (optional)*: Additional labels to add to the HTTPRoute resources

Once configured, you can check the status of your HTTPRoutes:

### Option 3: Istio Gateway

<Note>
  Istio Gateway support is available as of LangSmith v0.12.0
</Note>

If your cluster uses [Istio](https://istio.io/), you can configure LangSmith to provision VirtualService resources. This will create a VirtualService for LangSmith and a VirtualService for each [agent deployment](/langsmith/deployments).

* *name (optional)*: The name of the Istio Gateway resource to reference. Defaults to `"istio-gateway"`
* *namespace (optional)*: The namespace where the Istio Gateway resource is located. Defaults to `"istio-system"`
* *hostname (optional)*: The hostname that you would like to use for your LangSmith installation. E.g `"langsmith.example.com"`
* *basePath (optional)*: If you would like to serve LangSmith under a base path, you can specify it here. E.g "example.com/langsmith"
* *annotations (optional)*: Additional annotations to add to the VirtualService resources
* *labels (optional)*: Additional labels to add to the VirtualService resources

Once configured, you can check the status of your VirtualServices:

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/self-host-ingress.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
* *Labels (optional)*: Additional labels to add to the Ingress.

* *TLS (optional)*: If you would like to serve LangSmith over HTTPS, you can add TLS configuration here (many Ingress controllers may have other ways of controlling TLS so this is often not needed). This should be an array of TLS configurations. Each TLS configuration should have the following fields:

  * hosts: An array of hosts that the certificate should be valid for. E.g \["langsmith.example.com"]

  * secretName: The name of the Kubernetes secret that contains the certificate and private key. This secret should have the following keys:

    * tls.crt: The certificate
    * tls.key: The private key

  * You can read more about creating a TLS secret [here](https://kubernetes.io/do/langsmith/observability-concepts/services-networking/ingress/#tls).

## Configuration

You can configure your LangSmith instance to use one of three routing options: standard Ingress, Gateway API, or Istio Gateway. Choose the option that best fits your infrastructure.

### Option 1: Standard Ingress

With these parameters in hand, you can configure your LangSmith instance to use an Ingress. You can do this by modifying the `config.yaml` file for your LangSmith Helm Chart installation.
```

Example 2 (unknown):
```unknown
Once configured, you will need to update your LangSmith installation. If everything is configured correctly, your LangSmith instance should now be accessible via the Ingress. You can run the following to check the status of your Ingress:
```

Example 3 (unknown):
```unknown
You should see something like this in the output:
```

Example 4 (unknown):
```unknown
<Warning>
  If you do not have automated DNS setup, you will need to add the IP address to your DNS provider manually.
</Warning>

### Option 2: Gateway API

<Note>
  Gateway API support is available as of LangSmith v0.12.0
</Note>

If your cluster uses the [Kubernetes Gateway API](https://gateway-api.sigs.k8s.io/), you can configure LangSmith to provision HTTPRoute resources. This will create an HTTPRoute for LangSmith and an HTTPRoute for each [agent deployment](/langsmith/deployments).

#### Parameters

* *name (required)*: The name of the Gateway resource to reference
* *namespace (required)*: The namespace where the Gateway resource is located
* *hostname (optional)*: The hostname that you would like to use for your LangSmith installation. E.g `"langsmith.example.com"`
* *basePath (optional)*: If you would like to serve LangSmith under a base path, you can specify it here. E.g "example.com/langsmith"
* *sectionName (optional)*: The name of a specific listener section in the Gateway to use
* *annotations (optional)*: Additional annotations to add to the HTTPRoute resources
* *labels (optional)*: Additional labels to add to the HTTPRoute resources

#### Configuration
```

---

## Create a child run, linked to the parent

**URL:** llms-txt#create-a-child-run,-linked-to-the-parent

child_run = construct_run(
    name="Child Run",
    run_type="llm",
    inputs={"question": "What is the capital of France?"},
    parent_dotted_order=parent_run["dotted_order"],
)

---

## Create a code analysis prompt template

**URL:** llms-txt#create-a-code-analysis-prompt-template

code_analysis_prompt = """
Analyze the following code and provide insights:

Please provide:
1. A brief summary of what the code does
2. Any potential improvements
3. Code quality assessment
"""

prompt_template_config = PromptTemplateConfig(
    template=code_analysis_prompt,
    name="code_analyzer",
    template_format="semantic-kernel",
    input_variables=[
        InputVariable(name="code", description="The code to analyze", is_required=True),
    ],
)

---

## Create a code reviewer agent

**URL:** llms-txt#create-a-code-reviewer-agent

code_reviewer = autogen.AssistantAgent(
    name="code_reviewer",
    llm_config={"config_list": config_list},
    system_message="""You are an expert code reviewer. Your role is to:
    1. Review code for bugs, security issues, and best practices
    2. Suggest improvements and optimizations
    3. Provide constructive feedback
    Always be thorough but constructive in your reviews.""",
)

---

## Create a custom agent graph

**URL:** llms-txt#create-a-custom-agent-graph

custom_graph = create_agent(
    model=your_model,
    tools=specialized_tools,
    prompt="You are a specialized agent for data analysis..."
)

---

## Create a custom LangGraph graph

**URL:** llms-txt#create-a-custom-langgraph-graph

def create_weather_graph():
    workflow = StateGraph(...)
    # Build your custom graph
    return workflow.compile()

weather_graph = create_weather_graph()

---

## Create a dataset

**URL:** llms-txt#create-a-dataset

**Contents:**
- Run a single experiment

examples = [
    {
        "inputs": {"text": "Shut up, idiot"},
        "outputs": {"label": "Toxic"},
    },
    {
        "inputs": {"text": "You're a wonderful person"},
        "outputs": {"label": "Not toxic"},
    },
    {
        "inputs": {"text": "This is the worst thing ever"},
        "outputs": {"label": "Toxic"},
    },
    {
        "inputs": {"text": "I had a great day today"},
        "outputs": {"label": "Not toxic"},
    },
    {
        "inputs": {"text": "Nobody likes you"},
        "outputs": {"label": "Toxic"},
    },
    {
        "inputs": {"text": "This is unacceptable. I want to speak to the manager."},
        "outputs": {"label": "Not toxic"},
    },
]

dataset_name = "Toxic Queries - API Example"
dataset = client.create_dataset(dataset_name=dataset_name)
client.create_examples(dataset_id=dataset.id, examples=examples)
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
## Run a single experiment

First, pull all of the examples you'd want to use in your experiment.
```

---

## Create a developer agent

**URL:** llms-txt#create-a-developer-agent

developer = autogen.AssistantAgent(
    name="developer",
    llm_config={"config_list": config_list},
    system_message="""You are a senior software developer. Your role is to:
    1. Write clean, efficient code
    2. Address feedback from code reviews
    3. Explain your implementation decisions
    4. Implement requested features and fixes""",
)

---

## Create a documentation generator

**URL:** llms-txt#create-a-documentation-generator

**Contents:**
- Advanced usage
  - Custom metadata and tags

doc_prompt = """
Generate comprehensive documentation for the following function:

Include:
- Purpose and functionality
- Parameters and return values
- Usage examples
- Any important notes
"""

doc_template_config = PromptTemplateConfig(
    template=doc_prompt,
    name="doc_generator",
    template_format="semantic-kernel",
    input_variables=[
        InputVariable(name="function_code", description="The function code to document", is_required=True),
    ],
)

doc_generator = kernel.add_function(
    function_name="generateDocs",
    plugin_name="documentationPlugin",
    prompt_template_config=doc_template_config,
)

async def main():
    # Example code to analyze
    sample_code = """
def fibonacci(n):
    if n <= 1:
        return n
    return fibonacci(n-1) + fibonacci(n-2)
    """

# Analyze the code
    analysis_result = await kernel.invoke(code_analyzer, code=sample_code)
    print("Code Analysis:")
    print(analysis_result)
    print("\n" + "="*50 + "\n")

# Generate documentation
    doc_result = await kernel.invoke(doc_generator, function_code=sample_code)
    print("Generated Documentation:")
    print(doc_result)

return {"analysis": str(analysis_result), "documentation": str(doc_result)}

if __name__ == "__main__":
    asyncio.run(main())
python  theme={null}
from opentelemetry import trace

**Examples:**

Example 1 (unknown):
```unknown
## Advanced usage

### Custom metadata and tags

You can add custom metadata to your traces by setting span attributes:
```

---

## Create a function that will take in a list of examples and format them into a string

**URL:** llms-txt#create-a-function-that-will-take-in-a-list-of-examples-and-format-them-into-a-string

**Contents:**
  - NEW CODE ###
- Semantic search over examples

def create_example_string(examples):
    final_strings = []
    for e in examples:
        final_strings.append(f"Input: {e.inputs['topic']}\n> {e.outputs['output']}")
    return "\n\n".join(final_strings)
### NEW CODE ###

client = openai.Client()

available_topics = [
    "bug",
    "improvement",
    "new_feature",
    "documentation",
    "integration",
]

prompt_template = """Classify the type of the issue as one of {topics}.

Here are some examples:
{examples}

Begin!
Issue: {text}
>"""

@traceable(
    run_type="chain",
    name="Classifier",
)
def topic_classifier(
    topic: str):
    # We can now pull down the examples from the dataset
    # We do this inside the function so it always get the most up-to-date examples,
    # But this can be done outside and cached for speed if desired
    examples = list(ls_client.list_examples(dataset_name="classifier-github-issues"))  # <- New Code
    example_string = create_example_string(examples)
    return client.chat.completions.create(
        model="gpt-4o-mini",
        temperature=0,
        messages=[
            {
                "role": "user",
                "content": prompt_template.format(
                    topics=','.join(available_topics),
                    text=topic,
                    examples=example_string,
                )
            }
        ],
    ).choices[0].message.content
python  theme={null}
ls_client = Client()
run_id = uuid.uuid4()
topic_classifier(
    "address bug in documentation",
    langsmith_extra={"run_id": run_id})
python  theme={null}
import numpy as np

def find_similar(examples, topic, k=5):
    inputs = [e.inputs['topic'] for e in examples] + [topic]
    vectors = client.embeddings.create(input=inputs, model="text-embedding-3-small")
    vectors = [e.embedding for e in vectors.data]
    vectors = np.array(vectors)
    args = np.argsort(-vectors.dot(vectors[-1])[:-1])[:5]
    examples = [examples[i] for i in args]
    return examples
python  theme={null}
ls_client = Client()

def create_example_string(examples):
    final_strings = []
    for e in examples:
        final_strings.append(f"Input: {e.inputs['topic']}\n> {e.outputs['output']}")
    return "\n\n".join(final_strings)

client = openai.Client()

available_topics = [
    "bug",
    "improvement",
    "new_feature",
    "documentation",
    "integration",
]

prompt_template = """Classify the type of the issue as one of {topics}.

Here are some examples:
{examples}

Begin!
Issue: {text}
>"""

@traceable(
    run_type="chain",
    name="Classifier",
)
def topic_classifier(
    topic: str):
    examples = list(ls_client.list_examples(dataset_name="classifier-github-issues"))
    examples = find_similar(examples, topic)
    example_string = create_example_string(examples)
    return client.chat.completions.create(
        model="gpt-4o-mini",
        temperature=0,
        messages=[
            {
                "role": "user",
                "content": prompt_template.format(
                    topics=','.join(available_topics),
                    text=topic,
                    examples=example_string,
                )
            }
        ],
    ).choices[0].message.content
```

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/optimize-classifier.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
If now run the application with a similar input as before, we can see that it correctly learns that anything related to docs (even if a bug) should be classified as `documentation`
```

Example 2 (unknown):
```unknown
## Semantic search over examples

One additional thing we can do is only use the most semantically similar examples. This is useful when you start to build up a lot of examples.

In order to do this, we can first define an example to find the `k` most similar examples:
```

Example 3 (unknown):
```unknown
We can then use that in the application
```

---

## Create a new experiment using the /sessions endpoint

**URL:** llms-txt#create-a-new-experiment-using-the-/sessions-endpoint

---

## Create a parent run

**URL:** llms-txt#create-a-parent-run

parent_run = construct_run(
    name="Parent Run",
    run_type="chain",
    inputs={"main_question": "Tell me about France"},
)

---

## Create a prompt

**URL:** llms-txt#create-a-prompt

**Contents:**
- Compose your prompt
  - Template format
  - Add a template variable
  - Structured output
  - Tools
- Run the prompt
- Save your prompt
- View your prompts
- Add metadata

Source: https://docs.langchain.com/langsmith/create-a-prompt

Navigate to the  in the left-hand sidebar or from the application homepage.

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/empty-playground.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=2bede4ae9332bdf43ae20580d5bb957d" alt="" data-og-width="1747" width="1747" data-og-height="1285" height="1285" data-path="langsmith/images/empty-playground.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/empty-playground.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=20602d3b2e7b4219a8dc3612fee194b7 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/empty-playground.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=141b0ab234c54970f4e86f24ed13a954 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/empty-playground.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=1fa1d0cb9075f4fbdcae487ea4348116 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/empty-playground.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=bb3583d9357a597754396ee94e52c0da 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/empty-playground.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=c21b3e9881e95a0b954d578fa1d8aa47 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/empty-playground.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=3dcdde6a3fd4f3365059e181f53f68a0 2500w" />

## Compose your prompt

On the left is an editable view of the prompt.

The prompt is made up of messages, each of which has a "role" - including `system`, `human`, and `ai`.

The default template format is `f-string`, but you can change the prompt template format to `mustache` by clicking on the settings icon next to the model -> prompt format -> template format. Learn more about template formats [here](/langsmith/prompt-engineering-concepts#f-string-vs-mustache).
<img src="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/template-format.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=417fa567135babb46d2bf080b7eb44f0" alt="" data-og-width="938" width="938" data-og-height="352" height="352" data-path="langsmith/images/template-format.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/template-format.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=5bbcf83ad9e251ea9fdeb6c0f7dd49eb 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/template-format.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=02776bb45c6ffe0df98775886e75eaeb 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/template-format.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=c63b67b6afe893f7944ee2fb8b76bbaf 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/template-format.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=9df27878d7eaf60e953b9438fcf3f8c4 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/template-format.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=a8b2e1b3fd0977e5b6beb984119bc6fd 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/template-format.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=d30b1d76394200f9473f0b8fb08d2a5e 2500w" />

### Add a template variable

The power of prompts comes from the ability to use variables in your prompt. You can use variables to add dynamic content to your prompt. Add a template variable in one of two ways:

1. Add `{{variable_name}}` to your prompt (with one curly brace on each side for `f-string` and two for `mustache`). <img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-with-variable.png?fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=b250a57ef0e0a40a56822af750d52810" alt="" data-og-width="726" width="726" data-og-height="169" height="169" data-path="langsmith/images/prompt-with-variable.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-with-variable.png?w=280&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=ea82252d99c7ee63c15d1c1036db8c55 280w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-with-variable.png?w=560&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=fc56d047cf631b1e66ecfd09ab4c03a5 560w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-with-variable.png?w=840&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=7cc8cb861e9361445dcee545cbac84b7 840w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-with-variable.png?w=1100&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=a7ed018754ebb8dad675a75c4aa638cb 1100w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-with-variable.png?w=1650&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=5d360aac28b60689789e8d3274103611 1650w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-with-variable.png?w=2500&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=b9b98555c36f929004d234cc7fadaea5 2500w" />

2. Highlight text you want to templatize and click the tooltip button that shows up. Enter a name for your variable, and convert. <img src="https://mintlify.s3.us-west-1.amazonaws.com/langchain-5e9cc07a/langsmith/images/convert-to-variable.gif" alt="" />

When we add a variable, we see a place to enter sample inputs for our prompt variables. Fill these in with values to test the prompt. <img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-inputs.png?fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=35674518c53e340d02719cbb7b5fd782" alt="" data-og-width="775" width="775" data-og-height="134" height="134" data-path="langsmith/images/prompt-inputs.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-inputs.png?w=280&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=59775af2fccde924b7ad7657db2b4656 280w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-inputs.png?w=560&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=df9b24f1eb306578590ee772720ced08 560w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-inputs.png?w=840&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=df549771817326a72a9387d0133ea590 840w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-inputs.png?w=1100&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=4bc399d02630e4873ae34afbbb60057e 1100w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-inputs.png?w=1650&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=78917ac5c4c938d926a5fe420d4d2780 1650w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-inputs.png?w=2500&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=89368aa76e0764d3f3fcb90623b2f28b 2500w" />

### Structured output

Adding an output schema to your prompt will get output in a structured format. Learn more about structured output [here](/langsmith/prompt-engineering-concepts#structured-output). <img src="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/structured-output.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=51cdef35a620c225896dbf2f3ab07528" alt="" data-og-width="814" width="814" data-og-height="574" height="574" data-path="langsmith/images/structured-output.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/structured-output.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=7af3797e6106f2c7858cc67dac7cfe60 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/structured-output.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=7a3b8bb3d32eda103213856e4e05e74c 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/structured-output.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=e973d61622e18ab678631c734c4b16a8 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/structured-output.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=c63196b1604baef88db3f56375458005 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/structured-output.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=e672a8f8af25d414d61592a1b22205c0 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/structured-output.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=9d57196b5940203b66a31196ea464243 2500w" />

You can also add a tool by clicking the `+ Tool` button at the bottom of the prompt editor. See [here](/langsmith/use-tools) for more information on how to use tools.

Click "Start" to run the prompt.

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-a-prompt-run.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=9dec20b94326e5c8b11775b56eca55b4" alt="" data-og-width="1525" width="1525" data-og-height="766" height="766" data-path="langsmith/images/create-a-prompt-run.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-a-prompt-run.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=10cdcbb52db991d37478cdd199f51baf 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-a-prompt-run.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=14986d2ef1ad2aafd69d5005413604c8 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-a-prompt-run.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=20cc523bb2ffa9756d95908b0b434d91 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-a-prompt-run.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=5bdcb8118c13d9e9710ccc663058f47c 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-a-prompt-run.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=1f6760ed7f81ce2d9c7907c8539d6122 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-a-prompt-run.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=4c47787a81ad1ffed842e630ec80a362 2500w" />

To save your prompt, click the "Save" button, name your prompt, and decide if you want it to be "private" or "public". Private prompts are only visible to your workspace, while public prompts are discoverable to anyone.

The model and configuration you select in the Playground settings will be saved with the prompt. When you reopen the prompt, the model and configuration will automatically load from the saved version. <img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/save-prompt.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=180e2c79fb9d1ee8d7869fc279e2d94a" alt="" data-og-width="465" width="465" data-og-height="306" height="306" data-path="langsmith/images/save-prompt.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/save-prompt.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=2a2ea17f2ffb787ce2bbbfc88302636e 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/save-prompt.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=7120851531e79e13d8717ba14eb64483 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/save-prompt.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=6b7d47e08120d2329fa923be8c19a612 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/save-prompt.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=b6a4ed33c82c54f093ad4f07db1b7a1c 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/save-prompt.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=5746af64213ab0bc08d7315a895a0d28 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/save-prompt.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=470e4700c6ed64348f25ae23a8d462c5 2500w" />

<Check>
  The first time you create a public prompt, you'll be asked to set a LangChain Hub handle. All your public prompts will be linked to this handle. In a shared workspace, this handle will be set for the whole workspace.
</Check>

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/public-handle.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=56dbd809c50fb2abc816c73c599f0baf" alt="" data-og-width="575" width="575" data-og-height="357" height="357" data-path="langsmith/images/public-handle.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/public-handle.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=2e87d4538d4413b9ce13aed26f43e075 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/public-handle.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=df539980aec2f06214ee13668d271e38 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/public-handle.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=0b19f257c2d9eb78b88c0df6e7920d8d 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/public-handle.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=06c1efd1463437913cbdd6e19274f354 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/public-handle.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=648394098abe45d6d916ee1fe3f9066b 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/public-handle.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=844bf001cfa48ba449c873d228cc5fa3 2500w" />

You've just created your first prompt! View a table of your prompts in the prompts tab.

<img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-table.png?fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=9f8f5567bb93a0add181a51531474796" alt="" data-og-width="1508" width="1508" data-og-height="309" height="309" data-path="langsmith/images/prompt-table.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-table.png?w=280&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=6421b33ff02f9af3f994e665fbcddf96 280w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-table.png?w=560&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=bf3d973fd239d93c0c4b0dd45e2e7129 560w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-table.png?w=840&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=d2d60a1b18eb7baaeaa5a04fa67d4ac7 840w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-table.png?w=1100&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=9d11895d724a60910231494927f86e24 1100w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-table.png?w=1650&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=52afb51d257f64c5e59ad77e3a3cb67a 1650w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-table.png?w=2500&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=15e2b24c349a9f21729b572fdb23734f 2500w" />

To add metadata to your prompt, click the prompt and then click the "Edit" pencil icon next to the name. This brings you to where you can add additional information about the prompt, including a description, a README, and use cases. For public prompts this information will be visible to anyone who views your prompt in the LangChain Hub.

<img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/pencil.png?fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=e54ca6c0c8283b027f5848f79d1cf064" alt="" data-og-width="1167" width="1167" data-og-height="1067" height="1067" data-path="langsmith/images/pencil.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/pencil.png?w=280&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=0336448912188bb85366b0c49556d207 280w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/pencil.png?w=560&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=6456cdae3e5743b770ca530748efd21b 560w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/pencil.png?w=840&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=7f16e9adaa3e2d3063623317344fb7d7 840w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/pencil.png?w=1100&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=12e5f2c3a2cc2f91d66d467cff910809 1100w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/pencil.png?w=1650&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=ce6cb1b3011638d9bec990c1e8131837 1650w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/pencil.png?w=2500&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=b61eb5331a47c9e6998261799729fdbf 2500w" /> <img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/edit-prompt.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=febb5f53b2f917cf3e5a2ff2566eaef4" alt="" data-og-width="1508" width="1508" data-og-height="1084" height="1084" data-path="langsmith/images/edit-prompt.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/edit-prompt.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=440242ac2e20e092c50810ca84d37286 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/edit-prompt.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=46ad1078bdd4342cc49ecd1a70837149 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/edit-prompt.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=f4998ff77574085ec2eb2fdc8c755fdc 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/edit-prompt.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=2065392afa627416297cc8dc5a10780d 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/edit-prompt.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=bda747e312cbabdd107a7b54b4e1c03e 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/edit-prompt.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=846eaa58fed8f3a14aa76dbb610a0e41 2500w" />

---

## Create a thread and chat

**URL:** llms-txt#create-a-thread-and-chat

**Contents:**
- Next steps

thread = await client.threads.create()
print(f"✅ Created thread as Alice: {thread['thread_id']}")

response = await client.runs.create(
    thread_id=thread["thread_id"],
    assistant_id="agent",
    input={"messages": [{"role": "user", "content": "Hello!"}]},
)
print("✅ Bot responded:")
print(response)
```

1. Without a valid token, we can't access the bot
2. With a valid token, we can create threads and chat

Congratulations! You've built a chatbot that only lets "authenticated" users access it. While this system doesn't (yet) implement a production-ready security scheme, we've learned the basic mechanics of how to control access to our bot. In the next tutorial, we'll learn how to give each user their own private conversations.

Now that you can control who accesses your bot, you might want to:

1. Continue the tutorial by going to [Make conversations private](/langsmith/resource-auth) to learn about resource authorization.
2. Read more about [authentication concepts](/langsmith/auth).
3. Check out the [API reference](https://langchain-ai.github.io/langgraph/cloud/reference/sdk/python-sdk_ref) for more authentication details.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/set-up-custom-auth.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Create a thread as user 1

**URL:** llms-txt#create-a-thread-as-user-1

thread = await user1_client.threads.create()
print(f"✅ User 1 created thread: {thread['thread_id']}")

---

## Create a user proxy agent

**URL:** llms-txt#create-a-user-proxy-agent

user_proxy = autogen.UserProxyAgent(
    name="user_proxy",
    human_input_mode="NEVER",
    max_consecutive_auto_reply=8,
    is_termination_msg=lambda x: x.get("content", "").rstrip().endswith("TERMINATE"),
    code_execution_config={"work_dir": "workspace"},
    llm_config={"config_list": config_list},
)

def run_code_review_session(task_description: str):
    """Run a multi-agent code review session."""

# Create a group chat with the agents
    groupchat = autogen.GroupChat(
        agents=[user_proxy, developer, code_reviewer],
        messages=[],
        max_round=10
    )

# Create a group chat manager
    manager = autogen.GroupChatManager(
        groupchat=groupchat,
        llm_config={"config_list": config_list}
    )

# Start the conversation
    user_proxy.initiate_chat(
        manager,
        message=f"""
        Task: {task_description}

Developer: Please implement the requested feature.
        Code Reviewer: Please review the implementation and provide feedback.

Work together to create a high-quality solution.
        """
    )

return "Code review session completed"

---

## Create child run

**URL:** llms-txt#create-child-run

child_run_id = uuid4()
post_run(child_run_id, "OpenAI Call", "llm", {"messages": messages}, parent_run_id)

---

## Create clients for both users

**URL:** llms-txt#create-clients-for-both-users

alice = get_client(
    url="http://localhost:2024",
    headers={"Authorization": "Bearer user1-token"}
)

bob = get_client(
    url="http://localhost:2024",
    headers={"Authorization": "Bearer user2-token"}
)

---

## Create clients with different sampling rates

**URL:** llms-txt#create-clients-with-different-sampling-rates

client_1 = Client(tracing_sampling_rate=0.5)  # 50% sampling
client_2 = Client(tracing_sampling_rate=0.25)  # 25% sampling
client_no_trace = Client(tracing_sampling_rate=0.0)  # No tracing

---

## Create config with thread_id for state persistence

**URL:** llms-txt#create-config-with-thread_id-for-state-persistence

config = {"configurable": {"thread_id": str(uuid.uuid4())}}

---

## Create dataset

**URL:** llms-txt#create-dataset

examples = [
    {
        "inputs": {"messages": [{"role": "user", "content": "i bought some tracks recently and i dont like them"}]},
        "outputs": {"route": "refund_agent"},
    },
    {
        "inputs": {"messages": [{"role": "user", "content": "I was thinking of purchasing some Rolling Stones tunes, any recommendations?"}]},
        "outputs": {"route": "question_answering_agent"},
    },
    {
        "inputs": {"messages": [{"role": "user", "content": "i want a refund on purchase 237"}, {"role": "assistant", "content": "I've refunded you a total of $1.98. How else can I help you today?"}, {"role": "user", "content": "did prince release any albums in 2000?"}]},
        "outputs": {"route": "question_answering_agent"},
    },
    {
        "inputs": {"messages": [{"role": "user", "content": "i purchased a cover of Yesterday recently but can't remember who it was by, which versions of it do you have?"}]},
        "outputs": {"route": "question_answering_agent"},
    },
]

dataset_name = "Chinook Customer Service Bot: Intent Classifier"
if not client.has_dataset(dataset_name=dataset_name):
    dataset = client.create_dataset(dataset_name=dataset_name)
    client.create_examples(
        dataset_id=dataset.id,
        examples=examples
    )

---

## Create parent run

**URL:** llms-txt#create-parent-run

parent_run_id = uuid4()
post_run(parent_run_id, "Chat Pipeline", "chain", {"question": question})

---

## Create store with semantic search enabled

**URL:** llms-txt#create-store-with-semantic-search-enabled

**Contents:**
- Manage short-term memory
  - Trim messages
  - Delete messages
  - Summarize messages
  - Manage checkpoints
- Prebuilt memory tools

embeddings = init_embeddings("openai:text-embedding-3-small")
store = InMemoryStore(
    index={
        "embed": embeddings,
        "dims": 1536,
    }
)

store.put(("user_123", "memories"), "1", {"text": "I love pizza"})
store.put(("user_123", "memories"), "2", {"text": "I am a plumber"})

items = store.search(
    ("user_123", "memories"), query="I'm hungry", limit=1
)
python  theme={null}

from langchain.embeddings import init_embeddings
  from langchain.chat_models import init_chat_model
  from langgraph.store.base import BaseStore
  from langgraph.store.memory import InMemoryStore
  from langgraph.graph import START, MessagesState, StateGraph

model = init_chat_model("gpt-4o-mini")

# Create store with semantic search enabled
  embeddings = init_embeddings("openai:text-embedding-3-small")
  store = InMemoryStore(
      index={
          "embed": embeddings,
          "dims": 1536,
      }
  )

store.put(("user_123", "memories"), "1", {"text": "I love pizza"})
  store.put(("user_123", "memories"), "2", {"text": "I am a plumber"})

def chat(state, *, store: BaseStore):
      # Search based on user's last message
      items = store.search(
          ("user_123", "memories"), query=state["messages"][-1].content, limit=2
      )
      memories = "\n".join(item.value["text"] for item in items)
      memories = f"## Memories of user\n{memories}" if memories else ""
      response = model.invoke(
          [
              {"role": "system", "content": f"You are a helpful assistant.\n{memories}"},
              *state["messages"],
          ]
      )
      return {"messages": [response]}

builder = StateGraph(MessagesState)
  builder.add_node(chat)
  builder.add_edge(START, "chat")
  graph = builder.compile(store=store)

for message, metadata in graph.stream(
      input={"messages": [{"role": "user", "content": "I'm hungry"}]},
      stream_mode="messages",
  ):
      print(message.content, end="")
  python  theme={null}
from langchain_core.messages.utils import (  # [!code highlight]
    trim_messages,  # [!code highlight]
    count_tokens_approximately  # [!code highlight]
)  # [!code highlight]

def call_model(state: MessagesState):
    messages = trim_messages(  # [!code highlight]
        state["messages"],
        strategy="last",
        token_counter=count_tokens_approximately,
        max_tokens=128,
        start_on="human",
        end_on=("human", "tool"),
    )
    response = model.invoke(messages)
    return {"messages": [response]}

builder = StateGraph(MessagesState)
builder.add_node(call_model)
...
python  theme={null}
  from langchain_core.messages.utils import (
      trim_messages,  # [!code highlight]
      count_tokens_approximately  # [!code highlight]
  )
  from langchain.chat_models import init_chat_model
  from langgraph.graph import StateGraph, START, MessagesState

model = init_chat_model("claude-sonnet-4-5-20250929")
  summarization_model = model.bind(max_tokens=128)

def call_model(state: MessagesState):
      messages = trim_messages(  # [!code highlight]
          state["messages"],
          strategy="last",
          token_counter=count_tokens_approximately,
          max_tokens=128,
          start_on="human",
          end_on=("human", "tool"),
      )
      response = model.invoke(messages)
      return {"messages": [response]}

checkpointer = InMemorySaver()
  builder = StateGraph(MessagesState)
  builder.add_node(call_model)
  builder.add_edge(START, "call_model")
  graph = builder.compile(checkpointer=checkpointer)

config = {"configurable": {"thread_id": "1"}}
  graph.invoke({"messages": "hi, my name is bob"}, config)
  graph.invoke({"messages": "write a short poem about cats"}, config)
  graph.invoke({"messages": "now do the same but for dogs"}, config)
  final_response = graph.invoke({"messages": "what's my name?"}, config)

final_response["messages"][-1].pretty_print()
  
  ================================== Ai Message ==================================

Your name is Bob, as you mentioned when you first introduced yourself.
  python  theme={null}
from langchain.messages import RemoveMessage  # [!code highlight]

def delete_messages(state):
    messages = state["messages"]
    if len(messages) > 2:
        # remove the earliest two messages
        return {"messages": [RemoveMessage(id=m.id) for m in messages[:2]]}  # [!code highlight]
python  theme={null}
from langgraph.graph.message import REMOVE_ALL_MESSAGES  # [!code highlight]

def delete_messages(state):
    return {"messages": [RemoveMessage(id=REMOVE_ALL_MESSAGES)]}  # [!code highlight]
python  theme={null}
  from langchain.messages import RemoveMessage  # [!code highlight]

def delete_messages(state):
      messages = state["messages"]
      if len(messages) > 2:
          # remove the earliest two messages
          return {"messages": [RemoveMessage(id=m.id) for m in messages[:2]]}  # [!code highlight]

def call_model(state: MessagesState):
      response = model.invoke(state["messages"])
      return {"messages": response}

builder = StateGraph(MessagesState)
  builder.add_sequence([call_model, delete_messages])
  builder.add_edge(START, "call_model")

checkpointer = InMemorySaver()
  app = builder.compile(checkpointer=checkpointer)

for event in app.stream(
      {"messages": [{"role": "user", "content": "hi! I'm bob"}]},
      config,
      stream_mode="values"
  ):
      print([(message.type, message.content) for message in event["messages"]])

for event in app.stream(
      {"messages": [{"role": "user", "content": "what's my name?"}]},
      config,
      stream_mode="values"
  ):
      print([(message.type, message.content) for message in event["messages"]])
  
  [('human', "hi! I'm bob")]
  [('human', "hi! I'm bob"), ('ai', 'Hi Bob! How are you doing today? Is there anything I can help you with?')]
  [('human', "hi! I'm bob"), ('ai', 'Hi Bob! How are you doing today? Is there anything I can help you with?'), ('human', "what's my name?")]
  [('human', "hi! I'm bob"), ('ai', 'Hi Bob! How are you doing today? Is there anything I can help you with?'), ('human', "what's my name?"), ('ai', 'Your name is Bob.')]
  [('human', "what's my name?"), ('ai', 'Your name is Bob.')]
  python  theme={null}
from langgraph.graph import MessagesState
class State(MessagesState):
    summary: str
python  theme={null}
def summarize_conversation(state: State):

# First, we get any existing summary
    summary = state.get("summary", "")

# Create our summarization prompt
    if summary:

# A summary already exists
        summary_message = (
            f"This is a summary of the conversation to date: {summary}\n\n"
            "Extend the summary by taking into account the new messages above:"
        )

else:
        summary_message = "Create a summary of the conversation above:"

# Add prompt to our history
    messages = state["messages"] + [HumanMessage(content=summary_message)]
    response = model.invoke(messages)

# Delete all but the 2 most recent messages
    delete_messages = [RemoveMessage(id=m.id) for m in state["messages"][:-2]]
    return {"summary": response.content, "messages": delete_messages}
python  theme={null}
  from typing import Any, TypedDict

from langchain.chat_models import init_chat_model
  from langchain.messages import AnyMessage
  from langchain_core.messages.utils import count_tokens_approximately
  from langgraph.graph import StateGraph, START, MessagesState
  from langgraph.checkpoint.memory import InMemorySaver
  from langmem.short_term import SummarizationNode, RunningSummary  # [!code highlight]

model = init_chat_model("claude-sonnet-4-5-20250929")
  summarization_model = model.bind(max_tokens=128)

class State(MessagesState):
      context: dict[str, RunningSummary]  # [!code highlight]

class LLMInputState(TypedDict):  # [!code highlight]
      summarized_messages: list[AnyMessage]
      context: dict[str, RunningSummary]

summarization_node = SummarizationNode(  # [!code highlight]
      token_counter=count_tokens_approximately,
      model=summarization_model,
      max_tokens=256,
      max_tokens_before_summary=256,
      max_summary_tokens=128,
  )

def call_model(state: LLMInputState):  # [!code highlight]
      response = model.invoke(state["summarized_messages"])
      return {"messages": [response]}

checkpointer = InMemorySaver()
  builder = StateGraph(State)
  builder.add_node(call_model)
  builder.add_node("summarize", summarization_node)  # [!code highlight]
  builder.add_edge(START, "summarize")
  builder.add_edge("summarize", "call_model")
  graph = builder.compile(checkpointer=checkpointer)

# Invoke the graph
  config = {"configurable": {"thread_id": "1"}}
  graph.invoke({"messages": "hi, my name is bob"}, config)
  graph.invoke({"messages": "write a short poem about cats"}, config)
  graph.invoke({"messages": "now do the same but for dogs"}, config)
  final_response = graph.invoke({"messages": "what's my name?"}, config)

final_response["messages"][-1].pretty_print()
  print("\nSummary:", final_response["context"]["running_summary"].summary)
  
  ================================== Ai Message ==================================

From our conversation, I can see that you introduced yourself as Bob. That's the name you shared with me when we began talking.

Summary: In this conversation, I was introduced to Bob, who then asked me to write a poem about cats. I composed a poem titled "The Mystery of Cats" that captured cats' graceful movements, independent nature, and their special relationship with humans. Bob then requested a similar poem about dogs, so I wrote "The Joy of Dogs," which highlighted dogs' loyalty, enthusiasm, and loving companionship. Both poems were written in a similar style but emphasized the distinct characteristics that make each pet special.
  python  theme={null}
    config = {
        "configurable": {
            "thread_id": "1",  # [!code highlight]
            # optionally provide an ID for a specific checkpoint,
            # otherwise the latest checkpoint is shown
            # "checkpoint_id": "1f029ca3-1f5b-6704-8004-820c16b69a5a"  # [!code highlight]

}
    }
    graph.get_state(config)  # [!code highlight]
    
    StateSnapshot(
        values={'messages': [HumanMessage(content="hi! I'm bob"), AIMessage(content='Hi Bob! How are you doing today?), HumanMessage(content="what's my name?"), AIMessage(content='Your name is Bob.')]}, next=(),
        config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1f5b-6704-8004-820c16b69a5a'}},
        metadata={
            'source': 'loop',
            'writes': {'call_model': {'messages': AIMessage(content='Your name is Bob.')}},
            'step': 4,
            'parents': {},
            'thread_id': '1'
        },
        created_at='2025-05-05T16:01:24.680462+00:00',
        parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1790-6b0a-8003-baf965b6a38f'}},
        tasks=(),
        interrupts=()
    )
    python  theme={null}
    config = {
        "configurable": {
            "thread_id": "1",  # [!code highlight]
            # optionally provide an ID for a specific checkpoint,
            # otherwise the latest checkpoint is shown
            # "checkpoint_id": "1f029ca3-1f5b-6704-8004-820c16b69a5a"  # [!code highlight]

}
    }
    checkpointer.get_tuple(config)  # [!code highlight]
    
    CheckpointTuple(
        config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1f5b-6704-8004-820c16b69a5a'}},
        checkpoint={
            'v': 3,
            'ts': '2025-05-05T16:01:24.680462+00:00',
            'id': '1f029ca3-1f5b-6704-8004-820c16b69a5a',
            'channel_versions': {'__start__': '00000000000000000000000000000005.0.5290678567601859', 'messages': '00000000000000000000000000000006.0.3205149138784782', 'branch:to:call_model': '00000000000000000000000000000006.0.14611156755133758'}, 'versions_seen': {'__input__': {}, '__start__': {'__start__': '00000000000000000000000000000004.0.5736472536395331'}, 'call_model': {'branch:to:call_model': '00000000000000000000000000000005.0.1410174088651449'}},
            'channel_values': {'messages': [HumanMessage(content="hi! I'm bob"), AIMessage(content='Hi Bob! How are you doing today?), HumanMessage(content="what's my name?"), AIMessage(content='Your name is Bob.')]},
        },
        metadata={
            'source': 'loop',
            'writes': {'call_model': {'messages': AIMessage(content='Your name is Bob.')}},
            'step': 4,
            'parents': {},
            'thread_id': '1'
        },
        parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1790-6b0a-8003-baf965b6a38f'}},
        pending_writes=[]
    )
    python  theme={null}
    config = {
        "configurable": {
            "thread_id": "1"  # [!code highlight]
        }
    }
    list(graph.get_state_history(config))  # [!code highlight]
    
    [
        StateSnapshot(
            values={'messages': [HumanMessage(content="hi! I'm bob"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?'), HumanMessage(content="what's my name?"), AIMessage(content='Your name is Bob.')]},
            next=(),
            config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1f5b-6704-8004-820c16b69a5a'}},
            metadata={'source': 'loop', 'writes': {'call_model': {'messages': AIMessage(content='Your name is Bob.')}}, 'step': 4, 'parents': {}, 'thread_id': '1'},
            created_at='2025-05-05T16:01:24.680462+00:00',
            parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1790-6b0a-8003-baf965b6a38f'}},
            tasks=(),
            interrupts=()
        ),
        StateSnapshot(
            values={'messages': [HumanMessage(content="hi! I'm bob"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?'), HumanMessage(content="what's my name?")]},
            next=('call_model',),
            config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1790-6b0a-8003-baf965b6a38f'}},
            metadata={'source': 'loop', 'writes': None, 'step': 3, 'parents': {}, 'thread_id': '1'},
            created_at='2025-05-05T16:01:23.863421+00:00',
            parent_config={...}
            tasks=(PregelTask(id='8ab4155e-6b15-b885-9ce5-bed69a2c305c', name='call_model', path=('__pregel_pull', 'call_model'), error=None, interrupts=(), state=None, result={'messages': AIMessage(content='Your name is Bob.')}),),
            interrupts=()
        ),
        StateSnapshot(
            values={'messages': [HumanMessage(content="hi! I'm bob"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')]},
            next=('__start__',),
            config={...},
            metadata={'source': 'input', 'writes': {'__start__': {'messages': [{'role': 'user', 'content': "what's my name?"}]}}, 'step': 2, 'parents': {}, 'thread_id': '1'},
            created_at='2025-05-05T16:01:23.863173+00:00',
            parent_config={...}
            tasks=(PregelTask(id='24ba39d6-6db1-4c9b-f4c5-682aeaf38dcd', name='__start__', path=('__pregel_pull', '__start__'), error=None, interrupts=(), state=None, result={'messages': [{'role': 'user', 'content': "what's my name?"}]}),),
            interrupts=()
        ),
        StateSnapshot(
            values={'messages': [HumanMessage(content="hi! I'm bob"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')]},
            next=(),
            config={...},
            metadata={'source': 'loop', 'writes': {'call_model': {'messages': AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')}}, 'step': 1, 'parents': {}, 'thread_id': '1'},
            created_at='2025-05-05T16:01:23.862295+00:00',
            parent_config={...}
            tasks=(),
            interrupts=()
        ),
        StateSnapshot(
            values={'messages': [HumanMessage(content="hi! I'm bob")]},
            next=('call_model',),
            config={...},
            metadata={'source': 'loop', 'writes': None, 'step': 0, 'parents': {}, 'thread_id': '1'},
            created_at='2025-05-05T16:01:22.278960+00:00',
            parent_config={...}
            tasks=(PregelTask(id='8cbd75e0-3720-b056-04f7-71ac805140a0', name='call_model', path=('__pregel_pull', 'call_model'), error=None, interrupts=(), state=None, result={'messages': AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')}),),
            interrupts=()
        ),
        StateSnapshot(
            values={'messages': []},
            next=('__start__',),
            config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-0870-6ce2-bfff-1f3f14c3e565'}},
            metadata={'source': 'input', 'writes': {'__start__': {'messages': [{'role': 'user', 'content': "hi! I'm bob"}]}}, 'step': -1, 'parents': {}, 'thread_id': '1'},
            created_at='2025-05-05T16:01:22.277497+00:00',
            parent_config=None,
            tasks=(PregelTask(id='d458367b-8265-812c-18e2-33001d199ce6', name='__start__', path=('__pregel_pull', '__start__'), error=None, interrupts=(), state=None, result={'messages': [{'role': 'user', 'content': "hi! I'm bob"}]}),),
            interrupts=()
        )
    ]
    python  theme={null}
    config = {
        "configurable": {
            "thread_id": "1"  # [!code highlight]
        }
    }
    list(checkpointer.list(config))  # [!code highlight]
    
    [
        CheckpointTuple(
            config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1f5b-6704-8004-820c16b69a5a'}},
            checkpoint={
                'v': 3,
                'ts': '2025-05-05T16:01:24.680462+00:00',
                'id': '1f029ca3-1f5b-6704-8004-820c16b69a5a',
                'channel_versions': {'__start__': '00000000000000000000000000000005.0.5290678567601859', 'messages': '00000000000000000000000000000006.0.3205149138784782', 'branch:to:call_model': '00000000000000000000000000000006.0.14611156755133758'},
                'versions_seen': {'__input__': {}, '__start__': {'__start__': '00000000000000000000000000000004.0.5736472536395331'}, 'call_model': {'branch:to:call_model': '00000000000000000000000000000005.0.1410174088651449'}},
                'channel_values': {'messages': [HumanMessage(content="hi! I'm bob"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?'), HumanMessage(content="what's my name?"), AIMessage(content='Your name is Bob.')]},
            },
            metadata={'source': 'loop', 'writes': {'call_model': {'messages': AIMessage(content='Your name is Bob.')}}, 'step': 4, 'parents': {}, 'thread_id': '1'},
            parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1790-6b0a-8003-baf965b6a38f'}},
            pending_writes=[]
        ),
        CheckpointTuple(
            config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1790-6b0a-8003-baf965b6a38f'}},
            checkpoint={
                'v': 3,
                'ts': '2025-05-05T16:01:23.863421+00:00',
                'id': '1f029ca3-1790-6b0a-8003-baf965b6a38f',
                'channel_versions': {'__start__': '00000000000000000000000000000005.0.5290678567601859', 'messages': '00000000000000000000000000000006.0.3205149138784782', 'branch:to:call_model': '00000000000000000000000000000006.0.14611156755133758'},
                'versions_seen': {'__input__': {}, '__start__': {'__start__': '00000000000000000000000000000004.0.5736472536395331'}, 'call_model': {'branch:to:call_model': '00000000000000000000000000000005.0.1410174088651449'}},
                'channel_values': {'messages': [HumanMessage(content="hi! I'm bob"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?'), HumanMessage(content="what's my name?")], 'branch:to:call_model': None}
            },
            metadata={'source': 'loop', 'writes': None, 'step': 3, 'parents': {}, 'thread_id': '1'},
            parent_config={...},
            pending_writes=[('8ab4155e-6b15-b885-9ce5-bed69a2c305c', 'messages', AIMessage(content='Your name is Bob.'))]
        ),
        CheckpointTuple(
            config={...},
            checkpoint={
                'v': 3,
                'ts': '2025-05-05T16:01:23.863173+00:00',
                'id': '1f029ca3-1790-616e-8002-9e021694a0cd',
                'channel_versions': {'__start__': '00000000000000000000000000000004.0.5736472536395331', 'messages': '00000000000000000000000000000003.0.7056767754077798', 'branch:to:call_model': '00000000000000000000000000000003.0.22059023329132854'},
                'versions_seen': {'__input__': {}, '__start__': {'__start__': '00000000000000000000000000000001.0.7040775356287469'}, 'call_model': {'branch:to:call_model': '00000000000000000000000000000002.0.9300422176788571'}},
                'channel_values': {'__start__': {'messages': [{'role': 'user', 'content': "what's my name?"}]}, 'messages': [HumanMessage(content="hi! I'm bob"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')]}
            },
            metadata={'source': 'input', 'writes': {'__start__': {'messages': [{'role': 'user', 'content': "what's my name?"}]}}, 'step': 2, 'parents': {}, 'thread_id': '1'},
            parent_config={...},
            pending_writes=[('24ba39d6-6db1-4c9b-f4c5-682aeaf38dcd', 'messages', [{'role': 'user', 'content': "what's my name?"}]), ('24ba39d6-6db1-4c9b-f4c5-682aeaf38dcd', 'branch:to:call_model', None)]
        ),
        CheckpointTuple(
            config={...},
            checkpoint={
                'v': 3,
                'ts': '2025-05-05T16:01:23.862295+00:00',
                'id': '1f029ca3-178d-6f54-8001-d7b180db0c89',
                'channel_versions': {'__start__': '00000000000000000000000000000002.0.18673090920108737', 'messages': '00000000000000000000000000000003.0.7056767754077798', 'branch:to:call_model': '00000000000000000000000000000003.0.22059023329132854'},
                'versions_seen': {'__input__': {}, '__start__': {'__start__': '00000000000000000000000000000001.0.7040775356287469'}, 'call_model': {'branch:to:call_model': '00000000000000000000000000000002.0.9300422176788571'}},
                'channel_values': {'messages': [HumanMessage(content="hi! I'm bob"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')]}
            },
            metadata={'source': 'loop', 'writes': {'call_model': {'messages': AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')}}, 'step': 1, 'parents': {}, 'thread_id': '1'},
            parent_config={...},
            pending_writes=[]
        ),
        CheckpointTuple(
            config={...},
            checkpoint={
                'v': 3,
                'ts': '2025-05-05T16:01:22.278960+00:00',
                'id': '1f029ca3-0874-6612-8000-339f2abc83b1',
                'channel_versions': {'__start__': '00000000000000000000000000000002.0.18673090920108737', 'messages': '00000000000000000000000000000002.0.30296526818059655', 'branch:to:call_model': '00000000000000000000000000000002.0.9300422176788571'},
                'versions_seen': {'__input__': {}, '__start__': {'__start__': '00000000000000000000000000000001.0.7040775356287469'}},
                'channel_values': {'messages': [HumanMessage(content="hi! I'm bob")], 'branch:to:call_model': None}
            },
            metadata={'source': 'loop', 'writes': None, 'step': 0, 'parents': {}, 'thread_id': '1'},
            parent_config={...},
            pending_writes=[('8cbd75e0-3720-b056-04f7-71ac805140a0', 'messages', AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?'))]
        ),
        CheckpointTuple(
            config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-0870-6ce2-bfff-1f3f14c3e565'}},
            checkpoint={
                'v': 3,
                'ts': '2025-05-05T16:01:22.277497+00:00',
                'id': '1f029ca3-0870-6ce2-bfff-1f3f14c3e565',
                'channel_versions': {'__start__': '00000000000000000000000000000001.0.7040775356287469'},
                'versions_seen': {'__input__': {}},
                'channel_values': {'__start__': {'messages': [{'role': 'user', 'content': "hi! I'm bob"}]}}
            },
            metadata={'source': 'input', 'writes': {'__start__': {'messages': [{'role': 'user', 'content': "hi! I'm bob"}]}}, 'step': -1, 'parents': {}, 'thread_id': '1'},
            parent_config=None,
            pending_writes=[('d458367b-8265-812c-18e2-33001d199ce6', 'messages', [{'role': 'user', 'content': "hi! I'm bob"}]), ('d458367b-8265-812c-18e2-33001d199ce6', 'branch:to:call_model', None)]
        )
    ]
    python  theme={null}
thread_id = "1"
checkpointer.delete_thread(thread_id)
```

## Prebuilt memory tools

**LangMem** is a LangChain-maintained library that offers tools for managing long-term memories in your agent. See the [LangMem documentation](https://langchain-ai.github.io/langmem/) for usage examples.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/add-memory.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
<Accordion title="Long-term memory with semantic search">
```

Example 2 (unknown):
```unknown
</Accordion>

## Manage short-term memory

With [short-term memory](#add-short-term-memory) enabled, long conversations can exceed the LLM's context window. Common solutions are:

* [Trim messages](#trim-messages): Remove first or last N messages (before calling LLM)
* [Delete messages](#delete-messages) from LangGraph state permanently
* [Summarize messages](#summarize-messages): Summarize earlier messages in the history and replace them with a summary
* [Manage checkpoints](#manage-checkpoints) to store and retrieve message history
* Custom strategies (e.g., message filtering, etc.)

This allows the agent to keep track of the conversation without exceeding the LLM's context window.

### Trim messages

Most LLMs have a maximum supported context window (denominated in tokens). One way to decide when to truncate messages is to count the tokens in the message history and truncate whenever it approaches that limit. If you're using LangChain, you can use the trim messages utility and specify the number of tokens to keep from the list, as well as the `strategy` (e.g., keep the last `max_tokens`) to use for handling the boundary.

To trim message history, use the [`trim_messages`](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.utils.trim_messages.html) function:
```

Example 3 (unknown):
```unknown
<Accordion title="Full example: trim messages">
```

Example 4 (unknown):
```unknown

```

---

## Create the dataset

**URL:** llms-txt#create-the-dataset

ls_client = Client()
dataset_name = "attachment-test-dataset"
dataset = ls_client.create_dataset(
  dataset_name=dataset_name,
  description="Test dataset for evals with publicly available attachments",
)

inputs = {
  "audio_question": "What is in this audio clip?",
  "image_question": "What is in this image?",
}

outputs = {
  "audio_answer": "The sun rises in the east and sets in the west. This simple fact has been observed by humans for thousands of years.",
  "image_answer": "A mug with a blanket over it.",
}

---

## Create the example

**URL:** llms-txt#create-the-example

**Contents:**
- 2. Run evaluations
  - Define a target function

ls_client.create_examples(
  dataset_id=dataset.id,
  examples=[example],
  # Uncomment this flag if you'd like to upload attachments from local files:
  # dangerously_allow_filesystem=True
)
typescript  theme={null}
import { Client } from "langsmith";
import { v4 as uuid4 } from "uuid";

// Publicly available test files
const pdfUrl = "https://www.w3.org/WAI/ER/tests/xhtml/testfiles/resources/pdf/dummy.pdf";
const wavUrl = "https://openaiassets.blob.core.windows.net/$web/API/docs/audio/alloy.wav";
const pngUrl = "https://www.w3.org/Graphics/PNG/nurbcup2si.png";

// Helper function to fetch file as ArrayBuffer
async function fetchArrayBuffer(url: string): Promise<ArrayBuffer> {
  const response = await fetch(url);
  if (!response.ok) {
    throw new Error(`Failed to fetch ${url}: ${response.statusText}`);
  }
  return response.arrayBuffer();
}

// Fetch files as ArrayBuffer
const pdfArrayBuffer = await fetchArrayBuffer(pdfUrl);
const wavArrayBuffer = await fetchArrayBuffer(wavUrl);
const pngArrayBuffer = await fetchArrayBuffer(pngUrl);

// Create the LangSmith client (Ensure LANGSMITH_API_KEY is set in env)
const langsmithClient = new Client();

// Create a unique dataset name
const datasetName = "attachment-test-dataset:" + uuid4().substring(0, 8);

// Create the dataset
const dataset = await langsmithClient.createDataset(datasetName, {
  description: "Test dataset for evals with publicly available attachments",
});

// Define the example with attachments
const exampleId = uuid4();
const example = {
  id: exampleId,
  inputs: {
      audio_question: "What is in this audio clip?",
      image_question: "What is in this image?",
  },
  outputs: {
      audio_answer: "The sun rises in the east and sets in the west. This simple fact has been observed by humans for thousands of years.",
      image_answer: "A mug with a blanket over it.",
  },
  attachments: {
    my_pdf: {
      mimeType: "application/pdf",
      data: pdfArrayBuffer
    },
    my_wav: {
      mimeType: "audio/wav",
      data: wavArrayBuffer
    },
    my_img: {
      mimeType: "image/png",
      data: pngArrayBuffer
    },
  },
};

// Upload the example with attachments to the dataset
await langsmithClient.uploadExamplesMultipart(dataset.id, [example]);
python  theme={null}
  client.create_examples(..., dangerously_allow_filesystem=True)
  python  theme={null}
{
    "presigned_url": str,
    "mime_type": str,
    "reader": BinaryIO
}
python  theme={null}
from langsmith.wrappers import wrap_openai
import base64
from openai import OpenAI

client = wrap_openai(OpenAI())

**Examples:**

Example 1 (unknown):
```unknown
#### TypeScript

Requires version >= 0.2.13

You can use the `uploadExamplesMultipart` method to upload examples with attachments.

Note that this is a different method from the standard `createExamples` method, which currently does not support attachments. Each attachment requires either a `Uint8Array` or an `ArrayBuffer` as the data type.

* `Uint8Array`: Useful for handling binary data directly.
* `ArrayBuffer`: Represents fixed-length binary data, which can be converted to `Uint8Array` as needed.

Note that you cannot directly pass in a file path in the TypeScript SDK, as accessing local files is not supported in all runtime environments.
```

Example 2 (unknown):
```unknown
<Info>
  Along with being passed in as bytes, attachments can be specified as paths to local files. To do so pass in a path for the attachment `data` value and specify arg `dangerously_allow_filesystem=True`:
```

Example 3 (unknown):
```unknown
</Info>

## 2. Run evaluations

### Define a target function

Now that we have a dataset that includes examples with attachments, we can define a target function to run over these examples. The following example simply uses OpenAI's GPT-4o model to answer questions about an image and an audio clip.

#### Python

The target function you are evaluating must have two positional arguments in order to consume the attachments associated with the example, the first must be called `inputs` and the second must be called `attachments`.

* The `inputs` argument is a dictionary that contains the input data for the example, excluding the attachments.
* The `attachments` argument is a dictionary that maps the attachment name to a dictionary containing a presigned url, mime\_type, and a reader of the bytes content of the file. You can use either the presigned url or the reader to get the file contents. Each value in the attachments dictionary is a dictionary with the following structure:
```

Example 4 (unknown):
```unknown

```

---

## Create the graph with memory for persistence

**URL:** llms-txt#create-the-graph-with-memory-for-persistence

checkpointer = MemorySaver()

---

## Create two test users

**URL:** llms-txt#create-two-test-users

print(f"Creating test users: {email1} and {email2}")
await sign_up(email1, password)
await sign_up(email2, password)
python  theme={null}
async def login(email: str, password: str):
    """Get an access token for an existing user."""
    async with httpx.AsyncClient() as client:
        response = await client.post(
            f"{SUPABASE_URL}/auth/v1/token?grant_type=password",
            json={
                "email": email,
                "password": password
            },
            headers={
                "apikey": SUPABASE_ANON_KEY,
                "Content-Type": "application/json"
            },
        )
        assert response.status_code == 200
        return response.json()["access_token"]

**Examples:**

Example 1 (unknown):
```unknown
⚠️ Before continuing: Check your email and click both confirmation links. Supabase will reject `/login` requests until after you have confirmed your users' email.

Now test that users can only see their own data. Make sure the server is running (run `langgraph dev`) before proceeding. The following snippet requires the "anon public" key that you copied from the Supabase dashboard while [setting up the auth provider](#setup-auth-provider) previously.
```

---

## Create your underlying embeddings model

**URL:** llms-txt#create-your-underlying-embeddings-model

underlying_embeddings = ... # e.g., OpenAIEmbeddings(), HuggingFaceEmbeddings(), etc.

---

## Customize Deep Agents

**URL:** llms-txt#customize-deep-agents

**Contents:**
- Model
- System prompt
- Tools

Source: https://docs.langchain.com/oss/javascript/deepagents/customization

Learn how to customize deep agents with system prompts, tools, subagents, and more

By default, `deepagents` uses `"claude-sonnet-4-5-20250929"`. You can customize this by passing any [LangChain model object](https://python.langchain.com/docs/integrations/chat/).

Deep agents come with a built-in system prompt inspired by Claude Code's system prompt. The default system prompt contains detailed instructions for using the built-in planning tool, file system tools, and subagents.

Each deep agent tailored to a use case should include a custom system prompt specific to that use case.

Just like tool-calling agents, a deep agent gets a set of top level tools that it has access to.

In addition to any tools that you provide, deep agents also get access to a number of default tools:

* `write_todos` – Update the agent's to-do list
* `ls` – List all files in the agent's filesystem
* `read_file` – Read a file from the agent's filesystem
* `write_file` – Write a new file in the agent's filesystem
* `edit_file` – Edit an existing file in the agent's filesystem
* `task` – Spawn a subagent to handle a specific task

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/deepagents/customization.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
## System prompt

Deep agents come with a built-in system prompt inspired by Claude Code's system prompt. The default system prompt contains detailed instructions for using the built-in planning tool, file system tools, and subagents.

Each deep agent tailored to a use case should include a custom system prompt specific to that use case.
```

Example 2 (unknown):
```unknown
## Tools

Just like tool-calling agents, a deep agent gets a set of top level tools that it has access to.
```

---

## Customize user management

**URL:** llms-txt#customize-user-management

**Contents:**
- Features
  - Workspace level invites to an organization
  - SSO New Member Login Flow
  - Disabling Organization Creating
  - Disabling Personal Organizations

Source: https://docs.langchain.com/langsmith/self-host-user-management

<Note>
  This guide assumes you have read the [admin guide](/langsmith/administration-overview) and [organization setup guide](/langsmith/set-up-a-workspace#set-up-an-organization).
</Note>

LangSmith offers additional customization features for user management using feature flags.

### Workspace level invites to an organization

The default behavior in LangSmith requires a user to be an Organization Admin in order to invite new users to an organization. For self-hosted customers that would like to delegate this responsibility to workspace Admins, a feature flag may be set that enables workspace Admins to invite new users to the organization as well as their specific workspace **at the workspace level**.

Once this feature is enabled via the configuration option below, workspace Admins may add new users in the `Workspace members` tab under `Settings` > `Workspaces`. Both of the following cases are supported when inviting at the workspace level, while the organization level invite functions the same as before.

1. Invite users who are NOT already active in the organization: this will add the users as pending to the organization and specific workspace
2. Invite users who ARE already active in the organization: adds the users directly to the workspace as an active member (no pending state).

Admins may invite users for both cases at the same time.

### SSO New Member Login Flow

As of helm **v0.11.10**, self-hosted deployments using OAuth SSO will no longer need to manually add members in LangSmith settings for them to join. Deployments will have a <b>default</b> organization, to which new users will automatically be added upon their first login to LangSmith.

For your **default** organization, you can set which workspace(s) and workspace role is assigned to new members. For **non-default** organizations, the invitation flow remains the same.
Once a user joins an organization, any changes to their workspaces or roles beyond the default organization settings must be managed either through LangSmith settings (as before) or via SCIM.

<Note>
  By default, all new users are added to the organization’s initially provisioned workspace (**Workspace 1** by default) with the **Workspace Editor** role.
</Note>

<img src="https://mintcdn.com/langchain-5e9cc07a/QEp_iTXiY5U9rQvE/langsmith/images/sso-member-settings-update.png?fit=max&auto=format&n=QEp_iTXiY5U9rQvE&q=85&s=e7274ed7fdd47fe7c4c1f514d78f3ac7" alt="Update SSO Member Settings" data-og-width="1769" width="1769" data-og-height="1251" height="1251" data-path="langsmith/images/sso-member-settings-update.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/QEp_iTXiY5U9rQvE/langsmith/images/sso-member-settings-update.png?w=280&fit=max&auto=format&n=QEp_iTXiY5U9rQvE&q=85&s=2acde74eb4c622771decfe6d750f7c35 280w, https://mintcdn.com/langchain-5e9cc07a/QEp_iTXiY5U9rQvE/langsmith/images/sso-member-settings-update.png?w=560&fit=max&auto=format&n=QEp_iTXiY5U9rQvE&q=85&s=47bcef317de7189eda2743a66ead2070 560w, https://mintcdn.com/langchain-5e9cc07a/QEp_iTXiY5U9rQvE/langsmith/images/sso-member-settings-update.png?w=840&fit=max&auto=format&n=QEp_iTXiY5U9rQvE&q=85&s=eaeb449a5dc84d481fee92b7bdd0e163 840w, https://mintcdn.com/langchain-5e9cc07a/QEp_iTXiY5U9rQvE/langsmith/images/sso-member-settings-update.png?w=1100&fit=max&auto=format&n=QEp_iTXiY5U9rQvE&q=85&s=dd18c7a8aa8ee1fc2cce7d418334c713 1100w, https://mintcdn.com/langchain-5e9cc07a/QEp_iTXiY5U9rQvE/langsmith/images/sso-member-settings-update.png?w=1650&fit=max&auto=format&n=QEp_iTXiY5U9rQvE&q=85&s=a7d778198cdb93c3813b92341fc08b70 1650w, https://mintcdn.com/langchain-5e9cc07a/QEp_iTXiY5U9rQvE/langsmith/images/sso-member-settings-update.png?w=2500&fit=max&auto=format&n=QEp_iTXiY5U9rQvE&q=85&s=6029314be79253612b1465581144b170 2500w" />

<Note>
  To change your default organization, use **Set Default Organization** in the organization selector dropdown. (Org Admin permissions required in both the source and target organization.)
</Note>

### Disabling Organization Creating

By default, any user can create an organization in LangSmith. For self-hosted customers, an admin may want to restrict this ability after setting up initial organizations. This feature flag allows an admin to disable the ability for users to create new organizations.

<Note>
  The `userOrgCreationDisabled` feature flag is set to `true` by default for organizations using [basic auth](/langsmith/self-host-basic-auth) or [SSO](/langsmith/self-host-sso).
</Note>

### Disabling Personal Organizations

By default, any user who logs in to LangSmith will have a personal organization created for them. For self-hosted customers, an admin may want to restrict this ability. This feature flag allows an admin to disable the ability for users to create personal organizations.

<Note>
  The `personalOrgsDisabled` feature flag is set to `true` by default for organizations using [basic auth](/langsmith/self-host-basic-auth) or [SSO](/langsmith/self-host-sso).
</Note>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/self-host-user-management.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

### SSO New Member Login Flow

As of helm **v0.11.10**, self-hosted deployments using OAuth SSO will no longer need to manually add members in LangSmith settings for them to join. Deployments will have a <b>default</b> organization, to which new users will automatically be added upon their first login to LangSmith.

For your **default** organization, you can set which workspace(s) and workspace role is assigned to new members. For **non-default** organizations, the invitation flow remains the same.
Once a user joins an organization, any changes to their workspaces or roles beyond the default organization settings must be managed either through LangSmith settings (as before) or via SCIM.

<Note>
  By default, all new users are added to the organization’s initially provisioned workspace (**Workspace 1** by default) with the **Workspace Editor** role.
</Note>

<img src="https://mintcdn.com/langchain-5e9cc07a/QEp_iTXiY5U9rQvE/langsmith/images/sso-member-settings-update.png?fit=max&auto=format&n=QEp_iTXiY5U9rQvE&q=85&s=e7274ed7fdd47fe7c4c1f514d78f3ac7" alt="Update SSO Member Settings" data-og-width="1769" width="1769" data-og-height="1251" height="1251" data-path="langsmith/images/sso-member-settings-update.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/QEp_iTXiY5U9rQvE/langsmith/images/sso-member-settings-update.png?w=280&fit=max&auto=format&n=QEp_iTXiY5U9rQvE&q=85&s=2acde74eb4c622771decfe6d750f7c35 280w, https://mintcdn.com/langchain-5e9cc07a/QEp_iTXiY5U9rQvE/langsmith/images/sso-member-settings-update.png?w=560&fit=max&auto=format&n=QEp_iTXiY5U9rQvE&q=85&s=47bcef317de7189eda2743a66ead2070 560w, https://mintcdn.com/langchain-5e9cc07a/QEp_iTXiY5U9rQvE/langsmith/images/sso-member-settings-update.png?w=840&fit=max&auto=format&n=QEp_iTXiY5U9rQvE&q=85&s=eaeb449a5dc84d481fee92b7bdd0e163 840w, https://mintcdn.com/langchain-5e9cc07a/QEp_iTXiY5U9rQvE/langsmith/images/sso-member-settings-update.png?w=1100&fit=max&auto=format&n=QEp_iTXiY5U9rQvE&q=85&s=dd18c7a8aa8ee1fc2cce7d418334c713 1100w, https://mintcdn.com/langchain-5e9cc07a/QEp_iTXiY5U9rQvE/langsmith/images/sso-member-settings-update.png?w=1650&fit=max&auto=format&n=QEp_iTXiY5U9rQvE&q=85&s=a7d778198cdb93c3813b92341fc08b70 1650w, https://mintcdn.com/langchain-5e9cc07a/QEp_iTXiY5U9rQvE/langsmith/images/sso-member-settings-update.png?w=2500&fit=max&auto=format&n=QEp_iTXiY5U9rQvE&q=85&s=6029314be79253612b1465581144b170 2500w" />

<Note>
  To change your default organization, use **Set Default Organization** in the organization selector dropdown. (Org Admin permissions required in both the source and target organization.)
</Note>

### Disabling Organization Creating

By default, any user can create an organization in LangSmith. For self-hosted customers, an admin may want to restrict this ability after setting up initial organizations. This feature flag allows an admin to disable the ability for users to create new organizations.

#### Configuration

<Note>
  The `userOrgCreationDisabled` feature flag is set to `true` by default for organizations using [basic auth](/langsmith/self-host-basic-auth) or [SSO](/langsmith/self-host-sso).
</Note>

<CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>

### Disabling Personal Organizations

By default, any user who logs in to LangSmith will have a personal organization created for them. For self-hosted customers, an admin may want to restrict this ability. This feature flag allows an admin to disable the ability for users to create personal organizations.

#### Configuration

<Note>
  The `personalOrgsDisabled` feature flag is set to `true` by default for organizations using [basic auth](/langsmith/self-host-basic-auth) or [SSO](/langsmith/self-host-sso).
</Note>

<CodeGroup>
```

---

## Custom instrumentation

**URL:** llms-txt#custom-instrumentation

**Contents:**
- Use `@traceable` / `traceable`
- Use the `trace` context manager (Python only)
- Use the `RunTree` API
- Example usage

Source: https://docs.langchain.com/langsmith/annotate-code

<Note>
  If you've decided you no longer want to trace your runs, you can remove the `LANGSMITH_TRACING` environment variable. Note that this does not affect the `RunTree` objects or API users, as these are meant to be low-level and not affected by the tracing toggle.
</Note>

There are several ways to log traces to LangSmith.

<Check>
  If you are using LangChain (either Python or JS/TS), you can skip this section and go directly to the [LangChain-specific instructions](/langsmith/trace-with-langchain).
</Check>

## Use `@traceable` / `traceable`

LangSmith makes it easy to log traces with minimal changes to your existing code with the `@traceable` decorator in Python and `traceable` function in TypeScript.

<Note>
  The `LANGSMITH_TRACING` environment variable must be set to `'true'` in order for traces to be logged to LangSmith, even when using `@traceable` or `traceable`. This allows you to toggle tracing on and off without changing your code.

Additionally, you will need to set the `LANGSMITH_API_KEY` environment variable to your API key (see [Setup](/) for more information).

By default, the traces will be logged to a project named `default`. To log traces to a different project, see [this section](/langsmith/log-traces-to-project).
</Note>

The `@traceable` decorator is a simple way to log traces from the LangSmith Python SDK. Simply decorate any function with `@traceable`.

Note that when wrapping a sync function with `traceable`, (e.g. `formatPrompt` in the example below), you should use the `await` keyword when calling it to
ensure the trace is logged correctly.

<img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/annotate-code-trace.gif?s=bb81d0cb45382f2d793d43624db6e9ba" alt="" data-og-width="822" width="822" data-og-height="480" height="480" data-path="langsmith/images/annotate-code-trace.gif" data-optimize="true" data-opv="3" />

## Use the `trace` context manager (Python only)

In Python, you can use the `trace` context manager to log traces to LangSmith. This is useful in situations where:

1. You want to log traces for a specific block of code.
2. You want control over the inputs, outputs, and other attributes of the trace.
3. It is not feasible to use a decorator or wrapper.
4. Any or all of the above.

The context manager integrates seamlessly with the `traceable` decorator and `wrap_openai` wrapper, so you can use them together in the same application.

## Use the `RunTree` API

Another, more explicit way to log traces to LangSmith is via the `RunTree` API. This API allows you more control over your tracing - you can manually create runs and children runs to assemble your trace. You still need to set your `LANGSMITH_API_KEY`, but `LANGSMITH_TRACING` is not necessary for this method.

This method is not recommended, as it's easier to make mistakes in propagating trace context.

You can extend the utilities above to conveniently trace any code. Below are some example extensions:

Trace any public method in a class:

```python  theme={null}
from typing import Any, Callable, Type, TypeVar

def traceable_cls(cls: Type[T]) -> Type[T]:
    """Instrument all public methods in a class."""
    def wrap_method(name: str, method: Any) -> Any:
        if callable(method) and not name.startswith("__"):
            return traceable(name=f"{cls.__name__}.{name}")(method)
        return method

# Handle __dict__ case
    for name in dir(cls):
        if not name.startswith("_"):
            try:
                method = getattr(cls, name)
                setattr(cls, name, wrap_method(name, method))
            except AttributeError:
                # Skip attributes that can't be set (e.g., some descriptors)
                pass

# Handle __slots__ case
    if hasattr(cls, "__slots__"):
        for slot in cls.__slots__:  # type: ignore[attr-defined]
            if not slot.startswith("__"):
                try:
                    method = getattr(cls, slot)
                    setattr(cls, slot, wrap_method(slot, method))
                except AttributeError:
                    # Skip slots that don't have a value yet
                    pass

@traceable_cls
class MyClass:
    def __init__(self, some_val: int):
        self.some_val = some_val

def combine(self, other_val: int):
        return self.some_val + other_val

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

<img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/annotate-code-trace.gif?s=bb81d0cb45382f2d793d43624db6e9ba" alt="" data-og-width="822" width="822" data-og-height="480" height="480" data-path="langsmith/images/annotate-code-trace.gif" data-optimize="true" data-opv="3" />

## Use the `trace` context manager (Python only)

In Python, you can use the `trace` context manager to log traces to LangSmith. This is useful in situations where:

1. You want to log traces for a specific block of code.
2. You want control over the inputs, outputs, and other attributes of the trace.
3. It is not feasible to use a decorator or wrapper.
4. Any or all of the above.

The context manager integrates seamlessly with the `traceable` decorator and `wrap_openai` wrapper, so you can use them together in the same application.
```

Example 3 (unknown):
```unknown
## Use the `RunTree` API

Another, more explicit way to log traces to LangSmith is via the `RunTree` API. This API allows you more control over your tracing - you can manually create runs and children runs to assemble your trace. You still need to set your `LANGSMITH_API_KEY`, but `LANGSMITH_TRACING` is not necessary for this method.

This method is not recommended, as it's easier to make mistakes in propagating trace context.

<CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Custom output rendering

**URL:** llms-txt#custom-output-rendering

**Contents:**
- Configure custom output rendering
  - For datasets
  - For annotation queues
- Build a custom renderer
  - Understand the message format
  - Example implementation
- Where custom rendering appears

Source: https://docs.langchain.com/langsmith/custom-output-rendering

Custom output rendering allows you to visualize run outputs and dataset reference outputs using your own custom HTML pages. This is particularly useful for:

* **Domain-specific formatting**: Display medical records, legal documents, or other specialized data types in their native format.
* **Custom visualizations**: Create charts, graphs, or diagrams from numeric or structured output data.

In this page you'll learn how to:

* **[Configure custom rendering](#configure-custom-output-rendering)** in the LangSmith UI.
* **[Build a custom renderer](#build-a-custom-renderer)** to display output data.
* **[Understand where custom rendering appears](#where-custom-rendering-appears)** in LangSmith.

## Configure custom output rendering

Configure custom rendering at two levels:

* **For datasets**: Apply custom rendering to all runs associated with that dataset, wherever they appear—in experiments, run detail panes, or annotation queues.
* **For annotation queues**: Apply custom rendering to all runs within a specific annotation queue, regardless of which dataset they come from. This takes precedence over dataset-level configuration.

To configure custom output rendering for a dataset:

<img src="https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-menu.png?fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=7daf042ebae80eec20cd90a25c1d6087" alt="Dataset page with three-dot menu showing Custom Output Rendering option" data-og-width="3456" width="3456" data-og-height="2156" height="2156" data-path="langsmith/images/custom-output-rendering-menu.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-menu.png?w=280&fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=432f6a5bb6c79797c17fa8faa74169b1 280w, https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-menu.png?w=560&fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=c77ec3be9ea681180d5c82d2588814b5 560w, https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-menu.png?w=840&fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=3f8fdb966f809ad049585efe3271182b 840w, https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-menu.png?w=1100&fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=3bdd2738b8dc1d17eeea3951536dedc8 1100w, https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-menu.png?w=1650&fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=5428362058ab8e8f4f71009779e3134f 1650w, https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-menu.png?w=2500&fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=a31530b7bd16f2fe6f8c6b622f55d9de 2500w" />

1. Navigate to your dataset in the **Datasets & Experiments** page.
2. Click **⋮** (three-dot menu) in the top right corner.
3. Select **Custom Output Rendering**.
4. Toggle **Enable custom output rendering**.
5. Enter the webpage URL in the **URL** field.
6. Click **Save**.

<img src="https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-modal.png?fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=bffd3b40ca14bbebc05c998d1cb5fa7e" alt="Custom Output Rendering modal with fields filled in" data-og-width="3456" width="3456" data-og-height="2156" height="2156" data-path="langsmith/images/custom-output-rendering-modal.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-modal.png?w=280&fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=2fd8bdfcce6fdf5e9bd865590a8e0f79 280w, https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-modal.png?w=560&fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=c110a35a03909e146be961ac5386c888 560w, https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-modal.png?w=840&fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=1959a6a045a04e0d3290de4379a358bc 840w, https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-modal.png?w=1100&fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=4a154ab114547ff7e243acb36c9bd9a3 1100w, https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-modal.png?w=1650&fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=c2eae83b1d6ec160a37fd998511e9794 1650w, https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-modal.png?w=2500&fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=025b243cf994c130439ef55f6dc5e856 2500w" />

### For annotation queues

To configure custom output rendering for an annotation queue:

<img src="https://mintcdn.com/langchain-5e9cc07a/optUJrLvYf4z4j5I/langsmith/images/annotation-queue-custom-output-rendering-settings.png?fit=max&auto=format&n=optUJrLvYf4z4j5I&q=85&s=579aad04fa6990b220514280eef799f4" alt="Annotation queue settings showing custom output rendering configuration" data-og-width="3456" width="3456" data-og-height="1914" height="1914" data-path="langsmith/images/annotation-queue-custom-output-rendering-settings.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/optUJrLvYf4z4j5I/langsmith/images/annotation-queue-custom-output-rendering-settings.png?w=280&fit=max&auto=format&n=optUJrLvYf4z4j5I&q=85&s=525d5f11afb5bf8a5fd86f5de5063d72 280w, https://mintcdn.com/langchain-5e9cc07a/optUJrLvYf4z4j5I/langsmith/images/annotation-queue-custom-output-rendering-settings.png?w=560&fit=max&auto=format&n=optUJrLvYf4z4j5I&q=85&s=0d58600ddf45ff1e7bbad5cc31489f24 560w, https://mintcdn.com/langchain-5e9cc07a/optUJrLvYf4z4j5I/langsmith/images/annotation-queue-custom-output-rendering-settings.png?w=840&fit=max&auto=format&n=optUJrLvYf4z4j5I&q=85&s=d5aaa11700f5f58bcf157dc0fd32877a 840w, https://mintcdn.com/langchain-5e9cc07a/optUJrLvYf4z4j5I/langsmith/images/annotation-queue-custom-output-rendering-settings.png?w=1100&fit=max&auto=format&n=optUJrLvYf4z4j5I&q=85&s=51794d75925516c728177e91689dbdcd 1100w, https://mintcdn.com/langchain-5e9cc07a/optUJrLvYf4z4j5I/langsmith/images/annotation-queue-custom-output-rendering-settings.png?w=1650&fit=max&auto=format&n=optUJrLvYf4z4j5I&q=85&s=cf7c47a38a18a548f627dc56e94abfde 1650w, https://mintcdn.com/langchain-5e9cc07a/optUJrLvYf4z4j5I/langsmith/images/annotation-queue-custom-output-rendering-settings.png?w=2500&fit=max&auto=format&n=optUJrLvYf4z4j5I&q=85&s=bcd50a9547635188f2e7e458af0baa46 2500w" />

1. Navigate to the **Annotation Queues** page.
2. Click on an existing annotation queue or create a new one.
3. In the annotation queue settings pane, scroll to the **Custom Output Rendering** section.
4. Toggle **Enable custom output rendering**.
5. Enter the webpage URL in the **URL** field.
6. Click **Save** or **Create**.

<Info>
  When custom rendering is configured at both levels, annotation queue configuration takes precedence over dataset configuration for runs viewed within that queue.
</Info>

## Build a custom renderer

### Understand the message format

Your HTML page will receive output data via the [postMessage API](https://developer.mozilla.org/en-US/docs/Web/API/Window/postMessage). LangSmith sends messages with the following structure:

* `type`: Indicates whether this is an actual output (`"output"`) or a reference output (`"reference"`).
* `data`: The output data itself.
* `metadata.inputs`: The input data that generated this output, provided for context.

<Note>
  **Message delivery timing**: LangSmith uses an exponential backoff retry mechanism to ensure your page receives the data even if it loads slowly. Messages are sent up to 6 times with increasing delays (100ms, 200ms, 400ms, 800ms, 1600ms, 3200ms).
</Note>

### Example implementation

This example listens for incoming postMessage events and displays them on the page. Each message is numbered and formatted as JSON, making it easy to inspect the data structure LangSmith sends to your renderer.

## Where custom rendering appears

When enabled, your custom rendering will replace the default output view in:

* **Experiment comparison view**: When comparing outputs across multiple experiments:

<img src="https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-experiment-comparison.png?fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=6f1fd9d3ca4be55aa9a0b40140771e08" alt="Experiment comparison view showing custom rendering" data-og-width="3456" width="3456" data-og-height="2156" height="2156" data-path="langsmith/images/custom-output-rendering-experiment-comparison.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-experiment-comparison.png?w=280&fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=4010749197ec99216b6cbc004f8e2aed 280w, https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-experiment-comparison.png?w=560&fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=3eb5a8825d76c92f531c34912c476fb5 560w, https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-experiment-comparison.png?w=840&fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=1977c3020b1b2150beefc4980db857f9 840w, https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-experiment-comparison.png?w=1100&fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=2f8f86c1ad7ff02b65f78cd43bead446 1100w, https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-experiment-comparison.png?w=1650&fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=979e5426ee86673cd088aef614c52a6e 1650w, https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-experiment-comparison.png?w=2500&fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=292b4ac1322414a059b2ba5a4285caf3 2500w" />

* **Run detail panes**: When viewing runs that are associated with a dataset:

<img src="https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-run-details.png?fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=abec759e27bb3dfa827354d13746cf61" alt="Run detail pane showing custom rendering" data-og-width="3456" width="3456" data-og-height="2156" height="2156" data-path="langsmith/images/custom-output-rendering-run-details.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-run-details.png?w=280&fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=9d4d8e9e4d3e2f9e856d9da23e16d805 280w, https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-run-details.png?w=560&fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=1adfc78a0ba06a3b9b18433fc2e1f82c 560w, https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-run-details.png?w=840&fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=a882e8fb03c81526070c0e9f49de157b 840w, https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-run-details.png?w=1100&fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=39d6a85d5578c58cf2c6853ce1d5e704 1100w, https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-run-details.png?w=1650&fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=d9c37910f147b76685b018a620aa9d40 1650w, https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-run-details.png?w=2500&fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=24538e055a2363976a7f3ce5d0111356 2500w" />

* **Annotation queues**: When reviewing runs in annotation queues:

<img src="https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-annotation-queue.png?fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=8d1b66541ea7dcd0246354fca1568719" alt="Annotation queue showing custom rendering" data-og-width="3456" width="3456" data-og-height="2156" height="2156" data-path="langsmith/images/custom-output-rendering-annotation-queue.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-annotation-queue.png?w=280&fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=a42373180e5ab88ee80c3a670102f290 280w, https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-annotation-queue.png?w=560&fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=47be4504dd9d4a0c191cc426d7eaa7c4 560w, https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-annotation-queue.png?w=840&fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=36eda0a27d923057fbb4f0fc661e311e 840w, https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-annotation-queue.png?w=1100&fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=82593a5f663686eb61d9b90d9b1b8c39 1100w, https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-annotation-queue.png?w=1650&fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=dec8e45c1c77bd1cdd9b3aec093bf566 1650w, https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-annotation-queue.png?w=2500&fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=a8a978b3ec3d119fb16002d1a432c91a 2500w" />

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/custom-output-rendering.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
* `type`: Indicates whether this is an actual output (`"output"`) or a reference output (`"reference"`).
* `data`: The output data itself.
* `metadata.inputs`: The input data that generated this output, provided for context.

<Note>
  **Message delivery timing**: LangSmith uses an exponential backoff retry mechanism to ensure your page receives the data even if it loads slowly. Messages are sent up to 6 times with increasing delays (100ms, 200ms, 400ms, 800ms, 1600ms, 3200ms).
</Note>

### Example implementation

This example listens for incoming postMessage events and displays them on the page. Each message is numbered and formatted as JSON, making it easy to inspect the data structure LangSmith sends to your renderer.
```

---

## Custom state can be passed in invoke

**URL:** llms-txt#custom-state-can-be-passed-in-invoke

**Contents:**
- Common patterns
  - Trim messages
  - Delete messages
  - Summarize messages
- Access memory
  - Tools

result = agent.invoke(
    {
        "messages": [{"role": "user", "content": "Hello"}],
        "user_id": "user_123",  # [!code highlight]
        "preferences": {"theme": "dark"}  # [!code highlight]
    },
    {"configurable": {"thread_id": "1"}})
python  theme={null}
from langchain.messages import RemoveMessage
from langgraph.graph.message import REMOVE_ALL_MESSAGES
from langgraph.checkpoint.memory import InMemorySaver
from langchain.agents import create_agent, AgentState
from langchain.agents.middleware import before_model
from langgraph.runtime import Runtime
from langchain_core.runnables import RunnableConfig
from typing import Any

@before_model
def trim_messages(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:
    """Keep only the last few messages to fit context window."""
    messages = state["messages"]

if len(messages) <= 3:
        return None  # No changes needed

first_msg = messages[0]
    recent_messages = messages[-3:] if len(messages) % 2 == 0 else messages[-4:]
    new_messages = [first_msg] + recent_messages

return {
        "messages": [
            RemoveMessage(id=REMOVE_ALL_MESSAGES),
            *new_messages
        ]
    }

agent = create_agent(
    model,
    tools=tools,
    middleware=[trim_messages],
    checkpointer=InMemorySaver(),
)

config: RunnableConfig = {"configurable": {"thread_id": "1"}}

agent.invoke({"messages": "hi, my name is bob"}, config)
agent.invoke({"messages": "write a short poem about cats"}, config)
agent.invoke({"messages": "now do the same but for dogs"}, config)
final_response = agent.invoke({"messages": "what's my name?"}, config)

final_response["messages"][-1].pretty_print()
"""
================================== Ai Message ==================================

Your name is Bob. You told me that earlier.
If you'd like me to call you a nickname or use a different name, just say the word.
"""
python  theme={null}
from langchain.messages import RemoveMessage  # [!code highlight]

def delete_messages(state):
    messages = state["messages"]
    if len(messages) > 2:
        # remove the earliest two messages
        return {"messages": [RemoveMessage(id=m.id) for m in messages[:2]]}  # [!code highlight]
python  theme={null}
from langgraph.graph.message import REMOVE_ALL_MESSAGES  # [!code highlight]

def delete_messages(state):
    return {"messages": [RemoveMessage(id=REMOVE_ALL_MESSAGES)]}  # [!code highlight]
python  theme={null}
from langchain.messages import RemoveMessage
from langchain.agents import create_agent, AgentState
from langchain.agents.middleware import after_model
from langgraph.checkpoint.memory import InMemorySaver
from langgraph.runtime import Runtime
from langchain_core.runnables import RunnableConfig

@after_model
def delete_old_messages(state: AgentState, runtime: Runtime) -> dict | None:
    """Remove old messages to keep conversation manageable."""
    messages = state["messages"]
    if len(messages) > 2:
        # remove the earliest two messages
        return {"messages": [RemoveMessage(id=m.id) for m in messages[:2]]}
    return None

agent = create_agent(
    "gpt-5-nano",
    tools=[],
    system_prompt="Please be concise and to the point.",
    middleware=[delete_old_messages],
    checkpointer=InMemorySaver(),
)

config: RunnableConfig = {"configurable": {"thread_id": "1"}}

for event in agent.stream(
    {"messages": [{"role": "user", "content": "hi! I'm bob"}]},
    config,
    stream_mode="values",
):
    print([(message.type, message.content) for message in event["messages"]])

for event in agent.stream(
    {"messages": [{"role": "user", "content": "what's my name?"}]},
    config,
    stream_mode="values",
):
    print([(message.type, message.content) for message in event["messages"]])

[('human', "hi! I'm bob")]
[('human', "hi! I'm bob"), ('ai', 'Hi Bob! Nice to meet you. How can I help you today? I can answer questions, brainstorm ideas, draft text, explain things, or help with code.')]
[('human', "hi! I'm bob"), ('ai', 'Hi Bob! Nice to meet you. How can I help you today? I can answer questions, brainstorm ideas, draft text, explain things, or help with code.'), ('human', "what's my name?")]
[('human', "hi! I'm bob"), ('ai', 'Hi Bob! Nice to meet you. How can I help you today? I can answer questions, brainstorm ideas, draft text, explain things, or help with code.'), ('human', "what's my name?"), ('ai', 'Your name is Bob. How can I help you today, Bob?')]
[('human', "what's my name?"), ('ai', 'Your name is Bob. How can I help you today, Bob?')]
python  theme={null}
from langchain.agents import create_agent
from langchain.agents.middleware import SummarizationMiddleware
from langgraph.checkpoint.memory import InMemorySaver
from langchain_core.runnables import RunnableConfig

checkpointer = InMemorySaver()

agent = create_agent(
    model="gpt-4o",
    tools=[],
    middleware=[
        SummarizationMiddleware(
            model="gpt-4o-mini",
            max_tokens_before_summary=4000,  # Trigger summarization at 4000 tokens
            messages_to_keep=20,  # Keep last 20 messages after summary
        )
    ],
    checkpointer=checkpointer,
)

config: RunnableConfig = {"configurable": {"thread_id": "1"}}
agent.invoke({"messages": "hi, my name is bob"}, config)
agent.invoke({"messages": "write a short poem about cats"}, config)
agent.invoke({"messages": "now do the same but for dogs"}, config)
final_response = agent.invoke({"messages": "what's my name?"}, config)

final_response["messages"][-1].pretty_print()
"""
================================== Ai Message ==================================

Your name is Bob!
"""
python  theme={null}
from langchain.agents import create_agent, AgentState
from langchain.tools import tool, ToolRuntime

class CustomState(AgentState):
    user_id: str

@tool
def get_user_info(
    runtime: ToolRuntime
) -> str:
    """Look up user info."""
    user_id = runtime.state["user_id"]
    return "User is John Smith" if user_id == "user_123" else "Unknown user"

agent = create_agent(
    model="gpt-5-nano",
    tools=[get_user_info],
    state_schema=CustomState,
)

result = agent.invoke({
    "messages": "look up user information",
    "user_id": "user_123"
})
print(result["messages"][-1].content)

**Examples:**

Example 1 (unknown):
```unknown
## Common patterns

With [short-term memory](#add-short-term-memory) enabled, long conversations can exceed the LLM's context window. Common solutions are:

<CardGroup cols={2}>
  <Card title="Trim messages" icon="scissors" href="#trim-messages" arrow>
    Remove first or last N messages (before calling LLM)
  </Card>

  <Card title="Delete messages" icon="trash" href="#delete-messages" arrow>
    Delete messages from LangGraph state permanently
  </Card>

  <Card title="Summarize messages" icon="layer-group" href="#summarize-messages" arrow>
    Summarize earlier messages in the history and replace them with a summary
  </Card>

  <Card title="Custom strategies" icon="gears">
    Custom strategies (e.g., message filtering, etc.)
  </Card>
</CardGroup>

This allows the agent to keep track of the conversation without exceeding the LLM's context window.

### Trim messages

Most LLMs have a maximum supported context window (denominated in tokens).

One way to decide when to truncate messages is to count the tokens in the message history and truncate whenever it approaches that limit. If you're using LangChain, you can use the trim messages utility and specify the number of tokens to keep from the list, as well as the `strategy` (e.g., keep the last `max_tokens`) to use for handling the boundary.

To trim message history in an agent, use the [`@before_model`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.before_model) middleware decorator:
```

Example 2 (unknown):
```unknown
### Delete messages

You can delete messages from the graph state to manage the message history.

This is useful when you want to remove specific messages or clear the entire message history.

To delete messages from the graph state, you can use the `RemoveMessage`.

For `RemoveMessage` to work, you need to use a state key with [`add_messages`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.message.add_messages) [reducer](/oss/python/langgraph/graph-api#reducers).

The default [`AgentState`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.AgentState) provides this.

To remove specific messages:
```

Example 3 (unknown):
```unknown
To remove **all** messages:
```

Example 4 (unknown):
```unknown
<Warning>
  When deleting messages, **make sure** that the resulting message history is valid. Check the limitations of the LLM provider you're using. For example:

  * Some providers expect message history to start with a `user` message
  * Most providers require `assistant` messages with tool calls to be followed by corresponding `tool` result messages.
</Warning>
```

---

## Dataset prebuilt JSON schema types

**URL:** llms-txt#dataset-prebuilt-json-schema-types

Source: https://docs.langchain.com/langsmith/dataset-json-types

LangSmith recommends that you set a schema on the inputs and outputs of your dataset schemas to ensure data consistency and that your examples are in the right format for downstream processing, like running evals.

In order to better support LLM workflows, LangSmith has support for a few different predefined prebuilt types. These schemas are hosted publicly by the LangSmith API, and can be defined in your dataset schemas using [JSON Schema references](https://json-schema.org/understanding-json-schema/structuring#dollarref). The table of available schemas can be seen below

| Type    | JSON Schema Reference Link                                                                                                       | Usage                                                                                                                     |
| ------- | -------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------- |
| Message | [https://api.smith.langchain.com/public/schemas/v1/message.json](https://api.smith.langchain.com/public/schemas/v1/message.json) | Represents messages sent to a chat model, following the OpenAI standard format.                                           |
| Tool    | [https://api.smith.langchain.com/public/schemas/v1/tooldef.json](https://api.smith.langchain.com/public/schemas/v1/tooldef.json) | Tool definitions available to chat models for function calling, defined in OpenAI's JSON Schema inspired function format. |

LangSmith lets you define a series of transformations that collect the above prebuilt types from your traces and add them to your dataset. For more info on available transformations, see our [reference](/langsmith/dataset-transformations)

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/dataset-json-types.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Dataset transformations

**URL:** llms-txt#dataset-transformations

**Contents:**
- Transformation types
- Chat Model prebuilt schema
  - Compatibility
  - Enablement
  - Specs

Source: https://docs.langchain.com/langsmith/dataset-transformations

LangSmith allows you to attach transformations to fields in your dataset's schema that apply to your data before it is added to your dataset, whether that be from UI, API, or run rules.

Coupled with [LangSmith's prebuilt JSON schema types](/langsmith/dataset-json-types), these allow you to do easy preprocessing of your data before saving it into your datasets.

## Transformation types

| Transformation Type          | Target Types                                                              | Functionality                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
| ---------------------------- | ------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| remove\_system\_messages     | Array\[Message]                                                           | Filters a list of messages to remove any system messages.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
| convert\_to\_openai\_message | Message Array\[Message]                                                   | Converts any incoming data from LangChain's internal serialization format to OpenAI's standard message format using langchain's [convert\_to\_openai\_messages](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.utils.convert_to_openai_messages.html). If the target field is marked as required, and no matching message is found upon entry, it will attempt to extract a message (or list of messages) from several well-known LangSmith tracing formats (e.g., any traced LangChain [BaseChatModel](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.chat_models.BaseChatModel.html) run or traced run from the [LangSmith OpenAI wrapper](/langsmith/annotate-code#wrap-the-openai-client)), and remove the original key containing the message. |
| convert\_to\_openai\_tool    | Array\[Tool] Only available on top level fields in the inputs dictionary. | Converts any incoming data into OpenAI standard tool formats here using langchain's [convert\_to\_openai\_tool](https://python.langchain.com/api_reference/core/utils/langchain_core.utils.function_calling.convert_to_openai_tool.html) Will extract tool definitions from a run's invocation parameters if present / no tools are found at the specified key. This is useful because LangChain chat models trace tool definitions to the `extra.invocation_params` field of the run rather than inputs.                                                                                                                                                                                                                                                                                                                             |
| remove\_extra\_fields        | Object                                                                    | Removes any field not defined in the schema for this target object.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |

## Chat Model prebuilt schema

The main use case for transformations is to simplify collecting production traces into datasets in a format that can be standardized across model providers for usage in evaluations / few shot prompting / etc downstream.

To simplify setup of transformations for our end users, LangSmith offers a pre-defined schema that will do the following:

* Extract messages from your collected runs and transform them into the openai standard format, which makes them compatible all LangChain ChatModels and most model providers' SDK for downstream evaluation and experimentation
* Extract any tools used by your LLM and add them to your example's input to be used for reproducability in downstream evaluation

<Check>
  Users who want to iterate on their system prompts often also add the Remove System Messages transformation on their input messages when using our Chat Model schema, which will prevent you from saving the system prompt to your dataset.
</Check>

The LLM run collection schema is built to collect data from LangChain [BaseChatModel](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.chat_models.BaseChatModel.html) runs or traced runs from the [LangSmith OpenAI wrapper](/langsmith/annotate-code#wrap-the-openai-client).

Please reach out to [support@langchain.dev](mailto:support@langchain.dev) if you have an LLM run you are tracing that is not compatible and we can extend support.

If you want to apply transformations to other sorts of runs (for example, representing LangGraph state with message history), please define your schema directly and manually add the relevant transformations.

When adding a run from a tracing project or annotation queue to a dataset, if it has the LLM run type, we will apply the Chat Model schema by default.

For enablement on new datasets, see our [dataset management how-to guide](/langsmith/manage-datasets-in-application).

For the full API specs of the prebuilt schema, see the below sections:

And the transformations look as follows:

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/dataset-transformations.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
#### Output schema
```

Example 2 (unknown):
```unknown
#### Transformations

And the transformations look as follows:
```

---

## Data purging for compliance

**URL:** llms-txt#data-purging-for-compliance

**Contents:**
- Data retention
- Trace deletes
  - Deletion timeline
  - Delete specific traces
  - Delete by metadata
- Example deletes
  - Deleting examples is a two-step process
  - Deletion types

Source: https://docs.langchain.com/langsmith/data-purging-compliance

This guide covers the various features available after data reaches LangSmith Cloud servers to help you achieve your privacy goals.

LangSmith provides automatic data retention capabilities to help with compliance and storage management. Data retention policies can be configured at the organization and project levels.

For detailed information about data retention configuration and management, please refer to the [Data Retention concepts](/langsmith/administration-overview#data-retention) documentation.

You can use the API to complete trace deletes. The API supports two methods for deleting traces:

1. **By trace IDs and session ID**: Delete specific traces by providing a list of trace IDs and their corresponding session ID (up to 1000 traces per request)
2. **By metadata**: Delete traces across a workspace that match any of the specified metadata key-value pairs

For more details, refer to the [API spec](https://api.smith.langchain.com/redoc#tag/run/operation/delete_runs_api_v1_runs_delete_post).

<Warning>
  All trace deletions will delete related entities like feedbacks, aggregations, and stats across all data storages.
</Warning>

### Deletion timeline

Trace deletions are processed during non-peak usage times and are not instant. LangChain runs the delete job on the weekend. There is no confirmation of deletion - you'll need to query the data again to verify it has been removed.

### Delete specific traces

To delete specific traces by their trace IDs from a single session:

### Delete by metadata

When deleting by metadata:

* Accepts a `metadata` object of key/value pairs. KV pair matching uses an **or** condition. A trace will match if it has **any** of the key-value pairs specified in metadata (not all)
* You don't need to specify a session id when deleting by metadata. Deletes will apply across the workspace.

To delete traces based on metadata across a workspace (matches **any** of the metadata key-value pairs):

This will delete traces that have either `user_id: "user123"` **or** `environment: "staging"` in their metadata.

<Warning>
  Remember that you can only schedule up to 1000 traces per session per request. For larger deletions, you'll need to make multiple requests.
</Warning>

You can delete dataset examples self-serve via our API, which supports both soft and hard deletion methods depending on your data retention needs.

<Warning>
  Hard deletes will permanently remove inputs, outputs, and metadata from ALL versions of the specified examples across the entire dataset history.
</Warning>

### Deleting examples is a two-step process

For bulk operations, example deletion follows a two-step process:

#### 1. Search for examples by metadata

Find all examples with matching metadata across all datasets in a workspace.

[GET /examples](https://api.smith.langchain.com/redoc#tag/examples/operation/read_examples_api_v1_examples_get)

* `as_of` must be explicitly specified as a timestamp. Only examples created before the `as_of` date will be returned

This will return examples that have either `user_id: "user123"` **or** `environment: "staging"` in their metadata across all datasets in your workspace.

#### 2. Hard delete examples

Once you have the example IDs, send a delete request. This will zero-out the inputs, outputs, and metadata from all versions of the dataset for that example.

[DELETE /examples](https://api.smith.langchain.com/redoc#tag/examples/operation/delete_examples_api_v1_examples_delete)

* Specify example IDs and add `"hard_delete": true` to the query params of the request

#### Soft delete (default)

* Creates tombstoned entries with NULL inputs/outputs in the dataset
* Preserves historical data and maintains dataset versioning
* Only affects the current version of the dataset

* Permanently removes inputs, outputs, and metadata from ALL dataset versions
* Complete data removal when compliance requires zero-out across all versions
* Add `"hard_delete": true` to the query parameters

For more details, refer to the [API spec](https://api.smith.langchain.com/redoc#tag/examples/operation/delete_examples_api_v1_examples_delete).

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/data-purging-compliance.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
### Delete by metadata

When deleting by metadata:

* Accepts a `metadata` object of key/value pairs. KV pair matching uses an **or** condition. A trace will match if it has **any** of the key-value pairs specified in metadata (not all)
* You don't need to specify a session id when deleting by metadata. Deletes will apply across the workspace.

To delete traces based on metadata across a workspace (matches **any** of the metadata key-value pairs):
```

Example 2 (unknown):
```unknown
This will delete traces that have either `user_id: "user123"` **or** `environment: "staging"` in their metadata.

<Warning>
  Remember that you can only schedule up to 1000 traces per session per request. For larger deletions, you'll need to make multiple requests.
</Warning>

## Example deletes

You can delete dataset examples self-serve via our API, which supports both soft and hard deletion methods depending on your data retention needs.

<Warning>
  Hard deletes will permanently remove inputs, outputs, and metadata from ALL versions of the specified examples across the entire dataset history.
</Warning>

### Deleting examples is a two-step process

For bulk operations, example deletion follows a two-step process:

#### 1. Search for examples by metadata

Find all examples with matching metadata across all datasets in a workspace.

[GET /examples](https://api.smith.langchain.com/redoc#tag/examples/operation/read_examples_api_v1_examples_get)

* `as_of` must be explicitly specified as a timestamp. Only examples created before the `as_of` date will be returned
```

Example 3 (unknown):
```unknown
This will return examples that have either `user_id: "user123"` **or** `environment: "staging"` in their metadata across all datasets in your workspace.

#### 2. Hard delete examples

Once you have the example IDs, send a delete request. This will zero-out the inputs, outputs, and metadata from all versions of the dataset for that example.

[DELETE /examples](https://api.smith.langchain.com/redoc#tag/examples/operation/delete_examples_api_v1_examples_delete)

* Specify example IDs and add `"hard_delete": true` to the query params of the request
```

---

## Data storage and privacy

**URL:** llms-txt#data-storage-and-privacy

**Contents:**
- CLI
- Agent Server
  - LangSmith Tracing
  - In-memory development server
  - Standalone Server
- Studio
- Quick reference

Source: https://docs.langchain.com/langsmith/data-storage-and-privacy

This document describes how data is processed in the LangGraph CLI and the Agent Server for both the in-memory server (`langgraph dev`) and the local Docker server (`langgraph up`). It also describes what data is tracked when interacting with the hosted Studio frontend.

LangGraph **CLI** is the command-line interface for building and running LangGraph applications; see the [CLI guide](/langsmith/cli) to learn more.

By default, calls to most CLI commands log a single analytics event upon invocation. This helps us better prioritize improvements to the CLI experience. Each telemetry event contains the calling process's OS, OS version, Python version, the CLI version, the command name (`dev`, `up`, `run`, etc.), and booleans representing whether a flag was passed to the command. You can see the full analytics logic [here](https://github.com/langchain-ai/langgraph/blob/main/libs/cli/langgraph-cli/analytics.py).

You can disable all CLI telemetry by setting `LANGGRAPH_CLI_NO_ANALYTICS=1`.

<a id="in-memory-docker" />

The [Agent Server](/langsmith/agent-server) provides a durable execution runtime that relies on persisting checkpoints of your application state, long-term memories, thread metadata, assistants, and similar resources to the local file system or a database. Unless you have deliberately customized the storage location, this information is either written to local disk (for `langgraph dev`) or a PostgreSQL database (for `langgraph up` and in all deployments).

### LangSmith Tracing

When running the Agent server (either in-memory or in Docker), LangSmith tracing may be enabled to facilitate faster debugging and offer observability of graph state and LLM prompts in production. You can always disable tracing by setting `LANGSMITH_TRACING=false` in your server's runtime environment.

<a id="langgraph-dev" />

### In-memory development server

`langgraph dev` runs an [in-memory development server](/langsmith/local-server) as a single Python process, designed for quick development and testing. It saves all checkpointing and memory data to disk within a `.langgraph_api` directory in the current working directory. Apart from the telemetry data described in the [CLI](#cli) section, no data leaves the machine unless you have enabled tracing or your graph code explicitly contacts an external service.

<a id="langgraph-up" />

### Standalone Server

`langgraph up` builds your local package into a Docker image and runs the server as the [data plane](/langsmith/self-hosted) consisting of three containers: the API server, a PostgreSQL container, and a Redis container. All persistent data (checkpoints, assistants, etc.) are stored in the PostgreSQL database. Redis is used as a pubsub connection for real-time streaming of events. You can encrypt all checkpoints before saving to the database by setting a valid `LANGGRAPH_AES_KEY` environment variable. You can also specify [TTLs](/langsmith/configure-ttl) for checkpoints and cross-thread memories in `langgraph.json` to control how long data is stored. All persisted threads, memories, and other data can be deleted via the relevant API endpoints.

Additional API calls are made to confirm that the server has a valid license and to track the number of executed runs and tasks. Periodically, the API server validates the provided license key (or API key).

If you've disabled [tracing](#langsmith-tracing), no user data is persisted externally unless your graph code explicitly contacts an external service.

[Studio](/langsmith/studio) is a graphical interface for interacting with your Agent Server. It does not persist any private data (the data you send to your server is not sent to LangSmith). Though the Studio interface is served at [smith.langchain.com](https://smith.langchain.com), it is run in your browser and connects directly to your local Agent Server so that no data needs to be sent to LangSmith.

If you are logged in, LangSmith does collect some usage analytics to help improve the debugging user experience. This includes:

* Page visits and navigation patterns
* User actions (button clicks)
* Browser type and version
* Screen resolution and viewport size

Importantly, no application data or code (or other sensitive configuration details) are collected. All of that is stored in the persistence layer of your Agent Server. When using Studio anonymously, no account creation is required and usage analytics are not collected.

In summary, you can opt-out of server-side telemetry by turning off CLI analytics and disabling tracing.

| Variable                       | Purpose                   | Default                |
| ------------------------------ | ------------------------- | ---------------------- |
| `LANGGRAPH_CLI_NO_ANALYTICS=1` | Disable CLI analytics     | Analytics enabled      |
| `LANGSMITH_API_KEY`            | Enable LangSmith tracing  | Tracing disabled       |
| `LANGSMITH_TRACING=false`      | Disable LangSmith tracing | Depends on environment |

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/data-storage-and-privacy.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Decide whether to retrieve

**URL:** llms-txt#decide-whether-to-retrieve

workflow.add_conditional_edges(
    "generate_query_or_respond",
    # Assess LLM decision (call `retriever_tool` tool or respond to the user)
    tools_condition,
    {
        # Translate the condition outputs to nodes in our graph
        "tools": "retrieve",
        END: END,
    },
)

---

## Deep Agents

**URL:** llms-txt#deep-agents

Source: https://docs.langchain.com/oss/javascript/reference/deepagents-javascript

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/reference/deepagents-javascript.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Deep Agents Middleware

**URL:** llms-txt#deep-agents-middleware

**Contents:**
- To-do list middleware
- Filesystem middleware
  - Short-term vs. long-term filesystem
- Subagent middleware

Source: https://docs.langchain.com/oss/javascript/deepagents/middleware

Understand the middleware that powers deep agents

Deep agents are built with a modular middleware architecture. Deep agents have access to:

1. A planning tool
2. A filesystem for storing context and long-term memories
3. The ability to spawn subagents

Each feature is implemented as separate middleware. When you create a deep agent with `create_deep_agent`, we automatically attach `TodoListMiddleware`, `FilesystemMiddleware`, and `SubAgentMiddleware` to your agent.

Middleware is composable—you can add as many or as few middleware to an agent as needed. You can use any middleware independently.

The following sections explain what each middleware provides.

## To-do list middleware

Planning is integral to solving complex problems. If you've used Claude Code recently, you'll notice how it writes out a to-do list before tackling complex, multi-part tasks. You'll also notice how it can adapt and update this to-do list on the fly as more information comes in.

`TodoListMiddleware` provides your agent with a tool specifically for updating this to-do list. Before and while it executes a multi-part task, the agent is prompted to use the `write_todos` tool to keep track of what it's doing and what still needs to be done.

## Filesystem middleware

Context engineering is a main challenge in building effective agents. This is particularly difficult when using tools that return variable-length results (for example, web\_search and rag), as long tool results can quickly fill your context window.

`FilesystemMiddleware` provides four tools for interacting with both short-term and long-term memory:

* **ls**: List the files in the filesystem
* **read\_file**: Read an entire file or a certain number of lines from a file
* **write\_file**: Write a new file to the filesystem
* **edit\_file**: Edit an existing file in the filesystem

### Short-term vs. long-term filesystem

By default, these tools write to a local "filesystem" in your graph state. To enable persistent storage across threads, configure a `CompositeBackend` that routes specific paths (like `/memories/`) to a `StoreBackend`.

When you configure a `CompositeBackend` with a `StoreBackend` for `/memories/`, any files prefixed with **/memories/** are saved to persistent storage and survive across different threads. Files without this prefix remain in ephemeral state storage.

## Subagent middleware

Handing off tasks to subagents isolates context, keeping the main (supervisor) agent's context window clean while still going deep on a task.

The subagents middleware allows you to supply subagents through a `task` tool.

A subagent is defined with a **name**, **description**, **system prompt**, and **tools**. You can also provide a subagent with a custom **model**, or with additional **middleware**. This can be particularly useful when you want to give the subagent an additional state key to share with the main agent.

For more complex use cases, you can also provide your own pre-built LangGraph graph as a subagent.

In addition to any user-defined subagents, the main agent has access to a `general-purpose` subagent at all times. This subagent has the same instructions as the main agent and all the tools it has access to. The primary purpose of the `general-purpose` subagent is context isolation—the main agent can delegate a complex task to this subagent and get a concise answer back without bloat from intermediate tool calls.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/deepagents/middleware.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
## Filesystem middleware

Context engineering is a main challenge in building effective agents. This is particularly difficult when using tools that return variable-length results (for example, web\_search and rag), as long tool results can quickly fill your context window.

`FilesystemMiddleware` provides four tools for interacting with both short-term and long-term memory:

* **ls**: List the files in the filesystem
* **read\_file**: Read an entire file or a certain number of lines from a file
* **write\_file**: Write a new file to the filesystem
* **edit\_file**: Edit an existing file in the filesystem
```

Example 2 (unknown):
```unknown
### Short-term vs. long-term filesystem

By default, these tools write to a local "filesystem" in your graph state. To enable persistent storage across threads, configure a `CompositeBackend` that routes specific paths (like `/memories/`) to a `StoreBackend`.
```

Example 3 (unknown):
```unknown
When you configure a `CompositeBackend` with a `StoreBackend` for `/memories/`, any files prefixed with **/memories/** are saved to persistent storage and survive across different threads. Files without this prefix remain in ephemeral state storage.

## Subagent middleware

Handing off tasks to subagents isolates context, keeping the main (supervisor) agent's context window clean while still going deep on a task.

The subagents middleware allows you to supply subagents through a `task` tool.
```

Example 4 (unknown):
```unknown
A subagent is defined with a **name**, **description**, **system prompt**, and **tools**. You can also provide a subagent with a custom **model**, or with additional **middleware**. This can be particularly useful when you want to give the subagent an additional state key to share with the main agent.

For more complex use cases, you can also provide your own pre-built LangGraph graph as a subagent.
```

---

## Deep Agents overview

**URL:** llms-txt#deep-agents-overview

**Contents:**
- When to use deep agents
- Core capabilities
- Relationship to the LangChain ecosystem
- Get started

Source: https://docs.langchain.com/oss/javascript/deepagents/overview

Build agents that can plan, use subagents, and leverage file systems for complex tasks

[`deepagents`](https://www.npmjs.com/package/deepagents) is a standalone library for building agents that can tackle complex, multi-step tasks. Built on LangGraph and inspired by applications like Claude Code, Deep Research, and Manus, deep agents come with planning capabilities, file systems for context management, and the ability to spawn subagents.

## When to use deep agents

Use deep agents when you need agents that can:

* **Handle complex, multi-step tasks** that require planning and decomposition
* **Manage large amounts of context** through file system tools
* **Delegate work** to specialized subagents for context isolation
* **Persist memory** across conversations and threads

For simpler use cases, consider using LangChain's [`createAgent`](/oss/javascript/langchain/agents) or building a custom [LangGraph](/oss/javascript/langgraph/overview) workflow.

<Card title="Planning and task decomposition" icon="timeline">
  Deep agents include a built-in `write_todos` tool that enables agents to break down complex tasks into discrete steps, track progress, and adapt plans as new information emerges.
</Card>

<Card title="Context management" icon="scissors">
  File system tools (`ls`, `read_file`, `write_file`, `edit_file`) allow agents to offload large context to memory, preventing context window overflow and enabling work with variable-length tool results.
</Card>

<Card title="Subagent spawning" icon="people-group">
  A built-in `task` tool enables agents to spawn specialized subagents for context isolation. This keeps the main agent's context clean while still going deep on specific subtasks.
</Card>

<Card title="Long-term memory" icon="database">
  Extend agents with persistent memory across threads using LangGraph's Store. Agents can save and retrieve information from previous conversations.
</Card>

## Relationship to the LangChain ecosystem

Deep agents is built on top of:

* **LangGraph** - Provides the underlying graph execution and state management
* **LangChain** - Tools and model integrations work seamlessly with deep agents
* **LangSmith** - Observability and deployment through LangGraph Platform

Deep agents applications can be deployed via [LangSmith Deployment](/langsmith/deployments) and monitored with [LangSmith Observability](/langsmith/observability).

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/deepagents/overview.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Default: user-scoped token (works for any agent under this user)

**URL:** llms-txt#default:-user-scoped-token-(works-for-any-agent-under-this-user)

auth_result = await client.authenticate(
    provider="{provider_id}",
    scopes=["scopeA"],
    user_id="your_user_id"
)

if auth_result.needs_auth:
    print(f"Complete OAuth at: {auth_result.auth_url}")
    # Wait for completion
    completed_auth = await client.wait_for_completion(auth_result.auth_id)
    token = completed_auth.token
else:
    token = auth_result.token
```

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/agent-auth.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Define an example with attachments

**URL:** llms-txt#define-an-example-with-attachments

example_id = uuid.uuid4()
example = {
  "id": example_id,
  "inputs": inputs,
  "outputs": outputs,
  "attachments": {
      "my_pdf": {"mime_type": "application/pdf", "data": pdf_bytes},
      "my_wav": {"mime_type": "audio/wav", "data": wav_bytes},
      "my_img": {"mime_type": "image/png", "data": img_bytes},
      # Example of an attachment specified via a local file path:
      # "my_local_img": {"mime_type": "image/png", "data": Path(__file__).parent / "my_local_img.png"},
  },
}

---

## Define a new graph

**URL:** llms-txt#define-a-new-graph

workflow = StateGraph(State)

---

## Define a tool

**URL:** llms-txt#define-a-tool

def multiply(a: int, b: int) -> int:
    return a * b

---

## Define dataset: these are your test cases

**URL:** llms-txt#define-dataset:-these-are-your-test-cases

**Contents:**
- Define metrics
- Run Evaluations
- Comparing results
- Set up automated testing to run in CI/CD
- Track results over time
- Conclusion
- Reference code

dataset_name = "QA Example Dataset"
dataset = client.create_dataset(dataset_name)

client.create_examples(
    dataset_id=dataset.id,
    examples=[
        {
            "inputs": {"question": "What is LangChain?"},
            "outputs": {"answer": "A framework for building LLM applications"},
        },
        {
            "inputs": {"question": "What is LangSmith?"},
            "outputs": {"answer": "A platform for observing and evaluating LLM applications"},
        },
        {
            "inputs": {"question": "What is OpenAI?"},
            "outputs": {"answer": "A company that creates Large Language Models"},
        },
        {
            "inputs": {"question": "What is Google?"},
            "outputs": {"answer": "A technology company known for search"},
        },
        {
            "inputs": {"question": "What is Mistral?"},
            "outputs": {"answer": "A company that creates Large Language Models"},
        }
    ]
)
python  theme={null}
import openai
from langsmith import wrappers

openai_client = wrappers.wrap_openai(openai.OpenAI())

eval_instructions = "You are an expert professor specialized in grading students' answers to questions."

def correctness(inputs: dict, outputs: dict, reference_outputs: dict) -> bool:
    user_content = f"""You are grading the following question:
{inputs['question']}
Here is the real answer:
{reference_outputs['answer']}
You are grading the following predicted answer:
{outputs['response']}
Respond with CORRECT or INCORRECT:
Grade:"""
    response = openai_client.chat.completions.create(
        model="gpt-4o-mini",
        temperature=0,
        messages=[
            {"role": "system", "content": eval_instructions},
            {"role": "user", "content": user_content},
        ],
    ).choices[0].message.content
    return response == "CORRECT"
python  theme={null}
def concision(outputs: dict, reference_outputs: dict) -> bool:
    return int(len(outputs["response"]) < 2 * len(reference_outputs["answer"]))
python  theme={null}
default_instructions = "Respond to the users question in a short, concise manner (one short sentence)."

def my_app(question: str, model: str = "gpt-4o-mini", instructions: str = default_instructions) -> str:
    return openai_client.chat.completions.create(
        model=model,
        temperature=0,
        messages=[
            {"role": "system", "content": instructions},
            {"role": "user", "content": question},
        ],
    ).choices[0].message.content
python  theme={null}
def ls_target(inputs: str) -> dict:
    return {"response": my_app(inputs["question"])}
python  theme={null}
experiment_results = client.evaluate(
    ls_target, # Your AI system
    data=dataset_name, # The data to predict and grade over
    evaluators=[concision, correctness], # The evaluators to score the results
    experiment_prefix="openai-4o-mini", # A prefix for your experiment names to easily identify them
)
python  theme={null}
def ls_target_v2(inputs: str) -> dict:
    return {"response": my_app(inputs["question"], model="gpt-4-turbo")}

experiment_results = client.evaluate(
    ls_target_v2,
    data=dataset_name,
    evaluators=[concision, correctness],
    experiment_prefix="openai-4-turbo",
)
python  theme={null}
instructions_v3 = "Respond to the users question in a short, concise manner (one short sentence). Do NOT use more than ten words."

def ls_target_v3(inputs: str) -> dict:
    response = my_app(
        inputs["question"],
        model="gpt-4-turbo",
        instructions=instructions_v3
    )
    return {"response": response}

experiment_results = client.evaluate(
    ls_target_v3,
    data=dataset_name,
    evaluators=[concision, correctness],
    experiment_prefix="strict-openai-4-turbo",
)
python  theme={null}
def test_length_score() -> None:
    """Test that the length score is at least 80%."""
    experiment_results = evaluate(
        ls_target, # Your AI system
        data=dataset_name, # The data to predict and grade over
        evaluators=[concision, correctness], # The evaluators to score the results
    )
    # This will be cleaned up in the next release:
    feedback = client.list_feedback(
        run_ids=[r.id for r in client.list_runs(project_name=experiment_results.experiment_name)],
        feedback_key="concision"
    )
    scores = [f.score for f in feedback]
    assert sum(scores) / len(scores) >= 0.8, "Aggregate score should be at least .8"
python  theme={null}
  import openai
  from langsmith import Client, wrappers

# Application code
  openai_client = wrappers.wrap_openai(openai.OpenAI())

default_instructions = "Respond to the users question in a short, concise manner (one short sentence)."

def my_app(question: str, model: str = "gpt-4o-mini", instructions: str = default_instructions) -> str:
      return openai_client.chat.completions.create(
          model=model,
          temperature=0,
          messages=[
              {"role": "system", "content": instructions},
              {"role": "user", "content": question},
          ],
      ).choices[0].message.content

# Define dataset: these are your test cases
  dataset_name = "QA Example Dataset"
  dataset = client.create_dataset(dataset_name)

client.create_examples(
      dataset_id=dataset.id,
      examples=[
          {
              "inputs": {"question": "What is LangChain?"},
              "outputs": {"answer": "A framework for building LLM applications"},
          },
          {
              "inputs": {"question": "What is LangSmith?"},
              "outputs": {"answer": "A platform for observing and evaluating LLM applications"},
          },
          {
              "inputs": {"question": "What is OpenAI?"},
              "outputs": {"answer": "A company that creates Large Language Models"},
          },
          {
              "inputs": {"question": "What is Google?"},
              "outputs": {"answer": "A technology company known for search"},
          },
          {
              "inputs": {"question": "What is Mistral?"},
              "outputs": {"answer": "A company that creates Large Language Models"},
          }
      ]
  )

# Define evaluators
  eval_instructions = "You are an expert professor specialized in grading students' answers to questions."

def correctness(inputs: dict, outputs: dict, reference_outputs: dict) -> bool:
      user_content = f"""You are grading the following question:
  {inputs['question']}
  Here is the real answer:
  {reference_outputs['answer']}
  You are grading the following predicted answer:
  {outputs['response']}
  Respond with CORRECT or INCORRECT:
  Grade:"""
      response = openai_client.chat.completions.create(
          model="gpt-4o-mini",
          temperature=0,
          messages=[
              {"role": "system", "content": eval_instructions},
              {"role": "user", "content": user_content},
          ],
      ).choices[0].message.content
      return response == "CORRECT"

def concision(outputs: dict, reference_outputs: dict) -> bool:
      return int(len(outputs["response"]) < 2 * len(reference_outputs["answer"]))

# Run evaluations
  def ls_target(inputs: str) -> dict:
      return {"response": my_app(inputs["question"])}

experiment_results_v1 = client.evaluate(
      ls_target, # Your AI system
      data=dataset_name, # The data to predict and grade over
      evaluators=[concision, correctness], # The evaluators to score the results
      experiment_prefix="openai-4o-mini", # A prefix for your experiment names to easily identify them
  )

def ls_target_v2(inputs: str) -> dict:
      return {"response": my_app(inputs["question"], model="gpt-4-turbo")}

experiment_results_v2 = client.evaluate(
      ls_target_v2,
      data=dataset_name,
      evaluators=[concision, correctness],
      experiment_prefix="openai-4-turbo",
  )

instructions_v3 = "Respond to the users question in a short, concise manner (one short sentence). Do NOT use more than ten words."

def ls_target_v3(inputs: str) -> dict:
      response = my_app(
          inputs["question"],
          model="gpt-4-turbo",
          instructions=instructions_v3
      )
      return {"response": response}

experiment_results_v3 = client.evaluate(
      ls_target_v3,
      data=dataset_name,
      evaluators=[concision, correctness],
      experiment_prefix="strict-openai-4-turbo",
  )
  ```
</Accordion>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/evaluate-chatbot-tutorial.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
Now, if we go the LangSmith UI and look for `QA Example Dataset` in the `Datasets & Testing` page, when we click into it we should see that we have five new examples.

<img src="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/testing-tutorial-dataset.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=9ab5110714d009d5865ba0e2d8ee0ffa" alt="" data-og-width="1251" width="1251" data-og-height="560" height="560" data-path="langsmith/images/testing-tutorial-dataset.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/testing-tutorial-dataset.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=e4b38ded6968e649ed8ab507f63f1f3e 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/testing-tutorial-dataset.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=f7aee5327f8058dd99684cd43e44c791 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/testing-tutorial-dataset.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=9e853ed05b0a2ad40f9e4d0403e7004c 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/testing-tutorial-dataset.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=331654a31885b89a93924eaac4fa95da 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/testing-tutorial-dataset.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=833bf2a60b392323bba47fbe42655537 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/testing-tutorial-dataset.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=3410e4bc7ac5c28f8838fc5fb88026bd 2500w" />

## Define metrics

After creating our dataset, we can now define some metrics to evaluate our responses on. Since we have an expected answer, we can compare to that as part of our evaluation. However, we do not expect our application to output those **exact** answers, but rather something that is similar. This makes our evaluation a little trickier.

In addition to evaluating correctness, let's also make sure our answers are short and concise. This will be a little easier - we can define a simple Python function to measure the length of the response.

Let's go ahead and define these two metrics.

For the first, we will use an LLM to **judge** whether the output is correct (with respect to the expected output). This **LLM-as-a-judge** is relatively common for cases that are too complex to measure with a simple function. We can define our own prompt and LLM to use for evaluation here:
```

Example 2 (unknown):
```unknown
For evaluating the length of the response, this is a lot easier! We can just define a simple function that checks whether the actual output is less than 2x the length of the expected result.
```

Example 3 (unknown):
```unknown
## Run Evaluations

Great! So now how do we run evaluations? Now that we have a dataset and evaluators, all that we need is our application! We will build a simple application that just has a system message with instructions on how to respond and then passes it to the LLM. We will build this using the OpenAI SDK directly:
```

Example 4 (unknown):
```unknown
Before running this through LangSmith evaluations, we need to define a simple wrapper that maps the input keys from our dataset to the function we want to call, and then also maps the output of the function to the output key we expect.
```

---

## Define edges

**URL:** llms-txt#define-edges

**Contents:**
  - Impose a recursion limit
- Async
- Combine control flow and state updates with `Command`

def route(state: State) -> Literal["b", END]:
    if len(state["aggregate"]) < 7:
        return "b"
    else:
        return END

builder.add_edge(START, "a")
builder.add_conditional_edges("a", route)
builder.add_edge("b", "a")
graph = builder.compile()
python  theme={null}
from IPython.display import Image, display

display(Image(graph.get_graph().draw_mermaid_png()))
python  theme={null}
graph.invoke({"aggregate": []})

Node A sees []
Node B sees ['A']
Node A sees ['A', 'B']
Node B sees ['A', 'B', 'A']
Node A sees ['A', 'B', 'A', 'B']
Node B sees ['A', 'B', 'A', 'B', 'A']
Node A sees ['A', 'B', 'A', 'B', 'A', 'B']
python  theme={null}
from langgraph.errors import GraphRecursionError

try:
    graph.invoke({"aggregate": []}, {"recursion_limit": 4})
except GraphRecursionError:
    print("Recursion Error")

Node A sees []
Node B sees ['A']
Node C sees ['A', 'B']
Node D sees ['A', 'B']
Node A sees ['A', 'B', 'C', 'D']
Recursion Error
python  theme={null}
  import operator
  from typing import Annotated, Literal
  from typing_extensions import TypedDict
  from langgraph.graph import StateGraph, START, END
  from langgraph.managed.is_last_step import RemainingSteps

class State(TypedDict):
      aggregate: Annotated[list, operator.add]
      remaining_steps: RemainingSteps

def a(state: State):
      print(f'Node A sees {state["aggregate"]}')
      return {"aggregate": ["A"]}

def b(state: State):
      print(f'Node B sees {state["aggregate"]}')
      return {"aggregate": ["B"]}

# Define nodes
  builder = StateGraph(State)
  builder.add_node(a)
  builder.add_node(b)

# Define edges
  def route(state: State) -> Literal["b", END]:
      if state["remaining_steps"] <= 2:
          return END
      else:
          return "b"

builder.add_edge(START, "a")
  builder.add_conditional_edges("a", route)
  builder.add_edge("b", "a")
  graph = builder.compile()

# Test it out
  result = graph.invoke({"aggregate": []}, {"recursion_limit": 4})
  print(result)
  
  Node A sees []
  Node B sees ['A']
  Node A sees ['A', 'B']
  {'aggregate': ['A', 'B', 'A']}
  python  theme={null}
  import operator
  from typing import Annotated, Literal
  from typing_extensions import TypedDict
  from langgraph.graph import StateGraph, START, END

class State(TypedDict):
      aggregate: Annotated[list, operator.add]

def a(state: State):
      print(f'Node A sees {state["aggregate"]}')
      return {"aggregate": ["A"]}

def b(state: State):
      print(f'Node B sees {state["aggregate"]}')
      return {"aggregate": ["B"]}

def c(state: State):
      print(f'Node C sees {state["aggregate"]}')
      return {"aggregate": ["C"]}

def d(state: State):
      print(f'Node D sees {state["aggregate"]}')
      return {"aggregate": ["D"]}

# Define nodes
  builder = StateGraph(State)
  builder.add_node(a)
  builder.add_node(b)
  builder.add_node(c)
  builder.add_node(d)

# Define edges
  def route(state: State) -> Literal["b", END]:
      if len(state["aggregate"]) < 7:
          return "b"
      else:
          return END

builder.add_edge(START, "a")
  builder.add_conditional_edges("a", route)
  builder.add_edge("b", "c")
  builder.add_edge("b", "d")
  builder.add_edge(["c", "d"], "a")
  graph = builder.compile()
  python  theme={null}
  from IPython.display import Image, display

display(Image(graph.get_graph().draw_mermaid_png()))
  python  theme={null}
  result = graph.invoke({"aggregate": []})
  
  Node A sees []
  Node B sees ['A']
  Node D sees ['A', 'B']
  Node C sees ['A', 'B']
  Node A sees ['A', 'B', 'C', 'D']
  Node B sees ['A', 'B', 'C', 'D', 'A']
  Node D sees ['A', 'B', 'C', 'D', 'A', 'B']
  Node C sees ['A', 'B', 'C', 'D', 'A', 'B']
  Node A sees ['A', 'B', 'C', 'D', 'A', 'B', 'C', 'D']
  python  theme={null}
  from langgraph.errors import GraphRecursionError

try:
      result = graph.invoke({"aggregate": []}, {"recursion_limit": 4})
  except GraphRecursionError:
      print("Recursion Error")
  
  Node A sees []
  Node B sees ['A']
  Node C sees ['A', 'B']
  Node D sees ['A', 'B']
  Node A sees ['A', 'B', 'C', 'D']
  Recursion Error
  shell  theme={null}
    pip install -U "langchain[openai]"
    python init_chat_model theme={null}
      import os
      from langchain.chat_models import init_chat_model

os.environ["OPENAI_API_KEY"] = "sk-..."

model = init_chat_model("gpt-4.1")
      python Model Class theme={null}
      import os
      from langchain_openai import ChatOpenAI

os.environ["OPENAI_API_KEY"] = "sk-..."

model = ChatOpenAI(model="gpt-4.1")
      shell  theme={null}
    pip install -U "langchain[anthropic]"
    python init_chat_model theme={null}
      import os
      from langchain.chat_models import init_chat_model

os.environ["ANTHROPIC_API_KEY"] = "sk-..."

model = init_chat_model("claude-sonnet-4-5-20250929")
      python Model Class theme={null}
      import os
      from langchain_anthropic import ChatAnthropic

os.environ["ANTHROPIC_API_KEY"] = "sk-..."

model = ChatAnthropic(model="claude-sonnet-4-5-20250929")
      shell  theme={null}
    pip install -U "langchain[openai]"
    python init_chat_model theme={null}
      import os
      from langchain.chat_models import init_chat_model

os.environ["AZURE_OPENAI_API_KEY"] = "..."
      os.environ["AZURE_OPENAI_ENDPOINT"] = "..."
      os.environ["OPENAI_API_VERSION"] = "2025-03-01-preview"

model = init_chat_model(
          "azure_openai:gpt-4.1",
          azure_deployment=os.environ["AZURE_OPENAI_DEPLOYMENT_NAME"],
      )
      python Model Class theme={null}
      import os
      from langchain_openai import AzureChatOpenAI

os.environ["AZURE_OPENAI_API_KEY"] = "..."
      os.environ["AZURE_OPENAI_ENDPOINT"] = "..."
      os.environ["OPENAI_API_VERSION"] = "2025-03-01-preview"

model = AzureChatOpenAI(
          model="gpt-4.1",
          azure_deployment=os.environ["AZURE_OPENAI_DEPLOYMENT_NAME"]
      )
      shell  theme={null}
    pip install -U "langchain[google-genai]"
    python init_chat_model theme={null}
      import os
      from langchain.chat_models import init_chat_model

os.environ["GOOGLE_API_KEY"] = "..."

model = init_chat_model("google_genai:gemini-2.5-flash-lite")
      python Model Class theme={null}
      import os
      from langchain_google_genai import ChatGoogleGenerativeAI

os.environ["GOOGLE_API_KEY"] = "..."

model = ChatGoogleGenerativeAI(model="gemini-2.5-flash-lite")
      shell  theme={null}
    pip install -U "langchain[aws]"
    python init_chat_model theme={null}
      from langchain.chat_models import init_chat_model

# Follow the steps here to configure your credentials:
      # https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html

model = init_chat_model(
          "anthropic.claude-3-5-sonnet-20240620-v1:0",
          model_provider="bedrock_converse",
      )
      python Model Class theme={null}
      from langchain_aws import ChatBedrock

model = ChatBedrock(model="anthropic.claude-3-5-sonnet-20240620-v1:0")
      python  theme={null}
from langchain.chat_models import init_chat_model
from langgraph.graph import MessagesState, StateGraph

async def node(state: MessagesState):  # [!code highlight]
    new_message = await llm.ainvoke(state["messages"])  # [!code highlight]
    return {"messages": [new_message]}

builder = StateGraph(MessagesState).add_node(node).set_entry_point("node")
graph = builder.compile()

input_message = {"role": "user", "content": "Hello"}
result = await graph.ainvoke({"messages": [input_message]})  # [!code highlight]
python  theme={null}
def my_node(state: State) -> Command[Literal["my_other_node"]]:
    return Command(
        # state update
        update={"foo": "bar"},
        # control flow
        goto="my_other_node"
    )
python  theme={null}
import random
from typing_extensions import TypedDict, Literal
from langgraph.graph import StateGraph, START
from langgraph.types import Command

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
<img src="https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/graph_api_image_7.png?fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=e1b99e7efe45b1fdc5836d590d5fbbc3" alt="Simple loop graph" data-og-width="188" width="188" data-og-height="249" height="249" data-path="oss/images/graph_api_image_7.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/graph_api_image_7.png?w=280&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=a443c1ddc2f6a4e7c73f4482c7d63912 280w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/graph_api_image_7.png?w=560&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=f65d82d8aaeb024beb5da1aa2948bcdb 560w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/graph_api_image_7.png?w=840&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=b95f4df2fb69f28779a1d8dd113409d0 840w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/graph_api_image_7.png?w=1100&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=bdb4011d05756c10a1c7b5dea683fdb7 1100w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/graph_api_image_7.png?w=1650&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=dde791caa4279a6248b59b70df99dd2c 1650w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/graph_api_image_7.png?w=2500&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=e4d568719f1761ff3a3d2ea9175241d8 2500w" />

This architecture is similar to a [ReAct agent](/oss/python/langgraph/workflows-agents) in which node `"a"` is a tool-calling model, and node `"b"` represents the tools.

In our `route` conditional edge, we specify that we should end after the `"aggregate"` list in the state passes a threshold length.

Invoking the graph, we see that we alternate between nodes `"a"` and `"b"` before terminating once we reach the termination condition.
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
### Impose a recursion limit

In some applications, we may not have a guarantee that we will reach a given termination condition. In these cases, we can set the graph's [recursion limit](/oss/python/langgraph/graph-api#recursion-limit). This will raise a `GraphRecursionError` after a given number of [supersteps](/oss/python/langgraph/graph-api#graphs). We can then catch and handle this exception:
```

---

## Define graph state

**URL:** llms-txt#define-graph-state

class State(TypedDict):
    foo: str

---

## Define input schema

**URL:** llms-txt#define-input-schema

class InputState(TypedDict):
    question: str

---

## Define nodes

**URL:** llms-txt#define-nodes

builder = StateGraph(State)
builder.add_node(a)
builder.add_node(b)

---

## Define other parameters

**URL:** llms-txt#define-other-parameters

val = 42
text = "Hello, world!"

---

## Define output schema

**URL:** llms-txt#define-output-schema

class OutputState(TypedDict):
    answer: str

---

## Define regex patterns for various PII

**URL:** llms-txt#define-regex-patterns-for-various-pii

SSN_PATTERN = re.compile(r'\b\d{3}-\d{2}-\d{4}\b')
CREDIT_CARD_PATTERN = re.compile(r'\b(?:\d[ -]*?){13,16}\b')
EMAIL_PATTERN = re.compile(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,7}\b')
PHONE_PATTERN = re.compile(r'\b(?:\+?1[-.\s]?)?\(?\d{3}\)?[-.\s]?\d{3}[-.\s]?\d{4}\b')
FULL_NAME_PATTERN = re.compile(r'\b([A-Z][a-z]*\s[A-Z][a-z]*)\b')

def regex_anonymize(text):
    """
    Anonymize sensitive information in the text using regex patterns.
    Args:
        text (str): The input text to be anonymized.
    Returns:
        str: The anonymized text.
    """
    # Replace sensitive information with placeholders
    text = SSN_PATTERN.sub('[REDACTED SSN]', text)
    text = CREDIT_CARD_PATTERN.sub('[REDACTED CREDIT CARD]', text)
    text = EMAIL_PATTERN.sub('[REDACTED EMAIL]', text)
    text = PHONE_PATTERN.sub('[REDACTED PHONE]', text)
    text = FULL_NAME_PATTERN.sub('[REDACTED NAME]', text)
    return text

def recursive_anonymize(data, depth=10):
    """
    Recursively traverse the data structure and anonymize sensitive information.
    Args:
        data (any): The input data to be anonymized.
        depth (int): The current recursion depth to prevent excessive recursion.
    Returns:
        any: The anonymized data.
    """
    if depth == 0:
        return data
    if isinstance(data, dict):
        anonymized_dict = {}
        for k, v in data.items():
            anonymized_value = recursive_anonymize(v, depth - 1)
            anonymized_dict[k] = anonymized_value
        return anonymized_dict
    elif isinstance(data, list):
        anonymized_list = []
        for item in data:
            anonymized_item = recursive_anonymize(item, depth - 1)
            anonymized_list.append(anonymized_item)
        return anonymized_list
    elif isinstance(data, str):
        anonymized_data = regex_anonymize(data)
        return anonymized_data
    else:
        return data

openai_client = wrap_openai(openai.Client())

---

## Define target function that uses attachments

**URL:** llms-txt#define-target-function-that-uses-attachments

**Contents:**
  - Define custom evaluators
- Update examples with attachments
- UI
  - 1. Create examples with attachments
  - 2. Create a multimodal prompt
  - Define custom evaluators
  - Update examples with attachments

def file_qa(inputs, attachments):
    # Read the audio bytes from the reader and encode them in base64
    audio_reader = attachments["my_wav"]["reader"]
    audio_b64 = base64.b64encode(audio_reader.read()).decode('utf-8')

audio_completion = client.chat.completions.create(
        model="gpt-4o-audio-preview",
        messages=[
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": inputs["audio_question"]
                    },
                    {
                        "type": "input_audio",
                        "input_audio": {
                            "data": audio_b64,
                            "format": "wav"
                        }
                    }
                ]
            }
        ]
    )

# Most models support taking in an image URL directly in addition to base64 encoded images
    # You can pipe the image pre-signed URL directly to the model
    image_url = attachments["my_img"]["presigned_url"]
    image_completion = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
          {
            "role": "user",
            "content": [
              {"type": "text", "text": inputs["image_question"]},
              {
                "type": "image_url",
                "image_url": {
                  "url": image_url,
                },
              },
            ],
          }
        ],
    )

return {
        "audio_answer": audio_completion.choices[0].message.content,
        "image_answer": image_completion.choices[0].message.content,
    }
typescript  theme={null}
{
  presigned_url: string,
  mime_type: string,
}
typescript  theme={null}
import OpenAI from "openai";
import { wrapOpenAI } from "langsmith/wrappers";

const client: any = wrapOpenAI(new OpenAI());

async function fileQA(inputs: Record<string, any>, config?: Record<string, any>) {
  const presignedUrl = config?.attachments?.["my_wav"]?.presigned_url;
  if (!presignedUrl) {
    throw new Error("No presigned URL provided for audio.");
  }

const response = await fetch(presignedUrl);
  if (!response.ok) {
    throw new Error(`Failed to fetch audio: ${response.statusText}`);
  }

const arrayBuffer = await response.arrayBuffer();
  const uint8Array = new Uint8Array(arrayBuffer);
  const audioB64 = Buffer.from(uint8Array).toString("base64");

const audioCompletion = await client.chat.completions.create({
    model: "gpt-4o-audio-preview",
    messages: [
      {
        role: "user",
        content: [
          { type: "text", text: inputs["audio_question"] },
          {
            type: "input_audio",
            input_audio: {
              data: audioB64,
              format: "wav",
            },
          },
        ],
      },
    ],
  });

const imageUrl = config?.attachments?.["my_img"]?.presigned_url
  const imageCompletion = await client.chat.completions.create({
    model: "gpt-4o-mini",
    messages: [
      {
        role: "user",
        content: [
          { type: "text", text: inputs["image_question"] },
          {
            type: "image_url",
            image_url: {
              url: imageUrl,
            },
          },
        ],
      },
    ],
  });

return {
    audio_answer: audioCompletion.choices[0].message.content,
    image_answer: imageCompletion.choices[0].message.content,
  };
}
python Python theme={null}
  # Assumes you've installed pydantic
  from pydantic import BaseModel

def valid_image_description(outputs: dict, attachments: dict) -> bool:
    """Use an LLM to judge if the image description and images are consistent."""
    instructions = """
    Does the description of the following image make sense?
    Please carefully review the image and the description to determine if the description is valid.
    """

class Response(BaseModel):
        description_is_valid: bool

image_url = attachments["my_img"]["presigned_url"]
    response = client.beta.chat.completions.parse(
        model="gpt-4o",
        messages=[
            {
                "role": "system",
                "content": instructions
            },
            {
                "role": "user",
                "content": [
                    {"type": "image_url", "image_url": {"url": image_url}},
                    {"type": "text", "text": outputs["image_answer"]}
                ]
            }
        ],
        response_format=Response
    )
    return response.choices[0].message.parsed.description_is_valid

ls_client.evaluate(
    file_qa,
    data=dataset_name,
    evaluators=[valid_image_description],
  )
  typescript TypeScript theme={null}
  import { zodResponseFormat } from 'openai/helpers/zod';
  import { z } from 'zod';
  import { evaluate } from "langsmith/evaluation";

const DescriptionResponse = z.object({
    description_is_valid: z.boolean(),
  });

async function validImageDescription({
    outputs,
    attachments,
  }: {
    outputs?: any;
    attachments?: any;
  }): Promise<{ key: string; score: boolean}> {
    const instructions = `Does the description of the following image make sense?
  Please carefully review the image and the description to determine if the description is valid.`;

const imageUrl = attachments?.["my_img"]?.presigned_url
    const completion = await client.beta.chat.completions.parse({
        model: "gpt-4o",
        messages: [
            {
                role: "system",
                content: instructions,
            },
            {
                role: "user",
                content: [
                    { type: "image_url", image_url: { url: imageUrl } },
                    { type: "text", text: outputs?.image_answer },
                ],
            },
        ],
        response_format: zodResponseFormat(DescriptionResponse, 'imageResponse'),
    });

const score: boolean = completion.choices[0]?.message?.parsed?.description_is_valid ?? false;
    return { key: "valid_image_description", score };
  }

const resp = await evaluate(fileQA, {
    data: datasetName,
    // Need to pass flag to include attachments
    includeAttachments: true,
    evaluators: [validImageDescription],
    client: langsmithClient
  });
  python Python theme={null}
  example_update = {
    "id": example_id,
    "attachments": {
        # These are net new attachments
        "my_new_file": ("text/plain", b"foo bar"),
    },
    "inputs": inputs,
    "outputs": outputs,
    # Any attachments not in rename/retain will be deleted.
    # In this case, that would be "my_img" if we uploaded it.
    "attachments_operations": {
        # Retained attachments will stay exactly the same
        "retain": ["my_pdf"],
        # Renaming attachments preserves the original data
        "rename": {
            "my_wav": "my_new_wav",
        }
    },
  }

ls_client.update_examples(dataset_id=dataset.id, updates=[example_update])
  typescript TypeScript theme={null}
  import { ExampleUpdateWithAttachments } from "langsmith/schemas";

const exampleUpdate: ExampleUpdateWithAttachments = {
    id: exampleId,
    attachments: {
      // These are net new attachments
      "my_new_file": {
        mimeType: "text/plain",
        data: Buffer.from("foo bar")
      },
    },
    attachments_operations: {
      // Retained attachments will stay exactly the same
      retain: ["my_img"],
      // Renaming attachments preserves the original data
      rename: {
        "my_wav": "my_new_wav",
      },
      // Any attachments not in rename/retain will be deleted
      // In this case, that would be "my_pdf"
    },
  };

await langsmithClient.updateExamplesMultipart(dataset.id, [exampleUpdate]);
  ```
</CodeGroup>

### 1. Create examples with attachments

You can add examples with attachments to a dataset in a few different ways.

#### From existing runs

When adding runs to a LangSmith dataset, attachments can be selectively propagated from the source run to the destination example. To learn more, please see [this guide](/langsmith/manage-datasets-in-application#add-runs-from-the-tracing-project-ui).

<img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-trace-with-attachments-to-dataset.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=b8fa62cb39c4f1fc67d9b24fa78d1653" alt="Add trace with attachments to dataset" data-og-width="1662" width="1662" data-og-height="679" height="679" data-path="langsmith/images/add-trace-with-attachments-to-dataset.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-trace-with-attachments-to-dataset.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=72ee339359616ed8f03f4ddbfe86bc23 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-trace-with-attachments-to-dataset.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=cb6f558f8a0391588583a7b5d520a27f 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-trace-with-attachments-to-dataset.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=bc51ed2e68af972e488051fc1ae01caf 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-trace-with-attachments-to-dataset.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=d58d097b70ad0f6b7a327058c659d8d9 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-trace-with-attachments-to-dataset.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=cbac645e6534290036d24963c231a878 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-trace-with-attachments-to-dataset.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=b4ff5c59dd6c2b23d97bfd5e34206a50 2500w" />

You can create examples with attachments directly from the LangSmith UI. Click the `+ Example` button in the `Examples` tab of the dataset UI. Then upload attachments using the "Upload Files" button:

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-example-with-attachments.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=183f929c807f59157e93d40354057933" alt="Create example with attachments" data-og-width="3456" width="3456" data-og-height="1856" height="1856" data-path="langsmith/images/create-example-with-attachments.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-example-with-attachments.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=c80316a80e8359d14aa42aac6767b677 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-example-with-attachments.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=1921a813aea4e6fa62bcf7cfd169662e 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-example-with-attachments.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=35d6882ec8757e56d81eba2033220cf7 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-example-with-attachments.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=0b93d237b11ab6bf8693b287ee19dc4b 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-example-with-attachments.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=38384c5a8166b41535657ec8a6337b49 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-example-with-attachments.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=b0f8622286209391451895c2e4c7b03b 2500w" />

Once uploaded, you can view examples with attachments in the LangSmith UI. Each attachment will be rendered with a preview for easy inspection. <img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/attachments-with-examples.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=8f813bdf7d3bfd5a840e5f8c47693ed3" alt="Attachments with examples" data-og-width="1331" width="1331" data-og-height="593" height="593" data-path="langsmith/images/attachments-with-examples.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/attachments-with-examples.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=0ecc6a1a59da5972731c4f36fc0154d8 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/attachments-with-examples.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=f7985b2778973b594e8522964eb13770 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/attachments-with-examples.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=f062086524f7aff66c094ac0e259c04e 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/attachments-with-examples.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=970e61cff6a2cfc749c14481afe2a3f9 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/attachments-with-examples.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=4c34be3eb64547daf4879b2f2dab3ec4 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/attachments-with-examples.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=700119fd1a262ce0f85d924b684141a4 2500w" />

### 2. Create a multimodal prompt

The LangSmith UI allows you to include attachments in your prompts when evaluating multimodal models:

First, click the file icon in the message where you want to add multimodal content. Next, add a template variable for the attachment(s) you want to include for each example.

* For a single attachment type: Use the suggested variable name. Note: all examples must have an attachment with this name.
* For multiple attachments or if your attachments have varying names from one example to another: Use the `All attachments` variable to include all available attachments for each example.

<img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/adding-multimodal-variable.gif?s=07a15c9fc5e6fc743f92b6b41ab8c9e0" alt="Adding multimodal variable" data-og-width="1700" width="1700" data-og-height="1080" height="1080" data-path="langsmith/images/adding-multimodal-variable.gif" data-optimize="true" data-opv="3" />

### Define custom evaluators

<Note>
  The LangSmith playground does not currently support pulling multimodal content into evaluators. If this would be helpful for your use case, please let us know in the [LangChain Forum](https://forum.langchain.com/) (sign up [here](https://www.langchain.com/join-community) if you're not already a member)!
</Note>

You can evaluate a model's text output by adding an evaluator that takes in the example's inputs and outputs. Even without multimodal support in your evaluators, you can still run text-only evaluations. For example:

* OCR → text correction: Use a vision model to extract text from a document, then evaluate the accuracy of the extracted output.
* Speech-to-text → transcription quality: Use a voice model to transcribe audio to text, then evaluate the transcription against your reference.

For more information on defining custom evaluators, see the [LLM as Judge](/langsmith/llm-as-judge) guide.

### Update examples with attachments

<Note>
  Attachments are limited to 20MB in size in the UI.
</Note>

When editing an example in the UI, you can:

* Upload new attachments
* Rename and delete attachments
* Reset attachments to their previous state using the quick reset button

Changes are not saved until you click submit.

<img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/attachment-editing.gif?s=4f165ed98fe81722961778ebbe1691ed" alt="Attachment editing" data-og-width="1204" width="1204" data-og-height="720" height="720" data-path="langsmith/images/attachment-editing.gif" data-optimize="true" data-opv="3" />

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/evaluate-with-attachments.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
#### TypeScript

In the TypeScript SDK, the `config` argument is used to pass in the attachments to the target function if `includeAttachments` is set to `true`.

The `config` will contain `attachments` which is an object mapping the attachment name to an object of the form:
```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
### Define custom evaluators

The exact same rules apply as above to determine whether the evaluator should receive attachments.

The evaluator below uses an LLM to judge if the reasoning and the answer are consistent. To learn more about how to define llm-based evaluators, please see [this guide](/langsmith/llm-as-judge).

<CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Define the function that calls the model

**URL:** llms-txt#define-the-function-that-calls-the-model

def call_model(state: State):
    messages = state['messages']
    response = model.invoke(messages)

# We return a list, because this will get added to the existing list
    return {"messages": [response]}

---

## Define the function that determines whether to continue or not

**URL:** llms-txt#define-the-function-that-determines-whether-to-continue-or-not

def should_continue(state: State) -> Literal["tools", END]:
    messages = state['messages']
    last_message = messages[-1]

# If the LLM makes a tool call, then we route to the "tools" node
    if last_message.tool_calls:
        return "tools"

# Otherwise, we stop (reply to the user)
    return END

---

## Define the graph

**URL:** llms-txt#define-the-graph

**Contents:**
- Agent-to-agent communication

graph = (
    StateGraph(State, context_schema=Context)
    .add_node(call_model)
    .add_edge("__start__", "call_model")
    .compile()
)
python  theme={null}
#!/usr/bin/env python3
"""Agent-to-Agent conversation simulation using LangGraph A2A protocol."""

import asyncio
import aiohttp
import os

async def send_message(session, port, assistant_id, text):
    """Send a message to an agent and return the response text."""
    url = f"http://127.0.0.1:{port}/a2a/{assistant_id}"
    payload = {
        "jsonrpc": "2.0",
        "id": "",
        "method": "message/send",
        "params": {
            "message": {
                "role": "user",
                "parts": [{"kind": "text", "text": text}]
            },
            "messageId": "",
            "thread": {"threadId": ""}
        }
    }

headers = {"Accept": "application/json"}
    async with session.post(url, json=payload, headers=headers) as response:
        try:
            result = await response.json()
            return result["result"]["artifacts"][0]["parts"][0]["text"]
        except Exception as e:
            text = await response.text()
            print(f"Response error from port {port}: {response.status} - {text}")
            return f"Error from port {port}: {response.status}"

async def simulate_conversation():
    """Simulate a conversation between two agents."""
    agent_a_id = os.getenv("AGENT_A_ID")
    agent_b_id = os.getenv("AGENT_B_ID")

if not agent_a_id or not agent_b_id:
        print("Set AGENT_A_ID and AGENT_B_ID environment variables")
        return

message = "Hello! Let's have a conversation."

async with aiohttp.ClientSession() as session:
        for i in range(3):
            print(f"--- Round {i + 1} ---")

# Agent A responds
            message = await send_message(session, 2024, agent_a_id, message)
            print(f"🔵 Agent A: {message}")

# Agent B responds
            message = await send_message(session, 2025, agent_b_id, message)
            print(f"🔴 Agent B: {message}")
            print()

if __name__ == "__main__":
    asyncio.run(simulate_conversation())
```

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/server-a2a.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
## Agent-to-agent communication

Once your agents are running locally via `langgraph dev` or [deployed to production](/langsmith/deployments), you can facilitate communication between them using the A2A protocol.

This example demonstrates how two agents can communicate by sending JSON-RPC messages to each other's A2A endpoints. The script simulates a multi-turn conversation where each agent processes the other's response and continues the dialogue.
```

---

## Define the nodes

**URL:** llms-txt#define-the-nodes

def node_a(state: State) -> Command[Literal["node_b", "node_c"]]:
    print("Called A")
    value = random.choice(["b", "c"])
    # this is a replacement for a conditional edge function
    if value == "b":
        goto = "node_b"
    else:
        goto = "node_c"

# note how Command allows you to BOTH update the graph state AND route to the next node
    return Command(
        # this is the state update
        update={"foo": value},
        # this is a replacement for an edge
        goto=goto,
    )

def node_b(state: State):
    print("Called B")
    return {"foo": state["foo"] + "b"}

def node_c(state: State):
    print("Called C")
    return {"foo": state["foo"] + "c"}
python  theme={null}
builder = StateGraph(State)
builder.add_edge(START, "node_a")
builder.add_node(node_a)
builder.add_node(node_b)
builder.add_node(node_c)

**Examples:**

Example 1 (unknown):
```unknown
We can now create the [`StateGraph`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.StateGraph) with the above nodes. Notice that the graph doesn't have [conditional edges](/oss/python/langgraph/graph-api#conditional-edges) for routing! This is because control flow is defined with [`Command`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.Command) inside `node_a`.
```

---

## Define the nodes we will cycle between

**URL:** llms-txt#define-the-nodes-we-will-cycle-between

workflow.add_node(generate_query_or_respond)
workflow.add_node("retrieve", ToolNode([retriever_tool]))
workflow.add_node(rewrite_question)
workflow.add_node(generate_answer)

workflow.add_edge(START, "generate_query_or_respond")

---

## Define the node that processes the input and generates an answer

**URL:** llms-txt#define-the-node-that-processes-the-input-and-generates-an-answer

def answer_node(state: InputState):
    # Example answer and an extra key
    return {"answer": "bye", "question": state["question"]}

---

## Define the overall schema, combining both input and output

**URL:** llms-txt#define-the-overall-schema,-combining-both-input-and-output

class OverallState(InputState, OutputState):
    pass

---

## Define the processing node

**URL:** llms-txt#define-the-processing-node

def answer_node(state: InputState):
    # Replace with actual logic and do something useful
    return {"answer": "bye", "question": state["question"]}

---

## Define the runtime context

**URL:** llms-txt#define-the-runtime-context

**Contents:**
- Create the configuration file
- Next

class GraphContext(TypedDict):
    model_name: Literal["anthropic", "openai"]

workflow = StateGraph(AgentState, context_schema=GraphContext)
workflow.add_node("agent", call_model)
workflow.add_node("action", tool_node)
workflow.add_edge(START, "agent")
workflow.add_conditional_edges(
    "agent",
    should_continue,
    {
        "continue": "action",
        "end": END,
    },
)
workflow.add_edge("action", "agent")

graph = workflow.compile()
bash  theme={null}
my-app/
├── my_agent # all project code lies within here
│   ├── utils # utilities for your graph
│   │   ├── __init__.py
│   │   ├── tools.py # tools for your graph
│   │   ├── nodes.py # node functions for your graph
│   │   └── state.py # state definition of your graph
│   ├── __init__.py
│   └── agent.py # code for constructing your graph
├── .env
└── pyproject.toml
json  theme={null}
{
  "dependencies": ["."],
  "graphs": {
    "agent": "./my_agent/agent.py:graph"
  },
  "env": ".env"
}
bash  theme={null}
my-app/
├── my_agent # all project code lies within here
│   ├── utils # utilities for your graph
│   │   ├── __init__.py
│   │   ├── tools.py # tools for your graph
│   │   ├── nodes.py # node functions for your graph
│   │   └── state.py # state definition of your graph
│   ├── __init__.py
│   └── agent.py # code for constructing your graph
├── .env # environment variables
├── langgraph.json  # configuration file for LangGraph
└── pyproject.toml # dependencies for your project
```

After you setup your project and place it in a GitHub repository, it's time to [deploy your app](/langsmith/deployment-quickstart).

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/setup-pyproject.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
Example file directory:
```

Example 2 (unknown):
```unknown
## Create the configuration file

Create a [configuration file](/langsmith/cli#configuration-file) called `langgraph.json`. See the [configuration file reference](/langsmith/cli#configuration-file) for detailed explanations of each key in the JSON object of the configuration file.

Example `langgraph.json` file:
```

Example 3 (unknown):
```unknown
Note that the variable name of the `CompiledGraph` appears at the end of the value of each subkey in the top-level `graphs` key (i.e. `:<variable_name>`).

<Warning>
  **Configuration file location**
  The configuration file must be placed in a directory that is at the same level or higher than the Python files that contain compiled graphs and associated dependencies.
</Warning>

Example file directory:
```

---

## Define the schema for the input

**URL:** llms-txt#define-the-schema-for-the-input

class InputState(TypedDict):
    question: str

---

## Define the schema for the output

**URL:** llms-txt#define-the-schema-for-the-output

class OutputState(TypedDict):
    answer: str

---

## Define the structure for email classification

**URL:** llms-txt#define-the-structure-for-email-classification

**Contents:**
- Step 4: Build your nodes
  - Handle errors appropriately
  - Implementing our email agent nodes
- Step 5: Wire it together
  - Try out your agent
- Summary and next steps
  - Key Insights
  - Advanced considerations
  - Where to go from here

class EmailClassification(TypedDict):
    intent: Literal["question", "bug", "billing", "feature", "complex"]
    urgency: Literal["low", "medium", "high", "critical"]
    topic: str
    summary: str

class EmailAgentState(TypedDict):
    # Raw email data
    email_content: str
    sender_email: str
    email_id: str

# Classification result
    classification: EmailClassification | None

# Raw search/API results
    search_results: list[str] | None  # List of raw document chunks
    customer_history: dict | None  # Raw customer data from CRM

# Generated content
    draft_response: str | None
    messages: list[str] | None
python  theme={null}
    from langgraph.types import RetryPolicy

workflow.add_node(
        "search_documentation",
        search_documentation,
        retry_policy=RetryPolicy(max_attempts=3, initial_interval=1.0)
    )
    python  theme={null}
    from langgraph.types import Command

def execute_tool(state: State) -> Command[Literal["agent", "execute_tool"]]:
        try:
            result = run_tool(state['tool_call'])
            return Command(update={"tool_result": result}, goto="agent")
        except ToolError as e:
            # Let the LLM see what went wrong and try again
            return Command(
                update={"tool_result": f"Tool error: {str(e)}"},
                goto="agent"
            )
    python  theme={null}
    from langgraph.types import Command

def lookup_customer_history(state: State) -> Command[Literal["draft_response"]]:
        if not state.get('customer_id'):
            user_input = interrupt({
                "message": "Customer ID needed",
                "request": "Please provide the customer's account ID to look up their subscription history"
            })
            return Command(
                update={"customer_id": user_input['customer_id']},
                goto="lookup_customer_history"
            )
        # Now proceed with the lookup
        customer_data = fetch_customer_history(state['customer_id'])
        return Command(update={"customer_history": customer_data}, goto="draft_response")
    python  theme={null}
    def send_reply(state: EmailAgentState):
        try:
            email_service.send(state["draft_response"])
        except Exception:
            raise  # Surface unexpected errors
    python  theme={null}
    from typing import Literal
    from langgraph.graph import StateGraph, START, END
    from langgraph.types import interrupt, Command, RetryPolicy
    from langchain_openai import ChatOpenAI
    from langchain_core.messages import HumanMessage

llm = ChatOpenAI(model="gpt-4")

def read_email(state: EmailAgentState) -> dict:
        """Extract and parse email content"""
        # In production, this would connect to your email service
        return {
            "messages": [HumanMessage(content=f"Processing email: {state['email_content']}")]
        }

def classify_intent(state: EmailAgentState) -> Command[Literal["search_documentation", "human_review", "draft_response", "bug_tracking"]]:
        """Use LLM to classify email intent and urgency, then route accordingly"""

# Create structured LLM that returns EmailClassification dict
        structured_llm = llm.with_structured_output(EmailClassification)

# Format the prompt on-demand, not stored in state
        classification_prompt = f"""
        Analyze this customer email and classify it:

Email: {state['email_content']}
        From: {state['sender_email']}

Provide classification including intent, urgency, topic, and summary.
        """

# Get structured response directly as dict
        classification = structured_llm.invoke(classification_prompt)

# Determine next node based on classification
        if classification['intent'] == 'billing' or classification['urgency'] == 'critical':
            goto = "human_review"
        elif classification['intent'] in ['question', 'feature']:
            goto = "search_documentation"
        elif classification['intent'] == 'bug':
            goto = "bug_tracking"
        else:
            goto = "draft_response"

# Store classification as a single dict in state
        return Command(
            update={"classification": classification},
            goto=goto
        )
    python  theme={null}
    def search_documentation(state: EmailAgentState) -> Command[Literal["draft_response"]]:
        """Search knowledge base for relevant information"""

# Build search query from classification
        classification = state.get('classification', {})
        query = f"{classification.get('intent', '')} {classification.get('topic', '')}"

try:
            # Implement your search logic here
            # Store raw search results, not formatted text
            search_results = [
                "Reset password via Settings > Security > Change Password",
                "Password must be at least 12 characters",
                "Include uppercase, lowercase, numbers, and symbols"
            ]
        except SearchAPIError as e:
            # For recoverable search errors, store error and continue
            search_results = [f"Search temporarily unavailable: {str(e)}"]

return Command(
            update={"search_results": search_results},  # Store raw results or error
            goto="draft_response"
        )

def bug_tracking(state: EmailAgentState) -> Command[Literal["draft_response"]]:
        """Create or update bug tracking ticket"""

# Create ticket in your bug tracking system
        ticket_id = "BUG-12345"  # Would be created via API

return Command(
            update={
                "search_results": [f"Bug ticket {ticket_id} created"],
                "current_step": "bug_tracked"
            },
            goto="draft_response"
        )
    python  theme={null}
    def draft_response(state: EmailAgentState) -> Command[Literal["human_review", "send_reply"]]:
        """Generate response using context and route based on quality"""

classification = state.get('classification', {})

# Format context from raw state data on-demand
        context_sections = []

if state.get('search_results'):
            # Format search results for the prompt
            formatted_docs = "\n".join([f"- {doc}" for doc in state['search_results']])
            context_sections.append(f"Relevant documentation:\n{formatted_docs}")

if state.get('customer_history'):
            # Format customer data for the prompt
            context_sections.append(f"Customer tier: {state['customer_history'].get('tier', 'standard')}")

# Build the prompt with formatted context
        draft_prompt = f"""
        Draft a response to this customer email:
        {state['email_content']}

Email intent: {classification.get('intent', 'unknown')}
        Urgency level: {classification.get('urgency', 'medium')}

{chr(10).join(context_sections)}

Guidelines:
        - Be professional and helpful
        - Address their specific concern
        - Use the provided documentation when relevant
        """

response = llm.invoke(draft_prompt)

# Determine if human review needed based on urgency and intent
        needs_review = (
            classification.get('urgency') in ['high', 'critical'] or
            classification.get('intent') == 'complex'
        )

# Route to appropriate next node
        goto = "human_review" if needs_review else "send_reply"

return Command(
            update={"draft_response": response.content},  # Store only the raw response
            goto=goto
        )

def human_review(state: EmailAgentState) -> Command[Literal["send_reply", END]]:
        """Pause for human review using interrupt and route based on decision"""

classification = state.get('classification', {})

# interrupt() must come first - any code before it will re-run on resume
        human_decision = interrupt({
            "email_id": state.get('email_id',''),
            "original_email": state.get('email_content',''),
            "draft_response": state.get('draft_response',''),
            "urgency": classification.get('urgency'),
            "intent": classification.get('intent'),
            "action": "Please review and approve/edit this response"
        })

# Now process the human's decision
        if human_decision.get("approved"):
            return Command(
                update={"draft_response": human_decision.get("edited_response", state.get('draft_response',''))},
                goto="send_reply"
            )
        else:
            # Rejection means human will handle directly
            return Command(update={}, goto=END)

def send_reply(state: EmailAgentState) -> dict:
        """Send the email response"""
        # Integrate with email service
        print(f"Sending reply: {state['draft_response'][:100]}...")
        return {}
    python  theme={null}
  from langgraph.checkpoint.memory import MemorySaver
  from langgraph.types import RetryPolicy

# Create the graph
  workflow = StateGraph(EmailAgentState)

# Add nodes with appropriate error handling
  workflow.add_node("read_email", read_email)
  workflow.add_node("classify_intent", classify_intent)

# Add retry policy for nodes that might have transient failures
  workflow.add_node(
      "search_documentation",
      search_documentation,
      retry_policy=RetryPolicy(max_attempts=3)
  )
  workflow.add_node("bug_tracking", bug_tracking)
  workflow.add_node("draft_response", draft_response)
  workflow.add_node("human_review", human_review)
  workflow.add_node("send_reply", send_reply)

# Add only the essential edges
  workflow.add_edge(START, "read_email")
  workflow.add_edge("read_email", "classify_intent")
  workflow.add_edge("send_reply", END)

# Compile with checkpointer for persistence, in case run graph with Local_Server --> Please compile without checkpointer
  memory = MemorySaver()
  app = workflow.compile(checkpointer=memory)
  python  theme={null}
  # Test with an urgent billing issue
  initial_state = {
      "email_content": "I was charged twice for my subscription! This is urgent!",
      "sender_email": "customer@example.com",
      "email_id": "email_123",
      "messages": []
  }

# Run with a thread_id for persistence
  config = {"configurable": {"thread_id": "customer_123"}}
  result = app.invoke(initial_state, config)
  # The graph will pause at human_review
  print(f"Draft ready for review: {result['draft_response'][:100]}...")

# When ready, provide human input to resume
  from langgraph.types import Command

human_response = Command(
      resume={
          "approved": True,
          "edited_response": "We sincerely apologize for the double charge. I've initiated an immediate refund..."
      }
  )

# Resume execution
  final_result = app.invoke(human_response, config)
  print(f"Email sent successfully!")
  ```
</Accordion>

The graph pauses when it hits `interrupt()`, saves everything to the checkpointer, and waits. It can resume days later, picking up exactly where it left off. The `thread_id` ensures all state for this conversation is preserved together.

## Summary and next steps

Building this email agent has shown us the LangGraph way of thinking:

<CardGroup cols={2}>
  <Card title="Break into discrete steps" icon="sitemap" href="#step-1-map-out-your-workflow-as-discrete-steps">
    Each node does one thing well. This decomposition enables streaming progress updates, durable execution that can pause and resume, and clear debugging since you can inspect state between steps.
  </Card>

<Card title="State is shared memory" icon="database" href="#step-3-design-your-state">
    Store raw data, not formatted text. This lets different nodes use the same information in different ways.
  </Card>

<Card title="Nodes are functions" icon="code" href="#step-4-build-your-nodes">
    They take state, do work, and return updates. When they need to make routing decisions, they specify both the state updates and the next destination.
  </Card>

<Card title="Errors are part of the flow" icon="triangle-exclamation" href="#handle-errors-appropriately">
    Transient failures get retries, LLM-recoverable errors loop back with context, user-fixable problems pause for input, and unexpected errors bubble up for debugging.
  </Card>

<Card title="Human input is first-class" icon="user" href="/oss/python/langgraph/interrupts">
    The `interrupt()` function pauses execution indefinitely, saves all state, and resumes exactly where it left off when you provide input. When combined with other operations in a node, it must come first.
  </Card>

<Card title="Graph structure emerges naturally" icon="diagram-project" href="#step-5-wire-it-together">
    You define the essential connections, and your nodes handle their own routing logic. This keeps control flow explicit and traceable - you can always understand what your agent will do next by looking at the current node.
  </Card>
</CardGroup>

### Advanced considerations

<Accordion title="Node granularity trade-offs" icon="sliders">
  <Info>
    This section explores the trade-offs in node granularity design. Most applications can skip this and use the patterns shown above.
  </Info>

You might wonder: why not combine `Read Email` and `Classify Intent` into one node?

Or why separate Doc Search from Draft Reply?

The answer involves trade-offs between resilience and observability.

**The resilience consideration:** LangGraph's [durable execution](/oss/python/langgraph/durable-execution) creates checkpoints at node boundaries. When a workflow resumes after an interruption or failure, it starts from the beginning of the node where execution stopped. Smaller nodes mean more frequent checkpoints, which means less work to repeat if something goes wrong. If you combine multiple operations into one large node, a failure near the end means re-executing everything from the start of that node.

Why we chose this breakdown for the email agent:

* **Isolation of external services:** Doc Search and Bug Track are separate nodes because they call external APIs. If the search service is slow or fails, we want to isolate that from the LLM calls. We can add retry policies to these specific nodes without affecting others.

* **Intermediate visibility:** Having `Classify Intent` as its own node lets us inspect what the LLM decided before taking action. This is valuable for debugging and monitoring—you can see exactly when and why the agent routes to human review.

* **Different failure modes:** LLM calls, database lookups, and email sending have different retry strategies. Separate nodes let you configure these independently.

* **Reusability and testing:** Smaller nodes are easier to test in isolation and reuse in other workflows.

A different valid approach: You could combine `Read Email` and `Classify Intent` into a single node. You'd lose the ability to inspect the raw email before classification and would repeat both operations on any failure in that node. For most applications, the observability and debugging benefits of separate nodes are worth the trade-off.

Application-level concerns: The caching discussion in Step 2 (whether to cache search results) is an application-level decision, not a LangGraph framework feature. You implement caching within your node functions based on your specific requirements—LangGraph doesn't prescribe this.

Performance considerations: More nodes doesn't mean slower execution. LangGraph writes checkpoints in the background by default ([async durability mode](/oss/python/langgraph/durable-execution#durability-modes)), so your graph continues running without waiting for checkpoints to complete. This means you get frequent checkpoints with minimal performance impact. You can adjust this behavior if needed—use `"exit"` mode to checkpoint only at completion, or `"sync"` mode to block execution until each checkpoint is written.
</Accordion>

### Where to go from here

This was an introduction to thinking about building agents with LangGraph. You can extend this foundation with:

<CardGroup cols={2}>
  <Card title="Human-in-the-loop patterns" icon="user-check" href="/oss/python/langgraph/interrupts">
    Learn how to add tool approval before execution, batch approval, and other patterns
  </Card>

<Card title="Subgraphs" icon="diagram-nested" href="/oss/python/langgraph/use-subgraphs">
    Create subgraphs for complex multi-step operations
  </Card>

<Card title="Streaming" icon="tower-broadcast" href="/oss/python/langgraph/streaming">
    Add streaming to show real-time progress to users
  </Card>

<Card title="Observability" icon="chart-line" href="/oss/python/langgraph/observability">
    Add observability with LangSmith for debugging and monitoring
  </Card>

<Card title="Tool Integration" icon="wrench" href="/oss/python/langchain/tools">
    Integrate more tools for web search, database queries, and API calls
  </Card>

<Card title="Retry Logic" icon="rotate" href="/oss/python/langgraph/use-graph-api#add-retry-policies">
    Implement retry logic with exponential backoff for failed operations
  </Card>
</CardGroup>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/thinking-in-langgraph.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
Notice that the state contains only raw data - no prompt templates, no formatted strings, no instructions. The classification output is stored as a single dictionary, straight from the LLM.

## Step 4: Build your nodes

Now we implement each step as a function. A node in LangGraph is just a Python function that takes the current state and returns updates to it.

### Handle errors appropriately

Different errors need different handling strategies:

| Error Type                                                      | Who Fixes It       | Strategy                           | When to Use                                      |
| --------------------------------------------------------------- | ------------------ | ---------------------------------- | ------------------------------------------------ |
| Transient errors (network issues, rate limits)                  | System (automatic) | Retry policy                       | Temporary failures that usually resolve on retry |
| LLM-recoverable errors (tool failures, parsing issues)          | LLM                | Store error in state and loop back | LLM can see the error and adjust its approach    |
| User-fixable errors (missing information, unclear instructions) | Human              | Pause with `interrupt()`           | Need user input to proceed                       |
| Unexpected errors                                               | Developer          | Let them bubble up                 | Unknown issues that need debugging               |

<Tabs>
  <Tab title="Transient errors" icon="rotate">
    Add a retry policy to automatically retry network issues and rate limits:
```

Example 2 (unknown):
```unknown
</Tab>

  <Tab title="LLM-recoverable" icon="brain">
    Store the error in state and loop back so the LLM can see what went wrong and try again:
```

Example 3 (unknown):
```unknown
</Tab>

  <Tab title="User-fixable" icon="user">
    Pause and collect information from the user when needed (like account IDs, order numbers, or clarifications):
```

Example 4 (unknown):
```unknown
</Tab>

  <Tab title="Unexpected" icon="triangle-exclamation">
    Let them bubble up for debugging. Don't catch what you can't handle:
```

---

## Define the tools for the agent to use

**URL:** llms-txt#define-the-tools-for-the-agent-to-use

@tool
def search(query: str) -> str:
    """Call to surf the web."""
    # This is a placeholder, but don't tell the LLM that...
    if "sf" in query.lower() or "san francisco" in query.lower():
        return "It's 60 degrees and foggy."
    return "It's 90 degrees and sunny."

tools = [search]
tool_node = ToolNode(tools)
model = init_chat_model("claude-sonnet-4-5-20250929").bind_tools(tools)

---

## Define the tools our agent can use

**URL:** llms-txt#define-the-tools-our-agent-can-use

---

## Define the two nodes we will cycle between

**URL:** llms-txt#define-the-two-nodes-we-will-cycle-between

workflow.add_node("agent", call_model)
workflow.add_node("tools", tool_node)

---

## Define tools

**URL:** llms-txt#define-tools

@tool
def multiply(a: int, b: int) -> int:
    """Multiply `a` and `b`.

Args:
        a: First int
        b: Second int
    """
    return a * b

@tool
def add(a: int, b: int) -> int:
    """Adds `a` and `b`.

Args:
        a: First int
        b: Second int
    """
    return a + b

@tool
def divide(a: int, b: int) -> float:
    """Divide `a` and `b`.

Args:
        a: First int
        b: Second int
    """
    return a / b

---

## Define your agents

**URL:** llms-txt#define-your-agents

market_researcher = Agent(
    role="Senior Market Researcher",
    goal="Analyze market trends and consumer behavior in the tech industry",
    backstory="""You are an experienced market researcher with 10+ years of experience
    analyzing technology markets. You excel at identifying emerging trends and
    understanding consumer needs.""",
    verbose=True,
    allow_delegation=False,
)

content_strategist = Agent(
    role="Content Marketing Strategist",
    goal="Create compelling marketing content based on research insights",
    backstory="""You are a creative content strategist who transforms complex market
    research into engaging marketing materials. You understand how to communicate
    technical concepts to different audiences.""",
    verbose=True,
    allow_delegation=False,
)

data_analyst = Agent(
    role="Data Analyst",
    goal="Provide statistical analysis and data-driven insights",
    backstory="""You are a skilled data analyst who can interpret complex datasets
    and provide actionable insights. You excel at finding patterns and trends
    in data that others might miss.""",
    verbose=True,
    allow_delegation=False,
)

---

## Define your tasks

**URL:** llms-txt#define-your-tasks

research_task = Task(
    description="""Conduct comprehensive research on the current state of AI adoption
    in small to medium businesses. Focus on:
    1. Current adoption rates and trends
    2. Main barriers to adoption
    3. Most popular AI tools and use cases
    4. ROI and business impact metrics

Provide a detailed analysis with supporting data and statistics.""",
    agent=market_researcher,
    expected_output="A comprehensive market research report on AI adoption in SMBs with data, trends, and insights.",
)

analysis_task = Task(
    description="""Analyze the research findings and identify key statistical patterns.
    Create data visualizations and provide quantitative insights on:
    1. Adoption rate trends over time
    2. Industry-specific adoption patterns
    3. ROI correlation analysis
    4. Barrier impact assessment

Present findings in a clear, data-driven format.""",
    agent=data_analyst,
    expected_output="Statistical analysis report with key metrics, trends, and data-driven insights.",
    context=[research_task],
)

content_task = Task(
    description="""Based on the research and analysis, create a compelling marketing
    strategy document that includes:
    1. Executive summary of key findings
    2. Target audience personas based on adoption patterns
    3. Key messaging framework addressing main barriers
    4. Content recommendations for different business segments
    5. Campaign strategy to drive AI adoption

Make the content actionable and business-focused.""",
    agent=content_strategist,
    expected_output="Complete marketing strategy document with personas, messaging, and campaign recommendations.",
    context=[research_task, analysis_task],
)

---

## Define your tools

**URL:** llms-txt#define-your-tools

**Contents:**
- View traces in LangSmith
- Advanced usage
  - Custom metadata and tags

def get_flight_info(destination: str, departure_date: str) -> dict:
    """Get flight information for a destination."""
    return {
        "destination": destination,
        "departure_date": departure_date,
        "price": "$450",
        "duration": "5h 30m",
        "airline": "Example Airways"
    }

def get_hotel_recommendations(city: str, check_in: str) -> dict:
    """Get hotel recommendations for a city."""
    return {
        "city": city,
        "check_in": check_in,
        "hotels": [
            {"name": "Grand Plaza Hotel", "rating": 4.5, "price": "$120/night"},
            {"name": "City Center Inn", "rating": 4.2, "price": "$95/night"}
        ]
    }

async def main():
    # Create your ADK agent
    agent = LlmAgent(
        name="travel_assistant",
        tools=[get_flight_info, get_hotel_recommendations],
        model="gemini-2.5-flash-lite",
        instruction="You are a helpful travel assistant that can help with flights and hotels.",
    )

# Set up session service and runner
    session_service = InMemorySessionService()
    runner = Runner(
        app_name="travel_app",
        agent=agent,
        session_service=session_service
    )

# Create a session
    user_id = "traveler_456"
    session_id = "session_789"
    await session_service.create_session(
        app_name="travel_app",
        user_id=user_id,
        session_id=session_id
    )

# Send a message to the agent
    new_message = types.Content(
        parts=[types.Part(text="I need to book a flight to Paris for March 15th and find a good hotel.")],
        role="user",
    )

# Run the agent and process events
    events = runner.run(
        user_id=user_id,
        session_id=session_id,
        new_message=new_message,
    )

for event in events:
        print(event)

if __name__ == "__main__":
    asyncio.run(main())
python  theme={null}
from opentelemetry import trace

**Examples:**

Example 1 (unknown):
```unknown
## View traces in LangSmith

* **Agent conversations**: Complete conversation flows between users and your ADK agents.
* **Tool calls**: Individual function calls made by your agents.
* **Model interactions**: LLM requests and responses using Gemini models.
* **Session information**: User and session context for organizing related traces.
* **Model interactions**: LLM requests and responses using Gemini models

<img src="https://mintcdn.com/langchain-5e9cc07a/OEEzzB__isjPfBRD/langsmith/images/adk.png?fit=max&auto=format&n=OEEzzB__isjPfBRD&q=85&s=3495c7838ba7467b905a180fc9ce477b" alt="LangSmith dashboard with raw input from run and trace information." data-og-width="3022" width="3022" data-og-height="1444" height="1444" data-path="langsmith/images/adk.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/OEEzzB__isjPfBRD/langsmith/images/adk.png?w=280&fit=max&auto=format&n=OEEzzB__isjPfBRD&q=85&s=b6db8e92abed624dc492ee12d217b5d3 280w, https://mintcdn.com/langchain-5e9cc07a/OEEzzB__isjPfBRD/langsmith/images/adk.png?w=560&fit=max&auto=format&n=OEEzzB__isjPfBRD&q=85&s=1caecf02d785375c0cae8c97cccf093c 560w, https://mintcdn.com/langchain-5e9cc07a/OEEzzB__isjPfBRD/langsmith/images/adk.png?w=840&fit=max&auto=format&n=OEEzzB__isjPfBRD&q=85&s=3b7fe88b62d4c4bffa889bd79fa5fefd 840w, https://mintcdn.com/langchain-5e9cc07a/OEEzzB__isjPfBRD/langsmith/images/adk.png?w=1100&fit=max&auto=format&n=OEEzzB__isjPfBRD&q=85&s=d7a152d9255fc6d42f946b3601e79ecc 1100w, https://mintcdn.com/langchain-5e9cc07a/OEEzzB__isjPfBRD/langsmith/images/adk.png?w=1650&fit=max&auto=format&n=OEEzzB__isjPfBRD&q=85&s=1eb5ea91226c92628f474254eb177f80 1650w, https://mintcdn.com/langchain-5e9cc07a/OEEzzB__isjPfBRD/langsmith/images/adk.png?w=2500&fit=max&auto=format&n=OEEzzB__isjPfBRD&q=85&s=a98880df67b4dae4c56de2a3d27fefa7 2500w" />

## Advanced usage

### Custom metadata and tags

You can add custom metadata to your traces by setting span attributes in your ADK application:
```

---

## Delete organizations

**URL:** llms-txt#delete-organizations

**Contents:**
  - Prerequisites
  - Running the deletion script for a single organization

Source: https://docs.langchain.com/langsmith/script-delete-an-organization

The LangSmith UI does not currently support the deletion of an individual organization from a self-hosted instance of LangSmith. This, however, can be accomplished by directly removing all traces from all materialized views in ClickHouse (except the runs\_history views) and the runs and feedbacks tables and then removing the Organization from the Postgres tenants table.

This command using the Organization ID as an argument.

Ensure you have the following tools/items ready.

* [https://kubernetes.io/docs/tasks/tools/](https://kubernetes.io/docs/tasks/tools/)

* [https://www.postgresql.org/download/](https://www.postgresql.org/download/)

3. PostgreSQL database connection:

* Host
   * Port
   * Username
     * If using the bundled version, this is `postgres`
   * Password
     * If using the bundled version, this is `postgres`
   * Database name
     * If using the bundled version, this is `postgres`

4. Clickhouse database credentials

* Host
   * Port
   * Username
     * If using the bundled version, this is `default`
   * Password
     * If using the bundled version, this is `password`
   * Database name
     * If using the bundled version, this is `default`

5. Connectivity to the PostgreSQL database from the machine you will be running the migration script on.

* If you are using the bundled version, you may need to port forward the postgresql service to your local machine.
   * Run `kubectl port-forward svc/langsmith-postgres 5432:5432` to port forward the postgresql service to your local machine.

6. Connectivity to the Clickhouse database from the machine you will be running the migration script on.

* If you are using the bundled version, you may need to port forward the clickhouse service to your local machine.
     * Run `kubectl port-forward svc/langsmith-clickhouse 8123:8123` to port forward the clickhouse service to your local machine.
   * If you are using Clickhouse Cloud you will want to specify the --ssl flag and use port `8443`

7. The script to delete an organization

* You can download the script from [here](https://github.com/langchain-ai/helm/blob/main/charts/langsmith/scripts/delete_organization_sh)

### Running the deletion script for a single organization

Run the following command to run the organization removal script:

For example, if you are using the bundled version with port-forwarding, the command would look like:

If you visit the LangSmith UI, you should now see organization is no longer present.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/script-delete-an-organization.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
For example, if you are using the bundled version with port-forwarding, the command would look like:
```

---

## Delete traces

**URL:** llms-txt#delete-traces

**Contents:**
  - Prerequisites
  - Running the deletion script for a single trace
  - Running the deletion script for a multiple traces from a file with one trace ID per line

Source: https://docs.langchain.com/langsmith/script-delete-traces

The LangSmith UI does not currently support the deletion of an individual trace. This, however, can be accomplished by directly removing the trace from all materialized views in ClickHouse (except the runs\_history views) and the runs and feedback table themselves.

This command can either be run using a trace ID as an argument or using a file that is a list of trace IDs.

Ensure you have the following tools/items ready.

* [https://kubernetes.io/docs/tasks/tools/](https://kubernetes.io/docs/tasks/tools/)

2. Clickhouse database credentials

* Host
   * Port
   * Username
     * If using the bundled version, this is `default`
   * Password
     * If using the bundled version, this is `password`
   * Database name
     * If using the bundled version, this is `default`

3. Connectivity to the Clickhouse database from the machine you will be running the `delete_trace_by_id` script on.

* If you are using the bundled version, you may need to port forward the clickhouse service to your local machine.
   * Run `kubectl port-forward svc/langsmith-clickhouse 8123:8123` to port forward the clickhouse service to your local machine.

4. The script to delete a trace

* You can download the script from [here](https://github.com/langchain-ai/helm/blob/main/charts/langsmith/scripts/delete_trace_by_id.sh)

### Running the deletion script for a single trace

Run the following command to run the trace deletion script using a single trace ID:

For example, if you are using the bundled version with port-forwarding, the command would look like:

If you visit the LangSmith UI, you should now see specified trace ID is no longer present nor reflected in stats.

### Running the deletion script for a multiple traces from a file with one trace ID per line

Run the following command to run the trace deletion script using a list of trace IDs:

For example, if you are using the bundled version with port-forwarding, the command would look like:

If you visit the LangSmith UI, you should now see all the specified traces have been removed.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/script-delete-traces.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
For example, if you are using the bundled version with port-forwarding, the command would look like:
```

Example 2 (unknown):
```unknown
If you visit the LangSmith UI, you should now see specified trace ID is no longer present nor reflected in stats.

### Running the deletion script for a multiple traces from a file with one trace ID per line

Run the following command to run the trace deletion script using a list of trace IDs:
```

Example 3 (unknown):
```unknown
For example, if you are using the bundled version with port-forwarding, the command would look like:
```

---

## Delete workspaces

**URL:** llms-txt#delete-workspaces

**Contents:**
  - Prerequisites
  - Running the deletion script for a single workspace

Source: https://docs.langchain.com/langsmith/script-delete-a-workspace

<Note>
  Deleting a workspace is supported **nativley in LangSmith Self-Hosted v0.10**. View [instructions for deleting a workspace](/langsmith/set-up-a-workspace#delete-a-workspace).

Follow the guide below for Self-Hosted versions before v0.10.
</Note>

The LangSmith UI does not currently support the deletion of an individual workspace from an organization. This, however, can be accomplished by directly removing all traces from all materialized views in ClickHouse (except the runs\_history views) and the runs and feedbacks tables and then removing the Workspace from the Postgres tenants table.

This command using the Workspace ID as an argument.

Ensure you have the following tools/items ready.

* [https://kubernetes.io/docs/tasks/tools/](https://kubernetes.io/docs/tasks/tools/)

* [https://www.postgresql.org/download/](https://www.postgresql.org/download/)

3. PostgreSQL database connection:

* Host
   * Port
   * Username
     * If using the bundled version, this is `postgres`
   * Password
     * If using the bundled version, this is `postgres`
   * Database name
     * If using the bundled version, this is `postgres`

4. Clickhouse database credentials

* Host
   * Port
   * Username
     * If using the bundled version, this is `default`
   * Password
     * If using the bundled version, this is `password`
   * Database name
     * If using the bundled version, this is `default`

5. Connectivity to the PostgreSQL database from the machine you will be running the migration script on.

* If you are using the bundled version, you may need to port forward the postgresql service to your local machine.
   * Run `kubectl port-forward svc/langsmith-postgres 5432:5432` to port forward the postgresql service to your local machine.

6. Connectivity to the Clickhouse database from the machine you will be running the migration script on.

* If you are using the bundled version, you may need to port forward the clickhouse service to your local machine.
     * Run `kubectl port-forward svc/langsmith-clickhouse 8123:8123` to port forward the clickhouse service to your local machine.
   * If you are using Clickhouse Cloud you will want to specify the --ssl flag and use port `8443`

7. The script to delete a workspace

* You can download the script from [here](https://github.com/langchain-ai/helm/blob/main/charts/langsmith/scripts/delete_workspace.sh)

### Running the deletion script for a single workspace

Run the following command to run the workspace removal script:

For example, if you are using the bundled version with port-forwarding, the command would look like:

If you visit the LangSmith UI, you should now see workspace is deleted.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/script-delete-a-workspace.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
For example, if you are using the bundled version with port-forwarding, the command would look like:
```

---

## Deploy

**URL:** llms-txt#deploy

**Contents:**
- Prerequisites
- Deploy your agent
  - 1. Create a repository on GitHub
  - 2. Deploy to LangSmith
  - 3. Test your application in Studio
  - 4. Get the API URL for your deployment
  - 5. Test the API

Source: https://docs.langchain.com/oss/javascript/langgraph/deploy

LangSmith is the fastest way to turn agents into production systems. Traditional hosting platforms are built for stateless, short-lived web apps, while LangGraph is **purpose-built for stateful, long-running agents**, so you can go from repo to reliable cloud deployment in minutes.

Before you begin, ensure you have the following:

* A [GitHub account](https://github.com/)
* A [LangSmith account](https://smith.langchain.com/) (free to sign up)

### 1. Create a repository on GitHub

Your application's code must reside in a GitHub repository to be deployed on LangSmith. Both public and private repositories are supported. For this quickstart, first make sure your app is LangGraph-compatible by following the [local server setup guide](/oss/javascript/langgraph/studio#setup-local-agent-server). Then, push your code to the repository.

### 2. Deploy to LangSmith

<Steps>
  <Step title="Navigate to LangSmith Deployments">
    Log in to [LangSmith](https://smith.langchain.com/). In the left sidebar, select **Deployments**.
  </Step>

<Step title="Create new deployment">
    Click the **+ New Deployment** button. A pane will open where you can fill in the required fields.
  </Step>

<Step title="Link repository">
    If you are a first time user or adding a private repository that has not been previously connected, click the **Add new account** button and follow the instructions to connect your GitHub account.
  </Step>

<Step title="Deploy repository">
    Select your application's repository. Click **Submit** to deploy. This may take about 15 minutes to complete. You can check the status in the **Deployment details** view.
  </Step>
</Steps>

### 3. Test your application in Studio

Once your application is deployed:

1. Select the deployment you just created to view more details.
2. Click the **Studio** button in the top right corner. Studio will open to display your graph.

### 4. Get the API URL for your deployment

1. In the **Deployment details** view in LangGraph, click the **API URL** to copy it to your clipboard.
2. Click the `URL` to copy it to the clipboard.

You can now test the API:

<Tabs>
  <Tab title="Python">
    1. Install LangGraph Python:

2. Send a message to the agent:

<Tab title="Rest API">
    
  </Tab>
</Tabs>

<Tip>
  LangSmith offers additional hosting options, including self-hosted and hybrid. For more information, please see the [Platform setup overview](/langsmith/platform-setup).
</Tip>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/deploy.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
2. Send a message to the agent:
```

Example 2 (unknown):
```unknown
</Tab>

  <Tab title="Rest API">
```

---

## Deploy an observability stack for your LangSmith deployment

**URL:** llms-txt#deploy-an-observability-stack-for-your-langsmith-deployment

Source: https://docs.langchain.com/langsmith/observability-stack

<Warning>
  **This section is only applicable for Kubernetes deployments.**
</Warning>

LangSmith applications expose telemetry data that can be sent to the backend of your choice. If you don’t already have an observability stack, or prefer to keep LangSmith telemetry separate from your main application, you can use the LangSmith Observability Helm chart to deploy a basic observability stack.

---

## Deploy with control plane

**URL:** llms-txt#deploy-with-control-plane

**Contents:**
- Overview
- Prerequisites
- Step 1. Test locally
- Step 2. Build Docker image
- Step 3. Push to container registry
- Step 4. Deploy with the control plane UI
- Update deployment
- Private registry authentication
- Next steps

Source: https://docs.langchain.com/langsmith/deploy-with-control-plane

This guide shows you how to deploy your applications to [hybrid](/langsmith/hybrid) or [self-hosted](/langsmith/self-hosted) instances with a [control plane](/langsmith/control-plane). With a control plane, you build Docker images locally, push them to a registry that your Kubernetes cluster has access to, and deploy them with the [LangSmith UI](https://smith.langchain.com).

<Note>
  **This guide is for deploying applications, not setting up infrastructure.**

Before using this guide, you must have already completed infrastructure setup:

* **[Hybrid setup](/langsmith/deploy-hybrid)**: For hybrid hosting.
  * **[Enable LangSmith Deployment](/langsmith/deploy-self-hosted-full-platform)**: For self-hosted with control plane.

If you haven't set up your infrastructure yet, start with the [Platform setup section](/langsmith/platform-setup).
</Note>

Applications deployed to hybrid or self-hosted LangSmith instances with control plane use Docker images. In this guide, the application deployment workflow is:

1. Test your application locally using `langgraph dev` or [Studio](/langsmith/studio).
2. Build a Docker image using the `langgraph build` command.
3. Push the image to a container registry accessible by your infrastructure.
4. Deploy from the [control plane UI](/langsmith/control-plane#control-plane-ui) by specifying the image URL.

Before completing this guide, you'll need the following:

* Completed infrastructure setup to enable your [data plane](/langsmith/data-plane) to receive application deployments:
  * [Hybrid setup](/langsmith/deploy-hybrid): Installs data plane components (listener, operator, CRDs) in your Kubernetes cluster that connect to LangChain's managed control plane.
  * [Enable LangSmith Deployment](/langsmith/deploy-self-hosted-full-platform): Enables LangSmith Deployment on your self-hosted LangSmith instance.
* Access to the [LangSmith UI](https://smith.langchain.com) with LangSmith Deployment enabled.
* A container registry accessible by your Kubernetes cluster. If using a private registry that requires authentication, you must configure image pull secrets as part of your infrastructure setup. Refer to [Private registry authentication](#private-registry-authentication).

## Step 1. Test locally

Before deploying, test your application locally. You can use the [LangGraph CLI](/langsmith/cli#dev) to run an Agent server in development mode:

For a full guide local testing, refer to the [Local server quickstart](/langsmith/local-server).

## Step 2. Build Docker image

Build a Docker image of your application using the [`langgraph build`](/langsmith/cli#build) command:

Build command options include:

| Option               | Default          | Description                                                       |
| -------------------- | ---------------- | ----------------------------------------------------------------- |
| `-t, --tag TEXT`     | Required         | Tag for the Docker image                                          |
| `--platform TEXT`    |                  | Target platform(s) to build for (e.g., `linux/amd64,linux/arm64`) |
| `--pull / --no-pull` | `--pull`         | Build with latest remote Docker image                             |
| `-c, --config FILE`  | `langgraph.json` | Path to configuration file                                        |

Example with platform specification:

For full details, see the [CLI reference](/langsmith/cli#build).

## Step 3. Push to container registry

Push your image to a container registry accessible by your Kubernetes cluster. The specific commands depend on your registry provider.

<Tip>
  Tag your images with version information (e.g., `my-registry.com/my-app:v1.0.0`) to make rollbacks easier.
</Tip>

## Step 4. Deploy with the control plane UI

The [control plane UI](/langsmith/control-plane#control-plane-ui) allows you to create and manage deployments, view logs and metrics, and update configurations. To create a new deployment in the [LangSmith UI](https://smith.langchain.com):

1. In the left-hand navigation panel, select **Deployments**.
2. In the top-right corner, select **+ New Deployment**.
3. In the deployment configuration panel, provide:
   * **Image URL**: The full image URL you pushed in [Step 3](#step-3-push-to-container-registry).
   * **Listener/Compute ID**: Select the listener configured for your infrastructure.
   * **Namespace**: The Kubernetes namespace to deploy to.
   * **Environment variables**: Any required configuration (API keys, etc.).
   * Other deployment settings as needed.
4. Select **Submit**.

The control plane will coordinate with your [data plane](/langsmith/data-plane) listener to deploy your application.

After creating a deployment, the infrastructure is [provisioned asynchronously](/langsmith/control-plane#asynchronous-deployment). Deployment can take up to several minutes, with initial deployments taking longer due to database creation.

From the control plane UI, you can view build logs, server logs, and deployment metrics including CPU/memory usage, replicas, and API performance. For more details, refer to the [control plane monitoring documentation](/langsmith/control-plane#monitoring).

<Note>
  A [LangSmith Observability tracing project](/langsmith/observability) is automatically created for each deployment with the same name as the deployment. Tracing environment variables are set automatically by the control plane.
</Note>

To deploy a new version of your application, create a [new revision](/langsmith/control-plane#revisions):

Starting from the LangSmith UI:

1. In the left-hand navigation panel, select **Deployments**.
2. Select an existing deployment.
3. In the Deployment view, select **+ New Revision** in the top-right corner.
4. Update the configuration:
   * Update the **Image URL** to your new image version.
   * Update environment variables if needed.
   * Adjust other settings as needed.
5. Select **Submit**.

## Private registry authentication

If your container registry requires authentication (e.g., AWS ECR, Azure ACR, GCP Artifact Registry, private Docker registry), you must configure Kubernetes image pull secrets before deploying applications. This is a one-time infrastructure configuration.

<Note>
  **This configuration is done at the infrastructure level, not per-deployment.** Once configured, all deployments automatically inherit the registry credentials.
</Note>

The configuration steps depend on your deployment type:

* **Self-hosted with control plane**: Configure `imagePullSecrets` in your LangSmith Helm chart's `values.yaml` file. See the detailed steps in the [Enable LangSmith Deployment guide](/langsmith/deploy-self-hosted-full-platform#setup).
* **Hybrid**: Configure `imagePullSecrets` in your `langgraph-dataplane-values.yaml` file using the same format.

For detailed steps on creating image pull secrets for different registry providers, refer to the [Kubernetes documentation on pulling images from private registries](https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/).

* **[Control plane](/langsmith/control-plane)**: Learn more about control plane features.
* **[Data plane](/langsmith/data-plane)**: Understand data plane architecture.
* **[Observability](/langsmith/observability)**: Monitor your deployments with automatic tracing.
* **[Studio](/langsmith/studio)**: Test and debug deployed applications.
* **[LangGraph CLI](/langsmith/cli)**: Full CLI reference documentation.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/deploy-with-control-plane.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
For a full guide local testing, refer to the [Local server quickstart](/langsmith/local-server).

## Step 2. Build Docker image

Build a Docker image of your application using the [`langgraph build`](/langsmith/cli#build) command:
```

Example 2 (unknown):
```unknown
Build command options include:

| Option               | Default          | Description                                                       |
| -------------------- | ---------------- | ----------------------------------------------------------------- |
| `-t, --tag TEXT`     | Required         | Tag for the Docker image                                          |
| `--platform TEXT`    |                  | Target platform(s) to build for (e.g., `linux/amd64,linux/arm64`) |
| `--pull / --no-pull` | `--pull`         | Build with latest remote Docker image                             |
| `-c, --config FILE`  | `langgraph.json` | Path to configuration file                                        |

Example with platform specification:
```

---

## Deploy your app to Cloud

**URL:** llms-txt#deploy-your-app-to-cloud

**Contents:**
- Prerequisites
- 1. Create a repository on GitHub
- 2. Deploy to LangSmith
- 3. Test your application in Studio
- 4. Get the API URL for your deployment
- 5. Test the API
- Next steps

Source: https://docs.langchain.com/langsmith/deployment-quickstart

This is a quickstart guide for deploying your first application to LangSmith Cloud.

<Tip>
  For a comprehensive Cloud deployment guide with all configuration options, refer to the [Cloud deployment setup guide](/langsmith/deploy-to-cloud).
</Tip>

Before you begin, ensure you have the following:

* A [GitHub account](https://github.com/)
* A [LangSmith account](https://smith.langchain.com/) (free to sign up)

## 1. Create a repository on GitHub

To deploy an application to **LangSmith**, your application code must reside in a GitHub repository. Both public and private repositories are supported. For this quickstart, use the [`new-langgraph-project` template](https://github.com/langchain-ai/react-agent) for your application:

1. Go to the [`new-langgraph-project` repository](https://github.com/langchain-ai/new-langgraph-project) or [`new-langgraphjs-project` template](https://github.com/langchain-ai/new-langgraphjs-project).
2. Click the `Fork` button in the top right corner to fork the repository to your GitHub account.
3. Click **Create fork**.

## 2. Deploy to LangSmith

1. Log in to [LangSmith](https://smith.langchain.com/).
2. In the left sidebar, select **Deployments**.
3. Click the **+ New Deployment** button. A pane will open where you can fill in the required fields.
4. If you are a first time user or adding a private repository that has not been previously connected, click the **Import from GitHub** button and follow the instructions to connect your GitHub account.
5. Select your New LangGraph Project repository.
6. Click **Submit** to deploy.
   This may take about 15 minutes to complete. You can check the status in the **Deployment details** view.

## 3. Test your application in Studio

Once your application is deployed:

1. Select the deployment you just created to view more details.
2. Click the **Studio** button in the top right corner. [Studio](/langsmith/studio) will open to display your graph.

## 4. Get the API URL for your deployment

1. In the **Deployment details** view, click the **API URL** to copy it to your clipboard.
2. Click the `URL` to copy it to the clipboard.

You can now test the API:

<Tabs>
  <Tab title="Python SDK (Async)">
    1. Install the LangGraph Python SDK:

2. Send a message to the assistant (threadless run):

<Tab title="Python SDK (Sync)">
    1. Install the LangGraph Python SDK:

2. Send a message to the assistant (threadless run):

<Tab title="JavaScript SDK">
    1. Install the LangGraph JS SDK

2. Send a message to the assistant (threadless run):

<Tab title="Rest API">
    
  </Tab>
</Tabs>

You've successfully deployed your application to LangSmith Cloud. Here are some next steps:

* **Explore Studio**: Use [Studio](/langsmith/studio) to visualize and debug your graph interactively.
* **Monitor your app**: Set up [observability](/langsmith/observability) with traces, dashboards, and alerts.
* **Learn more about Cloud**: See the [complete Cloud setup guide](/langsmith/deploy-to-cloud) for all configuration options.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/deployment-quickstart.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
2. Send a message to the assistant (threadless run):
```

Example 2 (unknown):
```unknown
</Tab>

  <Tab title="Python SDK (Sync)">
    1. Install the LangGraph Python SDK:
```

Example 3 (unknown):
```unknown
2. Send a message to the assistant (threadless run):
```

Example 4 (unknown):
```unknown
</Tab>

  <Tab title="JavaScript SDK">
    1. Install the LangGraph JS SDK
```

---

## Deprecated method call

**URL:** llms-txt#deprecated-method-call

**Contents:**
  - `example` parameter removed from `AIMessage`
- Minor changes

text = response.text()
```

Existing usage patterns (i.e., `.text()`) will continue to function but now emit a warning. The method form will be removed in v2.

### `example` parameter removed from `AIMessage`

The `example` parameter has been removed from [`AIMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessage) objects. We recommend migrating to use `additional_kwargs` for passing extra metadata as needed.

* `AIMessageChunk` objects now include a `chunk_position` attribute with position `'last'` to indicate the final chunk in a stream. This allows for clearer handling of streamed messages. If the chunk is not the final one, `chunk_position` will be `None`.
* `LanguageModelOutputVar` is now typed to [`AIMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessage) instead of [`BaseMessage`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.messages.BaseMessage).
* The logic for merging message chunks (`AIMessageChunk.add`) has been updated with more sophisticated selection handling for the final id for the merged chunk. It prioritizes provider-assigned IDs over LangChain-generated IDs.
* We now open files with `utf-8` encoding by default.
* Standard tests now use multimodal content blocks.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/migrate/langchain-v1.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Discover errors and usage patterns with the Insights Agent

**URL:** llms-txt#discover-errors-and-usage-patterns-with-the-insights-agent

**Contents:**
- Prerequisites
- Generate your first Insights Report
- Understand the results
  - Top-level categories
  - Subcategories
  - Individual traces
- Configure a job
  - Autogenerating a config
  - Choose a model provider
  - Using a prebuilt config

Source: https://docs.langchain.com/langsmith/insights

The Insights Agent automatically analyzes your traces to detect usage patterns, common agent behaviors and failure modes — without requiring you to manually review thousands of traces.
Insights uses hierarchical categorization to make sense of your data and highlight actionable trends.

* An OpenAI API key (generate one [here](https://platform.openai.com/account/api-keys)) or an Anthropic API key (generate one [here](https://console.anthropic.com/settings/keys))
* Permissions to create rules in LangSmith (required to generate new Insights Reports)
* Permissions to view tracing projects LangSmith (required to view existing Insights Reports)

## Generate your first Insights Report

<Frame caption="Auto configuration flow for Insights Agent">
  <img src="https://mintcdn.com/langchain-5e9cc07a/rp5c1TvRWS7-YcPd/langsmith/images/insights-autogenerate-config.png?fit=max&auto=format&n=rp5c1TvRWS7-YcPd&q=85&s=1055fe5ac43cdce00c43297e818db6b6" data-og-width="1498" width="1498" data-og-height="1408" height="1408" data-path="langsmith/images/insights-autogenerate-config.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/rp5c1TvRWS7-YcPd/langsmith/images/insights-autogenerate-config.png?w=280&fit=max&auto=format&n=rp5c1TvRWS7-YcPd&q=85&s=0c83b31a5a183ba5935b39b7b9de711d 280w, https://mintcdn.com/langchain-5e9cc07a/rp5c1TvRWS7-YcPd/langsmith/images/insights-autogenerate-config.png?w=560&fit=max&auto=format&n=rp5c1TvRWS7-YcPd&q=85&s=72217621fca8f07947a9461d75f42913 560w, https://mintcdn.com/langchain-5e9cc07a/rp5c1TvRWS7-YcPd/langsmith/images/insights-autogenerate-config.png?w=840&fit=max&auto=format&n=rp5c1TvRWS7-YcPd&q=85&s=b23c7627f8e62d8bebb7e94a0ec068da 840w, https://mintcdn.com/langchain-5e9cc07a/rp5c1TvRWS7-YcPd/langsmith/images/insights-autogenerate-config.png?w=1100&fit=max&auto=format&n=rp5c1TvRWS7-YcPd&q=85&s=5399d7c632e4bde4b5aefd4d19c8a175 1100w, https://mintcdn.com/langchain-5e9cc07a/rp5c1TvRWS7-YcPd/langsmith/images/insights-autogenerate-config.png?w=1650&fit=max&auto=format&n=rp5c1TvRWS7-YcPd&q=85&s=6be2b9f0c90979149f86e416c7cd4d8c 1650w, https://mintcdn.com/langchain-5e9cc07a/rp5c1TvRWS7-YcPd/langsmith/images/insights-autogenerate-config.png?w=2500&fit=max&auto=format&n=rp5c1TvRWS7-YcPd&q=85&s=adf51929b885117ba87e00be8f54d306 2500w" />
</Frame>

#### From the [LangSmith UI](https://smith.langchain.com):

1. Navigate to **Tracing Projects** in the left-hand menu and select a tracing project.
2. Click **+New** in the top right corner then **New Insights Report** to generate new insights over the project.
3. Enter a name for your job.
4. Click the <Icon icon="key" /> icon in the top right of the job creation pane to set your OpenAI (or Anthropic) API key as a [workspace secret](/langsmith/administration-overview#workspaces). If your workspace already has an OpenAI API key set, you can skip this step.
5. Answer the guided questions to focus your Insights Report on what you want to learn about your agent, then click **Run job**.

<Tip>Toggle to Manual mode to try [prebuilt configs](#using-a-prebuilt-config) for common use cases or [build your own](#building-a-config-from-scratch).</Tip>

This will kick off a background Insights Report. Reports can take up to 30 minutes to complete.

#### From the [LangSmith SDK](https://reference.langchain.com/python/langsmith/observability/sdk/client/#langsmith.client.Client):

You can generate Insights Reports over data stored outside LangSmith using the Python SDK. This allows you to analyze chat histories from your production systems, logs, or other sources.

When you call `generate_insights()`, the SDK will:

1. Upload your chat histories as traces to a new LangSmith project
2. Generate an Insights Report over those uploaded traces
3. Return a link to your results in the LangSmith UI

<CodeGroup>
  
</CodeGroup>

<Note>Generating insights over 1,000 threads typically costs \$1.00-\$2.00 with OpenAI models and \$3.00-\$4.00 with current Anthropic models. The cost scales with the number of threads sampled and the size of each thread.</Note>

## Understand the results

Once your job has completed, you can navigate to the **Insights** tab where you'll see a table of Insights Report. Each Report contains insights generated over a specific sample of traces from the tracing project.

<Frame caption="Insights Reports for a single tracing project">
  <img src="https://mintcdn.com/langchain-5e9cc07a/4-kFQm9_42J5OnwH/langsmith/images/insights-job-results.png?fit=max&auto=format&n=4-kFQm9_42J5OnwH&q=85&s=6068ead08d93b27a31e85dd35bdbca01" data-og-width="2540" width="2540" data-og-height="836" height="836" data-path="langsmith/images/insights-job-results.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4-kFQm9_42J5OnwH/langsmith/images/insights-job-results.png?w=280&fit=max&auto=format&n=4-kFQm9_42J5OnwH&q=85&s=d89d356e627fe9b79a889f1b08f5b55e 280w, https://mintcdn.com/langchain-5e9cc07a/4-kFQm9_42J5OnwH/langsmith/images/insights-job-results.png?w=560&fit=max&auto=format&n=4-kFQm9_42J5OnwH&q=85&s=1e36efd2e207f240c943918bec0fb692 560w, https://mintcdn.com/langchain-5e9cc07a/4-kFQm9_42J5OnwH/langsmith/images/insights-job-results.png?w=840&fit=max&auto=format&n=4-kFQm9_42J5OnwH&q=85&s=81d1b513785c44c83e19e037ee2bac9c 840w, https://mintcdn.com/langchain-5e9cc07a/4-kFQm9_42J5OnwH/langsmith/images/insights-job-results.png?w=1100&fit=max&auto=format&n=4-kFQm9_42J5OnwH&q=85&s=bd6af403106f833511a03a2f2f58d866 1100w, https://mintcdn.com/langchain-5e9cc07a/4-kFQm9_42J5OnwH/langsmith/images/insights-job-results.png?w=1650&fit=max&auto=format&n=4-kFQm9_42J5OnwH&q=85&s=9de6145f9638aaa949b33cdae33de291 1650w, https://mintcdn.com/langchain-5e9cc07a/4-kFQm9_42J5OnwH/langsmith/images/insights-job-results.png?w=2500&fit=max&auto=format&n=4-kFQm9_42J5OnwH&q=85&s=f062f61dcc789fbc72a6fdf11fb76603 2500w" />
</Frame>

Click into your job to see traces organized into a set of auto-generated categories.
You can drill down through categories and subcategories to view the underlying traces, feedback, and run statistics.

<Frame caption="Common topics of conversations with the https://chat.langchain.com chatbot">
  <img src="https://mintcdn.com/langchain-5e9cc07a/4-kFQm9_42J5OnwH/langsmith/images/insights-nav.gif?s=6a22bfd0d94262b7aa78468a8379ea0f" data-og-width="800" width="800" data-og-height="516" height="516" data-path="langsmith/images/insights-nav.gif" data-optimize="true" data-opv="3" />
</Frame>

### Top-level categories

Your traces are automatically grouped into top-level categories that represent the broadest patterns in your data.

The distribution bars show how frequently each pattern occurs, making it easy to spot behaviors that happen more or less than expected.

Each category has a brief description and displays aggregated metrics over the traces it contains, including:

* Typical trace stats (like error rates, latency, cost)
* Feedback scores from your evaluators
* [Attributes](#attributes) extracted as part of the job

Clicking on any category shows a breakdown into subcategories, which gives you a more granular understanding of interaction patterns in that category of traces.

In the [Chat Langchain](https://chat.langchain.com) example pictured above, under "Data & Retrieval" there are subcategories like "Vector Stores" and "Data Ingestion".

### Individual traces

You can view the traces assigned to each category or subcategory by clicking through to see the traces table. From there, you can click into any trace to see the full conversation details.

You can create an Insights Report three ways. Start with the auto-generated flow to spin up a baseline, then iterate with saved or manual configs as you refine.

### Autogenerating a config

1. Open **New Insights** and make sure the **Auto** toggle is active.
2. Answer the natural-language questions about your agent’s purpose, what you want to learn, and how traces are structured. Insights will translate your answers into
   a draft config (job name, summary prompt, attributes, and sampling defaults).
3. Choose a provider, then click **Generate config** to preview or **Run job** to launch immediately.

**Providing useful context**

For best results, write a sentence or two for each prompt that gives the agent the context it needs—what you’re trying to learn, which signals or fields matter most, and anything you
already know isn’t useful. The clearer you are about what your agent does and how its traces are structured, the more the Insights Agent can group examples in a way
that’s specific, actionable, and aligned with how you reason about your data.

**Describing your traces**

Explain how your data is organized—are these single runs or multi-turn conversations? Which inputs and outputs contain the key information? This helps the Insights Agent generate summary prompts and attributes that focus on what matters. You can also directly specify variables from the [summary prompt](#summary-prompt) section if needed.

### Choose a model provider

You can select either OpenAI or Anthropic models to power the agent. You must have the corresponding [workspace secret](/langsmith/administration-overview#workspaces) set for whichever provider you choose (OPENAI\_API\_KEY or ANTHROPIC\_API\_KEY).

Note that using current Anthropic models costs \~3x as much as using OpenAI models.

### Using a prebuilt config

<Frame caption="Prebuilt configs in Manual mode">
  <img src="https://mintcdn.com/langchain-5e9cc07a/fy0PJHxgSvYe7jF3/langsmith/images/insights-manual-config.png?fit=max&auto=format&n=fy0PJHxgSvYe7jF3&q=85&s=fa979566d61807f4f40c91cf9c6928f4" data-og-width="2220" width="2220" data-og-height="1440" height="1440" data-path="langsmith/images/insights-manual-config.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/fy0PJHxgSvYe7jF3/langsmith/images/insights-manual-config.png?w=280&fit=max&auto=format&n=fy0PJHxgSvYe7jF3&q=85&s=9497dc72f079e1dcec91d6fa5dfa963f 280w, https://mintcdn.com/langchain-5e9cc07a/fy0PJHxgSvYe7jF3/langsmith/images/insights-manual-config.png?w=560&fit=max&auto=format&n=fy0PJHxgSvYe7jF3&q=85&s=cad05d034c0631cebc422b04bc271f9a 560w, https://mintcdn.com/langchain-5e9cc07a/fy0PJHxgSvYe7jF3/langsmith/images/insights-manual-config.png?w=840&fit=max&auto=format&n=fy0PJHxgSvYe7jF3&q=85&s=8bc66817b550b2b01b52bda398b02405 840w, https://mintcdn.com/langchain-5e9cc07a/fy0PJHxgSvYe7jF3/langsmith/images/insights-manual-config.png?w=1100&fit=max&auto=format&n=fy0PJHxgSvYe7jF3&q=85&s=0852ae47cb4fd9f36d5dd7a9b143db63 1100w, https://mintcdn.com/langchain-5e9cc07a/fy0PJHxgSvYe7jF3/langsmith/images/insights-manual-config.png?w=1650&fit=max&auto=format&n=fy0PJHxgSvYe7jF3&q=85&s=d9abc923c29876e8c4abd3217214be82 1650w, https://mintcdn.com/langchain-5e9cc07a/fy0PJHxgSvYe7jF3/langsmith/images/insights-manual-config.png?w=2500&fit=max&auto=format&n=fy0PJHxgSvYe7jF3&q=85&s=51763ece975ee35a4ff9ac02ae90d251 2500w" />
</Frame>

Use the **Saved configurations** dropdown to load presets for common jobs like **Usage Patterns** or **Error Analysis**. Run them directly for a fast start, or adjust filters, prompts, and providers before saving your customized version. To learn more about what you can customize, read the section below.

### Building a config from scratch

Building your own config helps when you need more control—for example, predefining categories you want your data to be grouped into or targeting traces that match specific feedback scores and filters.

* **Sample size**: The maximum number of traces to analyze. Currently capped at 1,000
* **Time range**: Traces are sampled from this time range
* **Filters**: Additional trace filters. As you adjust filters, you'll see how many traces match your criteria

By default, top-level categories are automatically generated bottom-up from the underlying traces.
In some instances, you know specific categories you're interested in upfront and want the job to bucket traces into those predefined categories.

The **Categories** section of the config lets you do this by enumerating the names and descriptions of the top-level categories you want to be used.
Subcategories are still auto-generated by the algorithm within the predefined top-level categories.

The first step of the job is to create a brief summary of every trace — it is these summaries that are then categorized.
Extracting the right information in the summary is essential for getting useful categories.
The prompt used to generate these summaries can be edited.

The two things to think about when editing the prompt are:

* Summarization instructions: Any information that isn't in the trace summary won't affect the categories that get generated, so make sure to provide clear instructions on what information is important to extract from each trace.
* Trace content: Use mustache formatting to specify which parts of each trace are passed to the summarizer. Large traces with lots of inputs and outputs can be expensive and noisy. Reducing the prompt to only include the most relevant parts of the trace can improve your results.

The Insights Agent analyzes [threads](https://docs.langchain.com/langsmith/threads) - groups of related traces that represent multi-turn conversations. You must specify what parts of the thread to send to the summarizer using at least one of these template variables:

| Variable | Best for                                                                | Example                                            |
| -------- | ----------------------------------------------------------------------- | -------------------------------------------------- |
| run.\*   | Access data from the most recent root run (i.e. final turn) in a thread | `{{run.inputs}}` `{{run.outputs}}` `{{run.error}}` |

You can also access nested fields using dot notation. For example, the prompt `"Summarize this: {{run.inputs.foo.bar}}"` will include only the "bar" value within the "foo" value of the last run's inputs.

Along with a summary, you can define additional categorical, numerical, and boolean attributes to be extracted from each trace.
These attributes will influence the categorization step — traces with similar attribute values will tend to be categorized together.
You can also see aggregations of these attributes per category.

As an example, you might want to extract the attribute `user_satisfied: boolean` from each trace to steer the algorithm towards categories that split up positive and negative user experiences, and to see the average user satisfaction per category.

#### Filter attributes

You can use the `filter_by` parameter on boolean attributes to pre-filter traces before generating insights. When enabled, only traces where the attribute evaluates to `true` are included in the analysis.

This is useful when you want to focus your Insights Report on a specific subset of traces—for example, only analyzing errors, only examining English-language conversations, or only including traces that meet certain quality criteria.

<Frame caption="Using filter attributes to generate Insights only on traces with agent errors">
  <img src="https://mintcdn.com/langchain-5e9cc07a/L4LVgASBXoDKblmJ/langsmith/images/insights-filter-by-attribute.png?fit=max&auto=format&n=L4LVgASBXoDKblmJ&q=85&s=8cb30778befb18af445c3f6db758e631" data-og-width="1244" width="1244" data-og-height="490" height="490" data-path="langsmith/images/insights-filter-by-attribute.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/L4LVgASBXoDKblmJ/langsmith/images/insights-filter-by-attribute.png?w=280&fit=max&auto=format&n=L4LVgASBXoDKblmJ&q=85&s=b2a047279e60e995ce9de7fb44be3fc3 280w, https://mintcdn.com/langchain-5e9cc07a/L4LVgASBXoDKblmJ/langsmith/images/insights-filter-by-attribute.png?w=560&fit=max&auto=format&n=L4LVgASBXoDKblmJ&q=85&s=7fd3e854f73803434bc6490a33c77a1f 560w, https://mintcdn.com/langchain-5e9cc07a/L4LVgASBXoDKblmJ/langsmith/images/insights-filter-by-attribute.png?w=840&fit=max&auto=format&n=L4LVgASBXoDKblmJ&q=85&s=84b9cc1cc25ad8cc98962b584bca3ad1 840w, https://mintcdn.com/langchain-5e9cc07a/L4LVgASBXoDKblmJ/langsmith/images/insights-filter-by-attribute.png?w=1100&fit=max&auto=format&n=L4LVgASBXoDKblmJ&q=85&s=5d091df89be4d677139a21be120448f8 1100w, https://mintcdn.com/langchain-5e9cc07a/L4LVgASBXoDKblmJ/langsmith/images/insights-filter-by-attribute.png?w=1650&fit=max&auto=format&n=L4LVgASBXoDKblmJ&q=85&s=a6d991b404f2da6551c2cc696428e030 1650w, https://mintcdn.com/langchain-5e9cc07a/L4LVgASBXoDKblmJ/langsmith/images/insights-filter-by-attribute.png?w=2500&fit=max&auto=format&n=L4LVgASBXoDKblmJ&q=85&s=8c3cf044ac694ceb4818407c74c96b2e 2500w" />
</Frame>

* Add `"filter_by": true` to any boolean attribute when creating a config for the Insights Agent
* The LLM evaluates each trace against the attribute description during summarization
* Traces where the attribute is `false` or missing are excluded before insights are generated

You can optionally save configs for future reuse using the 'save as' button.
This is especially useful if you want to compare Insights Reports over time to identify changes in user and agent behavior.

Select from previously saved configs in the dropdown in the top-left corner of the pane when creating a new Insights Report.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/insights.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Document API authentication in OpenAPI

**URL:** llms-txt#document-api-authentication-in-openapi

**Contents:**
- Default Schema
- Custom Security Schema
- Testing

Source: https://docs.langchain.com/langsmith/openapi-security

This guide shows how to customize the OpenAPI security schema for your LangSmith API documentation. A well-documented security schema helps API consumers understand how to authenticate with your API and even enables automatic client generation. See the [Authentication & Access Control conceptual guide](/langsmith/auth) for more details about LangGraph's authentication system.

<Note>
  **Implementation vs Documentation**
  This guide only covers how to document your security requirements in OpenAPI. To implement the actual authentication logic, see [How to add custom authentication](/langsmith/custom-auth).
</Note>

This guide applies to all LangSmith deployments (Cloud and self-hosted). It does not apply to usage of the LangGraph open source library if you are not using LangSmith.

The default security scheme varies by deployment type:

<Tabs>
  <Tab title="LangSmith" />
</Tabs>

By default, LangSmith requires a LangSmith API key in the `x-api-key` header:

When using one of the LangGraph SDK's, this can be inferred from environment variables.

<Tabs>
  <Tab title="Self-hosted" />
</Tabs>

By default, self-hosted deployments have no security scheme. This means they are to be deployed only on a secured network or with authentication. To add custom authentication, see [How to add custom authentication](/langsmith/custom-auth).

## Custom Security Schema

To customize the security schema in your OpenAPI documentation, add an `openapi` field to your `auth` configuration in `langgraph.json`. Remember that this only updates the API documentation - you must also implement the corresponding authentication logic as shown in [How to add custom authentication](/langsmith/custom-auth).

Note that LangSmith does not provide authentication endpoints - you'll need to handle user authentication in your client application and pass the resulting credentials to the LangGraph API.

<Tabs>
  <Tab title="OAuth2 with Bearer Token">
    
  </Tab>

<Tab title="API Key">
    
  </Tab>
</Tabs>

After updating your configuration:

1. Deploy your application
2. Visit `/docs` to see the updated OpenAPI documentation
3. Try out the endpoints using credentials from your authentication server (make sure you've implemented the authentication logic first)

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/openapi-security.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
When using one of the LangGraph SDK's, this can be inferred from environment variables.

<Tabs>
  <Tab title="Self-hosted" />
</Tabs>

By default, self-hosted deployments have no security scheme. This means they are to be deployed only on a secured network or with authentication. To add custom authentication, see [How to add custom authentication](/langsmith/custom-auth).

## Custom Security Schema

To customize the security schema in your OpenAPI documentation, add an `openapi` field to your `auth` configuration in `langgraph.json`. Remember that this only updates the API documentation - you must also implement the corresponding authentication logic as shown in [How to add custom authentication](/langsmith/custom-auth).

Note that LangSmith does not provide authentication endpoints - you'll need to handle user authentication in your client application and pass the resulting credentials to the LangGraph API.

<Tabs>
  <Tab title="OAuth2 with Bearer Token">
```

Example 2 (unknown):
```unknown
</Tab>

  <Tab title="API Key">
```

---

## Document transformers

**URL:** llms-txt#document-transformers

Source: https://docs.langchain.com/oss/javascript/integrations/document_transformers/index

<Columns cols={3}>
  <Card title="html-to-text" icon="link" href="/oss/javascript/integrations/document_transformers/html-to-text" arrow="true" cta="View guide" />

<Card title="mozilla/readability" icon="link" href="/oss/javascript/integrations/document_transformers/mozilla_readability" arrow="true" cta="View guide" />

<Card title="OpenAI functions metadata tagger" icon="link" href="/oss/javascript/integrations/document_transformers/openai_metadata_tagger" arrow="true" cta="View guide" />
</Columns>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/javascript/integrations/document_transformers/index.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Double texting

**URL:** llms-txt#double-texting

**Contents:**
- Reject
- Enqueue
- Interrupt
- Rollback

Source: https://docs.langchain.com/langsmith/double-texting

<Info>
  **Prerequisites**

* [Agent Server](/langsmith/agent-server)
</Info>

Many times users might interact with your graph in unintended ways.
For instance, a user may send one message and before the graph has finished running send a second message.
More generally, users may invoke the graph a second time before the first run has finished.
We call this "double texting".

<Note>
  Double texting is a feature of LangSmith Deployment. It is not available in the [LangGraph open source framework](/oss/python/langgraph/overview).
</Note>

<img src="https://mintcdn.com/langchain-5e9cc07a/Hucw5hmCzWXDanL-/langsmith/images/double-texting.png?fit=max&auto=format&n=Hucw5hmCzWXDanL-&q=85&s=1cae1e8cd4920872e7992460b081f76d" alt="Double-text strategies across first vs. second run: Reject keeps only the first; Enqueue runs the second afterward; Interrupt halts the first to run the second; Rollback reverts the first and reruns with the second." data-og-width="1886" width="1886" data-og-height="648" height="648" data-path="langsmith/images/double-texting.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Hucw5hmCzWXDanL-/langsmith/images/double-texting.png?w=280&fit=max&auto=format&n=Hucw5hmCzWXDanL-&q=85&s=67fc4d3817141da00d0f0e0b5c6de093 280w, https://mintcdn.com/langchain-5e9cc07a/Hucw5hmCzWXDanL-/langsmith/images/double-texting.png?w=560&fit=max&auto=format&n=Hucw5hmCzWXDanL-&q=85&s=2c9cf620db602c51a7e3804cb0815058 560w, https://mintcdn.com/langchain-5e9cc07a/Hucw5hmCzWXDanL-/langsmith/images/double-texting.png?w=840&fit=max&auto=format&n=Hucw5hmCzWXDanL-&q=85&s=7e16e946f3c616476fd99b40aa731a3c 840w, https://mintcdn.com/langchain-5e9cc07a/Hucw5hmCzWXDanL-/langsmith/images/double-texting.png?w=1100&fit=max&auto=format&n=Hucw5hmCzWXDanL-&q=85&s=98032b626677cf73744a3922112abda4 1100w, https://mintcdn.com/langchain-5e9cc07a/Hucw5hmCzWXDanL-/langsmith/images/double-texting.png?w=1650&fit=max&auto=format&n=Hucw5hmCzWXDanL-&q=85&s=2537ca1524e871001cd454f41dca6597 1650w, https://mintcdn.com/langchain-5e9cc07a/Hucw5hmCzWXDanL-/langsmith/images/double-texting.png?w=2500&fit=max&auto=format&n=Hucw5hmCzWXDanL-&q=85&s=d37e7e8cd1c4a5bad8cc399d0b878382 2500w" />

This option rejects any additional incoming runs while a current run is in progress and prevents concurrent execution or double texting.

For configuring the reject double text option, refer to the [how-to guide](/langsmith/reject-concurrent).

This option allows the current run to finish before processing any new input. Incoming requests are queued and executed sequentially once prior runs complete.

For configuring the enqueue double text option, refer to the [how-to guide](/langsmith/enqueue-concurrent).

This option halts the current execution and preserves the progress made up to the interruption point. The new user input is then inserted, and execution continues from that state.

When using this option, your graph must account for potential edge cases. For example, a tool call may have been initiated but not yet completed at the time of interruption. In these cases, handling or removing partial tool calls may be necessary to avoid unresolved operations.

For configuring the interrupt double text option, refer to the [how-to guide](/langsmith/interrupt-concurrent).

This option halts the current execution and reverts all progress—including the initial run input—before processing the new user input. The new input is treated as a fresh run, starting from the initial state.

For configuring the rollback double text option, refer to the [how-to guide](/langsmith/rollback-concurrent).

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/double-texting.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Do NOT mistake this for the secret service role key

**URL:** llms-txt#do-not-mistake-this-for-the-secret-service-role-key

SUPABASE_ANON_KEY = os.environ.get("SUPABASE_ANON_KEY")
if not SUPABASE_ANON_KEY:
    SUPABASE_ANON_KEY = getpass("Enter your public Supabase anon  key: ")

async def sign_up(email: str, password: str):
    """Create a new user account."""
    async with httpx.AsyncClient() as client:
        response = await client.post(
            f"{SUPABASE_URL}/auth/v1/signup",
            json={"email": email, "password": password},
            headers={"apiKey": SUPABASE_ANON_KEY},
        )
        assert response.status_code == 200
        return response.json()

---

## Durable execution

**URL:** llms-txt#durable-execution

**Contents:**
- Requirements
- Determinism and Consistent Replay
- Durability modes
  - `"exit"`
  - `"async"`
  - `"sync"`
- Using tasks in nodes
- Resuming Workflows
- Starting Points for Resuming Workflows

Source: https://docs.langchain.com/oss/javascript/langgraph/durable-execution

**Durable execution** is a technique in which a process or workflow saves its progress at key points, allowing it to pause and later resume exactly where it left off. This is particularly useful in scenarios that require [human-in-the-loop](/oss/javascript/langgraph/interrupts), where users can inspect, validate, or modify the process before continuing, and in long-running tasks that might encounter interruptions or errors (e.g., calls to an LLM timing out). By preserving completed work, durable execution enables a process to resume without reprocessing previous steps -- even after a significant delay (e.g., a week later).

LangGraph's built-in [persistence](/oss/javascript/langgraph/persistence) layer provides durable execution for workflows, ensuring that the state of each execution step is saved to a durable store. This capability guarantees that if a workflow is interrupted -- whether by a system failure or for [human-in-the-loop](/oss/javascript/langgraph/interrupts) interactions -- it can be resumed from its last recorded state.

<Tip>
  If you are using LangGraph with a checkpointer, you already have durable execution enabled. You can pause and resume workflows at any point, even after interruptions or failures.
  To make the most of durable execution, ensure that your workflow is designed to be [deterministic](#determinism-and-consistent-replay) and [idempotent](#determinism-and-consistent-replay) and wrap any side effects or non-deterministic operations inside [tasks](/oss/javascript/langgraph/functional-api#task). You can use [tasks](/oss/javascript/langgraph/functional-api#task) from both the [StateGraph (Graph API)](/oss/javascript/langgraph/graph-api) and the [Functional API](/oss/javascript/langgraph/functional-api).
</Tip>

To leverage durable execution in LangGraph, you need to:

1. Enable [persistence](/oss/javascript/langgraph/persistence) in your workflow by specifying a [checkpointer](/oss/javascript/langgraph/persistence#checkpointer-libraries) that will save workflow progress.

2. Specify a [thread identifier](/oss/javascript/langgraph/persistence#threads) when executing a workflow. This will track the execution history for a particular instance of the workflow.

3. Wrap any non-deterministic operations (e.g., random number generation) or operations with side effects (e.g., file writes, API calls) inside [tasks](https://langchain-ai.github.io/langgraphjs/reference/functions/langgraph.task.html) to ensure that when a workflow is resumed, these operations are not repeated for the particular run, and instead their results are retrieved from the persistence layer. For more information, see [Determinism and Consistent Replay](#determinism-and-consistent-replay).

## Determinism and Consistent Replay

When you resume a workflow run, the code does **NOT** resume from the **same line of code** where execution stopped; instead, it will identify an appropriate [starting point](#starting-points-for-resuming-workflows) from which to pick up where it left off. This means that the workflow will replay all steps from the [starting point](#starting-points-for-resuming-workflows) until it reaches the point where it was stopped.

As a result, when you are writing a workflow for durable execution, you must wrap any non-deterministic operations (e.g., random number generation) and any operations with side effects (e.g., file writes, API calls) inside [tasks](/oss/javascript/langgraph/functional-api#task) or [nodes](/oss/javascript/langgraph/graph-api#nodes).

To ensure that your workflow is deterministic and can be consistently replayed, follow these guidelines:

* **Avoid Repeating Work**: If a [node](/oss/javascript/langgraph/graph-api#nodes) contains multiple operations with side effects (e.g., logging, file writes, or network calls), wrap each operation in a separate **task**. This ensures that when the workflow is resumed, the operations are not repeated, and their results are retrieved from the persistence layer.
* **Encapsulate Non-Deterministic Operations:** Wrap any code that might yield non-deterministic results (e.g., random number generation) inside **tasks** or **nodes**. This ensures that, upon resumption, the workflow follows the exact recorded sequence of steps with the same outcomes.
* **Use Idempotent Operations**: When possible ensure that side effects (e.g., API calls, file writes) are idempotent. This means that if an operation is retried after a failure in the workflow, it will have the same effect as the first time it was executed. This is particularly important for operations that result in data writes. In the event that a **task** starts but fails to complete successfully, the workflow's resumption will re-run the **task**, relying on recorded outcomes to maintain consistency. Use idempotency keys or verify existing results to avoid unintended duplication, ensuring a smooth and predictable workflow execution.

For some examples of pitfalls to avoid, see the [Common Pitfalls](/oss/javascript/langgraph/functional-api#common-pitfalls) section in the functional API, which shows
how to structure your code using **tasks** to avoid these issues. The same principles apply to the [StateGraph (Graph API)](https://langchain-ai.github.io/langgraphjs/reference/classes/langgraph.StateGraph.html).

LangGraph supports three durability modes that allow you to balance performance and data consistency based on your application's requirements. The durability modes, from least to most durable, are as follows:

* [`"exit"`](#exit)
* [`"async"`](#async)
* [`"sync"`](#sync)

A higher durability mode adds more overhead to the workflow execution.

<Tip>
  **Added in v0.6.0**
  Use the `durability` parameter instead of `checkpoint_during` (deprecated in v0.6.0) for persistence policy management:

* `durability="async"` replaces `checkpoint_during=True`
  * `durability="exit"` replaces `checkpoint_during=False`

for persistence policy management, with the following mapping:

* `checkpoint_during=True` -> `durability="async"`
  * `checkpoint_during=False` -> `durability="exit"`
</Tip>

Changes are persisted only when graph execution completes (either successfully or with an error). This provides the best performance for long-running graphs but means intermediate state is not saved, so you cannot recover from mid-execution failures or interrupt the graph execution.

Changes are persisted asynchronously while the next step executes. This provides good performance and durability, but there's a small risk that checkpoints might not be written if the process crashes during execution.

Changes are persisted synchronously before the next step starts. This ensures that every checkpoint is written before continuing execution, providing high durability at the cost of some performance overhead.

You can specify the durability mode when calling any graph execution method:

## Using tasks in nodes

If a [node](/oss/javascript/langgraph/graph-api#nodes) contains multiple operations, you may find it easier to convert each operation into a **task** rather than refactor the operations into individual nodes.

<Tabs>
  <Tab title="Original">
    
  </Tab>

<Tab title="With task">
    
  </Tab>
</Tabs>

## Resuming Workflows

Once you have enabled durable execution in your workflow, you can resume execution for the following scenarios:

* **Pausing and Resuming Workflows:** Use the [interrupt](https://langchain-ai.github.io/langgraphjs/reference/functions/langgraph.interrupt-2.html) function to pause a workflow at specific points and the [`Command`](https://langchain-ai.github.io/langgraphjs/reference/classes/langgraph.Command.html) primitive to resume it with updated state. See [**Interrupts**](/oss/javascript/langgraph/interrupts) for more details.
* **Recovering from Failures:** Automatically resume workflows from the last successful checkpoint after an exception (e.g., LLM provider outage). This involves executing the workflow with the same thread identifier by providing it with a `null` as the input value (see this [example](/oss/javascript/langgraph/use-functional-api#resuming-after-an-error) with the functional API).

## Starting Points for Resuming Workflows

* If you're using a [StateGraph (Graph API)](/oss/javascript/langgraph/graph-api), the starting point is the beginning of the [**node**](/oss/javascript/langgraph/graph-api#nodes) where execution stopped.
* If you're making a subgraph call inside a node, the starting point will be the **parent** node that called the subgraph that was halted.
  Inside the subgraph, the starting point will be the specific [**node**](/oss/javascript/langgraph/graph-api#nodes) where execution stopped.
* If you're using the Functional API, the starting point is the beginning of the [**entrypoint**](/oss/javascript/langgraph/functional-api#entrypoint) where execution stopped.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/durable-execution.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
</Tab>

  <Tab title="With task">
```

---

## Dynamic few shot example selection

**URL:** llms-txt#dynamic-few-shot-example-selection

**Contents:**
- Pre-conditions
- Index your dataset for few shot search
- Test search quality in the few shot playground
- Adding few shot search to your application
  - Code snippets

Source: https://docs.langchain.com/langsmith/index-datasets-for-dynamic-few-shot-example-selection

<Note>
  This feature is in open beta. It is only available to paid team plans. Please reach out to [support@langchain.dev](mailto:support@langchain.dev) if you have questions about enablement.
</Note>

Configure your datasets so that you can search for few shot examples based on an incoming request.

1. Your dataset must use the KV store data type (we do not currently support chat model or LLM type datasets)
2. You must have an input schema defined for your dataset. See our docs on setting up schema validation [in our UI](/langsmith/manage-datasets-in-application#dataset-schema-validation) for details.
3. You must be on a paid team plan (e.g. Plus plan)
4. You must be on LangSmith cloud

## Index your dataset for few shot search

Navigate to the datasets UI, and click the new `Few-Shot search` tab. Hit the `Start sync` button, which will create a new index on your dataset to make it searchable.

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-tab-unsynced.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=7bc50f1a3d4ea7b9e7458f3ed9770179" alt="" data-og-width="3208" width="3208" data-og-height="1902" height="1902" data-path="langsmith/images/few-shot-tab-unsynced.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-tab-unsynced.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=c2ee3688a24b8d142d4a5dd89c8797c1 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-tab-unsynced.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=44a4d8b59948893b4bd20d4c1b62198c 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-tab-unsynced.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=16a303eab8e2aede574c12353f782789 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-tab-unsynced.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=fbcf4566ec48c219bb7828bdc2cfe6b9 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-tab-unsynced.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=15da2b6b3e4a29ac3a2b4e139e2cf345 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-tab-unsynced.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=6bd99520313da2fedbbbaf388c956ba7 2500w" />

By default, we sync to the latest version of your dataset. That means when new examples are added to your dataset, they will automatically be added to your index. This process runs every few minutes, so there should be a very short delay for indexing new examples. You can see whether your index is up to date under `Few-shot index` on the lefthand side of the screen in the next section.

## Test search quality in the few shot playground

Now that you have turned on indexing for your dataset, you will see the new few shot playground.

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-synced-empty-state.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=b2b714b5a281ab8f39761566afd452f7" alt="" data-og-width="3208" width="3208" data-og-height="1902" height="1902" data-path="langsmith/images/few-shot-synced-empty-state.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-synced-empty-state.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=b8e4e648ce36f12a141f98815fd2e63d 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-synced-empty-state.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=49044eef3af8c08b60bcfeabadde6d16 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-synced-empty-state.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=b0fb897716bfac9521d0490c11eb006e 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-synced-empty-state.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=f21c2ad1fc5177af66c5f0c5ac59f660 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-synced-empty-state.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=dd9e2f9aa42366063ca14ad2b365e6fc 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-synced-empty-state.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=72158d44bcf916f2548bc5c7a6dabbce 2500w" />

You can type in a sample input, and check which results would be returned by our search API.

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-search-results.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=f64d39010f85b68d4d9ae3febc2b59d7" alt="" data-og-width="3208" width="3208" data-og-height="1902" height="1902" data-path="langsmith/images/few-shot-search-results.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-search-results.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=cbbd01172c4514eb3d775a4c0f3ca2e4 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-search-results.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=4a15ea167365f8d9105ab5aab50ed241 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-search-results.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=94ac208df6e860b9c194a646e3183779 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-search-results.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=ec68d9679e4cc86eebe352a9364e8894 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-search-results.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=5650c218963b5e8bd8d56e0bf886cd3e 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-search-results.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=89e90c66c065bee2a728cd5dad69432a 2500w" />

Each result will have a score and a link to the example in the dataset. The scoring system works such that 0 is a completely random result, and higher scores are better. Results will be sorted in descending order according to score.

<Note>
  Search uses a BM25-like algorithm for keyword based similarity scores. The actual score is subject to change as we improve the search algorithm, so we recommend not relying on the scores themselves, as their meaning may evolve over time. They are simply used for convenience in vibe-testing outputs in the playground.
</Note>

## Adding few shot search to your application

Click the `Get Code Snippet` button in the previous diagram, you'll be taken to a screen that has code snippets from our LangSmith SDK in different languages.

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-code-snippet.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=015d9416ef4f2708a6f2dfedceb1ea07" alt="" data-og-width="3208" width="3208" data-og-height="1902" height="1902" data-path="langsmith/images/few-shot-code-snippet.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-code-snippet.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=a5b15cf813569f4f218d972f2dbe89a5 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-code-snippet.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=231a3aad9937aab1fa92e65839bb5869 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-code-snippet.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=8aa86a24694342bca8ab387c83338afe 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-code-snippet.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=d92247c22e6fb2fd85e80713616b7fba 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-code-snippet.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=28f6e81c67c52687cd24431b83145675 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-code-snippet.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=aeafca83ae2489c43aa38d24165e5d62 2500w" />

For code samples on using few shot search in LangChain python applications, please see our [how-to guide in the LangChain docs](https://python.langchain.com/v0.2/docs/how_to/example_selectors_langsmith/).

<Note>
  Please ensure you are using the python SDK with version >= 1.101 or the typescript SDK with version >= 1.43
</Note>

For copy and paste convenience, you can find the similar code snippets to the ones shown in the screenshot above here:

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/index-datasets-for-dynamic-few-shot-example-selection.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown

```

---

## Dynamic prompts

**URL:** llms-txt#dynamic-prompts

@dynamic_prompt
def dynamic_system_prompt(request: ModelRequest) -> str:
    user_name = request.runtime.context.user_name  # [!code highlight]
    system_prompt = f"You are a helpful assistant. Address the user as {user_name}."
    return system_prompt

---

## Edges taken after the `action` node is called.

**URL:** llms-txt#edges-taken-after-the-`action`-node-is-called.

workflow.add_conditional_edges(
    "retrieve",
    # Assess agent decision
    grade_documents,
)
workflow.add_edge("generate_answer", END)
workflow.add_edge("rewrite_question", "generate_query_or_respond")

---

## Egress for subscription metrics and operational metadata

**URL:** llms-txt#egress-for-subscription-metrics-and-operational-metadata

**Contents:**
- LangSmith Telemetry
  - What we use it for
  - What we collect
  - How to disable
- Example payloads
  - License Verification
  - Usage Reporting
  - Telemetry: Metrics
  - Telemetry: Traces
- Our Commitment

Source: https://docs.langchain.com/langsmith/self-host-egress

<Info>
  This section only applies to customers who are not running in offline mode and assumes you are using a self-hosted LangSmith instance serving version 0.9.0 or later. Previous versions of LangSmith did not have this feature.
</Info>

Self-Hosted LangSmith instances store all information locally and will never send sensitive information outside of your network. We currently only track platform usage for billing purposes according to the entitlements in your order. In order to better remotely support our customers, we do require egress to `https://beacon.langchain.com`.

In the future, we will be introducing support diagnostics to help us ensure that LangSmith is running at an optimal level within your environment.

<Warning>
  **This will require egress to `https://beacon.langchain.com` from your network. Refer to the [allowlisting IP section](/langsmith/cloud#allowlisting-ip-addresses) for static IP addresses, if needed.**
</Warning>

Generally, data that we send to Beacon can be categorized as follows:

* Subscription Metrics

* Subscription metrics are used to determine level of access and utilization of LangSmith. This includes, but are not limited to:

* Number of traces
    * Seats allocated per contract
    * Seats in currently use

* Operational Metadata
  * This metadata will contain and collect the above subscription metrics to assist with remote support, allowing the LangChain team to diagnose and troubleshoot performance issues more effectively and proactively.

## LangSmith Telemetry

As of version ***0.11***, LangSmith deployments will by default send telemetry data back to our backend. All telemetry data is associated with an organization and deployment, but never identified with individual users. We ***do not collect PII*** (personally identifiable information) in any form.

### What we use it for

* To provide more proactive support and faster troubleshooting of self-hosted instances.
* Assisting with performance tuning.
* Understanding real-world usage to prioritize improvements.

* **Request metadata**: anonymized request counts, sizes, and durations.
* **Database metrics**: query durations, error rates, and performance counters.
* **Distributed traces**: end-to-end traces with timing and error information for high-latency or failed requests.

<Info>
  We do not collect actual payload contents, database records, or any data that can identify your end users or customers.
</Info>

Set the following values in your `langsmith_config.yaml` file:

In an effort to maximize transparency, we provide sample payloads here:

### License Verification

`POST beacon.langchain.com/v1/beacon/verify`

`POST beacon.langchain.com/v1/beacon/ingest-traces`

### Telemetry: Metrics

`POST beacon.langchain.com/v1/beacon/v1/metrics`

### Telemetry: Traces

`POST beacon.langchain.com/v1/beacon/v1/traces`

LangChain will not store any sensitive information in the Subscription Metrics or Operational Metadata. Any data collected will not be shared with a third party. If you have any concerns about the data being sent, please reach out to your account team.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/self-host-egress.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
## Example payloads

In an effort to maximize transparency, we provide sample payloads here:

### License Verification

**Endpoint:**

`POST beacon.langchain.com/v1/beacon/verify`

**Request:**
```

Example 2 (unknown):
```unknown
**Response:**
```

Example 3 (unknown):
```unknown
### Usage Reporting

**Endpoint:**

`POST beacon.langchain.com/v1/beacon/ingest-traces`

**Request:**
```

Example 4 (unknown):
```unknown
**Response:**
```

---

## Embedding models

**URL:** llms-txt#embedding-models

**Contents:**
- Overview
  - How it works
  - Similarity metrics
- Interface
- Install and use
- Caching
- All integrations

Source: https://docs.langchain.com/oss/javascript/integrations/text_embedding/index

<Note>
  This overview covers **text-based embedding models**. LangChain does not currently support multimodal embeddings.
</Note>

Embedding models transform raw text—such as a sentence, paragraph, or tweet—into a fixed-length vector of numbers that captures its **semantic meaning**. These vectors allow machines to compare and search text based on meaning rather than exact words.

In practice, this means that texts with similar ideas are placed close together in the vector space. For example, instead of matching only the phrase *"machine learning"*, embeddings can surface documents that discuss related concepts even when different wording is used.

1. **Vectorization** — The model encodes each input string as a high-dimensional vector.
2. **Similarity scoring** — Vectors are compared using mathematical metrics to measure how closely related the underlying texts are.

### Similarity metrics

Several metrics are commonly used to compare embeddings:

* **Cosine similarity** — measures the angle between two vectors.
* **Euclidean distance** — measures the straight-line distance between points.
* **Dot product** — measures how much one vector projects onto another.

LangChain provides a standard interface for text embedding models (e.g., OpenAI, Cohere, Hugging Face) via the [Embeddings](https://v03.api.js.langchain.com/classes/_langchain_core.embeddings.Embeddings.html) interface.

Two main methods are available:

* `embedDocuments(documents: string[]) → number[][]`: Embeds a list of documents.
* `embedQuery(text: string) → number[]`: Embeds a single query.

<Note>
  The interface allows queries and documents to be embedded with different strategies, though most providers handle them the same way in practice.
</Note>

<AccordionGroup>
  <Accordion title="OpenAI">
    Install dependencies:

Add environment variables:

Instantiate the model:

<Accordion title="Azure">
    Install dependencies

Add environment variables:

Instantiate the model:

<Accordion title="AWS">
    Install dependencies:

Add environment variables:

Instantiate the model:

<Accordion title="Google Gemini">
    Install dependencies:

Add environment variables:

Instantiate the model:

<Accordion title="Google Vertex">
    Install dependencies:

Add environment variables:

Instantiate the model:

<Accordion title="MistralAI">
    Install dependencies:

Add environment variables:

Instantiate the model:

<Accordion title="Cohere">
    Install dependencies:

Add environment variables:

Instantiate the model:

<Accordion title="Ollama">
    Install dependencies:

Instantiate the model:

</Accordion>
</AccordionGroup>

Embeddings can be stored or temporarily cached to avoid needing to recompute them.

Caching embeddings can be done using a `CacheBackedEmbeddings`. This wrapper stores embeddings in a key-value store, where the text is hashed and the hash is used as the key in the cache.

The main supported way to initialize a `CacheBackedEmbeddings` is `fromBytesStore`. It takes the following parameters:

* **underlyingEmbeddings**: The embedder to use for embedding.
* **documentEmbeddingStore**: Any [`BaseStore`](/oss/javascript/integrations/stores/) for caching document embeddings.
* **options.namespace**: (optional, defaults to `""`) The namespace to use for the document cache. Helps avoid collisions (e.g., set it to the embedding model name).

<Important>
  - Always set the `namespace` parameter to avoid collisions when using different embedding models.
  - `CacheBackedEmbeddings` does not cache query embeddings by default. To enable this, specify a `query_embedding_store`.
</Important>

In production, you would typically use a more robust persistent store, such as a database or cloud storage. Please see [stores integrations](/oss/javascript/integrations/stores/) for options.

<Columns cols={3}>
  <Card title="Alibaba Tongyi" icon="link" href="/oss/javascript/integrations/text_embedding/alibaba_tongyi" arrow="true" cta="View guide" />

<Card title="Azure OpenAI" icon="link" href="/oss/javascript/integrations/text_embedding/azure_openai" arrow="true" cta="View guide" />

<Card title="Baidu Qianfan" icon="link" href="/oss/javascript/integrations/text_embedding/baidu_qianfan" arrow="true" cta="View guide" />

<Card title="Amazon Bedrock" icon="link" href="/oss/javascript/integrations/text_embedding/bedrock" arrow="true" cta="View guide" />

<Card title="ByteDance Doubao" icon="link" href="/oss/javascript/integrations/text_embedding/bytedance_doubao" arrow="true" cta="View guide" />

<Card title="Cloudflare Workers AI" icon="link" href="/oss/javascript/integrations/text_embedding/cloudflare_ai" arrow="true" cta="View guide" />

<Card title="Cohere" icon="link" href="/oss/javascript/integrations/text_embedding/cohere" arrow="true" cta="View guide" />

<Card title="DeepInfra" icon="link" href="/oss/javascript/integrations/text_embedding/deepinfra" arrow="true" cta="View guide" />

<Card title="Fireworks" icon="link" href="/oss/javascript/integrations/text_embedding/fireworks" arrow="true" cta="View guide" />

<Card title="Google Generative AI" icon="link" href="/oss/javascript/integrations/text_embedding/google_generativeai" arrow="true" cta="View guide" />

<Card title="Google Vertex AI" icon="link" href="/oss/javascript/integrations/text_embedding/google_vertex_ai" arrow="true" cta="View guide" />

<Card title="Gradient AI" icon="link" href="/oss/javascript/integrations/text_embedding/gradient_ai" arrow="true" cta="View guide" />

<Card title="HuggingFace Inference" icon="link" href="/oss/javascript/integrations/text_embedding/hugging_face_inference" arrow="true" cta="View guide" />

<Card title="IBM watsonx.ai" icon="link" href="/oss/javascript/integrations/text_embedding/ibm" arrow="true" cta="View guide" />

<Card title="Jina" icon="link" href="/oss/javascript/integrations/text_embedding/jina" arrow="true" cta="View guide" />

<Card title="Llama CPP" icon="link" href="/oss/javascript/integrations/text_embedding/llama_cpp" arrow="true" cta="View guide" />

<Card title="Minimax" icon="link" href="/oss/javascript/integrations/text_embedding/minimax" arrow="true" cta="View guide" />

<Card title="MistralAI" icon="link" href="/oss/javascript/integrations/text_embedding/mistralai" arrow="true" cta="View guide" />

<Card title="Mixedbread AI" icon="link" href="/oss/javascript/integrations/text_embedding/mixedbread_ai" arrow="true" cta="View guide" />

<Card title="Nomic" icon="link" href="/oss/javascript/integrations/text_embedding/nomic" arrow="true" cta="View guide" />

<Card title="Ollama" icon="link" href="/oss/javascript/integrations/text_embedding/ollama" arrow="true" cta="View guide" />

<Card title="OpenAI" icon="link" href="/oss/javascript/integrations/text_embedding/openai" arrow="true" cta="View guide" />

<Card title="Pinecone" icon="link" href="/oss/javascript/integrations/text_embedding/pinecone" arrow="true" cta="View guide" />

<Card title="Prem AI" icon="link" href="/oss/javascript/integrations/text_embedding/premai" arrow="true" cta="View guide" />

<Card title="Tencent Hunyuan" icon="link" href="/oss/javascript/integrations/text_embedding/tencent_hunyuan" arrow="true" cta="View guide" />

<Card title="TensorFlow" icon="link" href="/oss/javascript/integrations/text_embedding/tensorflow" arrow="true" cta="View guide" />

<Card title="TogetherAI" icon="link" href="/oss/javascript/integrations/text_embedding/togetherai" arrow="true" cta="View guide" />

<Card title="HuggingFace Transformers" icon="link" href="/oss/javascript/integrations/text_embedding/transformers" arrow="true" cta="View guide" />

<Card title="Voyage AI" icon="link" href="/oss/javascript/integrations/text_embedding/voyageai" arrow="true" cta="View guide" />

<Card title="ZhipuAI" icon="link" href="/oss/javascript/integrations/text_embedding/zhipuai" arrow="true" cta="View guide" />
</Columns>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/javascript/integrations/text_embedding/index.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
</CodeGroup>

    Add environment variables:
```

Example 4 (unknown):
```unknown
Instantiate the model:
```

---

## enabled: true

**URL:** llms-txt#enabled:-true

---

## Enables Claude Code to emit OTEL events

**URL:** llms-txt#enables-claude-code-to-emit-otel-events

export CLAUDE_CODE_ENABLE_TELEMETRY=1

---

## Enable blob storage

**URL:** llms-txt#enable-blob-storage

**Contents:**
- Requirements
- Authentication
  - Amazon S3
  - Google Cloud Storage
  - Azure Blob Storage
- CH Search
- Configuration
- TTL Configuration
  - Amazon S3
  - Google Cloud Storage

Source: https://docs.langchain.com/langsmith/self-host-blob-storage

By default, LangSmith stores run inputs, outputs, errors, manifests, extras, and events in ClickHouse. If you so choose, you can instead store this information in blob storage, which has a couple of notable benefits. For the best results in production deployments, we **strongly** recommend using blob storage, which offers the following benefits:

1. In high trace environments, inputs, outputs, errors, manifests, extras, and events may balloon the size of your databases.
2. If using LangSmith Managed ClickHouse, you may want sensitive information in blob storage that resides in your environment. To alleviate this, LangSmith supports storing run inputs, outputs, errors, manifests, extras, events, and attachments in an external blob storage system.

<Note>
  Azure blob storage is available in Helm chart versions 0.8.9 and greater. [Deleting trace projects](/langsmith/observability-concepts#deleting-traces-from-langsmith) is supported in Azure starting in Helm chart version 0.10.43.
</Note>

* Access to a valid blob storage service

* [Amazon S3](https://aws.amazon.com/s3/)
    * [Google Cloud Storage (GCS)](https://cloud.google.com/storage?hl=en)
  * [Azure Blob Storage](https://azure.microsoft.com/en-us/products/storage/blobs)

* A bucket/directory in your blob storage to store the data. We highly recommend creating a separate bucket/directory for LangSmith data.
  * **If you are using TTLs**, you will need to set up a lifecycle policy to delete old data. You can find more information on configuring TTLs [here](/langsmith/self-host-ttl). These policies should mirror the TTLs you have set in your LangSmith configuration, or you may experience data loss. See [here](#ttl-configuration) on how to setup the lifecycle rules for TTLs for blob storage.

* Credentials to permit LangSmith Services to access the bucket/directory
  * You will need to provide your LangSmith instance with the necessary credentials to access the bucket/directory. Read the authentication [section](#authentication) below for more information.

* If using S3 or GCS, an API url for your blob storage service

* This will be the URL that LangSmith uses to access your blob storage system
  * For Amazon S3, this will be the URL of the S3 endpoint. Something like: `https://s3.amazonaws.com` or `https://s3.us-west-1.amazonaws.com` if using a regional endpoint.
  * For Google Cloud Storage, this will be the URL of the GCS endpoint. Something like: `https://storage.googleapis.com`

To authenticate to [Amazon S3](https://aws.amazon.com/s3/), you will need to create an IAM policy granting the following permissions on your bucket.

Once you have the correct policy, there are three ways to authenticate with Amazon S3:

1. [**(Recommended) IAM Role for Service Account**](https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html): You can create an IAM role for your LangSmith instance and attach the policy to that role. You can then provide the role to LangSmith. This is the recommended way to authenticate with Amazon S3 in production.

1. You will need to create an IAM role with the policy attached.
   2. You will need to allow LangSmith service accounts to assume the role. The `langsmith-queue`, `langsmith-backend`, and `langsmith-platform-backend` service accounts will need to be able to assume the role.
      <Warning>
        The service account names will be different if you are using a custom release name. You can find the service account names by running `kubectl get serviceaccounts` in your cluster.
      </Warning>
   3. You will need to provide the role ARN to LangSmith. You can do this by adding the `eks.amazonaws.com/role-arn: "<role_arn>"` annotation to the `queue`, `backend`, and `platform-backend` services in your Helm Chart installation.

2. [**Access Key and Secret Key**](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html): You can provide LangSmith with an access key and secret key. This is the simplest way to authenticate with Amazon S3. However, it is not recommended for production use as it is less secure.
   1. You will need to create a user with the policy attached. Then you can provision an access key and secret key for that user.

3. [**VPC Endpoint Access**](https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-s3.html): You can enable access to your S3 bucket via a VPC endpoint, which allows traffic to flow securely from your VPC to your S3 bucket.

1. You'll need to provision a VPC endpoint and configure it to allow access to your S3 bucket.
   2. You can refer to our [public Terraform modules](https://github.com/langchain-ai/terraform/blob/main/modules/aws/s3/main.tf#L12) for guidance and an example of configuring this.

#### KMS encryption header support for S3

Starting with LangSmith Helm chart version **0.11.24**, you can pass a KMS encryption key header and enforce a specific KMS key for writes by providing its ARN. To enable this, set the following values in your Helm chart:

### Google Cloud Storage

To authenticate with [Google Cloud Storage](https://cloud.google.com/storage?hl=en), you will need to create a [`service account`](https://cloud.google.com/iam/docs/service-account-overview) with the necessary permissions to access your bucket.

Your service account will need the `Storage Admin` role or a custom role with equivalent permissions. This can be scoped to the bucket that LangSmith will be using.

Once you have a provisioned service account, you will need to generate a [`HMAC key`](https://cloud.google.com/storage/docs/authentication/hmackeys) for that service account. This key and secret will be used to authenticate with Google Cloud Storage.

### Azure Blob Storage

To authenticate with [Azure Blob Storage](https://azure.microsoft.com/en-us/products/storage/blobs), you will need to use one of the following methods to grant LangSmith workloads permission to access your [container](https://learn.microsoft.com/en-us/azure/storage/blobs/storage-blobs-introduction#containers) (listed in order of precedence):

1. [Storage account and access key](https://learn.microsoft.com/en-us/azure/storage/common/storage-account-keys-manage)
2. [Connection string](https://learn.microsoft.com/en-us/azure/storage/common/storage-configure-connection-string)
3. [Workload identity](https://azure.github.io/azure-workload-identity/docs/introduction.html) (recommended), managed identity, or environment variables supported by [`DefaultAzureCredential`](https://learn.microsoft.com/en-us/azure/developer/go/azure-sdk-authentication?tabs=bash#2-authenticate-with-azure). This is the default authentication method when configuration for either option above is not present.
   1. To use workload identity, add the label `azure.workload.identity/use: true` to the `queue`, `backend`, and `platform-backend` deployments. Additionally, add the `azure.workload.identity/client-id` annotation to the corresponding service accounts, which should be an existing Azure AD Application's client ID or user-assigned managed identity's client ID. See [Azure's documentation](https://azure.github.io/azure-workload-identity/docs/topics/service-account-labels-and-annotations.html) for additional details.

<Note>
  Some deployments may need further customization of the connection configuration using a Service URL Override instead of the default service URL (`https://<storage_account_name>.blob.core.windows.net/`). For example, this override is necessary in order to use a different blob storage domain (e.g. government or china).
</Note>

By default, LangSmith will still store tokens for search in ClickHouse. If you are using LangSmith Managed Clickhouse, you may want to disable this feature to avoid sending potentially sensitive information to ClickHouse. You can do this in your blob storage configuration.

After creating your bucket and obtaining the necessary credentials, you can configure LangSmith to use your blob storage system.

<Note>
  If using an access key and secret, you can also provide an existing Kubernetes secret that contains the authentication information. This is recommended over providing the access key and secret key directly in your config. See the [generated secret template](https://github.com/langchain-ai/helm/blob/main/charts/langsmith/templates/secrets.yaml) for the expected secret keys.
</Note>

If using the [TTL](/langsmith/self-host-ttl) feature with LangSmith, you'll also have to configure TTL rules for your blob storage. Trace information stored on blob storage is stored on a particular prefix path, which determines the TTL for the data. When a trace's retention is extended, its corresponding blob storage path changes to ensure that it matches the new extended retention.

The following TTL prefix are used:

* `ttl_s/`: Short term TTL, configured for 14 days.
* `ttl_l/`: Long term TTL, configured for 400 days.

If you have customized the TTLs in your LangSmith configuration, you will need to adjust the TTLs in your blob storage configuration to match.

If using S3 for your blob storage, you will need to setup a filter lifecycle configuration that matches the prefixes above. You can find information for this [in the Amazon Documentation](https://docs.aws.amazon.com/AmazonS3/latest/userguide/intro-lifecycle-rules.html#intro-lifecycle-rules-filter).

As an example, if you are using Terraform to manage your S3 bucket, you would setup something like this:

### Google Cloud Storage

You will need to setup lifecycle conditions for your GCS buckets that you are using. You can find information for this [in the Google Documentation](https://cloud.google.com/storage/docs/lifecycle#conditions), specifically using matchesPrefix.

As an example, if you are using Terraform to manage your GCS bucket, you would setup something like this:

### Azure blob storage

You will need to configure a [lifecycle management policy](https://learn.microsoft.com/en-us/azure/storage/blobs/lifecycle-management-policy-configure) on the container in order to expire objects matching the prefixes above.

As an example, if you are [using Terraform to manage your blob storage container](https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/storage_management_policy), you would setup something like this:

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/self-host-blob-storage.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
Once you have the correct policy, there are three ways to authenticate with Amazon S3:

1. [**(Recommended) IAM Role for Service Account**](https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html): You can create an IAM role for your LangSmith instance and attach the policy to that role. You can then provide the role to LangSmith. This is the recommended way to authenticate with Amazon S3 in production.

   1. You will need to create an IAM role with the policy attached.
   2. You will need to allow LangSmith service accounts to assume the role. The `langsmith-queue`, `langsmith-backend`, and `langsmith-platform-backend` service accounts will need to be able to assume the role.
      <Warning>
        The service account names will be different if you are using a custom release name. You can find the service account names by running `kubectl get serviceaccounts` in your cluster.
      </Warning>
   3. You will need to provide the role ARN to LangSmith. You can do this by adding the `eks.amazonaws.com/role-arn: "<role_arn>"` annotation to the `queue`, `backend`, and `platform-backend` services in your Helm Chart installation.

2. [**Access Key and Secret Key**](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html): You can provide LangSmith with an access key and secret key. This is the simplest way to authenticate with Amazon S3. However, it is not recommended for production use as it is less secure.
   1. You will need to create a user with the policy attached. Then you can provision an access key and secret key for that user.

3. [**VPC Endpoint Access**](https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-s3.html): You can enable access to your S3 bucket via a VPC endpoint, which allows traffic to flow securely from your VPC to your S3 bucket.

   1. You'll need to provision a VPC endpoint and configure it to allow access to your S3 bucket.
   2. You can refer to our [public Terraform modules](https://github.com/langchain-ai/terraform/blob/main/modules/aws/s3/main.tf#L12) for guidance and an example of configuring this.

#### KMS encryption header support for S3

Starting with LangSmith Helm chart version **0.11.24**, you can pass a KMS encryption key header and enforce a specific KMS key for writes by providing its ARN. To enable this, set the following values in your Helm chart:
```

Example 2 (unknown):
```unknown
### Google Cloud Storage

To authenticate with [Google Cloud Storage](https://cloud.google.com/storage?hl=en), you will need to create a [`service account`](https://cloud.google.com/iam/docs/service-account-overview) with the necessary permissions to access your bucket.

Your service account will need the `Storage Admin` role or a custom role with equivalent permissions. This can be scoped to the bucket that LangSmith will be using.

Once you have a provisioned service account, you will need to generate a [`HMAC key`](https://cloud.google.com/storage/docs/authentication/hmackeys) for that service account. This key and secret will be used to authenticate with Google Cloud Storage.

### Azure Blob Storage

To authenticate with [Azure Blob Storage](https://azure.microsoft.com/en-us/products/storage/blobs), you will need to use one of the following methods to grant LangSmith workloads permission to access your [container](https://learn.microsoft.com/en-us/azure/storage/blobs/storage-blobs-introduction#containers) (listed in order of precedence):

1. [Storage account and access key](https://learn.microsoft.com/en-us/azure/storage/common/storage-account-keys-manage)
2. [Connection string](https://learn.microsoft.com/en-us/azure/storage/common/storage-configure-connection-string)
3. [Workload identity](https://azure.github.io/azure-workload-identity/docs/introduction.html) (recommended), managed identity, or environment variables supported by [`DefaultAzureCredential`](https://learn.microsoft.com/en-us/azure/developer/go/azure-sdk-authentication?tabs=bash#2-authenticate-with-azure). This is the default authentication method when configuration for either option above is not present.
   1. To use workload identity, add the label `azure.workload.identity/use: true` to the `queue`, `backend`, and `platform-backend` deployments. Additionally, add the `azure.workload.identity/client-id` annotation to the corresponding service accounts, which should be an existing Azure AD Application's client ID or user-assigned managed identity's client ID. See [Azure's documentation](https://azure.github.io/azure-workload-identity/docs/topics/service-account-labels-and-annotations.html) for additional details.

<Note>
  Some deployments may need further customization of the connection configuration using a Service URL Override instead of the default service URL (`https://<storage_account_name>.blob.core.windows.net/`). For example, this override is necessary in order to use a different blob storage domain (e.g. government or china).
</Note>

## CH Search

By default, LangSmith will still store tokens for search in ClickHouse. If you are using LangSmith Managed Clickhouse, you may want to disable this feature to avoid sending potentially sensitive information to ClickHouse. You can do this in your blob storage configuration.

## Configuration

After creating your bucket and obtaining the necessary credentials, you can configure LangSmith to use your blob storage system.

<CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>

<Note>
  If using an access key and secret, you can also provide an existing Kubernetes secret that contains the authentication information. This is recommended over providing the access key and secret key directly in your config. See the [generated secret template](https://github.com/langchain-ai/helm/blob/main/charts/langsmith/templates/secrets.yaml) for the expected secret keys.
</Note>

## TTL Configuration

If using the [TTL](/langsmith/self-host-ttl) feature with LangSmith, you'll also have to configure TTL rules for your blob storage. Trace information stored on blob storage is stored on a particular prefix path, which determines the TTL for the data. When a trace's retention is extended, its corresponding blob storage path changes to ensure that it matches the new extended retention.

The following TTL prefix are used:

* `ttl_s/`: Short term TTL, configured for 14 days.
* `ttl_l/`: Long term TTL, configured for 400 days.

If you have customized the TTLs in your LangSmith configuration, you will need to adjust the TTLs in your blob storage configuration to match.

### Amazon S3

If using S3 for your blob storage, you will need to setup a filter lifecycle configuration that matches the prefixes above. You can find information for this [in the Amazon Documentation](https://docs.aws.amazon.com/AmazonS3/latest/userguide/intro-lifecycle-rules.html#intro-lifecycle-rules-filter).

As an example, if you are using Terraform to manage your S3 bucket, you would setup something like this:
```

---

## Enable LangSmith Deployment

**URL:** llms-txt#enable-langsmith-deployment

**Contents:**
- Overview
- Prerequisites
- Setup
- (Optional) Configure authentication for private registries
- Next steps

Source: https://docs.langchain.com/langsmith/deploy-self-hosted-full-platform

This guide shows you how to enable **LangSmith Deployment** on your [self-hosted LangSmith instance](/langsmith/kubernetes). This adds a [control plane](/langsmith/control-plane) and [data plane](/langsmith/data-plane) that let you deploy, scale, and manage agents and applications directly through the LangSmith UI.

After completing this guide, you'll have access to LangSmith [Observability](/langsmith/observability), [Evaluation](/langsmith/evaluation), and [Deployment](/langsmith/deployments).

<Info>**Important**<br /> Enabling LangSmith Deployment requires an [Enterprise](https://langchain.com/pricing) plan. </Info>

<Note>
  **This setup page is for enabling [LangSmith Deployment](/langsmith/deployments) on an existing LangSmith instance.**

Review the [self-hosted options](/langsmith/self-hosted) to understand:

* [LangSmith (observability)](/langsmith/self-hosted#langsmith): What you should install first.
  * [LangSmith Deployment](/langsmith/self-hosted#langsmith-deployment): What this guide enables.
  * [Standalone Server](/langsmith/self-hosted#standalone-server): Lightweight alternative without the UI.
</Note>

This guide builds on top of the [Kubernetes installation guide](/langsmith/kubernetes). **You must complete that guide first** before continuing. This page covers the additional setup steps required to enable LangSmith Deployment:

* Installing the LangGraph operator
* Configuring your ingress
* Connecting to the control plane

1. You are using Kubernetes.
2. You have an instance of [self-hosted LangSmith](/langsmith/kubernetes) running.
3. `KEDA` is installed on your cluster.

6. Ingress Configuration
   1. You must set up an ingress, gateway, or use Istio for your LangSmith instance. All agents will be deployed as Kubernetes services behind this ingress. Use this guide to [set up an ingress](/langsmith/self-host-ingress) for your instance.
7. You must have slack space in your cluster for multiple deployments. `Cluster-Autoscaler` is recommended to automatically provision new nodes.
8. A valid Dynamic PV provisioner or PVs available on your cluster. You can verify this by running:

9. Egress to `https://beacon.langchain.com` from your network. This is required for license verification and usage reporting if not running in air-gapped mode. See the [Egress documentation](/langsmith/self-host-egress) for more details.

1. As part of configuring your self-hosted LangSmith instance, you enable the `deployment` option. This will provision a few key resources.
   1. `listener`: This is a service that listens to the [control plane](/langsmith/control-plane) for changes to your deployments and creates/updates downstream CRDs.
   2. `LangGraphPlatform CRD`: A CRD for LangSmith Deployment. This contains the spec for managing an instance of a LangSmith deployment.
   3. `operator`: This operator handles changes to your LangSmith CRDs.
   4. `host-backend`: This is the [control plane](/langsmith/control-plane).

<Note>
  As of v0.12.0, the `langgraphPlatform` option is deprecated. Use `config.deployment` for any version after v0.12.0.
</Note>

2. Two additional images will be used by the chart. Use the images that are specified in the latest release.

3. In your config file for langsmith (usually `langsmith_config.yaml`), enable the `deployment` option. Note that you must also have a valid ingress setup:

4. In your `values.yaml` file, configure the `hostBackendImage` and `operatorImage` options (if you need to mirror images). If you are using a private container registry that requires authentication, you must also configure `imagePullSecrets`, refer to [Configure authentication for private registries](#optional-configure-authentication-for-private-registries).

5. You can also configure base templates for your agents by overriding the base templates [here](https://github.com/langchain-ai/helm/blob/main/charts/langsmith/values.yaml#L898).

Your self-hosted infrastructure is now ready to create deployments.

## (Optional) Configure authentication for private registries

If your [Agent Server deployments](/langsmith/agent-server) will use images from private container registries (e.g., AWS ECR, Azure ACR, GCP Artifact Registry, private Docker registry), configure image pull secrets. This is a one-time infrastructure configuration that allows all deployments to automatically authenticate with your private registry.

**Step 1: Create a Kubernetes image pull secret**

Replace the values with your registry credentials:

* `myregistry.com`: Your registry URL
* `your-username`: Your registry username
* `your-password`: Your registry password or access token
* `langsmith`: The Kubernetes namespace where LangSmith is installed

**Step 2: Configure the secret in your `values.yaml`**

**Step 3: Apply during Helm installation/upgrade**

When you deploy or upgrade your LangSmith instance using Helm, this configuration will be applied. All user deployments created through the LangSmith UI will automatically inherit these registry credentials.

For registry-specific authentication methods (AWS ECR, Azure ACR, GCP Artifact Registry, etc.), refer to the [Kubernetes documentation on pulling images from private registries](https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/).

Once your infrastructure is set up, you're ready to deploy applications. See the deployment guides in the [Deployment tab](/langsmith/deployments) for instructions on building and deploying your applications.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/deploy-self-hosted-full-platform.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
6. Ingress Configuration
   1. You must set up an ingress, gateway, or use Istio for your LangSmith instance. All agents will be deployed as Kubernetes services behind this ingress. Use this guide to [set up an ingress](/langsmith/self-host-ingress) for your instance.
7. You must have slack space in your cluster for multiple deployments. `Cluster-Autoscaler` is recommended to automatically provision new nodes.
8. A valid Dynamic PV provisioner or PVs available on your cluster. You can verify this by running:
```

Example 2 (unknown):
```unknown
9. Egress to `https://beacon.langchain.com` from your network. This is required for license verification and usage reporting if not running in air-gapped mode. See the [Egress documentation](/langsmith/self-host-egress) for more details.

## Setup

1. As part of configuring your self-hosted LangSmith instance, you enable the `deployment` option. This will provision a few key resources.
   1. `listener`: This is a service that listens to the [control plane](/langsmith/control-plane) for changes to your deployments and creates/updates downstream CRDs.
   2. `LangGraphPlatform CRD`: A CRD for LangSmith Deployment. This contains the spec for managing an instance of a LangSmith deployment.
   3. `operator`: This operator handles changes to your LangSmith CRDs.
   4. `host-backend`: This is the [control plane](/langsmith/control-plane).

<Note>
  As of v0.12.0, the `langgraphPlatform` option is deprecated. Use `config.deployment` for any version after v0.12.0.
</Note>

2. Two additional images will be used by the chart. Use the images that are specified in the latest release.
```

Example 3 (unknown):
```unknown
3. In your config file for langsmith (usually `langsmith_config.yaml`), enable the `deployment` option. Note that you must also have a valid ingress setup:
```

Example 4 (unknown):
```unknown
4. In your `values.yaml` file, configure the `hostBackendImage` and `operatorImage` options (if you need to mirror images). If you are using a private container registry that requires authentication, you must also configure `imagePullSecrets`, refer to [Configure authentication for private registries](#optional-configure-authentication-for-private-registries).

5. You can also configure base templates for your agents by overriding the base templates [here](https://github.com/langchain-ai/helm/blob/main/charts/langsmith/values.yaml#L898).

   Your self-hosted infrastructure is now ready to create deployments.

## (Optional) Configure authentication for private registries

If your [Agent Server deployments](/langsmith/agent-server) will use images from private container registries (e.g., AWS ECR, Azure ACR, GCP Artifact Registry, private Docker registry), configure image pull secrets. This is a one-time infrastructure configuration that allows all deployments to automatically authenticate with your private registry.

**Step 1: Create a Kubernetes image pull secret**
```

---

## Enable TTL and data retention

**URL:** llms-txt#enable-ttl-and-data-retention

**Contents:**
- Requirements
- ClickHouse TTL Cleanup Job
  - Default Schedule
  - Disabling the Job
  - Configuring the Schedule
  - Configuring Minimum Expired Rows Per Part
  - Configuring Maximum Active Mutations
  - Emergency: Stopping Running Mutations
  - Backups and Data Retention

Source: https://docs.langchain.com/langsmith/self-host-ttl

LangSmith Self-Hosted allows enablement of automatic TTL and Data Retention of traces. This can be useful if you're complying with data privacy regulations, or if you want to have more efficient space usage and auto cleanup of your traces. Traces will also have their data retention period automatically extended based on certain actions or run rule applications.

You can configure retention through helm or environment variable settings. There are a few options that are configurable:

* *Enabled:* Whether data retention is enabled or disabled. If enabled, via the UI you can your default organization and project TTL tiers to apply to traces (see [data retention guide](/langsmith/administration-overview#data-retention) for details).
* *Retention Periods:* You can configure system-wide retention periods for shortlived and longlived traces. Once configured, you can manage the retention level at each project as well as set an organization-wide default for new projects.

## ClickHouse TTL Cleanup Job

As of version **0.11**, a cron job runs on weekends to assist in deleting expired data that may not have been cleaned up by ClickHouse's built-in TTL mechanism.

<Warning>
  This job uses potentially long running **mutations** (`ALTER TABLE DELETE`), which are expensive operations that can impact ClickHouse's performance. We recommend running these operations only during off-peak hours (nights and weekends). During testing with **1 concurrent active** mutation (default), we did not observe significant CPU, memory, or latency increases.
</Warning>

By default, the cleanup job runs:

* **Saturday**: 8pm and 10pm UTC
* **Sunday**: 12am, 2am, and 4am UTC

### Disabling the Job

To disable the cleanup job entirely:

### Configuring the Schedule

You can customize when the cleanup job runs by modifying the cron expressions:

<Tip>
  To run the job on a single cron schedule, set both `CLICKHOUSE_TTL_CLEANUP_CRON_WEEKEND_EVENING` and `CLICKHOUSE_TTL_CLEANUP_CRON_WEEKEND_MORNING` to the same value. Job locking prevents overlapping executions.
</Tip>

### Configuring Minimum Expired Rows Per Part

The job goes table by table, scanning parts and deleting data from parts containing a minimum number of expired rows. This threshold balances efficiency and thoroughness:

* **Too low**: Job scans entire parts to clear minimal data (inefficient)
* **Too high**: Job misses parts with significant expired data

#### Checking Expired Rows

Use this query to analyze expired rows in your tables, and tweak your minimum value accordingly:

### Configuring Maximum Active Mutations

Delete operations can be time-consuming (\~50 minutes for a 100GB part). You can increase concurrent mutations to speed up the process:

<Warning>
  Increasing concurrent DELETE operations can severely impact system performance. Monitor your system carefully and only increase this value if you can tolerate potentially slower insert and read latencies.
</Warning>

### Emergency: Stopping Running Mutations

If you experience latency spikes and need to terminate a running mutation:

1. **Find active mutations**:

Look for the `mutation_id` where the `command` column contains a `DELETE` statement.

2. **Kill the mutation**:

### Backups and Data Retention

If disk space does not decrease after running this job, or if it continues to increase, backups may be causing the issue by creating file system hard links. These links prevent ClickHouse from cleaning up the data.

To verify, check the following directories inside your ClickHouse pod:

* `/var/lib/clickhouse/backup`
* `/var/lib/clickhouse/shadow`

If backups are present, copy them to an external filesystem or blob storage (e.g., S3), then clear the directories. Within a few minutes, you will notice disk space releasing.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/self-host-ttl.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

## ClickHouse TTL Cleanup Job

As of version **0.11**, a cron job runs on weekends to assist in deleting expired data that may not have been cleaned up by ClickHouse's built-in TTL mechanism.

<Warning>
  This job uses potentially long running **mutations** (`ALTER TABLE DELETE`), which are expensive operations that can impact ClickHouse's performance. We recommend running these operations only during off-peak hours (nights and weekends). During testing with **1 concurrent active** mutation (default), we did not observe significant CPU, memory, or latency increases.
</Warning>

### Default Schedule

By default, the cleanup job runs:

* **Saturday**: 8pm and 10pm UTC
* **Sunday**: 12am, 2am, and 4am UTC

### Disabling the Job

To disable the cleanup job entirely:
```

Example 3 (unknown):
```unknown
### Configuring the Schedule

You can customize when the cleanup job runs by modifying the cron expressions:
```

Example 4 (unknown):
```unknown
<Tip>
  To run the job on a single cron schedule, set both `CLICKHOUSE_TTL_CLEANUP_CRON_WEEKEND_EVENING` and `CLICKHOUSE_TTL_CLEANUP_CRON_WEEKEND_MORNING` to the same value. Job locking prevents overlapping executions.
</Tip>

### Configuring Minimum Expired Rows Per Part

The job goes table by table, scanning parts and deleting data from parts containing a minimum number of expired rows. This threshold balances efficiency and thoroughness:

* **Too low**: Job scans entire parts to clear minimal data (inefficient)
* **Too high**: Job misses parts with significant expired data
```

---

## End runs

**URL:** llms-txt#end-runs

**Contents:**
- Batch Ingestion

patch_run(child_run_id, chat_completion.dict())
patch_run(parent_run_id, {"answer": chat_completion.choices[0].message.content})
python  theme={null}
import json
import os
import uuid
from datetime import datetime, timezone
from typing import Dict, List
import requests
from requests_toolbelt import MultipartEncoder

def create_dotted_order(
    start_time: datetime | None = None,
    run_id: uuid.UUID | None = None
) -> str:
    """Create a dotted order string for run ordering and hierarchy.

The dotted order is used to establish the sequence and relationships between runs.
    It combines a timestamp with a unique identifier to ensure proper ordering and tracing.
    """
    st = start_time or datetime.now(timezone.utc)
    id_ = run_id or uuid.uuid4()
    return f"{st.strftime('%Y%m%dT%H%M%S%fZ')}{id_}"

def create_run_base(
    name: str,
    run_type: str,
    inputs: dict,
    start_time: datetime
) -> dict:
    """Create the base structure for a run."""
    run_id = uuid.uuid4()
    return {
        "id": str(run_id),
        "trace_id": str(run_id),
        "name": name,
        "start_time": start_time.isoformat(),
        "inputs": inputs,
        "run_type": run_type,
    }

def construct_run(
    name: str,
    run_type: str,
    inputs: dict,
    parent_dotted_order: str | None = None,
) -> dict:
    """Construct a run dictionary with the given parameters.

This function creates a run with a unique ID and dotted order, establishing its place
    in the trace hierarchy if it's a child run.
    """
    start_time = datetime.now(timezone.utc)
    run = create_run_base(name, run_type, inputs, start_time)
    current_dotted_order = create_dotted_order(start_time, uuid.UUID(run["id"]))

if parent_dotted_order:
        current_dotted_order = f"{parent_dotted_order}.{current_dotted_order}"
        run["trace_id"] = parent_dotted_order.split(".")[0].split("Z")[1]
        run["parent_run_id"] = parent_dotted_order.split(".")[-1].split("Z")[1]

run["dotted_order"] = current_dotted_order
    return run

def serialize_run(operation: str, run_data: dict) -> List[tuple]:
    """Serialize a run for the multipart request.

This function separates the run data into parts for efficient transmission and storage.
    The main run data and optional fields (inputs, outputs, events) are serialized separately.
    """
    run_id = run_data.get("id", str(uuid.uuid4()))

# Separate optional fields
    inputs = run_data.pop("inputs", None)
    outputs = run_data.pop("outputs", None)
    events = run_data.pop("events", None)

# Serialize main run data
    run_data_json = json.dumps(run_data).encode("utf-8")
    parts.append(
        (
            f"{operation}.{run_id}",
            (
                None,
                run_data_json,
                "application/json",
                {"Content-Length": str(len(run_data_json))},
            ),
        )
    )

# Serialize optional fields
    for key, value in [("inputs", inputs), ("outputs", outputs), ("events", events)]:
        if value:
            serialized_value = json.dumps(value).encode("utf-8")
            parts.append(
                (
                    f"{operation}.{run_id}.{key}",
                    (
                        None,
                        serialized_value,
                        "application/json",
                        {"Content-Length": str(len(serialized_value))},
                    ),
                )
            )

def batch_ingest_runs(
    api_url: str,
    api_key: str,
    posts: list[dict] | None = None,
    patches: list[dict] | None = None,
) -> None:
    """Ingest multiple runs in a single batch request.

This function handles both creating new runs (posts) and updating existing runs (patches).
    It's more efficient for ingesting multiple runs compared to individual API calls.
    """
    boundary = uuid.uuid4().hex
    all_parts = []

for operation, runs in zip(("post", "patch"), (posts, patches)):
        if runs:
            all_parts.extend(
                [part for run in runs for part in serialize_run(operation, run)]
            )

encoder = MultipartEncoder(fields=all_parts, boundary=boundary)
    headers = {"Content-Type": encoder.content_type, "x-api-key": api_key}

try:
        response = requests.post(
            f"{api_url}/runs/multipart",
            data=encoder,
            headers=headers
        )
        response.raise_for_status()
        print("Successfully ingested runs.")
    except requests.RequestException as e:
        print(f"Error ingesting runs: {e}")
        # In a production environment, you might want to log this error or handle it more robustly

**Examples:**

Example 1 (unknown):
```unknown
See the doc on the [Run (span) data format](/langsmith/run-data-format) for more information.

## Batch Ingestion

For faster ingestion of runs and higher rate limits, you can use the POST `/runs/multipart` [link](https://api.smith.langchain.com/redoc#tag/run/operation/multipart_ingest_runs_api_v1_runs_multipart_post) endpoint. Below is an example. It requires `orjson` (for fast json ) and `requests_toolbelt` to run
```

---

## Enforce previous behavior with output_version flag

**URL:** llms-txt#enforce-previous-behavior-with-output_version-flag

**Contents:**
  - Default `max_tokens` in `langchain-anthropic`
  - Legacy code moved to `langchain-classic`
  - Removal of deprecated APIs
  - Text property

model = ChatOpenAI(model="gpt-4o-mini", output_version="v0")
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
### Default `max_tokens` in `langchain-anthropic`

The `max_tokens` parameter in `langchain-anthropic` now defaults to higher values based on the model chosen, rather than the previous default of `1024`. If you relied on the old default, explicitly set `max_tokens=1024`.

### Legacy code moved to `langchain-classic`

Existing functionality outside the focus of standard interfaces and agents has been moved to the [`langchain-classic`](https://pypi.org/project/langchain-classic) package. See the [Simplified namespace](#simplified-namespace) section for details on what's available in the core `langchain` package and what moved to `langchain-classic`.

### Removal of deprecated APIs

Methods, functions, and other objects that were already deprecated and slated for removal in 1.0 have been deleted. Check the [deprecation notices](https://python.langchain.com/docs/versions/migrating_chains) from previous versions for replacement APIs.

### Text property

Use of the `.text()` method on message objects should drop the parentheses, as it is now a property:
```

---

## Enqueue concurrent

**URL:** llms-txt#enqueue-concurrent

**Contents:**
- Setup
- Create runs
- View run results

Source: https://docs.langchain.com/langsmith/enqueue-concurrent

This guide assumes knowledge of what double-texting is, which you can learn about in the [double-texting conceptual guide](/langsmith/double-texting).

The guide covers the `enqueue` option for double texting, which adds the interruptions to a queue and executes them in the order they are received by the client. Below is a quick example of using the `enqueue` option.

First, we will define a quick helper function for printing out JS and CURL model outputs (you can skip this if using Python):

<Tabs>
  <Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

Then, let's import our required packages and instantiate our client, assistant, and thread.

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

Now let's start two runs, with the second interrupting the first one with a multitask strategy of "enqueue":

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

Verify that the thread has data from both runs:

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/enqueue-concurrent.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
</Tab>

  <Tab title="CURL">
```

Example 2 (unknown):
```unknown
</Tab>
</Tabs>

Then, let's import our required packages and instantiate our client, assistant, and thread.

<Tabs>
  <Tab title="Python">
```

Example 3 (unknown):
```unknown
</Tab>

  <Tab title="Javascript">
```

Example 4 (unknown):
```unknown
</Tab>

  <Tab title="CURL">
```

---

## Environment variables

**URL:** llms-txt#environment-variables

**Contents:**
- `BG_JOB_ISOLATED_LOOPS`
- `BG_JOB_SHUTDOWN_GRACE_PERIOD_SECS`
- `BG_JOB_TIMEOUT_SECS`
- `DD_API_KEY`
- `LANGCHAIN_TRACING_SAMPLING_RATE`
- `LANGGRAPH_AUTH_TYPE`
- `LANGGRAPH_POSTGRES_POOL_MAX_SIZE`
- `LANGSMITH_API_KEY`
- `LANGSMITH_ENDPOINT`
- `LANGSMITH_TRACING`

Source: https://docs.langchain.com/langsmith/env-var

The Agent Server supports specific environment variables for configuring a deployment.

## `BG_JOB_ISOLATED_LOOPS`

Set `BG_JOB_ISOLATED_LOOPS` to `True` to execute background runs in an isolated event loop separate from the serving API event loop.

This environment variable should be set to `True` if the implementation of a graph/node contains synchronous code. In this situation, the synchronous code will block the serving API event loop, which may cause the API to be unavailable. A symptom of an unavailable API is continuous application restarts due to failing health checks.

## `BG_JOB_SHUTDOWN_GRACE_PERIOD_SECS`

Specifies, in seconds, how long the server will wait for background jobs to finish after the queue receives a shutdown signal. After this period, the server will force termination. Defaults to `180` seconds. Set this to ensure jobs have enough time to complete cleanly during shutdown. Added in `langgraph-api==0.2.16`.

## `BG_JOB_TIMEOUT_SECS`

The timeout of a background run can be increased. However, the infrastructure for a Cloud deployment enforces a 1 hour timeout limit for API requests. This means the connection between client and server will timeout after 1 hour. This is not configurable.

A background run can execute for longer than 1 hour, but a client must reconnect to the server (e.g. join stream via `POST /threads/{thread_id}/runs/{run_id}/stream`) to retrieve output from the run if the run is taking longer than 1 hour.

Specify `DD_API_KEY` (your [Datadog API Key](https://docs.datadoghq.com/account_management/api-app-keys/)) to automatically enable Datadog tracing for the deployment. Specify other [`DD_*` environment variables](https://ddtrace.readthedocs.io/en/stable/configuration.html) to configure the tracing instrumentation.

If `DD_API_KEY` is specified, the application process is wrapped in the [`ddtrace-run` command](https://ddtrace.readthedocs.io/en/stable/installation_quickstart.html). Other `DD_*` environment variables (e.g. `DD_SITE`, `DD_ENV`, `DD_SERVICE`, `DD_TRACE_ENABLED`) are typically needed to properly configure the tracing instrumentation. See [`DD_*` environment variables](https://ddtrace.readthedocs.io/en/stable/configuration.html) for more details. You can enable `DD_TRACE_DEBUG=true` and set `DD_LOG_LEVEL=debug` to troubleshoot.

<Note>
  Enabling `DD_API_KEY` (and thus `ddtrace-run`) can override or interfere with other auto-instrumentation solutions (such as OpenTelemetry) that you may have instrumented into your application code.
</Note>

## `LANGCHAIN_TRACING_SAMPLING_RATE`

Sampling rate for traces sent to LangSmith. Valid values: Any float between `0` and `1`.

For more details, refer to [Set a sampling rate for traces](/langsmith/sample-traces).

## `LANGGRAPH_AUTH_TYPE`

Type of authentication for the Agent Server deployment. Valid values: `langsmith`, `noop`.

For deployments to LangSmith, this environment variable is set automatically. For local development or deployments where authentication is handled externally (e.g. self-hosted), set this environment variable to `noop`.

## `LANGGRAPH_POSTGRES_POOL_MAX_SIZE`

Beginning with langgraph-api version `0.2.12`, the maximum size of the Postgres connection pool (per replica) can be controlled using the `LANGGRAPH_POSTGRES_POOL_MAX_SIZE` environment variable. By setting this variable, you can determine the upper bound on the number of simultaneous connections the server will establish with the Postgres database.

For example, if a deployment is scaled up to 10 replicas and `LANGGRAPH_POSTGRES_POOL_MAX_SIZE` is configured to `150`, then up to `1500` connections to Postgres can be established. This is particularly useful for deployments where database resources are limited (or more available) or where you need to tune connection behavior for performance or scaling reasons.

Defaults to `150` connections.

## `LANGSMITH_API_KEY`

For deployments with [self-hosted LangSmith](/langsmith/self-hosted) only.

To send traces to a self-hosted LangSmith instance, set `LANGSMITH_API_KEY` to an API key created from the self-hosted instance.

## `LANGSMITH_ENDPOINT`

For deployments with [self-hosted LangSmith](/langsmith/self-hosted) only.

To send traces to a self-hosted LangSmith instance, set `LANGSMITH_ENDPOINT` to the hostname of the self-hosted instance.

## `LANGSMITH_TRACING`

Set `LANGSMITH_TRACING` to `false` to disable tracing to LangSmith.

This is mainly relevant in the context of using the dev server via the `langgraph dev` command. Set `LOG_COLOR` to `true` to enable ANSI-colored console output when using the default console renderer. Disabling color output by setting this variable to `false` produces monochrome logs. Defaults to `true`.

Configure [log level](https://docs.python.org/3/library/logging.html#logging-levels). Defaults to `INFO`.

Set `LOG_JSON` to `true` to render all log messages as JSON objects using the configured `JSONRenderer`. This produces structured logs that can be easily parsed or ingested by log management systems. Defaults to `false`.

<Info>
  **Only Allowed in Self-Hosted Deployments**
  The `MOUNT_PREFIX` environment variable is only allowed in Self-Hosted Deployment models, LangSmith SaaS will not allow this environment variable.
</Info>

Set `MOUNT_PREFIX` to serve the Agent Server under a specific path prefix. This is useful for deployments where the server is behind a reverse proxy or load balancer that requires a specific path prefix.

For example, if the server is to be served under `https://example.com/langgraph`, set `MOUNT_PREFIX` to `/langgraph`.

## `N_JOBS_PER_WORKER`

Number of jobs per worker for the Agent Server task queue. Defaults to `10`.

## `POSTGRES_URI_CUSTOM`

<Info>
  **Only for Hybrid and Self-Hosted**
  Custom Postgres instances are only available for [Hybrid](/langsmith/hybrid) and [Self-Hosted](/langsmith/self-hosted) deployments.
</Info>

Specify `POSTGRES_URI_CUSTOM` to use a custom Postgres instance. The value of `POSTGRES_URI_CUSTOM` must be a valid [Postgres connection URI](https://www.postgresql.org/docs/current/libpq-connect.html#LIBPQ-CONNSTRING-URIS).

* Version 15.8 or higher.
* An initial database must be present and the connection URI must reference the database.

Control Plane Functionality:

* If `POSTGRES_URI_CUSTOM` is specified, the control plane will not provision a database for the server.
* If `POSTGRES_URI_CUSTOM` is removed, the control plane will not provision a database for the server and will not delete the externally managed Postgres instance.
* If `POSTGRES_URI_CUSTOM` is removed, deployment of the revision will not succeed. Once `POSTGRES_URI_CUSTOM` is specified, it must always be set for the lifecycle of the deployment.
* If the deployment is deleted, the control plane will not delete the externally managed Postgres instance.
* The value of `POSTGRES_URI_CUSTOM` can be updated. For example, a password in the URI can be updated.

Database Connectivity:

* The custom Postgres instance must be accessible by the Agent Server. The user is responsible for ensuring connectivity.

<Warning>
  This feature is in Alpha.
</Warning>

<Info>
  **Only Allowed in Self-Hosted Deployments**
  Redis Cluster mode is only available in Self-Hosted Deployment models, LangSmith SaaS will provision a redis instance for you by default.
</Info>

Set `REDIS_CLUSTER` to `True` to enable Redis Cluster mode. When enabled, the system will connect to Redis using cluster mode. This is useful when connecting to a Redis Cluster deployment.

## `REDIS_KEY_PREFIX`

<Info>
  **Available in API Server version 0.1.9+**
  This environment variable is supported in API Server version 0.1.9 and above.
</Info>

Specify a prefix for Redis keys. This allows multiple Agent Server instances to share the same Redis instance by using different key prefixes.

## `REDIS_URI_CUSTOM`

<Info>
  **Only for Hybrid and Self-Hosted**
  Custom Redis instances are only available for [Hybrid](/langsmith/hybrid) and [Self-Hosted](/langsmith/self-hosted) deployments.
</Info>

Specify `REDIS_URI_CUSTOM` to use a custom Redis instance. The value of `REDIS_URI_CUSTOM` must be a valid [Redis connection URI](https://redis-py.readthedocs.io/en/stable/connections.html#redis.Redis.from_url).

## `REDIS_MAX_CONNECTIONS`

The maximum size of the Redis connection pool (per replica) can be controlled using the `REDIS_MAX_CONNECTIONS` environment variable. By setting this variable, you can determine the upper bound on the number of simultaneous connections the server will establish with the Redis instance.

For example, if a deployment is scaled up to 10 replicas and `REDIS_MAX_CONNECTIONS` is configured to `150`, then up to `1500` connections to Redis can be established.

## `RESUMABLE_STREAM_TTL_SECONDS`

Time-to-live in seconds for resumable stream data in Redis.

When a run is created and the output is streamed, the stream can be configured to be resumable (e.g. `stream_resumable=True`). If a stream is resumable, output from the stream is temporarily stored in Redis. The TTL for this data can be configured by setting `RESUMABLE_STREAM_TTL_SECONDS`.

See the [Python](/langsmith/langgraph-python-sdk#langgraph_sdk.client.RunsClient.stream) and [JS/TS](https://langchain-ai.github.io/langgraphjs/reference/classes/sdk_client.RunsClient.html#stream) SDKs for more details on how to implement resumable streams.

Defaults to `120` seconds.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/env-var.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Errors

**URL:** llms-txt#errors

Source: https://docs.langchain.com/oss/javascript/common-errors

This page contains guides around resolving common errors you may find while building with LangChain and LangGraph.

Errors referenced below will have an `lc_error_code` property corresponding to one of the below codes when they are thrown in code.

| Error code                                                                                              |
| ------------------------------------------------------------------------------------------------------- |
| [GRAPH\_RECURSION\_LIMIT](/oss/javascript/langgraph/errors/GRAPH_RECURSION_LIMIT)                       |
| [INVALID\_CHAT\_HISTORY](/oss/javascript/langgraph/errors/INVALID_CHAT_HISTORY)                         |
| [INVALID\_CONCURRENT\_GRAPH\_UPDATE](/oss/javascript/langgraph/errors/INVALID_CONCURRENT_GRAPH_UPDATE)  |
| [INVALID\_GRAPH\_NODE\_RETURN\_VALUE](/oss/javascript/langgraph/errors/INVALID_GRAPH_NODE_RETURN_VALUE) |
| [INVALID\_PROMPT\_INPUT](/oss/javascript/langchain/errors/INVALID_PROMPT_INPUT)                         |
| [INVALID\_TOOL\_RESULTS](/oss/javascript/langchain/errors/INVALID_TOOL_RESULTS)                         |
| [MESSAGE\_COERCION\_FAILURE](/oss/javascript/langchain/errors/MESSAGE_COERCION_FAILURE)                 |
| [MISSING\_CHECKPOINTER](/oss/javascript/langgraph/errors/MISSING_CHECKPOINTER)                          |
| [MODEL\_AUTHENTICATION](/oss/javascript/langchain/errors/MODEL_AUTHENTICATION)                          |
| [MODEL\_NOT\_FOUND](/oss/javascript/langchain/errors/MODEL_NOT_FOUND)                                   |
| [MODEL\_RATE\_LIMIT](/oss/javascript/langchain/errors/MODEL_RATE_LIMIT)                                 |
| [MULTIPLE\_SUBGRAPHS](/oss/javascript/langgraph/errors/MULTIPLE_SUBGRAPHS)                              |
| [OUTPUT\_PARSING\_FAILURE](/oss/javascript/langchain/errors/OUTPUT_PARSING_FAILURE)                     |

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/common-errors.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Evaluate a chatbot

**URL:** llms-txt#evaluate-a-chatbot

**Contents:**
- Setup
- Create a dataset

Source: https://docs.langchain.com/langsmith/evaluate-chatbot-tutorial

In this guide we will set up evaluations for a chatbot. These allow you to measure how well your application is performing over a set of data. Being able to get this insight quickly and reliably will allow you to iterate with confidence.

At a high level, in this tutorial we will:

* *Create an initial golden dataset to measure performance*
* *Define metrics to use to measure performance*
* *Run evaluations on a few different prompts or models*
* *Compare results manually*
* *Track results over time*
* *Set up automated testing to run in CI/CD*

For more information on the evaluation workflows LangSmith supports, check out the [how-to guides](/langsmith/evaluation), or see the reference docs for [evaluate](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._runner.evaluate) and its asynchronous [aevaluate](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._arunner.aevaluate) counterpart.

Lots to cover, let's dive in!

First install the required dependencies for this tutorial. We happen to use OpenAI, but LangSmith can be used with any model:

And set environment variables to enable LangSmith tracing:

The first step when getting ready to test and evaluate your application is to define the datapoints you want to evaluate. There are a few aspects to consider here:

* What should the schema of each datapoint be?
* How many datapoints should I gather?
* How should I gather those datapoints?

**Schema:** Each datapoint should consist of, at the very least, the inputs to the application. If you are able, it is also very helpful to define the expected outputs - these represent what you would expect a properly functioning application to output. Often times you cannot define the perfect output - that's okay! Evaluation is an iterative process. Sometimes you may also want to define more information for each example - like the expected documents to fetch in RAG, or the expected steps to take as an agent. LangSmith datasets are very flexible and allow you to define arbitrary schemas.

**How many:** There's no hard and fast rule for how many you should gather. The main thing is to make sure you have proper coverage of edge cases you may want to guard against. Even 10-50 examples can provide a lot of value! Don't worry about getting a large number to start - you can (and should) always add over time!

**How to get:** This is maybe the trickiest part. Once you know you want to gather a dataset... how do you actually go about it? For most teams that are starting a new project, we generally see them start by collecting the first 10-20 datapoints by hand. After starting with these datapoints, these datasets are generally *living* constructs and grow over time. They generally grow after seeing how real users will use your application, seeing the pain points that exist, and then moving a few of those datapoints into this set. There are also methods like synthetically generating data that can be used to augment your dataset. To start, we recommend not worrying about those and just hand labeling \~10-20 examples.

Once you've got your dataset, there are a few different ways to upload them to LangSmith. For this tutorial, we will use the client, but you can also upload via the UI (or even create them in the UI).

For this tutorial, we will create 5 datapoints to evaluate on. We will be evaluating a question-answering application. The input will be a question, and the output will be an answer. Since this is a question-answering application, we can define the expected answer. Let's show how to create and upload this dataset to LangSmith!

```python  theme={null}
from langsmith import Client

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

And set environment variables to enable LangSmith tracing:
```

Example 3 (unknown):
```unknown
## Create a dataset

The first step when getting ready to test and evaluate your application is to define the datapoints you want to evaluate. There are a few aspects to consider here:

* What should the schema of each datapoint be?
* How many datapoints should I gather?
* How should I gather those datapoints?

**Schema:** Each datapoint should consist of, at the very least, the inputs to the application. If you are able, it is also very helpful to define the expected outputs - these represent what you would expect a properly functioning application to output. Often times you cannot define the perfect output - that's okay! Evaluation is an iterative process. Sometimes you may also want to define more information for each example - like the expected documents to fetch in RAG, or the expected steps to take as an agent. LangSmith datasets are very flexible and allow you to define arbitrary schemas.

**How many:** There's no hard and fast rule for how many you should gather. The main thing is to make sure you have proper coverage of edge cases you may want to guard against. Even 10-50 examples can provide a lot of value! Don't worry about getting a large number to start - you can (and should) always add over time!

**How to get:** This is maybe the trickiest part. Once you know you want to gather a dataset... how do you actually go about it? For most teams that are starting a new project, we generally see them start by collecting the first 10-20 datapoints by hand. After starting with these datapoints, these datasets are generally *living* constructs and grow over time. They generally grow after seeing how real users will use your application, seeing the pain points that exist, and then moving a few of those datapoints into this set. There are also methods like synthetically generating data that can be used to augment your dataset. To start, we recommend not worrying about those and just hand labeling \~10-20 examples.

Once you've got your dataset, there are a few different ways to upload them to LangSmith. For this tutorial, we will use the client, but you can also upload via the UI (or even create them in the UI).

For this tutorial, we will create 5 datapoints to evaluate on. We will be evaluating a question-answering application. The input will be a question, and the output will be an answer. Since this is a question-answering application, we can define the expected answer. Let's show how to create and upload this dataset to LangSmith!
```

---

## Evaluate a complex agent

**URL:** llms-txt#evaluate-a-complex-agent

**Contents:**
- Setup
  - Configure the environment
  - Download the database

Source: https://docs.langchain.com/langsmith/evaluate-complex-agent

<Info>
  [Agent evaluation](/langsmith/evaluation-concepts#agents) | [Evaluators](/langsmith/evaluation-concepts#evaluators) | [LLM-as-judge evaluators](/langsmith/evaluation-concepts#llm-as-judge)
</Info>

In this tutorial, we'll build a customer support bot that helps users navigate a digital music store. Then, we'll go through the three most effective types of evaluations to run on chat bots:

* [Final response](/langsmith/evaluation-concepts#evaluating-an-agents-final-response): Evaluate the agent's final response.
* [Trajectory](/langsmith/evaluation-concepts#evaluating-an-agents-trajectory): Evaluate whether the agent took the expected path (e.g., of tool calls) to arrive at the final answer.
* [Single step](/langsmith/evaluation-concepts#evaluating-a-single-step-of-an-agent): Evaluate any agent step in isolation (e.g., whether it selects the appropriate first tool for a given step).

We'll build our agent using [LangGraph](https://github.com/langchain-ai/langgraph), but the techniques and LangSmith functionality shown here are framework-agnostic.

### Configure the environment

Let's install the required dependencies:

Let's set up environment variables for OpenAI and [LangSmith](https://smith.langchain.com):

### Download the database

We will create a SQLite database for this tutorial. SQLite is a lightweight database that is easy to set up and use. We will load the `chinook` database, which is a sample database that represents a digital media store. Find more information about the database [here](https://www.sqlitetutorial.net/sqlite-sample-database/).

For convenience, we have hosted the database in a public GCS bucket:

Here's a sample of the data in the db:

```python  theme={null}
import sqlite3

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

Let's set up environment variables for OpenAI and [LangSmith](https://smith.langchain.com):
```

Example 3 (unknown):
```unknown
### Download the database

We will create a SQLite database for this tutorial. SQLite is a lightweight database that is easy to set up and use. We will load the `chinook` database, which is a sample database that represents a digital media store. Find more information about the database [here](https://www.sqlitetutorial.net/sqlite-sample-database/).

For convenience, we have hosted the database in a public GCS bucket:
```

Example 4 (unknown):
```unknown
Here's a sample of the data in the db:
```

---

## Evaluate a RAG application

**URL:** llms-txt#evaluate-a-rag-application

**Contents:**
- Overview
- Setup
  - Environment
  - Application
- Dataset
- Evaluators
  - Correctness: Response vs reference answer
  - Relevance: Response vs input
  - Groundedness: Response vs retrieved docs
  - Retrieval relevance: Retrieved docs vs input

Source: https://docs.langchain.com/langsmith/evaluate-rag-tutorial

<Info>
  [RAG evaluation](/langsmith/evaluation-concepts#retrieval-augmented-generation-rag) | [Evaluators](/langsmith/evaluation-concepts#evaluators) | [LLM-as-judge evaluators](/langsmith/evaluation-concepts#llm-as-judge)
</Info>

Retrieval Augmented Generation (RAG) is a technique that enhances Large Language Models (LLMs) by providing them with relevant external knowledge. It has become one of the most widely used approaches for building LLM applications.

This tutorial will show you how to evaluate your RAG applications using LangSmith. You'll learn:

1. How to create test datasets
2. How to run your RAG application on those datasets
3. How to measure your application's performance using different evaluation metrics

A typical RAG evaluation workflow consists of three main steps:

1. Creating a dataset with questions and their expected answers

2. Running your RAG application on those questions

3. Using evaluators to measure how well your application performed, looking at factors like:

* Answer relevance
   * Answer accuracy
   * Retrieval quality

For this tutorial, we'll create and evaluate a bot that answers questions about a few of [Lilian Weng's](https://lilianweng.github.io/) insightful blog posts.

First, let's set our environment variables:

And install the dependencies we'll need:

<Info>
  While this tutorial uses LangChain, the evaluation techniques and LangSmith functionality demonstrated here work with any framework. Feel free to use your preferred tools and libraries.
</Info>

In this section, we'll build a basic Retrieval-Augmented Generation (RAG) application.

We'll stick to a simple implementation that:

* Indexing: chunks and indexes a few of Lilian Weng's blogs in a vector store
* Retrieval: retrieves those chunks based on the user question
* Generation: passes the question and retrieved docs to an LLM.

#### Indexing and retrieval

First, lets load the blog posts we want to build a chatbot for and index them.

We can now define the generative pipeline.

Now that we've got our application, let's build a dataset to evaluate it. Our dataset will be very simple in this case: we'll have example questions and reference answers.

One way to think about different types of RAG evaluators is as a tuple of what is being evaluated X what its being evaluated against:

1. **Correctness**: Response vs reference answer

* `Goal`: Measure "*how similar/correct is the RAG chain answer, relative to a ground-truth answer*"
* `Mode`: Requires a ground truth (reference) answer supplied through a dataset
* `Evaluator`: Use LLM-as-judge to assess answer correctness.

2. **Relevance**: Response vs input

* `Goal`: Measure "*how well does the generated response address the initial user input*"
* `Mode`: Does not require reference answer, because it will compare the answer to the input question
* `Evaluator`: Use LLM-as-judge to assess answer relevance, helpfulness, etc.

3. **Groundedness**: Response vs retrieved docs

* `Goal`: Measure "*to what extent does the generated response agree with the retrieved context*"
* `Mode`: Does not require reference answer, because it will compare the answer to the retrieved context
* `Evaluator`: Use LLM-as-judge to assess faithfulness, hallucinations, etc.

4. **Retrieval relevance**: Retrieved docs vs input

* `Goal`: Measure "*how relevant are my retrieved results for this query*"
* `Mode`: Does not require reference answer, because it will compare the question to the retrieved context
* `Evaluator`: Use LLM-as-judge to assess relevance

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rag-eval-overview.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=6f303c2a284296b42e7d2d2e658f5171" alt="" data-og-width="1252" width="1252" data-og-height="547" height="547" data-path="langsmith/images/rag-eval-overview.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rag-eval-overview.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=b5531116cdd61ca9e8ea6fcd760643db 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rag-eval-overview.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=8f83816ac849c05dd8d3dee4c9670729 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rag-eval-overview.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=8656d8f8af7ffa7f2684376cf2f70874 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rag-eval-overview.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=4299367332fbefd15e938aefc23ca6fe 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rag-eval-overview.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=0bc19645279f119031a5cb8ca32f2f7d 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rag-eval-overview.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=185b7c4ba4f4127f780d5fa17b96c752 2500w" />

### Correctness: Response vs reference answer

### Relevance: Response vs input

The flow is similar to above, but we simply look at the `inputs` and `outputs` without needing the `reference_outputs`. Without a reference answer we can't grade accuracy, but can still grade relevance—as in, did the model address the user's question or not.

### Groundedness: Response vs retrieved docs

Another useful way to evaluate responses without needing reference answers is to check if the response is justified by (or "grounded in") the retrieved documents.

### Retrieval relevance: Retrieved docs vs input

We can now kick off our evaluation job with all of our different evaluators.

You can see an example of what these results look like here: [LangSmith link](https://smith.langchain.com/public/302573e2-20bf-4f8c-bdad-e97c20f33f1b/d)

<Accordion title="Here's a consolidated script with all the above code:">
  <CodeGroup>

</CodeGroup>
</Accordion>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/evaluate-rag-tutorial.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

And install the dependencies we'll need:

<CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>

### Application

<Info>
  While this tutorial uses LangChain, the evaluation techniques and LangSmith functionality demonstrated here work with any framework. Feel free to use your preferred tools and libraries.
</Info>

In this section, we'll build a basic Retrieval-Augmented Generation (RAG) application.

We'll stick to a simple implementation that:

* Indexing: chunks and indexes a few of Lilian Weng's blogs in a vector store
* Retrieval: retrieves those chunks based on the user question
* Generation: passes the question and retrieved docs to an LLM.

#### Indexing and retrieval

First, lets load the blog posts we want to build a chatbot for and index them.

<CodeGroup>
```

---

## Evaluation Concepts

**URL:** llms-txt#evaluation-concepts

**Contents:**
- Datasets
  - Examples
  - Dataset curation
  - Splits
  - Versions
- Evaluators
  - Human
  - Heuristic
  - LLM-as-judge
  - Pairwise

Source: https://docs.langchain.com/langsmith/evaluation-concepts

LangSmith makes building high-quality evaluations easy. This guide explains the key concepts of the LangSmith evaluation framework. The building blocks of the LangSmith framework are:

* [**Datasets**:](/langsmith/evaluation-concepts#datasets) Collections of test inputs and reference outputs.
* [**Evaluators**](/langsmith/evaluation-concepts#evaluators): Functions for scoring outputs. These can be [online evaluators](/langsmith/evaluation-concepts#online-evaluation) that run on traces in real time or [offline evaluators](/langsmith/evaluation-concepts#offline-evaluation) that run on a dataset.

A dataset is a collection of examples used for evaluating an application. An example is a test input, reference output pair.

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/dataset-concept.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=be2adaa8535dbb253bc0f199895da2e1" alt="Dataset" data-og-width="1279" width="1279" data-og-height="495" height="495" data-path="langsmith/images/dataset-concept.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/dataset-concept.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=b86cc15fe64ec6fdac0a14472b84c1e0 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/dataset-concept.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=b388730259225fbf466b9a8682303330 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/dataset-concept.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=bf8930b4598fbc88212e6e3dde2d0950 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/dataset-concept.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=10220cebf437b481fd655d8e1ddca492 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/dataset-concept.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=16e3d46679352d87e6358acc155d0bd9 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/dataset-concept.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=8b987fee69c354d78e7182a685ac38ed 2500w" />

Each example consists of:

* **Inputs**: a dictionary of input variables to pass to your application.
* **Reference outputs** (optional): a dictionary of reference outputs. These do not get passed to your application, they are only used in evaluators.
* **Metadata** (optional): a dictionary of additional information that can be used to create filtered views of a dataset.

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/example-concept.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=0c25674bd30e502e3034b754b8649d66" alt="Example" data-og-width="1281" width="1281" data-og-height="406" height="406" data-path="langsmith/images/example-concept.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/example-concept.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=3f49d9e298b397039ae97266ad8eda6e 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/example-concept.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=eb14858aac0f28b26cb1e6ee9ca920f4 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/example-concept.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=ae707f4b153d7c6e77215012018483c7 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/example-concept.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=9791ddf51677e2e4f046f32e46304e3d 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/example-concept.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=02b7df4950f45c8d4460b0b0cfad88c0 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/example-concept.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=db7340730c6025b03bcb96339fdc980e 2500w" />

There are various ways to build datasets for evaluation, including:

#### Manually curated examples

This is how we typically recommend people get started creating datasets. From building your application, you probably have some idea of what types of inputs you expect your application to be able to handle, and what "good" responses may be. You probably want to cover a few different common edge cases or situations you can imagine. Even 10-20 high-quality, manually-curated examples can go a long way.

#### Historical traces

Once you have an application in production, you start getting valuable information: how are users actually using it? These real-world runs make for great examples because they're, well, the most realistic!

If you're getting a lot of traffic, how can you determine which runs are valuable to add to a dataset? There are a few techniques you can use:

* **User feedback**: If possible - try to collect end user feedback. You can then see which datapoints got negative feedback. That is super valuable! These are spots where your application did not perform well. You should add these to your dataset to test against in the future.
* **Heuristics**: You can also use other heuristics to identify "interesting" datapoints. For example, runs that took a long time to complete could be interesting to look at and add to a dataset.
* **LLM feedback**: You can use another LLM to detect noteworthy runs. For example, you could use an LLM to label chatbot conversations where the user had to rephrase their question or correct the model in some way, indicating the chatbot did not initially respond correctly.

Once you have a few examples, you can try to artificially generate some more. It's generally advised to have a few good hand-crafted examples before this, as this synthetic data will often resemble them in some way. This can be a useful way to get a lot of datapoints, quickly.

When setting up your evaluation, you may want to partition your dataset into different splits. For example, you might use a smaller split for many rapid and cheap iterations and a larger split for your final evaluation. In addition, splits can be important for the interpretability of your experiments. For example, if you have a RAG application, you may want your dataset splits to focus on different types of questions (e.g., factual, opinion, etc) and to evaluate your application on each split separately.

Learn how to [create and manage dataset splits](/langsmith/manage-datasets-in-application#create-and-manage-dataset-splits).

Datasets are [versioned](/langsmith/manage-datasets#version-a-dataset) such that every time you add, update, or delete examples in your dataset, a new version of the dataset is created. This makes it easy to inspect and revert changes to your dataset in case you make a mistake. You can also [tag versions](/langsmith/manage-datasets#tag-a-version) of your dataset to give them a more human-readable name. This can be useful for marking important milestones in your dataset's history.

You can run evaluations on specific versions of a dataset. This can be useful when running evaluations in CI, to make sure that a dataset update doesn't accidentally break your CI pipelines.

Evaluators are functions that score how well your application performs on a particular example.

#### Evaluator inputs

Evaluators receive these inputs:

* [Example](/langsmith/evaluation-concepts#examples): The example(s) from your [Dataset](/langsmith/evaluation-concepts#datasets). Contains inputs, (reference) outputs, and metadata.
* [Run](/langsmith/observability-concepts#runs): The actual outputs and intermediate steps (child runs) from passing the example inputs to the application.

#### Evaluator outputs

An evaluator returns one or more metrics. These should be returned as a dictionary or list of dictionaries of the form:

* `key`: The name of the metric.
* `score` | `value`: The value of the metric. Use `score` if it's a numerical metric and `value` if it's categorical.
* `comment` (optional): The reasoning or additional string information justifying the score.

#### Defining evaluators

There are a number of ways to define and run evaluators:

* **Custom code**: Define [custom evaluators](/langsmith/code-evaluator) as Python or TypeScript functions and run them client-side using the SDKs or server-side via the UI.
* **Built-in evaluators**: LangSmith has a number of built-in evaluators that you can configure and run via the UI.

You can run evaluators using the LangSmith SDK ([Python](https://docs.smith.langchain.com/reference/python/reference) and [TypeScript](https://docs.smith.langchain.com/reference/js)), via the [Prompt Playground](/langsmith/observability-concepts#prompt-playground), or by configuring [Rules](/langsmith/rules) to automatically run them on particular tracing projects or datasets.

#### Evaluation techniques

There are a few high-level approaches to LLM evaluation:

Human evaluation is [often a great starting point for evaluation](https://hamel.dev/blog/posts/evals/#looking-at-your-traces). LangSmith makes it easy to review your LLM application outputs as well as the traces (all intermediate steps).

LangSmith's [annotation queues](/langsmith/evaluation-concepts#annotation-queues) make it easy to get human feedback on your application's outputs.

Heuristic evaluators are deterministic, rule-based functions. These are good for simple checks like making sure that a chatbot's response isn't empty, that a snippet of generated code can be compiled, or that a classification is exactly correct.

LLM-as-judge evaluators use LLMs to score the application's output. To use them, you typically encode the grading rules / criteria in the LLM prompt. They can be reference-free (e.g., check if system output contains offensive content or adheres to specific criteria). Or, they can compare task output to a reference output (e.g., check if the output is factually accurate relative to the reference).

With LLM-as-judge evaluators, it is important to carefully review the resulting scores and tune the grader prompt if needed. Often it is helpful to write these as few-shot evaluators, where you provide examples of inputs, outputs, and expected grades as part of the grader prompt.

Learn about [how to define an LLM-as-a-judge evaluator](/langsmith/llm-as-judge).

Pairwise evaluators allow you to compare the outputs of two versions of an application. This can use either a heuristic ("which response is longer"), an LLM (with a specific pairwise prompt), or human (asking them to manually annotate examples).

**When should you use pairwise evaluation?**

Pairwise evaluation is helpful when it is difficult to directly score an LLM output, but easier to compare two outputs. This can be the case for tasks like summarization - it may be hard to give a summary an absolute score, but easy to choose which of two summaries is more informative.

Learn [how run pairwise evaluations](/langsmith/evaluate-pairwise).

Each time we evaluate an application on a dataset, we are conducting an experiment. An experiment contains the results of running a specific version of your application on the dataset. To understand how to use the LangSmith experiment view, see [how to analyze experiment results](/langsmith/analyze-an-experiment).

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/experiment-view.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=89c78822157136d0e28e9a110dbdbfd5" alt="Experiment view" data-og-width="1633" width="1633" data-og-height="942" height="942" data-path="langsmith/images/experiment-view.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/experiment-view.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=f911d77a92d1fb020d6ddea937bd224e 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/experiment-view.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=11c80e9c9aad7df87ea97a540170809a 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/experiment-view.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=5b3722ed4fa19b7fcd7d73454dfd342a 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/experiment-view.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=97d5e419a79137652aeac9139473414a 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/experiment-view.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=4a79633866510d7f8ea1d92fa6f58138 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/experiment-view.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=53012b35d0f323587d5956b3a800818b 2500w" />

Typically, we will run multiple experiments on a given dataset, testing different configurations of our application (e.g., different prompts or LLMs). In LangSmith, you can easily view all the experiments associated with your dataset. Additionally, you can [compare multiple experiments in a comparison view](/langsmith/compare-experiment-results).

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/comparison-view.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=c4ed751dc3e74f5ad16634dff061bd77" alt="Comparison view" data-og-width="3018" width="3018" data-og-height="1532" height="1532" data-path="langsmith/images/comparison-view.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/comparison-view.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=4b6c96f4480262f4db39b7a06a971ca8 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/comparison-view.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=568b65404d8bbe7444d742fbf131bb4d 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/comparison-view.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=36c750c22fcc302d617f1e6e8a5014a0 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/comparison-view.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=fe9225cdcf381e2b6ef17c0bf99b3004 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/comparison-view.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=ac42818b24a858252bb320a23e06b1c2 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/comparison-view.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=bd4214772a68d401b48b704b56a35e4a 2500w" />

## Experiment configuration

LangSmith supports a number of experiment configurations which make it easier to run your evals in the manner you want.

Running an experiment multiple times can be helpful since LLM outputs are not deterministic and can differ from one repetition to the next. By running multiple repetitions, you can get a more accurate estimate of the performance of your system.

Repetitions can be configured by passing the `num_repetitions` argument to `evaluate` / `aevaluate` ([Python](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._runner.evaluate), [TypeScript](https://docs.smith.langchain.com/reference/js/interfaces/evaluation.EvaluateOptions#numrepetitions)). Repeating the experiment involves both re-running the target function to generate outputs and re-running the evaluators.

To learn more about running repetitions on experiments, read the [how-to-guide](/langsmith/repetition).

By passing the `max_concurrency` argument to `evaluate` / `aevaluate`, you can specify the concurrency of your experiment. The `max_concurrency` argument has slightly different semantics depending on whether you are using `evaluate` or `aevaluate`.

The `max_concurrency` argument to `evaluate` specifies the maximum number of concurrent threads to use when running the experiment. This is both for when running your target function as well as your evaluators.

The `max_concurrency` argument to `aevaluate` is fairly similar to `evaluate`, but instead uses a semaphore to limit the number of concurrent tasks that can run at once. `aevaluate` works by creating a task for each example in the dataset. Each task consists of running the target function as well as all of the evaluators on that specific example. The `max_concurrency` argument specifies the maximum number of concurrent tasks, or put another way - examples, to run at once.

Lastly, you can also cache the API calls made in your experiment by setting the `LANGSMITH_TEST_CACHE` to a valid folder on your device with write access. This will cause the API calls made in your experiment to be cached to disk, meaning future experiments that make the same API calls will be greatly sped up.

Human feedback is often the most valuable feedback you can gather on your application. With [annotation queues](/langsmith/annotation-queues) you can flag runs of your application for annotation. Human annotators then have a streamlined view to review and provide feedback on the runs in a queue. Often (some subset of) these annotated runs are then transferred to a [dataset](/langsmith/evaluation-concepts#datasets) for future evaluations. While you can always [annotate runs inline](/langsmith/annotate-traces-inline), annotation queues provide another option to group runs together, specify annotation criteria, and configure permissions.

Learn more about [annotation queues and human feedback](/langsmith/annotation-queues).

## Offline evaluation

Evaluating an application on a dataset is what we call "offline" evaluation. It is offline because we're evaluating on a pre-compiled set of data. An online evaluation, on the other hand, is one in which we evaluate a deployed application's outputs on real traffic, in near realtime. Offline evaluations are used for testing a version(s) of your application pre-deployment.

You can run offline evaluations client-side using the LangSmith SDK ([Python](https://docs.smith.langchain.com/reference/python/reference) and [TypeScript](https://docs.smith.langchain.com/reference/js)). You can run them server-side via the [Prompt Playground](/langsmith/observability-concepts#prompt-playground) or by configuring [automations](/langsmith/rules) to run certain evaluators on every new experiment against a specific dataset.

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/offline.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=879e4ee3616cecd7cff39879cfc6ec7b" alt="Offline" data-og-width="1581" width="1581" data-og-height="477" height="477" data-path="langsmith/images/offline.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/offline.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=ba01953933bebf30c6dc5d8112a3b3db 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/offline.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=3424f1efa82db871cba04c9a4bcac188 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/offline.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=ad0eca755f778a844465976b00a3efb6 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/offline.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=cddc86584ad7d9e82a60fc219cff886b 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/offline.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=e8cf5a07175523921ee1595e36ea1d73 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/offline.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=0f94c55d22273e06d6cde301b4a0a3f3 2500w" />

Perhaps the most common type of offline evaluation is one in which we curate a dataset of representative inputs, define the key performance metrics, and benchmark multiple versions of our application to find the best one. Benchmarking can be laborious because for many use cases you have to curate a dataset with gold-standard reference outputs and design good metrics for comparing experimental outputs to them. For a RAG Q\&A bot this might look like a dataset of questions and reference answers, and an LLM-as-judge evaluator that determines if the actual answer is semantically equivalent to the reference answer. For a ReACT agent this might look like a dataset of user requests and a reference set of all the tool calls the model is supposed to make, and a heuristic evaluator that checks if all of the reference tool calls were made.

Unit tests are used in software development to verify the correctness of individual system components. [Unit tests in the context of LLMs are often rule-based assertions](https://hamel.dev/blog/posts/evals/#level-1-unit-tests) on LLM inputs or outputs (e.g., checking that LLM-generated code can be compiled, JSON can be loaded, etc.) that validate basic functionality.

Unit tests are often written with the expectation that they should always pass. These types of tests are nice to run as part of CI. Note that when doing so it is useful to set up a cache to minimize LLM calls (because those can quickly rack up!).

Regression tests are used to measure performance across versions of your application over time. They are used to, at the very least, ensure that a new app version does not regress on examples that your current version correctly handles, and ideally to measure how much better your new version is relative to the current. Often these are triggered when you are making app updates (e.g. updating models or architectures) that are expected to influence the user experience.

LangSmith's comparison view has native support for regression testing, allowing you to quickly see examples that have changed relative to the baseline. Regressions are highlighted red, improvements green.

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/comparison-view.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=c4ed751dc3e74f5ad16634dff061bd77" alt="Comparison view" data-og-width="3018" width="3018" data-og-height="1532" height="1532" data-path="langsmith/images/comparison-view.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/comparison-view.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=4b6c96f4480262f4db39b7a06a971ca8 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/comparison-view.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=568b65404d8bbe7444d742fbf131bb4d 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/comparison-view.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=36c750c22fcc302d617f1e6e8a5014a0 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/comparison-view.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=fe9225cdcf381e2b6ef17c0bf99b3004 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/comparison-view.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=ac42818b24a858252bb320a23e06b1c2 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/comparison-view.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=bd4214772a68d401b48b704b56a35e4a 2500w" />

Backtesting is an approach that combines dataset creation (discussed above) with evaluation. If you have a collection of production logs, you can turn them into a dataset. Then, you can re-run those production examples with newer application versions. This allows you to assess performance on past and realistic user inputs.

This is commonly used to evaluate new model versions. Anthropic dropped a new model? No problem! Grab the 1000 most recent runs through your application and pass them through the new model. Then compare those results to what actually happened in production.

### Pairwise evaluation

For some tasks [it is easier](https://www.oreilly.com/radar/what-we-learned-from-a-year-of-building-with-llms-part-i/) for a human or LLM grader to determine if "version A is better than B" than to assign an absolute score to either A or B. Pairwise evaluations are just this — a scoring of the outputs of two versions against each other as opposed to against some reference output or absolute criteria. Pairwise evaluations are often useful when using LLM-as-judge evaluators on more general tasks. For example, if you have a summarizer application, it may be easier for an LLM-as-judge to determine "Which of these two summaries is more clear and concise?" than to give an absolute score like "Give this summary a score of 1-10 in terms of clarity and concision."

Learn [how run pairwise evaluations](/langsmith/evaluate-pairwise).

Evaluating a deployed application's outputs in (roughly) realtime is what we call "online" evaluation. In this case there is no dataset involved and no possibility of reference outputs — we're running evaluators on real inputs and real outputs as they're produced. This is useful for monitoring your application and flagging unintended behavior. Online evaluation can also work hand-in-hand with offline evaluation: for example, an online evaluator can be used to classify input questions into a set of categories that can later be used to curate a dataset for offline evaluation.

Online evaluators are generally intended to be run server-side. LangSmith has built-in [LLM-as-judge evaluators](/langsmith/llm-as-judge) that you can configure, or you can define custom code evaluators that are also run within LangSmith.

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/online.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=8d6c1b932e5487c4c01d84ae4f984240" alt="Online" data-og-width="1474" width="1474" data-og-height="521" height="521" data-path="langsmith/images/online.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/online.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=fd658bcfa1357196dd87ab6263a4896d 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/online.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=371e394c68e91e93efe1c80fb85d5484 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/online.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=5c65ce1c6487a959ce54bffcf8155bb8 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/online.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=b8e1815f6df4419f65294d2a658bc9f0 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/online.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=f737a486fdb5232e8db121a760075dd8 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/online.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=0ad2d3d938b8048c21f23f0052904940 2500w" />

### Evaluations vs testing

Testing and evaluation are very similar and overlapping concepts that often get confused.

**An evaluation measures performance according to a metric(s).** Evaluation metrics can be fuzzy or subjective, and are more useful in relative terms than absolute ones. That is, they're often used to compare two systems against each other rather than to assert something about an individual system.

**Testing asserts correctness.** A system can only be deployed if it passes all tests.

Evaluation metrics can be *turned into* tests. For example, you can write regression tests to assert that any new version of a system must outperform some baseline version of the system on the relevant evaluation metrics.

It can also be more resource efficient to run tests and evaluations together if your system is expensive to run and you have overlapping datasets for your tests and evaluations.

You can also choose to write evaluations using standard software testing tools like `pytest` or `vitest/jest` out of convenience.

### Using `pytest` and `Vitest/Jest`

The LangSmith SDKs come with integrations for [pytest](/langsmith/pytest) and [`Vitest/Jest`](/langsmith/vitest-jest). These make it easy to:

* Track test results in LangSmith
* Write evaluations as tests

Tracking test results in LangSmith makes it easy to share results, compare systems, and debug failing tests.

Writing evaluations as tests can be useful when each example you want to evaluate on requires custom logic for running the application and/or evaluators. The standard evaluation flows assume that you can run your application and evaluators in the same way on every example in a dataset. But for more complex systems or comprehensive evals, you may want to evaluate specific subsets of your system with specific types of inputs and metrics. These types of heterogenous evals are much easier to write as a suite of distinct test cases that all get tracked together rather than using the standard evaluate flow.

Using testing tools is also helpful when you want to *both* evaluate your system's outputs *and* assert some basic things about them.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/evaluation-concepts.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Evaluation job and results

**URL:** llms-txt#evaluation-job-and-results

**Contents:**
  - Trajectory evaluator
  - Single step evaluators

experiment_results = await client.aevaluate(
    run_graph,
    data=dataset_name,
    evaluators=[final_answer_correct],
    experiment_prefix="sql-agent-gpt4o-e2e",
    num_repetitions=1,
    max_concurrency=4,
)
experiment_results.to_pandas()
python  theme={null}
def trajectory_subsequence(outputs: dict, reference_outputs: dict) -> float:
    """Check how many of the desired steps the agent took."""
    if len(reference_outputs['trajectory']) > len(outputs['trajectory']):
        return False

i = j = 0
    while i < len(reference_outputs['trajectory']) and j < len(outputs['trajectory']):
        if reference_outputs['trajectory'][i] == outputs['trajectory'][j]:
            i += 1
        j += 1

return i / len(reference_outputs['trajectory'])
python  theme={null}
async def run_graph(inputs: dict) -> dict:
    """Run graph and track the trajectory it takes along with the final response."""
    trajectory = []
    # Set subgraph=True to stream events from subgraphs of the main graph: https://langchain-ai.github.io/langgraph/how-tos/streaming-subgraphs/
    # Set stream_mode="debug" to stream all possible events: https://langchain-ai.github.io/langgra/langsmith/observability-concepts/streaming
    async for namespace, chunk in graph.astream({"messages": [
            {
                "role": "user",
                "content": inputs['question'],
            }
        ]}, subgraphs=True, stream_mode="debug"):
        # Event type for entering a node
        if chunk['type'] == 'task':
            # Record the node name
            trajectory.append(chunk['payload']['name'])
            # Given how we defined our dataset, we also need to track when specific tools are
            # called by our question answering ReACT agent. These tool calls can be found
            # when the ToolsNode (named "tools") is invoked by looking at the AIMessage.tool_calls
            # of the latest input message.
            if chunk['payload']['name'] == 'tools' and chunk['type'] == 'task':
                for tc in chunk['payload']['input']['messages'][-1].tool_calls:
                    trajectory.append(tc['name'])
    return {"trajectory": trajectory}

experiment_results = await client.aevaluate(
    run_graph,
    data=dataset_name,
    evaluators=[trajectory_subsequence],
    experiment_prefix="sql-agent-gpt4o-trajectory",
    num_repetitions=1,
    max_concurrency=4,
)
experiment_results.to_pandas()
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
You can see what these results look like here: [LangSmith link](https://smith.langchain.com/public/708d08f4-300e-4c75-9677-c6b71b0d28c9/d).

### Trajectory evaluator

As agents become more complex, they have more potential points of failure. Rather than using simple pass/fail evaluations, it's often better to use evaluations that can give partial credit when an agent takes some correct steps, even if it doesn't reach the right final answer.

This is where trajectory evaluations come in. A trajectory evaluation:

1. Compares the actual sequence of steps the agent took against an expected sequence
2. Calculates a score based on how many of the expected steps were completed correctly

For this example, our end-to-end dataset contains an ordered list of steps that we expect the agent to take. Let's create an evaluator that checks the agent's actual trajectory against these expected steps and calculates what percentage were completed:
```

Example 2 (unknown):
```unknown
Now we can run our evaluation. Our evaluator assumes that our target function returns a 'trajectory' key, so lets define a target function that does so. We'll need to usage [LangGraph's streaming capabilities](https://langchain-ai.github.io/langgra/langsmith/observability-concepts/streaming/) to record the trajectory.

Note that we are reusing the same dataset as for our final response evaluation, so we could have run both evaluators together and defined a target function that returns both "response" and "trajectory". In practice it's often useful to have separate datasets for each type of evaluation, which is why we show them separately here:
```

Example 3 (unknown):
```unknown
You can see what these results look like here: [LangSmith link](https://smith.langchain.com/public/708d08f4-300e-4c75-9677-c6b71b0d28c9/d).

### Single step evaluators

While end-to-end tests give you the most signal about your agents performance, for the sake of debugging and iterating on your agent it can be helpful to pinpoint specific steps that are difficult and evaluate them directly.

In our case, a crucial part of our agent is that it routes the user's intention correctly into either the "refund" path or the "question answering" path. Let's create a dataset and run some evaluations to directly stress test this one component.
```

---

## Evaluation quickstart

**URL:** llms-txt#evaluation-quickstart

**Contents:**
- Prerequisites
- Video guide

Source: https://docs.langchain.com/langsmith/evaluation-quickstart

[*Evaluations*](/langsmith/evaluation-concepts) are a quantitative way to measure the performance of LLM applications. LLMs can behave unpredictably, even small changes to prompts, models, or inputs can significantly affect results. Evaluations provide a structured way to identify failures, compare versions, and build more reliable AI applications.

Running an evaluation in LangSmith requires three key components:

* [*Dataset*](/langsmith/evaluation-concepts#datasets): A set of test inputs (and optionally, expected outputs).
* [*Target function*](/langsmith/define-target-function): The part of your application you want to test—this might be a single LLM call with a new prompt, one module, or your entire workflow.
* [*Evaluators*](/langsmith/evaluation-concepts#evaluators): Functions that score your target function’s outputs.

This quickstart guides you through running a starter evaluation that checks the correctness of LLM responses, using either the LangSmith SDK or UI.

<Tip>
  If you prefer to watch a video on getting started with tracing, refer to the datasets and evaluations [Video guide](#video-guide).
</Tip>

Before you begin, make sure you have:

* **A LangSmith account**: Sign up or log in at [smith.langchain.com](https://smith.langchain.com).
* **A LangSmith API key**: Follow the [Create an API key](/langsmith/create-account-api-key#create-an-api-key) guide.
* **An OpenAI API key**: Generate this from the [OpenAI dashboard](https://platform.openai.com/account/api-keys).

**Select the UI or SDK filter for instructions:**

<Tabs>
  <Tab title="UI" icon="window">
    ## 1. Set workspace secrets

In the [LangSmith UI](https://smith.langchain.com), ensure that your OpenAI API key is set as a [workspace secret](/langsmith/administration-overview#workspace-secrets).

1. Navigate to <Icon icon="gear" /> **Settings** and then move to the **Secrets** tab.
    2. Select **Add secret** and enter the `OPENAI_API_KEY` and your API key as the **Value**.
    3. Select **Save secret**.

<Note> When adding workspace secrets in the LangSmith UI, make sure the secret keys match the environment variable names expected by your model provider.</Note>

## 2. Create a prompt

LangSmith's [Prompt Playground](/langsmith/observability-concepts#prompt-playground) makes it possible to run evaluations over different prompts, new models, or test different model configurations.

1. In the [LangSmith UI](https://smith.langchain.com), navigate to the **Playground** under **Prompt Engineering**.
    2. Under the **Prompts** panel, modify the **system** prompt to:

Leave the **Human** message as is: `{question}`.

## 3. Create a dataset

1. Click **Set up Evaluation**, which will open a **New Experiment** table at the bottom of the page.

2. In the **Select or create a new dataset** dropdown, click the **+ New** button to create a new dataset.

<div style={{ textAlign: 'center' }}>
         <img className="block dark:hidden" src="https://mintcdn.com/langchain-5e9cc07a/hVPHwyb3hetqtQnG/langsmith/images/playground-system-prompt-light.png?fit=max&auto=format&n=hVPHwyb3hetqtQnG&q=85&s=b068f4407a83e31403da9a5473960fee" alt="Playground with the edited system prompt and new experiment with the dropdown for creating a new dataset." data-og-width="1422" width="1422" data-og-height="743" height="743" data-path="langsmith/images/playground-system-prompt-light.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/hVPHwyb3hetqtQnG/langsmith/images/playground-system-prompt-light.png?w=280&fit=max&auto=format&n=hVPHwyb3hetqtQnG&q=85&s=d43b9466988d5077d0d2efe44b80b578 280w, https://mintcdn.com/langchain-5e9cc07a/hVPHwyb3hetqtQnG/langsmith/images/playground-system-prompt-light.png?w=560&fit=max&auto=format&n=hVPHwyb3hetqtQnG&q=85&s=1bf60ab2d71b1b9e734c28694f7974bc 560w, https://mintcdn.com/langchain-5e9cc07a/hVPHwyb3hetqtQnG/langsmith/images/playground-system-prompt-light.png?w=840&fit=max&auto=format&n=hVPHwyb3hetqtQnG&q=85&s=131d3d7bdc6c16e7738d3ea50fbc3abf 840w, https://mintcdn.com/langchain-5e9cc07a/hVPHwyb3hetqtQnG/langsmith/images/playground-system-prompt-light.png?w=1100&fit=max&auto=format&n=hVPHwyb3hetqtQnG&q=85&s=5396dc8c14902762a7499cf9dced6907 1100w, https://mintcdn.com/langchain-5e9cc07a/hVPHwyb3hetqtQnG/langsmith/images/playground-system-prompt-light.png?w=1650&fit=max&auto=format&n=hVPHwyb3hetqtQnG&q=85&s=b5eb2c32e461f50e7c85672cb5646f80 1650w, https://mintcdn.com/langchain-5e9cc07a/hVPHwyb3hetqtQnG/langsmith/images/playground-system-prompt-light.png?w=2500&fit=max&auto=format&n=hVPHwyb3hetqtQnG&q=85&s=db0ceb217623931ff1084e86b5d50981 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/langchain-5e9cc07a/hVPHwyb3hetqtQnG/langsmith/images/playground-system-prompt-dark.png?fit=max&auto=format&n=hVPHwyb3hetqtQnG&q=85&s=a114b1a83bf8d0a074b4ce2759207e4d" alt="Playground with the edited system prompt and new experiment with the dropdown for creating a new dataset." data-og-width="1421" width="1421" data-og-height="736" height="736" data-path="langsmith/images/playground-system-prompt-dark.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/hVPHwyb3hetqtQnG/langsmith/images/playground-system-prompt-dark.png?w=280&fit=max&auto=format&n=hVPHwyb3hetqtQnG&q=85&s=b88848b64b77bf1e2e997b956bbdd171 280w, https://mintcdn.com/langchain-5e9cc07a/hVPHwyb3hetqtQnG/langsmith/images/playground-system-prompt-dark.png?w=560&fit=max&auto=format&n=hVPHwyb3hetqtQnG&q=85&s=c036a354ec2e314d50426814028106d4 560w, https://mintcdn.com/langchain-5e9cc07a/hVPHwyb3hetqtQnG/langsmith/images/playground-system-prompt-dark.png?w=840&fit=max&auto=format&n=hVPHwyb3hetqtQnG&q=85&s=69fd6ef5aebac86623c203592a6038ae 840w, https://mintcdn.com/langchain-5e9cc07a/hVPHwyb3hetqtQnG/langsmith/images/playground-system-prompt-dark.png?w=1100&fit=max&auto=format&n=hVPHwyb3hetqtQnG&q=85&s=89ddb65e1ee37a1901e1f653ecd917ed 1100w, https://mintcdn.com/langchain-5e9cc07a/hVPHwyb3hetqtQnG/langsmith/images/playground-system-prompt-dark.png?w=1650&fit=max&auto=format&n=hVPHwyb3hetqtQnG&q=85&s=d8ec3af511ae661b55c9bcb79a18726f 1650w, https://mintcdn.com/langchain-5e9cc07a/hVPHwyb3hetqtQnG/langsmith/images/playground-system-prompt-dark.png?w=2500&fit=max&auto=format&n=hVPHwyb3hetqtQnG&q=85&s=824698630c1325a8082df4b8923492a9 2500w" />
       </div>

3. Add the following examples to the dataset:

| Inputs                                                   | Reference Outputs                                 |
       | -------------------------------------------------------- | ------------------------------------------------- |
       | question: Which country is Mount Kilimanjaro located in? | output: Mount Kilimanjaro is located in Tanzania. |
       | question: What is Earth's lowest point?                  | output: Earth's lowest point is The Dead Sea.     |

4. Click **Save** and enter a name to save your newly created dataset.

## 4. Add an evaluator

1. Click **+ Evaluator** and select **Correctness** from the **Pre-built Evaluator** options.
    2. In the **Correctness** panel, click **Save**.

## 5. Run your evaluation

1. Select <Icon icon="circle-play" /> **Start** on the top right to run your evaluation. This will create an [*experiment*](/langsmith/evaluation-concepts#experiment) with a preview in the **New Experiment** table. You can view in full by clicking the experiment name.

<div style={{ textAlign: 'center' }}>
         <img className="block dark:hidden" src="https://mintcdn.com/langchain-5e9cc07a/3SZlGm2zGXjJWzA5/langsmith/images/full-experiment-view-light.png?fit=max&auto=format&n=3SZlGm2zGXjJWzA5&q=85&s=efa004b4032d0e439a58d08567b75478" alt="Full experiment view of the results that used the example dataset." data-og-width="1241" width="1241" data-og-height="671" height="671" data-path="langsmith/images/full-experiment-view-light.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/3SZlGm2zGXjJWzA5/langsmith/images/full-experiment-view-light.png?w=280&fit=max&auto=format&n=3SZlGm2zGXjJWzA5&q=85&s=6d76d0e8d11cfdab4ac142f2d5c4bde1 280w, https://mintcdn.com/langchain-5e9cc07a/3SZlGm2zGXjJWzA5/langsmith/images/full-experiment-view-light.png?w=560&fit=max&auto=format&n=3SZlGm2zGXjJWzA5&q=85&s=b712e5a37af115a401d8d0d34812ef93 560w, https://mintcdn.com/langchain-5e9cc07a/3SZlGm2zGXjJWzA5/langsmith/images/full-experiment-view-light.png?w=840&fit=max&auto=format&n=3SZlGm2zGXjJWzA5&q=85&s=30c491438801b77eb4377401f26fd65d 840w, https://mintcdn.com/langchain-5e9cc07a/3SZlGm2zGXjJWzA5/langsmith/images/full-experiment-view-light.png?w=1100&fit=max&auto=format&n=3SZlGm2zGXjJWzA5&q=85&s=dc75651029fb4a83549714b41f06f541 1100w, https://mintcdn.com/langchain-5e9cc07a/3SZlGm2zGXjJWzA5/langsmith/images/full-experiment-view-light.png?w=1650&fit=max&auto=format&n=3SZlGm2zGXjJWzA5&q=85&s=7f3ade3f66b44d7080284112502a5812 1650w, https://mintcdn.com/langchain-5e9cc07a/3SZlGm2zGXjJWzA5/langsmith/images/full-experiment-view-light.png?w=2500&fit=max&auto=format&n=3SZlGm2zGXjJWzA5&q=85&s=e084175b15d368419c77835fbad3b53e 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/langchain-5e9cc07a/3SZlGm2zGXjJWzA5/langsmith/images/full-experiment-view-dark.png?fit=max&auto=format&n=3SZlGm2zGXjJWzA5&q=85&s=34c2921eeadd1b7782ac64b579bcef6a" alt="Full experiment view of the results that used the example dataset." data-og-width="1241" width="1241" data-og-height="665" height="665" data-path="langsmith/images/full-experiment-view-dark.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/3SZlGm2zGXjJWzA5/langsmith/images/full-experiment-view-dark.png?w=280&fit=max&auto=format&n=3SZlGm2zGXjJWzA5&q=85&s=e02feba8a82d493bf55c6801368b5c9b 280w, https://mintcdn.com/langchain-5e9cc07a/3SZlGm2zGXjJWzA5/langsmith/images/full-experiment-view-dark.png?w=560&fit=max&auto=format&n=3SZlGm2zGXjJWzA5&q=85&s=5f184bcab87fa6a55a948a54aa393a14 560w, https://mintcdn.com/langchain-5e9cc07a/3SZlGm2zGXjJWzA5/langsmith/images/full-experiment-view-dark.png?w=840&fit=max&auto=format&n=3SZlGm2zGXjJWzA5&q=85&s=c0bc9d71281293065f696cf85632179b 840w, https://mintcdn.com/langchain-5e9cc07a/3SZlGm2zGXjJWzA5/langsmith/images/full-experiment-view-dark.png?w=1100&fit=max&auto=format&n=3SZlGm2zGXjJWzA5&q=85&s=f8715bb40e847ad482fa1b5ff573ae2e 1100w, https://mintcdn.com/langchain-5e9cc07a/3SZlGm2zGXjJWzA5/langsmith/images/full-experiment-view-dark.png?w=1650&fit=max&auto=format&n=3SZlGm2zGXjJWzA5&q=85&s=5da023adf8b5bc693335baae7159169b 1650w, https://mintcdn.com/langchain-5e9cc07a/3SZlGm2zGXjJWzA5/langsmith/images/full-experiment-view-dark.png?w=2500&fit=max&auto=format&n=3SZlGm2zGXjJWzA5&q=85&s=64a53655134ae7554edaf47b1a957d26 2500w" />
       </div>

<Tip>
      To learn more about running experiments in LangSmith, read the [evaluation conceptual guide](/langsmith/evaluation-concepts).
    </Tip>

* For more details on evaluations, refer to the [Evaluation documentation](/langsmith/evaluation).
    * Learn how to [create and manage datasets in the UI](/langsmith/manage-datasets-in-application#set-up-your-dataset).
    * Learn how to [run an evaluation from the prompt playground](/langsmith/run-evaluation-from-prompt-playground).
  </Tab>

<Tab title="SDK" icon="code">
    <Tip>
      This guide uses prebuilt LLM-as-judge evaluators from the open-source [`openevals`](https://github.com/langchain-ai/openevals) package. OpenEvals includes a set of commonly used evaluators and is a great starting point if you're new to evaluations. If you want greater flexibility in how you evaluate your apps, you can also [define completely custom evaluators](/langsmith/code-evaluator).
    </Tip>

## 1. Install dependencies

In your terminal, create a directory for your project and install the dependencies in your environment:

<Info>
      If you are using `yarn` as your package manager, you will also need to manually install `@langchain/core` as a peer dependency of `openevals`. This is not required for LangSmith evals in general, you may define evaluators [using arbitrary custom code](/langsmith/code-evaluator).
    </Info>

## 2. Set up environment variables

Set the following environment variables:

* `LANGSMITH_TRACING`
    * `LANGSMITH_API_KEY`
    * `OPENAI_API_KEY` (or your LLM provider's API key)
    * (optional) `LANGSMITH_WORKSPACE_ID`: If your LangSmith API key is linked to multiple [workspaces](/langsmith/administration-overview#workspaces), set this variable to specify which workspace to use.

<Note>
      If you're using Anthropic, use the [Anthropic wrapper](/langsmith/annotate-code#wrap-the-anthropic-client-python-only) to trace your calls. For other providers, use [the traceable wrapper](/langsmith/annotate-code#use-%40traceable-%2F-traceable).
    </Note>

## 3. Create a dataset

1. Create a file and add the following code, which will:

* Import the `Client` to connect to LangSmith.
       * Create a dataset.
       * Define example [*inputs* and *outputs*](/langsmith/evaluation-concepts#examples).
       * Associate the input and output pairs with that dataset in LangSmith so they can be used in evaluations.

2. In your terminal, run the `dataset` file to create the datasets you'll use to evaluate your app:

You'll see the following output:

## 4. Create your target function

Define a [target function](/langsmith/define-target-function) that contains what you're evaluating. In this guide, you'll define a target function that contains a single LLM call to answer a question.

Add the following to an `eval` file:

## 5. Define an evaluator

In this step, you’re telling LangSmith how to grade the answers your app produces.

Import a prebuilt evaluation prompt (`CORRECTNESS_PROMPT`) from [`openevals`](https://github.com/langchain-ai/openevals) and a helper that wraps it into an [*LLM-as-judge evaluator*](/langsmith/evaluation-concepts#llm-as-judge), which will score the application's output.

<Info>
      `CORRECTNESS_PROMPT` is just an f-string with variables for `"inputs"`, `"outputs"`, and `"reference_outputs"`. See [here](https://github.com/langchain-ai/openevals#customizing-prompts) for more information on customizing OpenEvals prompts.
    </Info>

The evaluator compares:

* `inputs`: what was passed into your target function (e.g., the question text).
    * `outputs`: what your target function returned (e.g., the model’s answer).
    * `reference_outputs`: the ground truth answers you attached to each dataset example in [Step 3](#3-create-a-dataset).

Add the following highlighted code to your `eval` file:

## 6. Run and view results

To run the evaluation experiment, you'll call `evaluate(...)`, which:

* Pulls example from the dataset you created in [Step 3](#3-create-a-dataset).
    * Sends each example's inputs to your target function from [Step 4](#4-add-an-evaluator).
    * Collects the outputs (the model's answers).
    * Passes the outputs along with the `reference_outputs` to your evaluator from [Step 5](#5-define-an-evaluator).
    * Records all results in LangSmith as an experiment, so you can view them in the UI.

1. Add the highlighted code to your `eval` file:

2. Run your evaluator:

3. You'll receive a link to view the evaluation results and metadata for the experiment results:

4. Follow the link in the output of your evaluation run to access the **Datasets & Experiments** page in the [LangSmith UI](https://smith.langchain.com), and explore the results of the experiment. This will direct you to the created experiment with a table showing the **Inputs**, **Reference Output**, and **Outputs**. You can select a dataset to open an expanded view of the results.

<div style={{ textAlign: 'center' }}>
         <img className="block dark:hidden" src="https://mintcdn.com/langchain-5e9cc07a/DDMvkseOvrCjx9sx/langsmith/images/experiment-results-link-light.png?fit=max&auto=format&n=DDMvkseOvrCjx9sx&q=85&s=94341c15219e46866589140d87efb8f6" alt="Experiment results in the UI after following the link." data-og-width="1816" width="1816" data-og-height="464" height="464" data-path="langsmith/images/experiment-results-link-light.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/DDMvkseOvrCjx9sx/langsmith/images/experiment-results-link-light.png?w=280&fit=max&auto=format&n=DDMvkseOvrCjx9sx&q=85&s=a21a3329260ad62c96f334cda7956fe9 280w, https://mintcdn.com/langchain-5e9cc07a/DDMvkseOvrCjx9sx/langsmith/images/experiment-results-link-light.png?w=560&fit=max&auto=format&n=DDMvkseOvrCjx9sx&q=85&s=a35065f0c34ec47116cf07320d15feee 560w, https://mintcdn.com/langchain-5e9cc07a/DDMvkseOvrCjx9sx/langsmith/images/experiment-results-link-light.png?w=840&fit=max&auto=format&n=DDMvkseOvrCjx9sx&q=85&s=4f505f7b711829c9948e07aea7199869 840w, https://mintcdn.com/langchain-5e9cc07a/DDMvkseOvrCjx9sx/langsmith/images/experiment-results-link-light.png?w=1100&fit=max&auto=format&n=DDMvkseOvrCjx9sx&q=85&s=cc258bf607ab9c601e6770e35b03d6ca 1100w, https://mintcdn.com/langchain-5e9cc07a/DDMvkseOvrCjx9sx/langsmith/images/experiment-results-link-light.png?w=1650&fit=max&auto=format&n=DDMvkseOvrCjx9sx&q=85&s=dde47e9fa8c00e81776c32df132e1191 1650w, https://mintcdn.com/langchain-5e9cc07a/DDMvkseOvrCjx9sx/langsmith/images/experiment-results-link-light.png?w=2500&fit=max&auto=format&n=DDMvkseOvrCjx9sx&q=85&s=2f690dfd90d40401f5fa0cfabe08d070 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/langchain-5e9cc07a/DDMvkseOvrCjx9sx/langsmith/images/experiment-results-link-dark.png?fit=max&auto=format&n=DDMvkseOvrCjx9sx&q=85&s=d741b33219f7d130e80e1dfb7e743ac6" alt="Experiment results in the UI after following the link." data-og-width="1567" width="1567" data-og-height="455" height="455" data-path="langsmith/images/experiment-results-link-dark.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/DDMvkseOvrCjx9sx/langsmith/images/experiment-results-link-dark.png?w=280&fit=max&auto=format&n=DDMvkseOvrCjx9sx&q=85&s=49cc262a4941e5af43659dc1351e9ade 280w, https://mintcdn.com/langchain-5e9cc07a/DDMvkseOvrCjx9sx/langsmith/images/experiment-results-link-dark.png?w=560&fit=max&auto=format&n=DDMvkseOvrCjx9sx&q=85&s=177c8f916e53cb2d7396e7f10352eb50 560w, https://mintcdn.com/langchain-5e9cc07a/DDMvkseOvrCjx9sx/langsmith/images/experiment-results-link-dark.png?w=840&fit=max&auto=format&n=DDMvkseOvrCjx9sx&q=85&s=801e51a8002dac8add3ed14796b989bc 840w, https://mintcdn.com/langchain-5e9cc07a/DDMvkseOvrCjx9sx/langsmith/images/experiment-results-link-dark.png?w=1100&fit=max&auto=format&n=DDMvkseOvrCjx9sx&q=85&s=feab9e0fb21a720a8aefa80e4b6aedca 1100w, https://mintcdn.com/langchain-5e9cc07a/DDMvkseOvrCjx9sx/langsmith/images/experiment-results-link-dark.png?w=1650&fit=max&auto=format&n=DDMvkseOvrCjx9sx&q=85&s=0bf74f76e36d30e9520504281dd6b2ff 1650w, https://mintcdn.com/langchain-5e9cc07a/DDMvkseOvrCjx9sx/langsmith/images/experiment-results-link-dark.png?w=2500&fit=max&auto=format&n=DDMvkseOvrCjx9sx&q=85&s=84dd1248a09df55b3bd872276ef7c3ad 2500w" />
       </div>

Here are some topics you might want to explore next:

* [Evaluation concepts](/langsmith/evaluation-concepts) provides descriptions of the key terminology for evaluations in LangSmith.
    * [OpenEvals README](https://github.com/langchain-ai/openevals) to see all available prebuilt evaluators and how to customize them.
    * [Define custom evaluators](/langsmith/code-evaluator).
    * [Python](https://docs.smith.langchain.com/reference/python/reference) or [TypeScript](https://docs.smith.langchain.com/reference/js) SDK references for comprehensive descriptions of every class and function.
  </Tab>
</Tabs>

<iframe className="w-full aspect-video rounded-xl" src="https://www.youtube.com/embed/iEgjJyk3aTw?si=C7BPKXPmdE1yAflv" title="YouTube video player" frameBorder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowFullScreen />

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/evaluation-quickstart.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
Answer the following question accurately:
```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
</CodeGroup>

    <Info>
      If you are using `yarn` as your package manager, you will also need to manually install `@langchain/core` as a peer dependency of `openevals`. This is not required for LangSmith evals in general, you may define evaluators [using arbitrary custom code](/langsmith/code-evaluator).
    </Info>

    ## 2. Set up environment variables

    Set the following environment variables:

    * `LANGSMITH_TRACING`
    * `LANGSMITH_API_KEY`
    * `OPENAI_API_KEY` (or your LLM provider's API key)
    * (optional) `LANGSMITH_WORKSPACE_ID`: If your LangSmith API key is linked to multiple [workspaces](/langsmith/administration-overview#workspaces), set this variable to specify which workspace to use.
```

Example 4 (unknown):
```unknown
<Note>
      If you're using Anthropic, use the [Anthropic wrapper](/langsmith/annotate-code#wrap-the-anthropic-client-python-only) to trace your calls. For other providers, use [the traceable wrapper](/langsmith/annotate-code#use-%40traceable-%2F-traceable).
    </Note>

    ## 3. Create a dataset

    1. Create a file and add the following code, which will:

       * Import the `Client` to connect to LangSmith.
       * Create a dataset.
       * Define example [*inputs* and *outputs*](/langsmith/evaluation-concepts#examples).
       * Associate the input and output pairs with that dataset in LangSmith so they can be used in evaluations.

       <CodeGroup>
```

---

## Evaluator

**URL:** llms-txt#evaluator

def correct(outputs: dict, reference_outputs: dict) -> bool:
    """Check if the agent chose the correct route."""
    return outputs["route"] == reference_outputs["route"]

---

## Evaluator functions can be sync or async

**URL:** llms-txt#evaluator-functions-can-be-sync-or-async

def concise(inputs: dict, outputs: dict) -> bool:
    return len(outputs["output"]) < 3 * len(inputs["idea"])

ls_client = Client()
ideas = [
    "universal basic income",
    "nuclear fusion",
    "hyperloop",
    "nuclear powered rockets",
]
dataset = ls_client.create_dataset("research ideas")
ls_client.create_examples(
    dataset_name=dataset.name,
    examples=[{"inputs": {"idea": i}} for i in ideas],
)

---

## Evaluator function

**URL:** llms-txt#evaluator-function

async def final_answer_correct(inputs: dict, outputs: dict, reference_outputs: dict) -> bool:
    """Evaluate if the final response is equivalent to reference response."""

# Note that we assume the outputs has a 'response' dictionary. We'll need to make sure
    # that the target function we define includes this key.
    user = f"""QUESTION: {inputs['question']}
    GROUND TRUTH RESPONSE: {reference_outputs['response']}
    STUDENT RESPONSE: {outputs['response']}"""

grade = await grader_llm.ainvoke([{"role": "system", "content": grader_instructions}, {"role": "user", "content": user}])
    return grade["is_correct"]
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
Now we can run our evaluation. Our evaluator assumes that our target function returns a 'response' key, so lets define a target function that does so.

Also remember that in our refund graph we made the refund node configurable, so that if we specified `config={"env": "test"}`, we would mock out the refunds without actually updating the DB. We'll use this configurable variable in our target `run_graph` method when invoking our graph:
```

---

## Example: caching a query embedding

**URL:** llms-txt#example:-caching-a-query-embedding

tic = time.time()
print(cached_embedder.embed_query("Hello, world!"))
print(f"First call took: {time.time() - tic:.2f} seconds")

---

## Example Collector Configuration: Logs Sidecar

**URL:** llms-txt#example-collector-configuration:-logs-sidecar

---

## Example Collector Configuration: Metrics and Traces Gateway

**URL:** llms-txt#example-collector-configuration:-metrics-and-traces-gateway

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/langsmith-collector.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Example: create a predetermined tool call

**URL:** llms-txt#example:-create-a-predetermined-tool-call

def list_tables(state: MessagesState):
    tool_call = {
        "name": "sql_db_list_tables",
        "args": {},
        "id": "abc123",
        "type": "tool_call",
    }
    tool_call_message = AIMessage(content="", tool_calls=[tool_call])

list_tables_tool = next(tool for tool in tools if tool.name == "sql_db_list_tables")
    tool_message = list_tables_tool.invoke(tool_call)
    response = AIMessage(f"Available tables: {tool_message.content}")

return {"messages": [tool_call_message, tool_message, response]}

---

## Example data format

**URL:** llms-txt#example-data-format

Source: https://docs.langchain.com/langsmith/example-data-format

<Check>
  Before diving into this content, it might be helpful to read the following:

* [Conceptual guide on evaluation](/langsmith/evaluation-concepts)
</Check>

LangSmith stores examples in datasets as follows:

| Field Name          | Type     | Description                                                                                          |
| ------------------- | -------- | ---------------------------------------------------------------------------------------------------- |
| **id**              | UUID     | Unique identifier for the example.                                                                   |
| **name**            | string   | The name of the example.                                                                             |
| **created\_at**     | datetime | The time this example was created                                                                    |
| **modified\_at**    | datetime | The last time this example was modified                                                              |
| **inputs**          | object   | A map of inputs for the example.                                                                     |
| **outputs**         | object   | A map or set of outputs generated by the run.                                                        |
| **dataset\_id**     | UUID     | The dataset the example belongs to                                                                   |
| **source\_run\_id** | UUID     | If this example was created from a LangSmith [`Run`](/langsmith/run-data-format), the ID of said run |
| **metadata**        | object   | A map of additional, user or SDK defined information that can be stored on an example.               |

To learn more about how examples are used in evaluation, read our how-to guide on [evaluating LLM applications](/langsmith/evaluate-llm-application).

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/example-data-format.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Example: force a model to create a tool call

**URL:** llms-txt#example:-force-a-model-to-create-a-tool-call

**Contents:**
- 5. Implement the agent
- 6. Implement human-in-the-loop review
- Next steps

def call_get_schema(state: MessagesState):
    # Note that LangChain enforces that all models accept `tool_choice="any"`
    # as well as `tool_choice=<string name of tool>`.
    llm_with_tools = llm.bind_tools([get_schema_tool], tool_choice="any")
    response = llm_with_tools.invoke(state["messages"])

return {"messages": [response]}

generate_query_system_prompt = """
You are an agent designed to interact with a SQL database.
Given an input question, create a syntactically correct {dialect} query to run,
then look at the results of the query and return the answer. Unless the user
specifies a specific number of examples they wish to obtain, always limit your
query to at most {top_k} results.

You can order the results by a relevant column to return the most interesting
examples in the database. Never query for all the columns from a specific table,
only ask for the relevant columns given the question.

DO NOT make any DML statements (INSERT, UPDATE, DELETE, DROP etc.) to the database.
""".format(
    dialect=db.dialect,
    top_k=5,
)

def generate_query(state: MessagesState):
    system_message = {
        "role": "system",
        "content": generate_query_system_prompt,
    }
    # We do not force a tool call here, to allow the model to
    # respond naturally when it obtains the solution.
    llm_with_tools = llm.bind_tools([run_query_tool])
    response = llm_with_tools.invoke([system_message] + state["messages"])

return {"messages": [response]}

check_query_system_prompt = """
You are a SQL expert with a strong attention to detail.
Double check the {dialect} query for common mistakes, including:
- Using NOT IN with NULL values
- Using UNION when UNION ALL should have been used
- Using BETWEEN for exclusive ranges
- Data type mismatch in predicates
- Properly quoting identifiers
- Using the correct number of arguments for functions
- Casting to the correct data type
- Using the proper columns for joins

If there are any of the above mistakes, rewrite the query. If there are no mistakes,
just reproduce the original query.

You will call the appropriate tool to execute the query after running this check.
""".format(dialect=db.dialect)

def check_query(state: MessagesState):
    system_message = {
        "role": "system",
        "content": check_query_system_prompt,
    }

# Generate an artificial user message to check
    tool_call = state["messages"][-1].tool_calls[0]
    user_message = {"role": "user", "content": tool_call["args"]["query"]}
    llm_with_tools = llm.bind_tools([run_query_tool], tool_choice="any")
    response = llm_with_tools.invoke([system_message, user_message])
    response.id = state["messages"][-1].id

return {"messages": [response]}
python  theme={null}
def should_continue(state: MessagesState) -> Literal[END, "check_query"]:
    messages = state["messages"]
    last_message = messages[-1]
    if not last_message.tool_calls:
        return END
    else:
        return "check_query"

builder = StateGraph(MessagesState)
builder.add_node(list_tables)
builder.add_node(call_get_schema)
builder.add_node(get_schema_node, "get_schema")
builder.add_node(generate_query)
builder.add_node(check_query)
builder.add_node(run_query_node, "run_query")

builder.add_edge(START, "list_tables")
builder.add_edge("list_tables", "call_get_schema")
builder.add_edge("call_get_schema", "get_schema")
builder.add_edge("get_schema", "generate_query")
builder.add_conditional_edges(
    "generate_query",
    should_continue,
)
builder.add_edge("check_query", "run_query")
builder.add_edge("run_query", "generate_query")

agent = builder.compile()
python  theme={null}
from IPython.display import Image, display
from langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeStyles

display(Image(agent.get_graph().draw_mermaid_png()))
python  theme={null}
question = "Which genre on average has the longest tracks?"

for step in agent.stream(
    {"messages": [{"role": "user", "content": question}]},
    stream_mode="values",
):
    step["messages"][-1].pretty_print()

================================ Human Message =================================

Which genre on average has the longest tracks?
================================== Ai Message ==================================

Available tables: Album, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track
================================== Ai Message ==================================
Tool Calls:
  sql_db_schema (call_yzje0tj7JK3TEzDx4QnRR3lL)
 Call ID: call_yzje0tj7JK3TEzDx4QnRR3lL
  Args:
    table_names: Genre, Track
================================= Tool Message =================================
Name: sql_db_schema

CREATE TABLE "Genre" (
	"GenreId" INTEGER NOT NULL,
	"Name" NVARCHAR(120),
	PRIMARY KEY ("GenreId")
)

/*
3 rows from Genre table:
GenreId	Name
1	Rock
2	Jazz
3	Metal
*/

CREATE TABLE "Track" (
	"TrackId" INTEGER NOT NULL,
	"Name" NVARCHAR(200) NOT NULL,
	"AlbumId" INTEGER,
	"MediaTypeId" INTEGER NOT NULL,
	"GenreId" INTEGER,
	"Composer" NVARCHAR(220),
	"Milliseconds" INTEGER NOT NULL,
	"Bytes" INTEGER,
	"UnitPrice" NUMERIC(10, 2) NOT NULL,
	PRIMARY KEY ("TrackId"),
	FOREIGN KEY("MediaTypeId") REFERENCES "MediaType" ("MediaTypeId"),
	FOREIGN KEY("GenreId") REFERENCES "Genre" ("GenreId"),
	FOREIGN KEY("AlbumId") REFERENCES "Album" ("AlbumId")
)

/*
3 rows from Track table:
TrackId	Name	AlbumId	MediaTypeId	GenreId	Composer	Milliseconds	Bytes	UnitPrice
1	For Those About To Rock (We Salute You)	1	1	1	Angus Young, Malcolm Young, Brian Johnson	343719	11170334	0.99
2	Balls to the Wall	2	2	1	U. Dirkschneider, W. Hoffmann, H. Frank, P. Baltes, S. Kaufmann, G. Hoffmann	342562	5510424	0.99
3	Fast As a Shark	3	2	1	F. Baltes, S. Kaufman, U. Dirkscneider & W. Hoffman	230619	3990994	0.99
*/
================================== Ai Message ==================================
Tool Calls:
  sql_db_query (call_cb9ApLfZLSq7CWg6jd0im90b)
 Call ID: call_cb9ApLfZLSq7CWg6jd0im90b
  Args:
    query: SELECT Genre.Name, AVG(Track.Milliseconds) AS AvgMilliseconds FROM Track JOIN Genre ON Track.GenreId = Genre.GenreId GROUP BY Genre.GenreId ORDER BY AvgMilliseconds DESC LIMIT 5;
================================== Ai Message ==================================
Tool Calls:
  sql_db_query (call_DMVALfnQ4kJsuF3Yl6jxbeAU)
 Call ID: call_DMVALfnQ4kJsuF3Yl6jxbeAU
  Args:
    query: SELECT Genre.Name, AVG(Track.Milliseconds) AS AvgMilliseconds FROM Track JOIN Genre ON Track.GenreId = Genre.GenreId GROUP BY Genre.GenreId ORDER BY AvgMilliseconds DESC LIMIT 5;
================================= Tool Message =================================
Name: sql_db_query

[('Sci Fi & Fantasy', 2911783.0384615385), ('Science Fiction', 2625549.076923077), ('Drama', 2575283.78125), ('TV Shows', 2145041.0215053763), ('Comedy', 1585263.705882353)]
================================== Ai Message ==================================

The genre with the longest tracks on average is "Sci Fi & Fantasy," with an average track length of approximately 2,911,783 milliseconds. Other genres with relatively long tracks include "Science Fiction," "Drama," "TV Shows," and "Comedy."
python  theme={null}
from langchain_core.runnables import RunnableConfig
from langchain.tools import tool
from langgraph.types import interrupt

@tool(
    run_query_tool.name,
    description=run_query_tool.description,
    args_schema=run_query_tool.args_schema
)
def run_query_tool_with_interrupt(config: RunnableConfig, **tool_input):
    request = {
        "action": run_query_tool.name,
        "args": tool_input,
        "description": "Please review the tool call"
    }
    response = interrupt([request]) # [!code highlight]
    # approve the tool call
    if response["type"] == "accept":
        tool_response = run_query_tool.invoke(tool_input, config)
    # update tool call args
    elif response["type"] == "edit":
        tool_input = response["args"]["args"]
        tool_response = run_query_tool.invoke(tool_input, config)
    # respond to the LLM with user feedback
    elif response["type"] == "response":
        user_feedback = response["args"]
        tool_response = user_feedback
    else:
        raise ValueError(f"Unsupported interrupt response type: {response['type']}")

return tool_response
python  theme={null}
from langgraph.checkpoint.memory import InMemorySaver

def should_continue(state: MessagesState) -> Literal[END, "run_query"]:
    messages = state["messages"]
    last_message = messages[-1]
    if not last_message.tool_calls:
        return END
    else:
        return "run_query"

builder = StateGraph(MessagesState)
builder.add_node(list_tables)
builder.add_node(call_get_schema)
builder.add_node(get_schema_node, "get_schema")
builder.add_node(generate_query)
builder.add_node(run_query_node, "run_query")

builder.add_edge(START, "list_tables")
builder.add_edge("list_tables", "call_get_schema")
builder.add_edge("call_get_schema", "get_schema")
builder.add_edge("get_schema", "generate_query")
builder.add_conditional_edges(
    "generate_query",
    should_continue,
)
builder.add_edge("run_query", "generate_query")

checkpointer = InMemorySaver() # [!code highlight]
agent = builder.compile(checkpointer=checkpointer) # [!code highlight]
python  theme={null}
import json

config = {"configurable": {"thread_id": "1"}}

question = "Which genre on average has the longest tracks?"

for step in agent.stream(
    {"messages": [{"role": "user", "content": question}]},
    config,
    stream_mode="values",
):
    if "messages" in step:
        step["messages"][-1].pretty_print()
    elif "__interrupt__" in step:
        action = step["__interrupt__"][0]
        print("INTERRUPTED:")
        for request in action.value:
            print(json.dumps(request, indent=2))
    else:
        pass

INTERRUPTED:
{
  "action": "sql_db_query",
  "args": {
    "query": "SELECT Genre.Name, AVG(Track.Milliseconds) AS AvgLength FROM Track JOIN Genre ON Track.GenreId = Genre.GenreId GROUP BY Genre.Name ORDER BY AvgLength DESC LIMIT 5;"
  },
  "description": "Please review the tool call"
}
python  theme={null}
from langgraph.types import Command

for step in agent.stream(
    Command(resume={"type": "accept"}),
    # Command(resume={"type": "edit", "args": {"query": "..."}}),
    config,
    stream_mode="values",
):
    if "messages" in step:
        step["messages"][-1].pretty_print()
    elif "__interrupt__" in step:
        action = step["__interrupt__"][0]
        print("INTERRUPTED:")
        for request in action.value:
            print(json.dumps(request, indent=2))
    else:
        pass

================================== Ai Message ==================================
Tool Calls:
  sql_db_query (call_t4yXkD6shwdTPuelXEmY3sAY)
 Call ID: call_t4yXkD6shwdTPuelXEmY3sAY
  Args:
    query: SELECT Genre.Name, AVG(Track.Milliseconds) AS AvgLength FROM Track JOIN Genre ON Track.GenreId = Genre.GenreId GROUP BY Genre.Name ORDER BY AvgLength DESC LIMIT 5;
================================= Tool Message =================================
Name: sql_db_query

[('Sci Fi & Fantasy', 2911783.0384615385), ('Science Fiction', 2625549.076923077), ('Drama', 2575283.78125), ('TV Shows', 2145041.0215053763), ('Comedy', 1585263.705882353)]
================================== Ai Message ==================================

The genre with the longest average track length is "Sci Fi & Fantasy" with an average length of about 2,911,783 milliseconds. Other genres with long average track lengths include "Science Fiction," "Drama," "TV Shows," and "Comedy."
```

Refer to the [human-in-the-loop guide](/oss/python/langgraph/interrupts) for details.

Check out the [Evaluate a graph](/langsmith/evaluate-graph) guide for evaluating LangGraph applications, including SQL agents like this one, using LangSmith.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/sql-agent.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
## 5. Implement the agent

We can now assemble these steps into a workflow using the [Graph API](/oss/python/langgraph/graph-api). We define a [conditional edge](/oss/python/langgraph/graph-api#conditional-edges) at the query generation step that will route to the query checker if a query is generated, or end if there are no tool calls present, such that the LLM has delivered a response to the query.
```

Example 2 (unknown):
```unknown
We visualize the application below:
```

Example 3 (unknown):
```unknown
<img src="https://mintcdn.com/langchain-5e9cc07a/aAi4RLdXQAh8fThS/oss/images/sql-agent-langgraph.png?fit=max&auto=format&n=aAi4RLdXQAh8fThS&q=85&s=1ddd4aae369fb8c143edaccb0a09c81f" alt="SQL agent graph" style={{ height: "800px" }} data-og-width="308" width="308" data-og-height="645" height="645" data-path="oss/images/sql-agent-langgraph.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aAi4RLdXQAh8fThS/oss/images/sql-agent-langgraph.png?w=280&fit=max&auto=format&n=aAi4RLdXQAh8fThS&q=85&s=e5d3e67f17d65e438370f7d771e3ba7d 280w, https://mintcdn.com/langchain-5e9cc07a/aAi4RLdXQAh8fThS/oss/images/sql-agent-langgraph.png?w=560&fit=max&auto=format&n=aAi4RLdXQAh8fThS&q=85&s=dbcb80fdb2d00a6dc33dc90f05d100b5 560w, https://mintcdn.com/langchain-5e9cc07a/aAi4RLdXQAh8fThS/oss/images/sql-agent-langgraph.png?w=840&fit=max&auto=format&n=aAi4RLdXQAh8fThS&q=85&s=72be69a1e7ac39afad3d0aa03ecffffa 840w, https://mintcdn.com/langchain-5e9cc07a/aAi4RLdXQAh8fThS/oss/images/sql-agent-langgraph.png?w=1100&fit=max&auto=format&n=aAi4RLdXQAh8fThS&q=85&s=5ad351b8b6641defe17882f5e102cab0 1100w, https://mintcdn.com/langchain-5e9cc07a/aAi4RLdXQAh8fThS/oss/images/sql-agent-langgraph.png?w=1650&fit=max&auto=format&n=aAi4RLdXQAh8fThS&q=85&s=8a5cefc8ac6938d0b4b0946e0522ffaa 1650w, https://mintcdn.com/langchain-5e9cc07a/aAi4RLdXQAh8fThS/oss/images/sql-agent-langgraph.png?w=2500&fit=max&auto=format&n=aAi4RLdXQAh8fThS&q=85&s=0b5b7711b4b2ece3a3ccb10a2b012166 2500w" />

We can now invoke the graph:
```

Example 4 (unknown):
```unknown

```

---

## Example usage

**URL:** llms-txt#example-usage

**Contents:**
- Advanced usage
  - Custom metadata and tags

if __name__ == "__main__":
    task = """
    Create a Python function that implements a binary search algorithm.
    The function should:
    - Take a sorted list and a target value as parameters
    - Return the index of the target if found, or -1 if not found
    - Include proper error handling and documentation
    """

result = run_code_review_session(task)
    print(f"Result: {result}")
python  theme={null}
from opentelemetry import trace

**Examples:**

Example 1 (unknown):
```unknown
## Advanced usage

### Custom metadata and tags

You can add custom metadata to your traces by setting span attributes in your AutoGen application:
```

---

## Execute tool and create result message

**URL:** llms-txt#execute-tool-and-create-result-message

weather_result = "Sunny, 72°F"
tool_message = ToolMessage(
    content=weather_result,
    tool_call_id="call_123"  # Must match the call ID
)

---

## [{'expensive_node': {'result': 10}}]

**URL:** llms-txt#[{'expensive_node':-{'result':-10}}]

print(graph.invoke({"x": 5}, stream_mode='updates'))    # [!code highlight]

---

## [{'expensive_node': {'result': 10}, '__metadata__': {'cached': True}}]

**URL:** llms-txt#[{'expensive_node':-{'result':-10},-'__metadata__':-{'cached':-true}}]

**Contents:**
- Edges
  - Normal Edges
  - Conditional Edges
  - Entry Point
  - Conditional Entry Point
- `Send`
- `Command`
  - When should I use Command instead of conditional edges?
  - Navigating to a node in a parent graph
  - Using inside tools

python  theme={null}
graph.add_edge("node_a", "node_b")
python  theme={null}
graph.add_conditional_edges("node_a", routing_function)
python  theme={null}
graph.add_conditional_edges("node_a", routing_function, {True: "node_b", False: "node_c"})
python  theme={null}
from langgraph.graph import START

graph.add_edge(START, "node_a")
python  theme={null}
from langgraph.graph import START

graph.add_conditional_edges(START, routing_function)
python  theme={null}
graph.add_conditional_edges(START, routing_function, {True: "node_b", False: "node_c"})
python  theme={null}
def continue_to_jokes(state: OverallState):
    return [Send("generate_joke", {"subject": s}) for s in state['subjects']]

graph.add_conditional_edges("node_a", continue_to_jokes)
python  theme={null}
def my_node(state: State) -> Command[Literal["my_other_node"]]:
    return Command(
        # state update
        update={"foo": "bar"},
        # control flow
        goto="my_other_node"
    )
python  theme={null}
def my_node(state: State) -> Command[Literal["my_other_node"]]:
    if state["foo"] == "bar":
        return Command(update={"foo": "baz"}, goto="my_other_node")
python  theme={null}
def my_node(state: State) -> Command[Literal["other_subgraph"]]:
    return Command(
        update={"foo": "bar"},
        goto="other_subgraph",  # where `other_subgraph` is a node in the parent graph
        graph=Command.PARENT
    )
python  theme={null}
@dataclass
class ContextSchema:
    llm_provider: str = "openai"

graph = StateGraph(State, context_schema=ContextSchema)
python  theme={null}
graph.invoke(inputs, context={"llm_provider": "anthropic"})
python  theme={null}
from langgraph.runtime import Runtime

def node_a(state: State, runtime: Runtime[ContextSchema]):
    llm = get_llm(runtime.context.llm_provider)
    # ...
python  theme={null}
graph.invoke(inputs, config={"recursion_limit": 5}, context={"llm": "anthropic"})
python  theme={null}
from langchain_core.runnables import RunnableConfig
from langgraph.graph import StateGraph

def my_node(state: dict, config: RunnableConfig) -> dict:
    current_step = config["metadata"]["langgraph_step"]
    print(f"Currently on step: {current_step}")
    return state
python  theme={null}
from langchain_core.runnables import RunnableConfig
from langgraph.graph import StateGraph, END

def reasoning_node(state: dict, config: RunnableConfig) -> dict:
    current_step = config["metadata"]["langgraph_step"]
    recursion_limit = config["recursion_limit"]  # always present, defaults to 25

# Check if we're approaching the limit (e.g., 80% threshold)
    if current_step >= recursion_limit * 0.8:
        return {
            **state,
            "route_to": "fallback",
            "reason": "Approaching recursion limit"
        }

# Normal processing
    return {"messages": state["messages"] + ["thinking..."]}

def fallback_node(state: dict, config: RunnableConfig) -> dict:
    """Handle cases where recursion limit is approaching"""
    return {
        **state,
        "messages": state["messages"] + ["Reached complexity limit, providing best effort answer"]
    }

def route_based_on_state(state: dict) -> str:
    if state.get("route_to") == "fallback":
        return "fallback"
    elif state.get("done"):
        return END
    return "reasoning"

**Examples:**

Example 1 (unknown):
```unknown
1. First run takes two seconds to run (due to mocked expensive computation).
2. Second run utilizes cache and returns quickly.

## Edges

Edges define how the logic is routed and how the graph decides to stop. This is a big part of how your agents work and how different nodes communicate with each other. There are a few key types of edges:

* Normal Edges: Go directly from one node to the next.
* Conditional Edges: Call a function to determine which node(s) to go to next.
* Entry Point: Which node to call first when user input arrives.
* Conditional Entry Point: Call a function to determine which node(s) to call first when user input arrives.

A node can have MULTIPLE outgoing edges. If a node has multiple out-going edges, **all** of those destination nodes will be executed in parallel as a part of the next superstep.

### Normal Edges

If you **always** want to go from node A to node B, you can use the [`add_edge`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.StateGraph.add_edge) method directly.
```

Example 2 (unknown):
```unknown
### Conditional Edges

If you want to **optionally** route to one or more edges (or optionally terminate), you can use the [`add_conditional_edges`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.StateGraph.add_conditional_edges) method. This method accepts the name of a node and a "routing function" to call after that node is executed:
```

Example 3 (unknown):
```unknown
Similar to nodes, the `routing_function` accepts the current `state` of the graph and returns a value.

By default, the return value `routing_function` is used as the name of the node (or list of nodes) to send the state to next. All those nodes will be run in parallel as a part of the next superstep.

You can optionally provide a dictionary that maps the `routing_function`'s output to the name of the next node.
```

Example 4 (unknown):
```unknown
<Tip>
  Use [`Command`](#command) instead of conditional edges if you want to combine state updates and routing in a single function.
</Tip>

### Entry Point

The entry point is the first node(s) that are run when the graph starts. You can use the [`add_edge`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.StateGraph.add_edge) method from the virtual [`START`](https://reference.langchain.com/python/langgraph/constants/#langgraph.constants.START) node to the first node to execute to specify where to enter the graph.
```

---

## Explore the failed inputs and outputs.

**URL:** llms-txt#explore-the-failed-inputs-and-outputs.

for r in failed:
    print(r["example"].inputs)
    print(r["run"].outputs)

---

## Explore the results as a Pandas DataFrame.

**URL:** llms-txt#explore-the-results-as-a-pandas-dataframe.

---

## export LANGSMITH_API_KEY="your-api-key"

**URL:** llms-txt#export-langsmith_api_key="your-api-key"

**Contents:**
  - Installation
- Instantiation
- Indexing and Retrieval
- Direct Usage
  - Embed single texts
  - Embed multiple texts
- Specifying dimensions
- Custom URLs
- API reference

bash npm theme={null}
  npm install @langchain/openai @langchain/core
  bash yarn theme={null}
  yarn add @langchain/openai @langchain/core
  bash pnpm theme={null}
  pnpm add @langchain/openai @langchain/core
  typescript  theme={null}
import { OpenAIEmbeddings } from "@langchain/openai";

const embeddings = new OpenAIEmbeddings({
  apiKey: "YOUR-API-KEY", // In Node.js defaults to process.env.OPENAI_API_KEY
  batchSize: 512, // Default value if omitted is 512. Max is 2048
  model: "text-embedding-3-large",
});
typescript  theme={null}
// Create a vector store with a sample text
import { MemoryVectorStore } from "@langchain/classic/vectorstores/memory";

const text = "LangChain is the framework for building context-aware reasoning applications";

const vectorstore = await MemoryVectorStore.fromDocuments(
  [{ pageContent: text, metadata: {} }],
  embeddings,
);

// Use the vector store as a retriever that returns a single document
const retriever = vectorstore.asRetriever(1);

// Retrieve the most similar text
const retrievedDocuments = await retriever.invoke("What is LangChain?");

retrievedDocuments[0].pageContent;
output  theme={null}
LangChain is the framework for building context-aware reasoning applications
typescript  theme={null}
const singleVector = await embeddings.embedQuery(text);

console.log(singleVector.slice(0, 100));
output  theme={null}
[
    -0.01927683,  0.0037708976,  -0.032942563,  0.0037671267,  0.008175306,
   -0.012511838,  -0.009713832,   0.021403614,  -0.015377721, 0.0018684798,
    0.020574018,   0.022399133,   -0.02322873,   -0.01524951,  -0.00504169,
   -0.007375876,   -0.03448109, 0.00015130726,   0.021388533, -0.012564631,
   -0.020031009,   0.027406884,  -0.039217334,    0.03036327,  0.030393435,
   -0.021750538,   0.032610722,  -0.021162277,  -0.025898525,  0.018869571,
    0.034179416,  -0.013371604,  0.0037652412,   -0.02146395, 0.0012641934,
   -0.055688616,    0.05104287,  0.0024982197,  -0.019095825, 0.0037369595,
  0.00088757504,   0.025189597,  -0.018779071,   0.024978427,  0.016833287,
  -0.0025868358,  -0.011727491, -0.0021154736,  -0.017738303, 0.0013839195,
  -0.0131151825,   -0.05405959,   0.029729757,  -0.003393808,  0.019774588,
    0.028885076,   0.004355387,   0.026094612,    0.06479911,  0.038040817,
    -0.03478276,  -0.012594799,  -0.024767255, -0.0031430433,  0.017874055,
   -0.015294761,   0.005709139,   0.025355516,   0.044798266,   0.02549127,
    -0.02524993, 0.00014553308,  -0.019427665,  -0.023545485,  0.008748483,
    0.019850006,  -0.028417485,  -0.001860938,   -0.02318348, -0.010799851,
     0.04793565, -0.0048983963,    0.02193154,  -0.026411368,  0.026426451,
   -0.012149832,   0.035355937,  -0.047814984,  -0.027165547, -0.008228099,
   -0.007737882,   0.023726488,  -0.046487626,  -0.007783133, -0.019638835,
     0.01793439,  -0.018024892,  0.0030336871,  -0.019578502, 0.0042837397
]
typescript  theme={null}
const text2 = "LangGraph is a library for building stateful, multi-actor applications with LLMs";

const vectors = await embeddings.embedDocuments([text, text2]);

console.log(vectors[0].slice(0, 100));
console.log(vectors[1].slice(0, 100));
output  theme={null}
[
    -0.01927683,  0.0037708976,  -0.032942563,  0.0037671267,  0.008175306,
   -0.012511838,  -0.009713832,   0.021403614,  -0.015377721, 0.0018684798,
    0.020574018,   0.022399133,   -0.02322873,   -0.01524951,  -0.00504169,
   -0.007375876,   -0.03448109, 0.00015130726,   0.021388533, -0.012564631,
   -0.020031009,   0.027406884,  -0.039217334,    0.03036327,  0.030393435,
   -0.021750538,   0.032610722,  -0.021162277,  -0.025898525,  0.018869571,
    0.034179416,  -0.013371604,  0.0037652412,   -0.02146395, 0.0012641934,
   -0.055688616,    0.05104287,  0.0024982197,  -0.019095825, 0.0037369595,
  0.00088757504,   0.025189597,  -0.018779071,   0.024978427,  0.016833287,
  -0.0025868358,  -0.011727491, -0.0021154736,  -0.017738303, 0.0013839195,
  -0.0131151825,   -0.05405959,   0.029729757,  -0.003393808,  0.019774588,
    0.028885076,   0.004355387,   0.026094612,    0.06479911,  0.038040817,
    -0.03478276,  -0.012594799,  -0.024767255, -0.0031430433,  0.017874055,
   -0.015294761,   0.005709139,   0.025355516,   0.044798266,   0.02549127,
    -0.02524993, 0.00014553308,  -0.019427665,  -0.023545485,  0.008748483,
    0.019850006,  -0.028417485,  -0.001860938,   -0.02318348, -0.010799851,
     0.04793565, -0.0048983963,    0.02193154,  -0.026411368,  0.026426451,
   -0.012149832,   0.035355937,  -0.047814984,  -0.027165547, -0.008228099,
   -0.007737882,   0.023726488,  -0.046487626,  -0.007783133, -0.019638835,
     0.01793439,  -0.018024892,  0.0030336871,  -0.019578502, 0.0042837397
]
[
   -0.010181213,   0.023419594,   -0.04215527, -0.0015320902,  -0.023573855,
  -0.0091644935,  -0.014893179,   0.019016149,  -0.023475688,  0.0010219777,
    0.009255648,    0.03996757,   -0.04366983,   -0.01640774,  -0.020194141,
    0.019408813,  -0.027977299,  -0.022017224,   0.013539891,  -0.007769135,
    0.032647192,  -0.015089511,  -0.022900717,   0.023798235,   0.026084099,
   -0.024625633,   0.035003178,  -0.017978394,  -0.049615882,   0.013364594,
    0.031132633,   0.019142363,   0.023195215,  -0.038396914,   0.005584942,
   -0.031946007,   0.053682756, -0.0036356465,   0.011240003,  0.0056690844,
  -0.0062791156,   0.044146635,  -0.037387207,    0.01300699,   0.018946031,
   0.0050415234,   0.029618073,  -0.021750772,  -0.000649473, 0.00026951815,
   -0.014710871,  -0.029814405,    0.04204308,  -0.014710871,  0.0039616977,
   -0.021512369,   0.054608323,   0.021484323,    0.02790718,  -0.010573876,
   -0.023952495,  -0.035143413,  -0.048802506, -0.0075798146,   0.023279356,
   -0.022690361,  -0.016590048,  0.0060477243,   0.014100839,   0.005476258,
   -0.017221114, -0.0100059165,  -0.017922299,  -0.021989176,    0.01830094,
     0.05516927,   0.001033372,  0.0017310516,   -0.00960624,  -0.037864015,
    0.013063084,   0.006591143,  -0.010160177,  0.0011394264,    0.04953174,
    0.004806626,   0.029421741,  -0.037751824,   0.003618117,   0.007162609,
    0.027696826, -0.0021070621,  -0.024485396, -0.0042141243,   -0.02801937,
   -0.019605145,   0.016281527,  -0.035143413,    0.01640774,   0.042323552
]
typescript  theme={null}
import { OpenAIEmbeddings } from "@langchain/openai";

const embeddingsDefaultDimensions = new OpenAIEmbeddings({
  model: "text-embedding-3-large",
});

const vectorsDefaultDimensions = await embeddingsDefaultDimensions.embedDocuments(["some text"]);
console.log(vectorsDefaultDimensions[0].length);
output  theme={null}
3072
typescript  theme={null}
import { OpenAIEmbeddings } from "@langchain/openai";

const embeddings1024 = new OpenAIEmbeddings({
  model: "text-embedding-3-large",
  dimensions: 1024,
});

const vectors1024 = await embeddings1024.embedDocuments(["some text"]);
console.log(vectors1024[0].length);
output  theme={null}
1024
typescript  theme={null}
import { OpenAIEmbeddings } from "@langchain/openai";

const model = new OpenAIEmbeddings({
  configuration: {
    baseURL: "https://your_custom_url.com",
  },
});
```

You can also pass other `ClientOptions` parameters accepted by the official SDK.

If you are hosting on Azure OpenAI, see the [dedicated page instead](/oss/javascript/integrations/text_embedding/azure_openai).

For detailed documentation of all OpenAIEmbeddings features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_openai.OpenAIEmbeddings.html).

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/javascript/integrations/text_embedding/openai.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
### Installation

The LangChain OpenAIEmbeddings integration lives in the `@langchain/openai` package:

<CodeGroup>
```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>

## Instantiation

Now we can instantiate our model object and generate chat completions:
```

---

## Export LangSmith telemetry to your observability backend

**URL:** llms-txt#export-langsmith-telemetry-to-your-observability-backend

Source: https://docs.langchain.com/langsmith/export-backend

<Warning>
  **This section is only applicable for Kubernetes deployments.**
</Warning>

Self-Hosted LangSmith instances produce telemetry data in the form of logs, metrics and traces. This section will show you how to access and export that data to an observability collector or backend.

This section assumes that you have monitoring infrastructure set up already, or you will set up this infrastructure and want to know how to configure LangSmith to collect data from it.

Infrastructure refers to:

* Collectors, such as [OpenTelemetry](https://opentelemetry.io/docs/collector/), [FluentBit](https://docs.fluentbit.io/manual) or [Prometheus](https://prometheus.io/).
* Observability backends, such as [Datadog](https://www.datadoghq.com/) or the [Grafana](https://grafana.com/) ecosystem.

---

## export LANGSMITH_TRACING="true"

**URL:** llms-txt#export-langsmith_tracing="true"

---

## Extraction schema, mirrors the graph state.

**URL:** llms-txt#extraction-schema,-mirrors-the-graph-state.

class PurchaseInformation(TypedDict):
    """All of the known information about the invoice / invoice lines the customer would like refunded. Do not make up values, leave fields as null if you don't know their value."""

invoice_id: int | None
    invoice_line_ids: list[int] | None
    customer_first_name: str | None
    customer_last_name: str | None
    customer_phone: str | None
    track_name: str | None
    album_title: str | None
    artist_name: str | None
    purchase_date_iso_8601: str | None
    followup: Annotated[
        str | None,
        ...,
        "If the user hasn't enough identifying information, please tell them what the required information is and ask them to specify it.",
    ]

---

## Extract customer_id and customer_name using jq

**URL:** llms-txt#extract-customer_id-and-customer_name-using-jq

export CUSTOMER_ID=$(echo "$response" | jq -r '.customer_info.customer_id')
export CUSTOMER_NAME=$(echo "$response" | jq -r '.customer_info.customer_name')

---

## Feedback data format

**URL:** llms-txt#feedback-data-format

Source: https://docs.langchain.com/langsmith/feedback-data-format

<Check>
  Before diving into this content, it might be helpful to read the following:

* [Conceptual guide on tracing and feedback](/langsmith/observability-concepts)
</Check>

**Feedback** is LangSmith's way of storing the criteria and scores from evaluation on a particular trace or intermediate run (span). Feedback can be produced from a variety of ways, such as:

1. [Sent up along with a trace](/langsmith/attach-user-feedback) from the LLM application
2. Generated by a user in the app [inline](/langsmith/annotate-traces-inline) or in an [annotation queue](/langsmith/annotation-queues)
3. Generated by an automatic evaluator during [offline evaluation](/langsmith/evaluate-llm-application)
4. Generated by an [online evaluator](/langsmith/online-evaluations)

Feedback is stored in a simple format with the following fields:

| Field Name                | Type     | Description                                                                                            |
| ------------------------- | -------- | ------------------------------------------------------------------------------------------------------ |
| id                        | UUID     | Unique identifier for the record itself                                                                |
| created\_at               | datetime | Timestamp when the record was created                                                                  |
| modified\_at              | datetime | Timestamp when the record was last modified                                                            |
| session\_id               | UUID     | Unique identifier for the experiment or tracing project the run was a part of                          |
| run\_id                   | UUID     | Unique identifier for a specific run within a session                                                  |
| key                       | string   | A key describing the criteria of the feedback, eg "correctness"                                        |
| score                     | number   | Numerical score associated with the feedback key                                                       |
| value                     | string   | Reserved for storing a value associated with the score. Useful for categorical feedback.               |
| comment                   | string   | Any comment or annotation associated with the record. This can be a justification for the score given. |
| correction                | object   | Reserved for storing correction details, if any                                                        |
| feedback\_source          | object   | Object containing information about the feedback source                                                |
| feedback\_source.type     | string   | The type of source where the feedback originated, eg "api", "app", "evaluator"                         |
| feedback\_source.metadata | object   | Reserved for additional metadata, currently                                                            |
| feedback\_source.user\_id | UUID     | Unique identifier for the user providing feedback                                                      |

Here is an example JSON representation of a feedback record in the above format:

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/feedback-data-format.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Fetch the comparative experiment

**URL:** llms-txt#fetch-the-comparative-experiment

resp = requests.get(
    f"https://api.smith.langchain.com/api/v1/datasets/{str(dataset_id)}/comparative",
    params={"id": comparative_experiment_id},
    headers={"x-api-key": os.environ["LANGSMITH_API_KEY"]}
)

comparative_experiment = resp.json()[0]
experiment_ids = [info["id"] for info in comparative_experiment["experiments_info"]]

from collections import defaultdict
example_id_to_runs_map = defaultdict(list)

---

## Fetch the files as bytes

**URL:** llms-txt#fetch-the-files-as-bytes

pdf_bytes = requests.get(pdf_url).content
wav_bytes = requests.get(wav_url).content
img_bytes = requests.get(img_url).content

---

## Fetch the runs we want to convert to a dataset/experiment

**URL:** llms-txt#fetch-the-runs-we-want-to-convert-to-a-dataset/experiment

---

## FilesystemMiddleware is included by default in create_deep_agent

**URL:** llms-txt#filesystemmiddleware-is-included-by-default-in-create_deep_agent

---

## Filter traces

**URL:** llms-txt#filter-traces

**Contents:**
- Creating and Applying Filters
  - Filtering by run attributes
  - Filtering by time range
  - Filter operators
- Specific Filtering Techniques
  - Filter for intermediate runs (spans)
  - Filter based on inputs and outputs
  - Filter based on input / output key-value pairs
  - Example: Filtering for tool calls
  - Negative filtering on key-value pairs

Source: https://docs.langchain.com/langsmith/filter-traces-in-application

<Tip>**Recommended reading**: It might be helpful to read the [Conceptual guide on tracing](/langsmith/observability-concepts) to gain familiarity with the concepts mentioned on this page.</Tip>

Tracing projects can contain a significant amount of data. Filters are used for effectively navigating and analyzing this data, allowing you to:

* **Have focused investigations**: Quickly narrow down to specific runs for ad-hoc analysis
* **Debug and analyze**: Identify and examine errors, failed runs, and performance bottlenecks

This page contains a series of guides for how to filter runs in a tracing project. If you are programmatically exporting runs for analysis via the [API](https://api.smith.langchain.com/redoc#tag/run/operation/query_runs_api_v1_runs_query_post) or [SDK](https://docs.smith.langchain.com/reference/python/client/langsmith.client.Client#langsmith.client.Client.list_runs), please refer to the [exporting traces guide](./export-traces) for more information.

## Creating and Applying Filters

### Filtering by run attributes

There are two ways to filter runs in a tracing project:

1. **Filters**: Located towards the top-left of the tracing projects page. This is where you construct and manage detailed filter criteria.

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=d3751c19f28b7dffecec87e27d6c6d53" alt="Filtering" data-og-width="1156" width="1156" data-og-height="551" height="551" data-path="langsmith/images/filter.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=f03bd4a75a5b8b0cc2cb2d96b0908000 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=68f5583f17be5060e8815ae14edd6619 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=35af2365bef2b299b70e336209c54797 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=fc2bc02daf5f56c914cd0cf7f274d200 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=e45165a47a471aaf0e94dcb5e2abbb7d 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=98ee26df269a2bdabad8a3f5bea1211a 2500w" />

2. **Filter Shortcuts**: Positioned on the right sidebar of the tracing projects page. The filter shortcuts bar provides quick access to filters based on the most frequently occurring attributes in your project's runs.

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-shortcuts.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=c70333d17e9f38f813a6ddef017c7a29" alt="Filter Shortcuts" data-og-width="1330" width="1330" data-og-height="1078" height="1078" data-path="langsmith/images/filter-shortcuts.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-shortcuts.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=b2e073e3f070674704f3f6506e851bc2 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-shortcuts.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=dbdcf6272d1f115a158f4b1dad02f4b7 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-shortcuts.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=808645fd9ebcd95e424b8b0a47a75cde 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-shortcuts.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=de2a53f560cfaecb13673b400a46de55 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-shortcuts.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=73ac5c2913b9e13ae99822c2a80b290e 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-shortcuts.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=f126680bfdf0fe958a7cc06429387582 2500w" />

<Info>
  **Default filter**

By default, the `IsTrace` is `true` filter is applied. This displays only top-level traces. Removing this filter will show all runs, including intermediate spans, in the project.
</Info>

### Filtering by time range

In addition to filtering by run attributes, you can also filter runs within a specific time range. This option is available towards the top-left of the tracing projects page.

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-time.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=214ec0a0bcec5e5bf07a54becec35a80" alt="Filtering on time" data-og-width="1325" width="1325" data-og-height="680" height="680" data-path="langsmith/images/filter-time.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-time.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=cf9a2351d13ebfcada25090fc8a56f6e 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-time.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=35ef8e40fcb515e8b9b9b98d49312521 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-time.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=7c69f28b9a1debb36dd1929292d0dca3 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-time.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=a538ee5b3b711f1d257cc733e701ef0c 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-time.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=acb075b10621f69b098b929fc64d54d3 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-time.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=e11f9d866b5ad3b89a29fad8167f5aac 2500w" />

The available filter operators depend on the data type of the attribute you are filtering on. Here's an overview of common operators:

* **is**: Exact match on the filter value
* **is not**: Negative match on the filter value
* **contains**: Partial match on the filter value
* **does not contain**: Negative partial match on the filter value
* **is one of**: Match on any of the values in the list
* `>` / `<`: Available for numeric fields

## Specific Filtering Techniques

### Filter for intermediate runs (spans)

In order to filter for intermediate runs (spans), you first need to remove the default `IsTrace` is `true` filter. For example, you would do this if you wanted to filter by `run name` for sub runs or filter by `run type`.

Run metadata and tags are also powerful to filter on. These rely on good tagging across all parts of your pipeline. To learn more, you can check out [this guide](./add-metadata-tags).

### Filter based on inputs and outputs

You can filter runs based on the content in the inputs and outputs of the run.

To filter either inputs or outputs, you can use the `Full-Text Search` filter which will match keywords in either field. For more targeted search, you can use the `Input` or `Output` filters which will only match content based on the respective field.

<Note>
  For performance, we index up to 250 characters of data for full-text search. If your search query exceeds this limit, we recommend using [Input/Output key-value search](/langsmith/filter-traces-in-application#filter-based-on-input-%2F-output-key-value-pairs) instead.
</Note>

You can also specify multiple matches, either by including multiple terms separated by whitespace, or adding multiple filters - which will try to match all terms provided.

Note that keyword search is done by splitting the text and finding any partial matches on the search keywords, so it is not done in specific order. We exclude common stop words from the search (from the nltk stop word list along with a few other common JSON keywords).

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-full-text.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=9d233d67218491a50cc759335a8ce6fa" alt="Filtering" data-og-width="368" width="368" data-og-height="301" height="301" data-path="langsmith/images/filter-full-text.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-full-text.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=bf7a2397bf1fda0c41a460bd8c2ae9f2 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-full-text.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=2ca993100e989cdfe8628b0d788fbd37 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-full-text.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=1c379c825a1759dc3e13f7c182475159 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-full-text.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=7e1eab353b75b25d22a901a01caff403 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-full-text.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=8fd894a5a8725b5f048bcc9d03b27d07 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-full-text.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=f3a68f1e2dc9ab3451f1d5e10733322c 2500w" />

Based on the filters above, the system will search for `python` and `tensorflow` in either inputs or outputs, and `embedding` in the inputs along with `fine` and `tune` in the outputs.

### Filter based on input / output key-value pairs

In addition to full-text search, you can filter runs based on specific key-value pairs in the inputs and outputs. This allows for more precise filtering, especially when dealing with structured data.

<Note>
  We index up to 100 unique keys to keep your data organized and searchable. Each key also has a character limit of 250 characters per value. If your data exceeds either of these limits, the text won't be indexed. This helps us ensure fast, reliable performance.
</Note>

To filter based on key-value pairs, select the `Input Key` or `Output Key` filter from the filters dropdown.

For example, to match the following input:

Select `Filters`, `Add Filter` to bring up the filtering options. Then select `Input Key`, enter `input` as the key and enter `What is the capital of France?` as the value.

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-input.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=fef9d6d6bb8c6d285df898ce9c93f192" alt="Filtering" data-og-width="575" width="575" data-og-height="132" height="132" data-path="langsmith/images/search-kv-input.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-input.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=6625f651706eae30b9f1ce9fcbfdb9b2 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-input.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=184a5582ff5deba63ca77aa203af7fbc 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-input.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=5d7c647a7249e766a43b9f0995a60ab1 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-input.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=901d0c7b928a44a753d42058aad52a8e 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-input.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=cbb901c4a190590ebc194999cf525260 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-input.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=7614d23d5ba9e06b8673905ef93e6fb5 2500w" />

You can also match nested keys by using dot notation to select the nested key name. For example, to match nested keys in the output:

Select `Output Key`, enter `documents.page_content` as the key and enter `The capital of France is Paris` as the value. This will match the nested key `documents.page_content` with the specified value.

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-output.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=6d66e1d62691463e05a7933bb3b2c0ce" alt="Filtering" data-og-width="708" width="708" data-og-height="95" height="95" data-path="langsmith/images/search-kv-output.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-output.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=a7f623484a5b5b5ab4c8a83f1288bed0 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-output.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=1756043b13d848b8b61043a88c06aa43 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-output.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=eef83852d80520b9941d72fe41cc4d64 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-output.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=683073a1b186796b7b9892748c1fbd94 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-output.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=1fdf9cf2ef7eaf699e0d7212ef2ea2ea 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-output.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=e8ab34552881a97fe451bfc3794b6cbb 2500w" />

You can add multiple key-value filters to create more complex queries. You can also use the `Filter Shortcuts` on the right side to quickly filter based on common key-value pairs as shown below:

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-filter-shortcut.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=9cec4279c975c71e67710c606a2dc700" alt="Filtering" data-og-width="637" width="637" data-og-height="702" height="702" data-path="langsmith/images/search-kv-filter-shortcut.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-filter-shortcut.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=667fbfc50987837e0f0188b8d1dbf1ca 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-filter-shortcut.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=ef422e47304d43568ae954e35c7b1764 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-filter-shortcut.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=c84e0935eb05be46679d57bb86219d07 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-filter-shortcut.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=ee8dcaf98f8fd9dc65d5b7f17df1804a 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-filter-shortcut.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=d0b79382681256ab905af3fc0d7c5660 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-filter-shortcut.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=8da0c834fad83b17d5cdf69bdd1ccb64 2500w" />

### Example: Filtering for tool calls

It's common to want to search for traces that contain specific tool calls. Tool calls are typically indicated in the output of an LLM run. To filter for tool calls, you would use the `Output Key` filter.

While this example will show you how to filter for tool calls, the same logic can be applied to filter for any key-value pair in the output.

In this case, let's assume this is the output you want to filter for:

With the example above, the KV search will map each nested JSON path as a key-value pair that you can use to search and filter.

LangSmith will break it into the following set of searchable key-value pairs:

| Key                                                | Value                                                                        |
| -------------------------------------------------- | ---------------------------------------------------------------------------- |
| `generations.type`                                 | `ChatGeneration`                                                             |
| `generations.message.type`                         | `constructor`                                                                |
| `generations.message.kwargs.type`                  | `ai`                                                                         |
| `generations.message.kwargs.id`                    | `run-ca7f7531-f4de-4790-9c3e-960be7f8b109`                                   |
| `generations.message.kwargs.tool_calls.name`       | `Plan`                                                                       |
| `generations.message.kwargs.tool_calls.args.steps` | `Research LangGraph's node configuration capabilities`                       |
| `generations.message.kwargs.tool_calls.args.steps` | `Investigate how to add a Python code execution node`                        |
| `generations.message.kwargs.tool_calls.args.steps` | `Find an example or create a sample implementation of a code execution node` |
| `generations.message.kwargs.tool_calls.id`         | `toolu_01XexPzAVknT3gRmUB5PK5BP`                                             |
| `generations.message.kwargs.tool_calls.type`       | `tool_call`                                                                  |
| `type`                                             | `LLMResult`                                                                  |

To search for a specific tool call, you can use the following Output Key search while removing the root runs filter:

`generations.message.kwargs.tool_calls.name` = `Plan`

This will match root and non-root runs where the `tool_calls` name is `Plan`.

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-tool.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=9d763b8572d14db7f1bc59a3cf4b08a5" alt="Filtering" data-og-width="629" width="629" data-og-height="98" height="98" data-path="langsmith/images/search-kv-tool.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-tool.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=9cd9ec450a02572f4bdcec3850bbf4c2 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-tool.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=a0988bd110eb51fc73f7e5e2875117b2 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-tool.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=21710580f012a7a62eeaf528f735f4c9 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-tool.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=5778f4ef1f156e7d2d0a04f45339aa72 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-tool.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=ae736245285787c485a6a085e5eec65e 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-tool.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=178a7e4dec57f7260393ee8d0b16030a 2500w" />

### Negative filtering on key-value pairs

Different types of negative filtering can be applied to `Metadata`, `Input Key`, and `Output Key` fields to exclude specific runs from your results.

For example, to find all runs where the metadata key `phone` is not equal to `1234567890`, set the `Metadata` `Key` operator to `is` and `Key` field to `phone`, then set the `Value` operator to `is not` and the `Value` field to `1234567890`. This will match all runs that have a metadata key `phone` with any value except `1234567890`.

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/negative-filtering-1.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=acbd683725e073e1ac78b8ba132e6d43" alt="Filtering" data-og-width="549" width="549" data-og-height="100" height="100" data-path="langsmith/images/negative-filtering-1.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/negative-filtering-1.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=a0fd6f85165cae0ca7a7d96be57805e1 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/negative-filtering-1.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=714cdc9f0ad4e6c2b67d49d79740e9b2 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/negative-filtering-1.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=cf74951b9cf760cbd6c5653b6d15b95a 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/negative-filtering-1.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=4ae07131bdb12d5e6c5676e51f54ee7c 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/negative-filtering-1.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=bfb45574bec84ad88d9866b025ac61ba 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/negative-filtering-1.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=e4a74fca06b1bf2940f15528b04f1b21 2500w" />

To find runs that don't have a specific metadata key, set the `Key` operator to `is not`. For example, setting the `Key` operator to `is not` with `phone` as the key will match all runs that don't have a `phone` field in their metadata.

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/negative-filtering-2.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=2c298181ed0aef202ca52f5c30b3d62b" alt="Filtering" data-og-width="419" width="419" data-og-height="128" height="128" data-path="langsmith/images/negative-filtering-2.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/negative-filtering-2.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=cb11546a30cf22c9653c05519b263409 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/negative-filtering-2.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=6a7f5a2ac54f187e449f24011af6f160 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/negative-filtering-2.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=161ed2a1433d0fa695ef527b32292ee6 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/negative-filtering-2.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=d7f8b99d1af28d2e27cdab3c3b03fdad 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/negative-filtering-2.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=ef65a43c85924cf25da164bc049e7029 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/negative-filtering-2.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=5c79162abd3187782b27f06f28a9d590 2500w" />

You can also filter for runs that neither have a specific key nor a specific value. To find runs where the metadata has neither the key `phone` nor any field with the value `1234567890`, set the `Key` operator to `is not` with key `phone`, and the `Value` operator to `is not` with value `1234567890`.

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/negative-filtering-3.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=18ca1b82babfda201b184a8884d120eb" alt="Filtering" data-og-width="571" width="571" data-og-height="125" height="125" data-path="langsmith/images/negative-filtering-3.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/negative-filtering-3.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=50c9bee0afc47df246ddb3313e1b7784 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/negative-filtering-3.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=5bbf4d4da70355e861ce7b7adf5ca4b5 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/negative-filtering-3.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=ba52269765451c26687cc7b74a572a68 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/negative-filtering-3.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=6b2b21485dc7073c4edc8b0d1b6718b1 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/negative-filtering-3.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=18b134952892cc7e5203a5dfaa25eda4 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/negative-filtering-3.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=f546a96e2b6b229cc593e8a31ec7572f 2500w" />

Finally, you can also filter for runs that do not have a specific key but have a specific value. To find runs where there is no `phone` key but there is a value of `1234567890` for some other key, set the `Key` operator to `is not` with key `phone`, and the `Value` operator to `is` with value `1234567890`.

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/negative-filtering-4.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=60e118205cc6524432058ce968aad478" alt="Filtering" data-og-width="546" width="546" data-og-height="126" height="126" data-path="langsmith/images/negative-filtering-4.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/negative-filtering-4.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=edd2fee83ba3d63bd8a44f82cfc2c66c 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/negative-filtering-4.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=20bfead7865f62ee4db7890734c811ac 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/negative-filtering-4.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=42d6832b0db4843441c5a8dcd95c88b7 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/negative-filtering-4.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=6361c16f6321f6f47ca83867f3f5bfbd 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/negative-filtering-4.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=6fcbe01d2be9672356101c4b9a585e78 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/negative-filtering-4.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=4dcb53d4331f8a1802ec49b1c3d1971a 2500w" />

Note that you can use the `does not contain` operator instead of `is not` to perform a substring match.

Saving filters allows you to store and reuse frequently used filter configurations. Saved filters are specific to a tracing project.

In the filter box, click the **Save filter** button after you have constructed your filter. This will bring up a dialog to specify the name and a description of the filter.

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/save-a-filter.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=27236a7b0e60a352d16b7affd9b913e5" alt="Filtering" data-og-width="854" width="854" data-og-height="775" height="775" data-path="langsmith/images/save-a-filter.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/save-a-filter.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=8bf93a5ab643e0f0ef5194529a084879 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/save-a-filter.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=6cb4d039bf74e97e65688465707560ab 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/save-a-filter.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=381acf9bb9f3c2443faeb4dc0c84506f 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/save-a-filter.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=fd1660d33217beb5031de6546ff2cdf7 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/save-a-filter.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=4b4bc3898549d5f505d0e8989307d5f4 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/save-a-filter.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=a9927179e81ace45255aef6a99c77aa3 2500w" />

#### Use a saved filter

After saving a filter, it is available in the filter bar as a quick filter for you to use. If you have more than three saved filters, only two will be displayed directly, with the rest accessible via a "more" menu. You can use the settings icon in the saved filter bar to optionally hide default saved filters.

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/selecting-a-filter.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=89774c393f6303c981adeba6750f7cad" alt="Filtering" data-og-width="1256" width="1256" data-og-height="404" height="404" data-path="langsmith/images/selecting-a-filter.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/selecting-a-filter.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=a204feab24664b271ddc70ea8dd60698 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/selecting-a-filter.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=4806fcac0a82770fccdb66b06181736e 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/selecting-a-filter.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=9600c7b41c39c9759347a7a5f6ba7d4d 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/selecting-a-filter.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=2f4a06c4a4dd87c995b54cae9f177e23 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/selecting-a-filter.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=5f5df78152c343362d6f3ba0c7a9a82a 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/selecting-a-filter.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=9d681b4c6a42c7c4a9440525bb1a3557 2500w" />

#### Update a saved filter

With the filter selected, make any changes to filter parameters. Then click **Update filter** > **Update** to update the filter.

In the same menu, you can also create a new saved filter by clicking **Update filter** > **Create new**.

#### Delete a saved filter

Click the settings icon in the saved filter bar, and delete a filter using the trash icon.

You can copy a constructed filter to share it with colleagues, reuse it later, or query runs programmatically in the [API](https://api.smith.langchain.com/redoc#tag/run/operation/query_runs_api_v1_runs_query_post) or [SDK](https://docs.smith.langchain.com/reference/python/client/langsmith.client.Client#langsmith.client.Client.list_runs).

In order to copy the filter, you can first create it in the UI. From there, you can click the copy button in the upper right hand corner. If you have constructed tree or trace filters, you can also copy those.

This will give you a string representing the filter in the LangSmith query language. For example: `and(eq(is_root, true), and(eq(feedback_key, "user_score"), eq(feedback_score, 1)))`. For more information on the query language syntax, please refer to [this reference](/langsmith/trace-query-syntax#filter-query-language).

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/copy-filter.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=f9f60cca965c1466fe58ec4e8cc52ea5" alt="Copy Filter" data-og-width="1123" width="1123" data-og-height="382" height="382" data-path="langsmith/images/copy-filter.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/copy-filter.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=8966107385058659add5005faa0570d5 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/copy-filter.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=54cdbad3c852e011f1bbf9bf59cd553d 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/copy-filter.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=198de895cd5c70de24e72b9eb5f8975d 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/copy-filter.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=b500fb05f2705eb74157d5a1ba2bebbe 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/copy-filter.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=67cdd3110def30e481ec0266eaffac66 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/copy-filter.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=f2366d6a3ab857b74bcbd551122ec350 2500w" />

## Filtering runs within the trace view

You can also apply filters directly within the trace view, which is useful for sifting through traces with a large number of runs. The same filters available in the main runs table view can be applied here.

By default, only the runs that match the filters will be shown. To see the matched runs within the broader context of the trace tree, switch the view option from "Filtered Only" to "Show All" or "Most relevant".

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-runs-in-trace-view.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=13ea9f30d6efd9bb3f96c8c2376a6858" alt="Filtering within trace view" data-og-width="1341" width="1341" data-og-height="764" height="764" data-path="langsmith/images/filter-runs-in-trace-view.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-runs-in-trace-view.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=506069ddcc28d8959213f1bfa5d4d2eb 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-runs-in-trace-view.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=8b4e2a130a76953f7291e983fb13c86d 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-runs-in-trace-view.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=f702ad52060f28de4febdc556daafc6e 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-runs-in-trace-view.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=4c6506211f69e03b54793fce9c85760b 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-runs-in-trace-view.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=ad5849855b303c6357705708e038a9d6 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-runs-in-trace-view.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=6fd17200291f3c6eb680861aa1ab7d80 2500w" />

## Manually specify a raw query in LangSmith query language

If you have [copied a previously constructed filter](/langsmith/filter-traces-in-application#copy-the-filter), you may want to manually apply this raw query in a future session.

In order to do this, you can click on **Advanced filters** on the bottom of the filters popover. From there you can paste a raw query into the text box.

Note that this will add that query to the existing queries, not overwrite it.

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/raw-query.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=0657c7420e760fb57782531d039c0b0d" alt="Raw Query" data-og-width="495" width="495" data-og-height="570" height="570" data-path="langsmith/images/raw-query.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/raw-query.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=8ebe5718f7e50c0e5186d23ea6467620 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/raw-query.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=121dab45e0a3f1599ad9dd26132f2c59 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/raw-query.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=0d3ca66c6498f2c1d4e33bf4b63068e8 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/raw-query.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=ef61bc19b9fe41eb0e5d80566049e288 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/raw-query.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=ccaeac2952f3a2fde4d99c5d69c8055d 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/raw-query.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=f15ec18a7e3cb748defc6d10c3033d28 2500w" />

## Use an AI Query to auto-generate a query (Experimental)

Sometimes figuring out the exact query to specify can be difficult! In order to make it easier, we've added an `AI Query` functionality. With this, you can type in the filter you want to construct in natural language and it will convert it into a valid query.

For example: "All runs longer than 10 seconds"

<img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/ai-query.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=ef4266d159753dacafbd5af2bcad0ab8" alt="AI Query" data-og-width="464" width="464" data-og-height="319" height="319" data-path="langsmith/images/ai-query.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/ai-query.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=9589aa2ef6d4fd16533c0b9123ab552e 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/ai-query.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=f03177102e2c6fa2ec2043bce0d3aa7d 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/ai-query.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=3f6c3c22f37565790e77fa8a55e459a6 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/ai-query.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=8ba2568d2044c01364206560d7dd8efc 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/ai-query.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=8bf30e3c64545cca142bf0a6745e19f6 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/ai-query.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=c38f6e1b2e00ef271d25f9f674aa27a4 2500w" />

### Filter for intermediate runs (spans) on properties of the root

A common concept is to filter for intermediate runs which are part of a trace whose root run has some attribute. An example is filtering for intermediate runs of a particular type whose root run has positive (or negative) feedback associated with it.

In order to do this, first set up a filter for intermediate runs (per the above section). After that, you can then add another filter rule. You can then click the `Advanced Filters` link all the way at the bottom of the filter. This will open up a new modal where you can add `Trace filters`. These filters will apply to the traces of all the parent runs of the individual runs you've already filtered for.

<img src="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-filter.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=a520558efb53e4dd3d6751db76739f90" alt="Filtering" data-og-width="551" width="551" data-og-height="542" height="542" data-path="langsmith/images/trace-filter.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-filter.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=5303c1b8db95a047593d52ff52ebc55f 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-filter.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=d21e37d06556b6cd20d88c6b72293f68 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-filter.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=700e831f6c8d473bb9eb3beca7131b4e 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-filter.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=43467c081d7cfc83582220fe6b7266fe 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-filter.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=9cda650d2069d852c95d8e51893e424d 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-filter.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=f0fa57df70f665ef672c917fe6231c15 2500w" />

### Filter for runs (spans) whose child runs have some attribute

This is the opposite of the above. You may want to search for runs who have specific types of sub runs. An example of this could be searching for all traces that had a sub run with name `Foo`. This is useful when `Foo` is not always called, but you want to analyze the cases where it is.

In order to do this, you can click on the `Advanced Filters` link all the way at the bottom of the filter. This will open up a new modal where you can add `Tree filters`. This will make the rule you specify apply to all child runs of the individual runs you've already filtered for.

<img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/child-runs.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=6b70fc03e9f46333d7af3566619204f0" alt="Filtering" data-og-width="471" width="471" data-og-height="602" height="602" data-path="langsmith/images/child-runs.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/child-runs.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=4e7f54072ed1f198804ffca3e073710c 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/child-runs.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=2c9e7b6627d5edece1b4f90879c03310 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/child-runs.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=265d349fc921cf06be7910f20f275a5e 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/child-runs.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=8445bba9df0671da9e718e63c4e2cc48 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/child-runs.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=cfd020e86ca4b5e2cc77ee43a0c35746 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/child-runs.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=199dab9b8f3adbaecadf3c2206ab008c 2500w" />

### Example: Filtering on all runs whose tree contains the tool call filter

Extending the [tool call filtering example](/langsmith/filter-traces-in-application#example-filtering-for-tool-calls) from above, if you would like to filter for all runs *whose tree contains* the tool filter call, you can use the tree filter in the [advanced filters](/langsmith/filter-traces-in-application#advanced-filters) setting:

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-tool-tree.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=b11deac10cd42be7efff94134658b25b" alt="Filtering" data-og-width="669" width="669" data-og-height="462" height="462" data-path="langsmith/images/search-kv-tool-tree.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-tool-tree.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=3f487a02af0891c8c97935b8674d8aba 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-tool-tree.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=4334fbf4536432920e35d1df4f5cff2e 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-tool-tree.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=c7b2edb49f520ebe3e2f03651c8d9d39 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-tool-tree.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=6c107e48006c1ee4d2246935b1259595 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-tool-tree.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=be785994ef014f06767d1001f5d5dbf1 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-tool-tree.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=c686bc45b1d33a12ddcc89aa9c5c0a35 2500w" />

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/filter-traces-in-application.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
Select `Filters`, `Add Filter` to bring up the filtering options. Then select `Input Key`, enter `input` as the key and enter `What is the capital of France?` as the value.

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-input.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=fef9d6d6bb8c6d285df898ce9c93f192" alt="Filtering" data-og-width="575" width="575" data-og-height="132" height="132" data-path="langsmith/images/search-kv-input.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-input.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=6625f651706eae30b9f1ce9fcbfdb9b2 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-input.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=184a5582ff5deba63ca77aa203af7fbc 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-input.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=5d7c647a7249e766a43b9f0995a60ab1 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-input.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=901d0c7b928a44a753d42058aad52a8e 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-input.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=cbb901c4a190590ebc194999cf525260 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-input.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=7614d23d5ba9e06b8673905ef93e6fb5 2500w" />

You can also match nested keys by using dot notation to select the nested key name. For example, to match nested keys in the output:
```

Example 2 (unknown):
```unknown
Select `Output Key`, enter `documents.page_content` as the key and enter `The capital of France is Paris` as the value. This will match the nested key `documents.page_content` with the specified value.

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-output.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=6d66e1d62691463e05a7933bb3b2c0ce" alt="Filtering" data-og-width="708" width="708" data-og-height="95" height="95" data-path="langsmith/images/search-kv-output.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-output.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=a7f623484a5b5b5ab4c8a83f1288bed0 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-output.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=1756043b13d848b8b61043a88c06aa43 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-output.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=eef83852d80520b9941d72fe41cc4d64 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-output.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=683073a1b186796b7b9892748c1fbd94 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-output.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=1fdf9cf2ef7eaf699e0d7212ef2ea2ea 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-output.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=e8ab34552881a97fe451bfc3794b6cbb 2500w" />

You can add multiple key-value filters to create more complex queries. You can also use the `Filter Shortcuts` on the right side to quickly filter based on common key-value pairs as shown below:

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-filter-shortcut.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=9cec4279c975c71e67710c606a2dc700" alt="Filtering" data-og-width="637" width="637" data-og-height="702" height="702" data-path="langsmith/images/search-kv-filter-shortcut.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-filter-shortcut.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=667fbfc50987837e0f0188b8d1dbf1ca 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-filter-shortcut.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=ef422e47304d43568ae954e35c7b1764 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-filter-shortcut.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=c84e0935eb05be46679d57bb86219d07 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-filter-shortcut.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=ee8dcaf98f8fd9dc65d5b7f17df1804a 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-filter-shortcut.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=d0b79382681256ab905af3fc0d7c5660 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-filter-shortcut.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=8da0c834fad83b17d5cdf69bdd1ccb64 2500w" />

### Example: Filtering for tool calls

It's common to want to search for traces that contain specific tool calls. Tool calls are typically indicated in the output of an LLM run. To filter for tool calls, you would use the `Output Key` filter.

While this example will show you how to filter for tool calls, the same logic can be applied to filter for any key-value pair in the output.

In this case, let's assume this is the output you want to filter for:
```

---

## Finally, we compile it!

**URL:** llms-txt#finally,-we-compile-it!

---

## Find memories about food preferences

**URL:** llms-txt#find-memories-about-food-preferences

---

## First call

**URL:** llms-txt#first-call

config = {"configurable": {"thread_id": "my-thread"}}
result = agent.invoke(input, config=config)

---

## First invocation

**URL:** llms-txt#first-invocation

agent.invoke(HumanMessage(content="I live in Sydney, Australia."))

---

## First let's just say hi to the AI

**URL:** llms-txt#first-let's-just-say-hi-to-the-ai

for update in graph.stream(
    {"messages": [{"role": "user", "content": "hi"}]}, config, stream_mode="updates"
):
    print(update)
python  theme={null}
def update_memory(state: MessagesState, config: RunnableConfig, *, store: BaseStore):

# Get the user id from the config
    user_id = config["configurable"]["user_id"]

# Namespace the memory
    namespace = (user_id, "memories")

# ... Analyze conversation and create a new memory

# Create a new memory ID
    memory_id = str(uuid.uuid4())

# We create a new memory
    store.put(namespace, memory_id, {"memory": memory})

python  theme={null}
memories[-1].dict()
{'value': {'food_preference': 'I like pizza'},
 'key': '07e0caf4-1631-47b7-b15f-65515d4c1843',
 'namespace': ['1', 'memories'],
 'created_at': '2024-10-02T17:22:31.590602+00:00',
 'updated_at': '2024-10-02T17:22:31.590605+00:00'}
python  theme={null}
def call_model(state: MessagesState, config: RunnableConfig, *, store: BaseStore):
    # Get the user id from the config
    user_id = config["configurable"]["user_id"]

# Namespace the memory
    namespace = (user_id, "memories")

# Search based on the most recent message
    memories = store.search(
        namespace,
        query=state["messages"][-1].content,
        limit=3
    )
    info = "\n".join([d.value["memory"] for d in memories])

# ... Use memories in the model call
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
We can access the `in_memory_store` and the `user_id` in *any node* by passing `store: BaseStore` and `config: RunnableConfig` as node arguments. Here's how we might use semantic search in a node to find relevant memories:
```

Example 2 (unknown):
```unknown
As we showed above, we can also access the store in any node and use the `store.search` method to get memories. Recall the memories are returned as a list of objects that can be converted to a dictionary.
```

Example 3 (unknown):
```unknown
We can access the memories and use them in our model call.
```

Example 4 (unknown):
```unknown
If we create a new thread, we can still access the same memories so long as the `user_id` is the same.
```

---

## First, post the runs to create them

**URL:** llms-txt#first,-post-the-runs-to-create-them

posts = [parent_run, child_run]
batch_ingest_runs(api_url, api_key, posts=posts)

---

## First session: save user info

**URL:** llms-txt#first-session:-save-user-info

agent.invoke({
    "messages": [{"role": "user", "content": "Save the following user: userid: abc123, name: Foo, age: 25, email: foo@langchain.dev"}]
})

---

## For certain unit tests, you may need to set certain flags and environment variables:

**URL:** llms-txt#for-certain-unit-tests,-you-may-need-to-set-certain-flags-and-environment-variables:

TIKTOKEN_CACHE_DIR=tiktoken_cache uv run --group test pytest --disable-socket --allow-unix-socket tests/unit_tests/

---

## for every request. This will determine whether the request is allowed or not

**URL:** llms-txt#for-every-request.-this-will-determine-whether-the-request-is-allowed-or-not

**Contents:**
- 3. Test your bot
- 4. Chat with your bot

@auth.authenticate
async def get_current_user(authorization: str | None) -> Auth.types.MinimalUserDict:
    """Check if the user's token is valid."""
    assert authorization
    scheme, token = authorization.split()
    assert scheme.lower() == "bearer"
    # Check if token is valid
    if token not in VALID_TOKENS:
        raise Auth.exceptions.HTTPException(status_code=401, detail="Invalid token")

# Return user info if valid
    user_data = VALID_TOKENS[token]
    return {
        "identity": user_data["id"],
    }
json {highlight={7-9}} title="langgraph.json" theme={null}
{
  "dependencies": ["."],
  "graphs": {
    "agent": "./src/agent/graph.py:graph"
  },
  "env": ".env",
  "auth": {
    "path": "src/security/auth.py:auth"
  }
}
bash  theme={null}
langgraph dev --no-browser
json  theme={null}
{
    "auth": {
        "path": "src/security/auth.py:auth",
        "disable_studio_auth": true
    }
}
python  theme={null}
from langgraph_sdk import get_client

**Examples:**

Example 1 (unknown):
```unknown
Notice that your [authentication](https://langchain-ai.github.io/langgraph/cloud/reference/sdk/python-sdk_ref#langgraph_sdk.auth.Auth.authenticate) handler does two important things:

1. Checks if a valid token is provided in the request's [Authorization header](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Authorization)
2. Returns the user's [identity](https://langchain-ai.github.io/langgraph/cloud/reference/sdk/python-sdk_ref#langgraph_sdk.auth.types.MinimalUserDict)

Now tell LangGraph to use authentication by adding the following to the [`langgraph.json`](/langsmith/cli#configuration-file) configuration:
```

Example 2 (unknown):
```unknown
## 3. Test your bot

Start the server again to test everything out:
```

Example 3 (unknown):
```unknown
If you didn't add the `--no-browser`, the Studio UI will open in the browser. By default, we also permit access from Studio, even when using custom auth. This makes it easier to develop and test your bot in Studio. You can remove this alternative authentication option by setting `disable_studio_auth: true` in your auth configuration:
```

Example 4 (unknown):
```unknown
## 4. Chat with your bot

You should now only be able to access the bot if you provide a valid token in the request header. Users will still, however, be able to access each other's resources until you add [resource authorization handlers](/langsmith/auth#resource-specific-handlers) in the next section of the tutorial.

<img src="https://mintcdn.com/langchain-5e9cc07a/IMK8wJkjSpMCGODD/langsmith/images/authentication.png?fit=max&auto=format&n=IMK8wJkjSpMCGODD&q=85&s=3ccfa86789baea630b8f418e9eb5b648" alt="Auth gate passes requests with a valid token, but no per-resource filters are applied yet—so users share visibility until authorization handlers are added in the next step." data-og-width="2617" width="2617" data-og-height="1673" height="1673" data-path="langsmith/images/authentication.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/IMK8wJkjSpMCGODD/langsmith/images/authentication.png?w=280&fit=max&auto=format&n=IMK8wJkjSpMCGODD&q=85&s=0216c0cf0cb74f67f43e65561a787c96 280w, https://mintcdn.com/langchain-5e9cc07a/IMK8wJkjSpMCGODD/langsmith/images/authentication.png?w=560&fit=max&auto=format&n=IMK8wJkjSpMCGODD&q=85&s=c4a9ab37e2413a38dc61ef6f4288c1b1 560w, https://mintcdn.com/langchain-5e9cc07a/IMK8wJkjSpMCGODD/langsmith/images/authentication.png?w=840&fit=max&auto=format&n=IMK8wJkjSpMCGODD&q=85&s=d1146dee549ab13c949efae260706988 840w, https://mintcdn.com/langchain-5e9cc07a/IMK8wJkjSpMCGODD/langsmith/images/authentication.png?w=1100&fit=max&auto=format&n=IMK8wJkjSpMCGODD&q=85&s=e430d8ebe4534d20c1d4d3d887f0c938 1100w, https://mintcdn.com/langchain-5e9cc07a/IMK8wJkjSpMCGODD/langsmith/images/authentication.png?w=1650&fit=max&auto=format&n=IMK8wJkjSpMCGODD&q=85&s=3aeaa334e3ff1457c165821966721f94 1650w, https://mintcdn.com/langchain-5e9cc07a/IMK8wJkjSpMCGODD/langsmith/images/authentication.png?w=2500&fit=max&auto=format&n=IMK8wJkjSpMCGODD&q=85&s=74d6dd4c73baa0182624085d13f08530 2500w" />

Run the following code in a file or notebook:
```

---

## For LangSmith API keys linked to multiple workspaces, set the LANGSMITH_WORKSPACE_ID environment variable to specify which workspace to use.

**URL:** llms-txt#for-langsmith-api-keys-linked-to-multiple-workspaces,-set-the-langsmith_workspace_id-environment-variable-to-specify-which-workspace-to-use.

**Contents:**
  - 3. Log a trace
- Without LangChain
  - 1. Installation
  - 2. Configure your environment

export LANGSMITH_WORKSPACE_ID=<your-workspace-id>
python Python theme={null}
  from typing import Literal
  from langchain.messages import HumanMessage
  from langchain_openai import ChatOpenAI
  from langchain.tools import tool
  from langgraph.prebuilt import ToolNode
  from langgraph.graph import StateGraph, MessagesState

@tool
  def search(query: str):
      """Call to surf the web."""
      if "sf" in query.lower() or "san francisco" in query.lower():
          return "It's 60 degrees and foggy."
      return "It's 90 degrees and sunny."

tools = [search]
  tool_node = ToolNode(tools)

model = ChatOpenAI(model="gpt-4o", temperature=0).bind_tools(tools)

def should_continue(state: MessagesState) -> Literal["tools", "__end__"]:
      messages = state['messages']
      last_message = messages[-1]
      if last_message.tool_calls:
          return "tools"
      return "__end__"

def call_model(state: MessagesState):
      messages = state['messages']
      # Invoking `model` will automatically infer the correct tracing context
      response = model.invoke(messages)
      return {"messages": [response]}

workflow = StateGraph(MessagesState)
  workflow.add_node("agent", call_model)
  workflow.add_node("tools", tool_node)
  workflow.add_edge("__start__", "agent")
  workflow.add_conditional_edges(
      "agent",
      should_continue,
  )
  workflow.add_edge("tools", 'agent')

app = workflow.compile()

final_state = app.invoke(
      {"messages": [HumanMessage(content="what is the weather in sf")]},
      config={"configurable": {"thread_id": 42}}
  )

final_state["messages"][-1].content
  typescript TypeScript theme={null}
  import { HumanMessage, AIMessage } from "@langchain/core/messages";
  import { tool } from "@langchain/core/tools";
  import { z } from "zod";
  import { ChatOpenAI } from "@langchain/openai";
  import { StateGraph, StateGraphArgs } from "@langchain/langgraph";
  import { ToolNode } from "@langchain/langgraph/prebuilt";

interface AgentState {
    messages: HumanMessage[];
  }

const graphState: StateGraphArgs<AgentState>["channels"] = {
    messages: {
      reducer: (x: HumanMessage[], y: HumanMessage[]) => x.concat(y),
    },
  };

const searchTool = tool(async ({ query }: { query: string }) => {
    if (query.toLowerCase().includes("sf") || query.toLowerCase().includes("san francisco")) {
      return "It's 60 degrees and foggy."
    }
    return "It's 90 degrees and sunny."
  }, {
    name: "search",
    description:
      "Call to surf the web.",
    schema: z.object({
      query: z.string().describe("The query to use in your search."),
    }),
  });

const tools = [searchTool];
  const toolNode = new ToolNode<AgentState>(tools);

const model = new ChatOpenAI({
    model: "gpt-4o",
    temperature: 0,
  }).bindTools(tools);

function shouldContinue(state: AgentState) {
    const messages = state.messages;
    const lastMessage = messages[messages.length - 1] as AIMessage;
    if (lastMessage.tool_calls?.length) {
      return "tools";
    }
    return "__end__";
  }

async function callModel(state: AgentState) {
    const messages = state.messages;
    // Invoking `model` will automatically infer the correct tracing context
    const response = await model.invoke(messages);
    return { messages: [response] };
  }

const workflow = new StateGraph<AgentState>({ channels: graphState })
    .addNode("agent", callModel)
    .addNode("tools", toolNode)
    .addEdge("__start__", "agent")
    .addConditionalEdges("agent", shouldContinue)
    .addEdge("tools", "agent");

const app = workflow.compile();

const finalState = await app.invoke(
    { messages: [new HumanMessage("what is the weather in sf")] },
    { configurable: { thread_id: "42" } }
  );

finalState.messages[finalState.messages.length - 1].content;
  bash pip theme={null}
  pip install openai langsmith langgraph
  bash yarn theme={null}
  yarn add openai langsmith @langchain/langgraph
  bash npm theme={null}
  npm install openai langsmith @langchain/langgraph
  bash pnpm theme={null}
  pnpm add openai langsmith @langchain/langgraph
  bash wrap theme={null}
export LANGSMITH_TRACING=true
export LANGSMITH_API_KEY=<your-api-key>

**Examples:**

Example 1 (unknown):
```unknown
<Info>
  If you are using LangChain.js with LangSmith and are not in a serverless environment, we also recommend setting the following explicitly to reduce latency:

  `export LANGCHAIN_CALLBACKS_BACKGROUND=true`

  If you are in a serverless environment, we recommend setting the reverse to allow tracing to finish before your function ends:

  `export LANGCHAIN_CALLBACKS_BACKGROUND=false`

  See [this LangChain.js guide](https://js.langchain.com/docs/how_to/callbacks_serverless) for more information.
</Info>

### 3. Log a trace

Once you've set up your environment, you can call LangChain runnables as normal. LangSmith will infer the proper tracing config:

<CodeGroup>
```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
</CodeGroup>

An example trace from running the above code [looks like this](https://smith.langchain.com/public/10863294-ee79-484a-927f-0558230f1547/r):

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-with-langchain-trace.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=a589f14351fb48e721205d1e363753ea" alt="Trace tree for a LangGraph run with LangChain" data-og-width="3314" width="3314" data-og-height="1766" height="1766" data-path="langsmith/images/langgraph-with-langchain-trace.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-with-langchain-trace.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=01650d2547ba6e440a66ceb0bdeb566a 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-with-langchain-trace.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=c0d7d3f04d58edd25e9b99d2a61890ce 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-with-langchain-trace.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=cf074976fbc12b0baad0e8320fd7fa47 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-with-langchain-trace.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=627f66d7b9e2b8a054e66e8dbc7afa73 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-with-langchain-trace.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=7946baa3ad54fa851cd8a34eb31b47b0 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-with-langchain-trace.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=d6b8e6a216b312f5ae3c74503fabba63 2500w" />

## Without LangChain

If you are using other SDKs or custom functions within LangGraph, you will need to [wrap or decorate them appropriately](/langsmith/annotate-code#use-traceable--traceable) (with the `@traceable` decorator in Python or the `traceable` function in JS, or something like e.g. `wrap_openai` for SDKs). If you do so, LangSmith will automatically nest traces from those wrapped methods.

Here's an example. You can also see this page for more information.

### 1. Installation

Install the LangGraph library and the OpenAI SDK for Python and JS (we use the OpenAI integration for the code snippets below).

<CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## For large datasets, lazily load documents

**URL:** llms-txt#for-large-datasets,-lazily-load-documents

**Contents:**
- By category
  - Webpages
  - PDFs
  - Cloud Providers
  - Social Platforms
  - Messaging Services
  - Productivity tools
  - Common File Types
- All document loaders

for document in loader.lazy_load():
    print(document)
```

The below document loaders allow you to load webpages.

| Document Loader                                                             | Description                                                                                                          | Package/API |
| --------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------- | ----------- |
| [Web](/oss/python/integrations/document_loaders/web_base)                   | Uses urllib and BeautifulSoup to load and parse HTML web pages                                                       | Package     |
| [Unstructured](/oss/python/integrations/document_loaders/unstructured_file) | Uses Unstructured to load and parse web pages                                                                        | Package     |
| [RecursiveURL](/oss/python/integrations/document_loaders/recursive_url)     | Recursively scrapes all child links from a root URL                                                                  | Package     |
| [Sitemap](/oss/python/integrations/document_loaders/sitemap)                | Scrapes all pages on a given sitemap                                                                                 | Package     |
| [Spider](/oss/python/integrations/document_loaders/spider)                  | Crawler and scraper that returns LLM-ready data                                                                      | API         |
| [Firecrawl](/oss/python/integrations/document_loaders/firecrawl)            | API service that can be deployed locally                                                                             | API         |
| [Docling](/oss/python/integrations/document_loaders/docling)                | Uses Docling to load and parse web pages                                                                             | Package     |
| [Hyperbrowser](/oss/python/integrations/document_loaders/hyperbrowser)      | Platform for running and scaling headless browsers, can be used to scrape/crawl any site                             | API         |
| [AgentQL](/oss/python/integrations/document_loaders/agentql)                | Web interaction and structured data extraction from any web page using an AgentQL query or a Natural Language prompt | API         |

The below document loaders allow you to load PDF documents.

| Document Loader                                                                    | Description                                          | Package/API |
| ---------------------------------------------------------------------------------- | ---------------------------------------------------- | ----------- |
| [PyPDF](/oss/python/integrations/document_loaders/pypdfloader)                     | Uses `pypdf` to load and parse PDFs                  | Package     |
| [Unstructured](/oss/python/integrations/document_loaders/unstructured_file)        | Uses Unstructured's open source library to load PDFs | Package     |
| [Amazon Textract](/oss/python/integrations/document_loaders/amazon_textract)       | Uses AWS API to load PDFs                            | API         |
| [MathPix](/oss/python/integrations/document_loaders/mathpix)                       | Uses MathPix to load PDFs                            | Package     |
| [PDFPlumber](/oss/python/integrations/document_loaders/pdfplumber)                 | Load PDF files using PDFPlumber                      | Package     |
| [PyPDFDirectry](/oss/python/integrations/document_loaders/pypdfdirectory)          | Load a directory with PDF files                      | Package     |
| [PyPDFium2](/oss/python/integrations/document_loaders/pypdfium2)                   | Load PDF files using PyPDFium2                       | Package     |
| [PyMuPDF](/oss/python/integrations/document_loaders/pymupdf)                       | Load PDF files using PyMuPDF                         | Package     |
| [PyMuPDF4LLM](/oss/python/integrations/document_loaders/pymupdf4llm)               | Load PDF content to Markdown using PyMuPDF4LLM       | Package     |
| [PDFMiner](/oss/python/integrations/document_loaders/pdfminer)                     | Load PDF files using PDFMiner                        | Package     |
| [Upstage Document Parse Loader](/oss/python/integrations/document_loaders/upstage) | Load PDF files using UpstageDocumentParseLoader      | Package     |
| [Docling](/oss/python/integrations/document_loaders/docling)                       | Load PDF files using Docling                         | Package     |
| [UnDatasIO](/oss/python/integrations/document_loaders/undatasio)                   | Load PDF files using UnDatasIO                       | Package     |
| [OpenDataLoader PDF](/oss/python/integrations/document_loaders/opendataloader_pdf) | Load PDF files using OpenDataLoader PDF              | Package     |

The below document loaders allow you to load documents from your favorite cloud providers.

| Document Loader                                                                                            | Description                                                 | Partner Package | API reference                                                                                                                                                                                  |
| ---------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------- | --------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [AWS S3 Directory](/oss/python/integrations/document_loaders/aws_s3_directory)                             | Load documents from an AWS S3 directory                     | ❌               | [`S3DirectoryLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.s3_directory.S3DirectoryLoader.html)                          |
| [AWS S3 File](/oss/python/integrations/document_loaders/aws_s3_file)                                       | Load documents from an AWS S3 file                          | ❌               | [`S3FileLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.s3_file.S3FileLoader.html)                                         |
| [Azure AI Data](/oss/python/integrations/document_loaders/azure_ai_data)                                   | Load documents from Azure AI services                       | ❌               | [`AzureAIDataLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.azure_ai_data.AzureAIDataLoader.html)                         |
| [Azure Blob Storage](/oss/python/integrations/document_loaders/azure_blob_storage)                         | Load documents from Azure Blob Storage                      | ✅               | [`AzureBlobStorageLoader`](https://reference.langchain.com/python/integrations/langchain_azure/storage/)                                                                                       |
| [Dropbox](/oss/python/integrations/document_loaders/dropbox)                                               | Load documents from Dropbox                                 | ❌               | [`DropboxLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.dropbox.DropboxLoader.html)                                       |
| [Google Cloud Storage Directory](/oss/python/integrations/document_loaders/google_cloud_storage_directory) | Load documents from GCS bucket                              | ✅               | [`GCSDirectoryLoader`](https://python.langchain.com/api_reference/google_community/gcs_directory/langchain_google_community.gcs_directory.GCSDirectoryLoader.html)                             |
| [Google Cloud Storage File](/oss/python/integrations/document_loaders/google_cloud_storage_file)           | Load documents from GCS file object                         | ✅               | [`GCSFileLoader`](https://python.langchain.com/api_reference/google_community/gcs_file/langchain_google_community.gcs_file.GCSFileLoader.html)                                                 |
| [Google Drive](/oss/python/integrations/document_loaders/google_drive)                                     | Load documents from Google Drive (Google Docs only)         | ✅               | [`GoogleDriveLoader`](https://python.langchain.com/api_reference/google_community/drive/langchain_google_community.drive.GoogleDriveLoader.html)                                               |
| [Huawei OBS Directory](/oss/python/integrations/document_loaders/huawei_obs_directory)                     | Load documents from Huawei Object Storage Service Directory | ❌               | [`OBSDirectoryLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.obs_directory.OBSDirectoryLoader.html)                       |
| [Huawei OBS File](/oss/python/integrations/document_loaders/huawei_obs_file)                               | Load documents from Huawei Object Storage Service File      | ❌               | [`OBSFileLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.obs_file.OBSFileLoader.html)                                      |
| [Microsoft OneDrive](/oss/python/integrations/document_loaders/microsoft_onedrive)                         | Load documents from Microsoft OneDrive                      | ❌               | [`OneDriveLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.onedrive.OneDriveLoader.html)                                    |
| [Microsoft SharePoint](/oss/python/integrations/document_loaders/microsoft_sharepoint)                     | Load documents from Microsoft SharePoint                    | ❌               | [`SharePointLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.sharepoint.SharePointLoader.html)                              |
| [Tencent COS Directory](/oss/python/integrations/document_loaders/tencent_cos_directory)                   | Load documents from Tencent Cloud Object Storage Directory  | ❌               | [`TencentCOSDirectoryLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.tencent_cos_directory.TencentCOSDirectoryLoader.html) |
| [Tencent COS File](/oss/python/integrations/document_loaders/tencent_cos_file)                             | Load documents from Tencent Cloud Object Storage File       | ❌               | [`TencentCOSFileLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.tencent_cos_file.TencentCOSFileLoader.html)                |

The below document loaders allow you to load documents from different social media platforms.

| Document Loader                                              | API reference                                                                                                                                                      |
| ------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| [Twitter](/oss/python/integrations/document_loaders/twitter) | [`TwitterTweetLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.twitter.TwitterTweetLoader.html) |
| [Reddit](/oss/python/integrations/document_loaders/reddit)   | [`RedditPostsLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.reddit.RedditPostsLoader.html)    |

### Messaging Services

The below document loaders allow you to load data from different messaging platforms.

| Document Loader                                                          | API reference                                                                                                                                                               |
| ------------------------------------------------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [Telegram](/oss/python/integrations/document_loaders/telegram)           | [`TelegramChatFileLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.telegram.TelegramChatFileLoader.html) |
| [WhatsApp](/oss/python/integrations/document_loaders/whatsapp_chat)      | [`WhatsAppChatLoader`](https://python.langchain.com/api_reference/community/chat_loaders/langchain_community.chat_loaders.whatsapp.WhatsAppChatLoader.html)                 |
| [Discord](/oss/python/integrations/document_loaders/discord)             | [`DiscordChatLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.discord.DiscordChatLoader.html)            |
| [Facebook Chat](/oss/python/integrations/document_loaders/facebook_chat) | [`FacebookChatLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.facebook_chat.FacebookChatLoader.html)    |
| [Mastodon](/oss/python/integrations/document_loaders/mastodon)           | [`MastodonTootsLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.mastodon.MastodonTootsLoader.html)       |

### Productivity tools

The below document loaders allow you to load data from commonly used productivity tools.

| Document Loader                                            | API reference                                                                                                                                                                  |
| ---------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| [Figma](/oss/python/integrations/document_loaders/figma)   | [`FigmaFileLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.figma.FigmaFileLoader.html)                     |
| [Notion](/oss/python/integrations/document_loaders/notion) | [`NotionDirectoryLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.notion.NotionDirectoryLoader.html)        |
| [Slack](/oss/python/integrations/document_loaders/slack)   | [`SlackDirectoryLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.slack_directory.SlackDirectoryLoader.html) |
| [Quip](/oss/python/integrations/document_loaders/quip)     | [`QuipLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.quip.QuipLoader.html)                                |
| [Trello](/oss/python/integrations/document_loaders/trello) | [`TrelloLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.trello.TrelloLoader.html)                          |
| [Roam](/oss/python/integrations/document_loaders/roam)     | [`RoamLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.roam.RoamLoader.html)                                |
| [GitHub](/oss/python/integrations/document_loaders/github) | [`GithubFileLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.github.GithubFileLoader.html)                  |

### Common File Types

The below document loaders allow you to load data from common data formats.

| Document Loader                                                                                | Data Type                                                                                                                                                                    |
| ---------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [CSVLoader](/oss/python/integrations/document_loaders/csv)                                     | CSV files                                                                                                                                                                    |
| [Unstructured](/oss/python/integrations/document_loaders/unstructured_file)                    | Many file types (see [https://docs.unstructured.io/platform/supported-file-types](https://docs.unstructured.io/platform/supported-file-types))                               |
| [JSONLoader](/oss/python/integrations/document_loaders/json)                                   | JSON files                                                                                                                                                                   |
| [BSHTMLLoader](/oss/python/integrations/document_loaders/bshtml)                               | HTML files                                                                                                                                                                   |
| [DoclingLoader](/oss/python/integrations/document_loaders/docling)                             | Various file types (see [https://ds4sd.github.io/docling/](https://ds4sd.github.io/docling/))                                                                                |
| [PolarisAIDataInsightLoader](/oss/python/integrations/document_loaders/polaris_ai_datainsight) | Various file types (see [https://datainsight.polarisoffice.com/documentation?docType=doc\_extract](https://datainsight.polarisoffice.com/documentation?docType=doc_extract)) |

## All document loaders

<Columns cols={3}>
  <Card title="acreom" icon="link" href="/oss/python/integrations/document_loaders/acreom" arrow="true" cta="View guide" />

<Card title="AgentQLLoader" icon="link" href="/oss/python/integrations/document_loaders/agentql" arrow="true" cta="View guide" />

<Card title="AirbyteLoader" icon="link" href="/oss/python/integrations/document_loaders/airbyte" arrow="true" cta="View guide" />

<Card title="Airtable" icon="link" href="/oss/python/integrations/document_loaders/airtable" arrow="true" cta="View guide" />

<Card title="Alibaba Cloud MaxCompute" icon="link" href="/oss/python/integrations/document_loaders/alibaba_cloud_maxcompute" arrow="true" cta="View guide" />

<Card title="Amazon Textract" icon="link" href="/oss/python/integrations/document_loaders/amazon_textract" arrow="true" cta="View guide" />

<Card title="Apify Dataset" icon="link" href="/oss/python/integrations/document_loaders/apify_dataset" arrow="true" cta="View guide" />

<Card title="ArxivLoader" icon="link" href="/oss/python/integrations/document_loaders/arxiv" arrow="true" cta="View guide" />

<Card title="AssemblyAI Audio Transcripts" icon="link" href="/oss/python/integrations/document_loaders/assemblyai" arrow="true" cta="View guide" />

<Card title="AstraDB" icon="link" href="/oss/python/integrations/document_loaders/astradb" arrow="true" cta="View guide" />

<Card title="Async Chromium" icon="link" href="/oss/python/integrations/document_loaders/async_chromium" arrow="true" cta="View guide" />

<Card title="AsyncHtml" icon="link" href="/oss/python/integrations/document_loaders/async_html" arrow="true" cta="View guide" />

<Card title="Athena" icon="link" href="/oss/python/integrations/document_loaders/athena" arrow="true" cta="View guide" />

<Card title="AWS S3 Directory" icon="link" href="/oss/python/integrations/document_loaders/aws_s3_directory" arrow="true" cta="View guide" />

<Card title="AWS S3 File" icon="link" href="/oss/python/integrations/document_loaders/aws_s3_file" arrow="true" cta="View guide" />

<Card title="AZLyrics" icon="link" href="/oss/python/integrations/document_loaders/azlyrics" arrow="true" cta="View guide" />

<Card title="Azure AI Data" icon="link" href="/oss/python/integrations/document_loaders/azure_ai_data" arrow="true" cta="View guide" />

<Card title="Azure Blob Storage" icon="link" href="/oss/python/integrations/document_loaders/azure_blob_storage" arrow="true" cta="View guide" />

<Card title="Azure AI Document Intelligence" icon="link" href="/oss/python/integrations/document_loaders/azure_document_intelligence" arrow="true" cta="View guide" />

<Card title="BibTeX" icon="link" href="/oss/python/integrations/document_loaders/bibtex" arrow="true" cta="View guide" />

<Card title="BiliBili" icon="link" href="/oss/python/integrations/document_loaders/bilibili" arrow="true" cta="View guide" />

<Card title="Blackboard" icon="link" href="/oss/python/integrations/document_loaders/blackboard" arrow="true" cta="View guide" />

<Card title="Blockchain" icon="link" href="/oss/python/integrations/document_loaders/blockchain" arrow="true" cta="View guide" />

<Card title="Box" icon="link" href="/oss/python/integrations/document_loaders/box" arrow="true" cta="View guide" />

<Card title="Brave Search" icon="link" href="/oss/python/integrations/document_loaders/brave_search" arrow="true" cta="View guide" />

<Card title="Browserbase" icon="link" href="/oss/python/integrations/document_loaders/browserbase" arrow="true" cta="View guide" />

<Card title="Browserless" icon="link" href="/oss/python/integrations/document_loaders/browserless" arrow="true" cta="View guide" />

<Card title="BSHTMLLoader" icon="link" href="/oss/python/integrations/document_loaders/bshtml" arrow="true" cta="View guide" />

<Card title="Cassandra" icon="link" href="/oss/python/integrations/document_loaders/cassandra" arrow="true" cta="View guide" />

<Card title="ChatGPT Data" icon="link" href="/oss/python/integrations/document_loaders/chatgpt_loader" arrow="true" cta="View guide" />

<Card title="College Confidential" icon="link" href="/oss/python/integrations/document_loaders/college_confidential" arrow="true" cta="View guide" />

<Card title="Concurrent Loader" icon="link" href="/oss/python/integrations/document_loaders/concurrent" arrow="true" cta="View guide" />

<Card title="Confluence" icon="link" href="/oss/python/integrations/document_loaders/confluence" arrow="true" cta="View guide" />

<Card title="CoNLL-U" icon="link" href="/oss/python/integrations/document_loaders/conll-u" arrow="true" cta="View guide" />

<Card title="Copy Paste" icon="link" href="/oss/python/integrations/document_loaders/copypaste" arrow="true" cta="View guide" />

<Card title="Couchbase" icon="link" href="/oss/python/integrations/document_loaders/couchbase" arrow="true" cta="View guide" />

<Card title="CSV" icon="link" href="/oss/python/integrations/document_loaders/csv" arrow="true" cta="View guide" />

<Card title="Cube Semantic Layer" icon="link" href="/oss/python/integrations/document_loaders/cube_semantic" arrow="true" cta="View guide" />

<Card title="Datadog Logs" icon="link" href="/oss/python/integrations/document_loaders/datadog_logs" arrow="true" cta="View guide" />

<Card title="Dedoc" icon="link" href="/oss/python/integrations/document_loaders/dedoc" arrow="true" cta="View guide" />

<Card title="Diffbot" icon="link" href="/oss/python/integrations/document_loaders/diffbot" arrow="true" cta="View guide" />

<Card title="Discord" icon="link" href="/oss/python/integrations/document_loaders/discord" arrow="true" cta="View guide" />

<Card title="Docling" icon="link" href="/oss/python/integrations/document_loaders/docling" arrow="true" cta="View guide" />

<Card title="Docugami" icon="link" href="/oss/python/integrations/document_loaders/docugami" arrow="true" cta="View guide" />

<Card title="Docusaurus" icon="link" href="/oss/python/integrations/document_loaders/docusaurus" arrow="true" cta="View guide" />

<Card title="Dropbox" icon="link" href="/oss/python/integrations/document_loaders/dropbox" arrow="true" cta="View guide" />

<Card title="Email" icon="link" href="/oss/python/integrations/document_loaders/email" arrow="true" cta="View guide" />

<Card title="EPub" icon="link" href="/oss/python/integrations/document_loaders/epub" arrow="true" cta="View guide" />

<Card title="Etherscan" icon="link" href="/oss/python/integrations/document_loaders/etherscan" arrow="true" cta="View guide" />

<Card title="EverNote" icon="link" href="/oss/python/integrations/document_loaders/evernote" arrow="true" cta="View guide" />

<Card title="Facebook Chat" icon="link" href="/oss/python/integrations/document_loaders/facebook_chat" arrow="true" cta="View guide" />

<Card title="Fauna" icon="link" href="/oss/python/integrations/document_loaders/fauna" arrow="true" cta="View guide" />

<Card title="Figma" icon="link" href="/oss/python/integrations/document_loaders/figma" arrow="true" cta="View guide" />

<Card title="FireCrawl" icon="link" href="/oss/python/integrations/document_loaders/firecrawl" arrow="true" cta="View guide" />

<Card title="Geopandas" icon="link" href="/oss/python/integrations/document_loaders/geopandas" arrow="true" cta="View guide" />

<Card title="Git" icon="link" href="/oss/python/integrations/document_loaders/git" arrow="true" cta="View guide" />

<Card title="GitBook" icon="link" href="/oss/python/integrations/document_loaders/gitbook" arrow="true" cta="View guide" />

<Card title="GitHub" icon="link" href="/oss/python/integrations/document_loaders/github" arrow="true" cta="View guide" />

<Card title="Glue Catalog" icon="link" href="/oss/python/integrations/document_loaders/glue_catalog" arrow="true" cta="View guide" />

<Card title="Google AlloyDB for PostgreSQL" icon="link" href="/oss/python/integrations/document_loaders/google_alloydb" arrow="true" cta="View guide" />

<Card title="Google BigQuery" icon="link" href="/oss/python/integrations/document_loaders/google_bigquery" arrow="true" cta="View guide" />

<Card title="Google Bigtable" icon="link" href="/oss/python/integrations/document_loaders/google_bigtable" arrow="true" cta="View guide" />

<Card title="Google Cloud SQL for SQL Server" icon="link" href="/oss/python/integrations/document_loaders/google_cloud_sql_mssql" arrow="true" cta="View guide" />

<Card title="Google Cloud SQL for MySQL" icon="link" href="/oss/python/integrations/document_loaders/google_cloud_sql_mysql" arrow="true" cta="View guide" />

<Card title="Google Cloud SQL for PostgreSQL" icon="link" href="/oss/python/integrations/document_loaders/google_cloud_sql_pg" arrow="true" cta="View guide" />

<Card title="Google Cloud Storage Directory" icon="link" href="/oss/python/integrations/document_loaders/google_cloud_storage_directory" arrow="true" cta="View guide" />

<Card title="Google Cloud Storage File" icon="link" href="/oss/python/integrations/document_loaders/google_cloud_storage_file" arrow="true" cta="View guide" />

<Card title="Google Firestore in Datastore Mode" icon="link" href="/oss/python/integrations/document_loaders/google_datastore" arrow="true" cta="View guide" />

<Card title="Google Drive" icon="link" href="/oss/python/integrations/document_loaders/google_drive" arrow="true" cta="View guide" />

<Card title="Google El Carro for Oracle Workloads" icon="link" href="/oss/python/integrations/document_loaders/google_el_carro" arrow="true" cta="View guide" />

<Card title="Google Firestore (Native Mode)" icon="link" href="/oss/python/integrations/document_loaders/google_firestore" arrow="true" cta="View guide" />

<Card title="Google Memorystore for Redis" icon="link" href="/oss/python/integrations/document_loaders/google_memorystore_redis" arrow="true" cta="View guide" />

<Card title="Google Spanner" icon="link" href="/oss/python/integrations/document_loaders/google_spanner" arrow="true" cta="View guide" />

<Card title="Google Speech-to-Text" icon="link" href="/oss/python/integrations/document_loaders/google_speech_to_text" arrow="true" cta="View guide" />

<Card title="Grobid" icon="link" href="/oss/python/integrations/document_loaders/grobid" arrow="true" cta="View guide" />

<Card title="Gutenberg" icon="link" href="/oss/python/integrations/document_loaders/gutenberg" arrow="true" cta="View guide" />

<Card title="Hacker News" icon="link" href="/oss/python/integrations/document_loaders/hacker_news" arrow="true" cta="View guide" />

<Card title="Huawei OBS Directory" icon="link" href="/oss/python/integrations/document_loaders/huawei_obs_directory" arrow="true" cta="View guide" />

<Card title="Huawei OBS File" icon="link" href="/oss/python/integrations/document_loaders/huawei_obs_file" arrow="true" cta="View guide" />

<Card title="HuggingFace Dataset" icon="link" href="/oss/python/integrations/document_loaders/hugging_face_dataset" arrow="true" cta="View guide" />

<Card title="HyperbrowserLoader" icon="link" href="/oss/python/integrations/document_loaders/hyperbrowser" arrow="true" cta="View guide" />

<Card title="iFixit" icon="link" href="/oss/python/integrations/document_loaders/ifixit" arrow="true" cta="View guide" />

<Card title="Images" icon="link" href="/oss/python/integrations/document_loaders/image" arrow="true" cta="View guide" />

<Card title="Image Captions" icon="link" href="/oss/python/integrations/document_loaders/image_captions" arrow="true" cta="View guide" />

<Card title="IMSDb" icon="link" href="/oss/python/integrations/document_loaders/imsdb" arrow="true" cta="View guide" />

<Card title="Iugu" icon="link" href="/oss/python/integrations/document_loaders/iugu" arrow="true" cta="View guide" />

<Card title="Joplin" icon="link" href="/oss/python/integrations/document_loaders/joplin" arrow="true" cta="View guide" />

<Card title="JSONLoader" icon="link" href="/oss/python/integrations/document_loaders/json" arrow="true" cta="View guide" />

<Card title="Jupyter Notebook" icon="link" href="/oss/python/integrations/document_loaders/jupyter_notebook" arrow="true" cta="View guide" />

<Card title="Kinetica" icon="link" href="/oss/python/integrations/document_loaders/kinetica" arrow="true" cta="View guide" />

<Card title="lakeFS" icon="link" href="/oss/python/integrations/document_loaders/lakefs" arrow="true" cta="View guide" />

<Card title="LangSmith" icon="link" href="/oss/python/integrations/document_loaders/langsmith" arrow="true" cta="View guide" />

<Card title="LarkSuite (FeiShu)" icon="link" href="/oss/python/integrations/document_loaders/larksuite" arrow="true" cta="View guide" />

<Card title="LLM Sherpa" icon="link" href="/oss/python/integrations/document_loaders/llmsherpa" arrow="true" cta="View guide" />

<Card title="Mastodon" icon="link" href="/oss/python/integrations/document_loaders/mastodon" arrow="true" cta="View guide" />

<Card title="MathPixPDFLoader" icon="link" href="/oss/python/integrations/document_loaders/mathpix" arrow="true" cta="View guide" />

<Card title="MediaWiki Dump" icon="link" href="/oss/python/integrations/document_loaders/mediawikidump" arrow="true" cta="View guide" />

<Card title="Merge Documents Loader" icon="link" href="/oss/python/integrations/document_loaders/merge_doc" arrow="true" cta="View guide" />

<Card title="MHTML" icon="link" href="/oss/python/integrations/document_loaders/mhtml" arrow="true" cta="View guide" />

<Card title="Microsoft Excel" icon="link" href="/oss/python/integrations/document_loaders/microsoft_excel" arrow="true" cta="View guide" />

<Card title="Microsoft OneDrive" icon="link" href="/oss/python/integrations/document_loaders/microsoft_onedrive" arrow="true" cta="View guide" />

<Card title="Microsoft OneNote" icon="link" href="/oss/python/integrations/document_loaders/microsoft_onenote" arrow="true" cta="View guide" />

<Card title="Microsoft PowerPoint" icon="link" href="/oss/python/integrations/document_loaders/microsoft_powerpoint" arrow="true" cta="View guide" />

<Card title="Microsoft SharePoint" icon="link" href="/oss/python/integrations/document_loaders/microsoft_sharepoint" arrow="true" cta="View guide" />

<Card title="Microsoft Word" icon="link" href="/oss/python/integrations/document_loaders/microsoft_word" arrow="true" cta="View guide" />

<Card title="Near Blockchain" icon="link" href="/oss/python/integrations/document_loaders/mintbase" arrow="true" cta="View guide" />

<Card title="Modern Treasury" icon="link" href="/oss/python/integrations/document_loaders/modern_treasury" arrow="true" cta="View guide" />

<Card title="MongoDB" icon="link" href="/oss/python/integrations/document_loaders/mongodb" arrow="true" cta="View guide" />

<Card title="Needle Document Loader" icon="link" href="/oss/python/integrations/document_loaders/needle" arrow="true" cta="View guide" />

<Card title="News URL" icon="link" href="/oss/python/integrations/document_loaders/news" arrow="true" cta="View guide" />

<Card title="Notion DB" icon="link" href="/oss/python/integrations/document_loaders/notion" arrow="true" cta="View guide" />

<Card title="Nuclia" icon="link" href="/oss/python/integrations/document_loaders/nuclia" arrow="true" cta="View guide" />

<Card title="Obsidian" icon="link" href="/oss/python/integrations/document_loaders/obsidian" arrow="true" cta="View guide" />

<Card title="OpenDataLoader PDF" icon="link" href="/oss/python/integrations/document_loaders/opendataloader_pdf" arrow="true" cta="View guide" />

<Card title="Open Document Format (ODT)" icon="link" href="/oss/python/integrations/document_loaders/odt" arrow="true" cta="View guide" />

<Card title="Open City Data" icon="link" href="/oss/python/integrations/document_loaders/open_city_data" arrow="true" cta="View guide" />

<Card title="Oracle Autonomous Database" icon="link" href="/oss/python/integrations/document_loaders/oracleadb_loader" arrow="true" cta="View guide" />

<Card title="Oracle AI Vector Search" icon="link" href="/oss/python/integrations/document_loaders/oracleai" arrow="true" cta="View guide" />

<Card title="Org-mode" icon="link" href="/oss/python/integrations/document_loaders/org_mode" arrow="true" cta="View guide" />

<Card title="Outline Document Loader" icon="link" href="/oss/python/integrations/document_loaders/outline" arrow="true" cta="View guide" />

<Card title="Pandas DataFrame" icon="link" href="/oss/python/integrations/document_loaders/pandas_dataframe" arrow="true" cta="View guide" />

<Card title="PDFMinerLoader" icon="link" href="/oss/python/integrations/document_loaders/pdfminer" arrow="true" cta="View guide" />

<Card title="PDFPlumber" icon="link" href="/oss/python/integrations/document_loaders/pdfplumber" arrow="true" cta="View guide" />

<Card title="Pebblo Safe DocumentLoader" icon="link" href="/oss/python/integrations/document_loaders/pebblo" arrow="true" cta="View guide" />

<Card title="Polaris AI DataInsight" icon="link" href="/oss/python/integrations/document_loaders/polaris_ai_datainsight" arrow="true" cta="View guide" />

<Card title="Polars DataFrame" icon="link" href="/oss/python/integrations/document_loaders/polars_dataframe" arrow="true" cta="View guide" />

<Card title="Dell PowerScale" icon="link" href="/oss/python/integrations/document_loaders/powerscale" arrow="true" cta="View guide" />

<Card title="Psychic" icon="link" href="/oss/python/integrations/document_loaders/psychic" arrow="true" cta="View guide" />

<Card title="PubMed" icon="link" href="/oss/python/integrations/document_loaders/pubmed" arrow="true" cta="View guide" />

<Card title="PullMdLoader" icon="link" href="/oss/python/integrations/document_loaders/pull_md" arrow="true" cta="View guide" />

<Card title="PyMuPDFLoader" icon="link" href="/oss/python/integrations/document_loaders/pymupdf" arrow="true" cta="View guide" />

<Card title="PyMuPDF4LLM" icon="link" href="/oss/python/integrations/document_loaders/pymupdf4llm" arrow="true" cta="View guide" />

<Card title="PyPDFDirectoryLoader" icon="link" href="/oss/python/integrations/document_loaders/pypdfdirectory" arrow="true" cta="View guide" />

<Card title="PyPDFium2Loader" icon="link" href="/oss/python/integrations/document_loaders/pypdfium2" arrow="true" cta="View guide" />

<Card title="PyPDFLoader" icon="link" href="/oss/python/integrations/document_loaders/pypdfloader" arrow="true" cta="View guide" />

<Card title="PySpark" icon="link" href="/oss/python/integrations/document_loaders/pyspark_dataframe" arrow="true" cta="View guide" />

<Card title="Quip" icon="link" href="/oss/python/integrations/document_loaders/quip" arrow="true" cta="View guide" />

<Card title="ReadTheDocs Documentation" icon="link" href="/oss/python/integrations/document_loaders/readthedocs_documentation" arrow="true" cta="View guide" />

<Card title="Recursive URL" icon="link" href="/oss/python/integrations/document_loaders/recursive_url" arrow="true" cta="View guide" />

<Card title="Reddit" icon="link" href="/oss/python/integrations/document_loaders/reddit" arrow="true" cta="View guide" />

<Card title="Roam" icon="link" href="/oss/python/integrations/document_loaders/roam" arrow="true" cta="View guide" />

<Card title="Rockset" icon="link" href="/oss/python/integrations/document_loaders/rockset" arrow="true" cta="View guide" />

<Card title="rspace" icon="link" href="/oss/python/integrations/document_loaders/rspace" arrow="true" cta="View guide" />

<Card title="RSS Feeds" icon="link" href="/oss/python/integrations/document_loaders/rss" arrow="true" cta="View guide" />

<Card title="RST" icon="link" href="/oss/python/integrations/document_loaders/rst" arrow="true" cta="View guide" />

<Card title="scrapfly" icon="link" href="/oss/python/integrations/document_loaders/scrapfly" arrow="true" cta="View guide" />

<Card title="ScrapingAnt" icon="link" href="/oss/python/integrations/document_loaders/scrapingant" arrow="true" cta="View guide" />

<Card title="SingleStore" icon="link" href="/oss/python/integrations/document_loaders/singlestore" arrow="true" cta="View guide" />

<Card title="Sitemap" icon="link" href="/oss/python/integrations/document_loaders/sitemap" arrow="true" cta="View guide" />

<Card title="Slack" icon="link" href="/oss/python/integrations/document_loaders/slack" arrow="true" cta="View guide" />

<Card title="Snowflake" icon="link" href="/oss/python/integrations/document_loaders/snowflake" arrow="true" cta="View guide" />

<Card title="Source Code" icon="link" href="/oss/python/integrations/document_loaders/source_code" arrow="true" cta="View guide" />

<Card title="Spider" icon="link" href="/oss/python/integrations/document_loaders/spider" arrow="true" cta="View guide" />

<Card title="Spreedly" icon="link" href="/oss/python/integrations/document_loaders/spreedly" arrow="true" cta="View guide" />

<Card title="Stripe" icon="link" href="/oss/python/integrations/document_loaders/stripe" arrow="true" cta="View guide" />

<Card title="Subtitle" icon="link" href="/oss/python/integrations/document_loaders/subtitle" arrow="true" cta="View guide" />

<Card title="SurrealDB" icon="link" href="/oss/python/integrations/document_loaders/surrealdb" arrow="true" cta="View guide" />

<Card title="Telegram" icon="link" href="/oss/python/integrations/document_loaders/telegram" arrow="true" cta="View guide" />

<Card title="Tencent COS Directory" icon="link" href="/oss/python/integrations/document_loaders/tencent_cos_directory" arrow="true" cta="View guide" />

<Card title="Tencent COS File" icon="link" href="/oss/python/integrations/document_loaders/tencent_cos_file" arrow="true" cta="View guide" />

<Card title="TensorFlow Datasets" icon="link" href="/oss/python/integrations/document_loaders/tensorflow_datasets" arrow="true" cta="View guide" />

<Card title="TiDB" icon="link" href="/oss/python/integrations/document_loaders/tidb" arrow="true" cta="View guide" />

<Card title="2Markdown" icon="link" href="/oss/python/integrations/document_loaders/tomarkdown" arrow="true" cta="View guide" />

<Card title="TOML" icon="link" href="/oss/python/integrations/document_loaders/toml" arrow="true" cta="View guide" />

<Card title="Trello" icon="link" href="/oss/python/integrations/document_loaders/trello" arrow="true" cta="View guide" />

<Card title="TSV" icon="link" href="/oss/python/integrations/document_loaders/tsv" arrow="true" cta="View guide" />

<Card title="Twitter" icon="link" href="/oss/python/integrations/document_loaders/twitter" arrow="true" cta="View guide" />

<Card title="UnDatasIO" icon="link" href="/oss/python/integrations/document_loaders/undatasio" arrow="true" cta="View guide" />

<Card title="Unstructured" icon="link" href="/oss/python/integrations/document_loaders/unstructured_file" arrow="true" cta="View guide" />

<Card title="UnstructuredMarkdownLoader" icon="link" href="/oss/python/integrations/document_loaders/unstructured_markdown" arrow="true" cta="View guide" />

<Card title="UnstructuredPDFLoader" icon="link" href="/oss/python/integrations/document_loaders/unstructured_pdfloader" arrow="true" cta="View guide" />

<Card title="Upstage" icon="link" href="/oss/python/integrations/document_loaders/upstage" arrow="true" cta="View guide" />

<Card title="URL" icon="link" href="/oss/python/integrations/document_loaders/url" arrow="true" cta="View guide" />

<Card title="Vsdx" icon="link" href="/oss/python/integrations/document_loaders/vsdx" arrow="true" cta="View guide" />

<Card title="Weather" icon="link" href="/oss/python/integrations/document_loaders/weather" arrow="true" cta="View guide" />

<Card title="WebBaseLoader" icon="link" href="/oss/python/integrations/document_loaders/web_base" arrow="true" cta="View guide" />

<Card title="WhatsApp Chat" icon="link" href="/oss/python/integrations/document_loaders/whatsapp_chat" arrow="true" cta="View guide" />

<Card title="Wikipedia" icon="link" href="/oss/python/integrations/document_loaders/wikipedia" arrow="true" cta="View guide" />

<Card title="UnstructuredXMLLoader" icon="link" href="/oss/python/integrations/document_loaders/xml" arrow="true" cta="View guide" />

<Card title="Xorbits Pandas DataFrame" icon="link" href="/oss/python/integrations/document_loaders/xorbits" arrow="true" cta="View guide" />

<Card title="YouTube Audio" icon="link" href="/oss/python/integrations/document_loaders/youtube_audio" arrow="true" cta="View guide" />

<Card title="YouTube Transcripts" icon="link" href="/oss/python/integrations/document_loaders/youtube_transcript" arrow="true" cta="View guide" />

<Card title="YoutubeLoaderDL" icon="link" href="/oss/python/integrations/document_loaders/yt_dlp" arrow="true" cta="View guide" />

<Card title="Yuque" icon="link" href="/oss/python/integrations/document_loaders/yuque" arrow="true" cta="View guide" />

<Card title="ZeroxPDFLoader" icon="link" href="/oss/python/integrations/document_loaders/zeroxpdfloader" arrow="true" cta="View guide" />
</Columns>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/document_loaders/index.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## For production use, consider using a configuration file or environment variables

**URL:** llms-txt#for-production-use,-consider-using-a-configuration-file-or-environment-variables

api_url = "https://api.smith.langchain.com"
api_key = os.environ.get("LANGSMITH_API_KEY")

if not api_key:
    raise ValueError("LANGSMITH_API_KEY environment variable is not set")

---

## Frequently Asked Questions

**URL:** llms-txt#frequently-asked-questions

**Contents:**
- Questions and Answers
  - I've been using LangSmith since before pricing took effect for new users. When will pricing go into effect for my account?
  - Which plan is right for me?
  - What is a seat?
  - What is a trace?
  - What is an ingested event?
  - I've hit my rate or usage limits. What can I do?
  - I have a developer account, can I upgrade my account to the Plus or Enterprise plan?
  - How does billing work?
  - Can I limit how much I spend on tracing?

Source: https://docs.langchain.com/langsmith/pricing-faq

## Questions and Answers

### I've been using LangSmith since before pricing took effect for new users. When will pricing go into effect for my account?

If you've been using LangSmith already, your usage will be billable starting in July 2024. At that point if you want to add seats or use more than the monthly allotment of free traces, you will need to add a credit card to LangSmith or contact sales. If you are interested in the Enterprise plan with higher rate limits and special deployment options, you can learn more or make a purchase by [contacting our sales team](https://www.langchain.com/contact-sales).

### Which plan is right for me?

If you're an individual developer, the Developer plan is a great choice for small projects.

For teams that want to collaborate in LangSmith, check out the Plus plan. **If you are an early-stage startup building an AI application**, you may be eligible for our Startup plan with discounted prices and a generous free monthly trace allotment. Please reach out via our [Startup Contact Form](https://airtable.com/app8ZrGLtHAtFVO1o/pagfLAmdTz4ep7TGu/form) for more details.

If you need more advanced administration, authentication and authorization, deployment options, support, or annual invoicing, the Enterprise plan is right for you. Please reach out via our [Sales Contact Form](https://www.langchain.com/contact-sales) for more details.

A seat is a distinct user inside your organization. We consider the total number of users (including invited users) to determine the number of seats to bill.

A trace is one complete invocation of your application chain or agent, evaluator run, or playground run. Here is an [example](https://smith.langchain.com/public/17c24270-9f74-47e7-b70c-d508afc448fa/r) of a single trace.

### What is an ingested event?

An ingested event is any distinct, trace-related data sent to LangSmith. This includes:

* Inputs, outputs and metadata sent at the start of a run step within a trace
* Inputs, outputs and metadata sent at the end of a run step within a trace
* Feedback on run steps or traces

### I've hit my rate or usage limits. What can I do?

When you first sign up for a LangSmith account, you get a Personal organization that is limited to 5000 monthly traces. To continue sending traces after reaching this limit, upgrade to the Developer or Plus plans by adding a credit card. Head to [Plans and Billing](https://smith.langchain.com/settings/payments) to upgrade.

Similarly, if you've hit the rate limits on your current plan, you can upgrade to a higher plan to get higher limits, or reach out to [support@langchain.dev](mailto:support@langchain.dev) with questions.

### I have a developer account, can I upgrade my account to the Plus or Enterprise plan?

Yes, Developer plan users can easily upgrade to the Plus plan on the [Plans and Billing](https://smith.langchain.com/settings/payments) page. For the Enterprise plan, please [contact our sales team](https://www.langchain.com/contact-sales) to discuss your needs.

### How does billing work?

Seats are billed monthly on the first of the month. Additional seats purchased mid-month are pro-rated and billed within one day of the purchase. Seats removed mid-month will not be credited.

As long as you have a card on file in your account, we'll service your traces and bill you on the first of the month for traces that you submitted in the previous month. You will be able to set usage limits if you so choose to limit the maximum charges you could incur in any given month.

### Can I limit how much I spend on tracing?

You can set limits on the number of traces that can be sent to LangSmith per month on the [Usage configuration](https://smith.langchain.com/settings/payments) page.

<Note>
  While we do show you the dollar value of your usage limit for convenience, this limit evaluated in terms of number of traces instead of dollar amount. For example, if you are approved for our startup plan tier where you are given a generous allotment of free traces, your usage limit will not automatically change.

You are not currently able to set a spend limit in the product.
</Note>

### How can I track my usage so far this month?

Under the Settings section for your Organization you will see subsection for **Usage**. There, you will be able to see a graph of the daily number of billable LangSmith traces from the last 30, 60, or 90 days. Note that this data is delayed by 1-2 hours and so may trail your actual number of runs slightly for the current day.

### I have a question about my bill...

Customers on the Developer and Plus plan tiers should email [support@langchain.dev](mailto:support@langchain.dev). Customers on the Enterprise plan should contact their sales representative directly.

Enterprise plan customers are billed annually by invoice.

### What can I expect from Support?

On the Developer plan, community-based support is available on [LangChain community Slack](https://www.langchain.com/join-community).

On the Plus plan, you will also receive preferential, email support at [support@langchain.dev](mailto:support@langchain.dev) for LangSmith-related questions only and we'll do our best to respond within the next business day.

On the Enterprise plan, you'll get white-glove support with a Slack channel, a dedicated customer success manager, and monthly check-ins to go over LangSmith and LangChain questions. We can help with anything from debugging, agent and RAG techniques, evaluation approaches, and cognitive architecture reviews. If you purchase the add-on to run LangSmith in your environment, we'll also support deployments and new releases with our infra engineering team on-call.

### Where is my data stored?

You may choose to sign up in either the US or EU region. See the [cloud architecture reference](/langsmith/cloud#cloud-architecture-and-scalability) for more details. If you're on the Enterprise plan, we can deliver LangSmith to run on your kubernetes cluster in AWS, GCP, or Azure so that data never leaves your environment.

### Which security frameworks is LangSmith compliant with?

We are SOC 2 Type II, GDPR, and HIPAA compliant.

You can request more information about our security policies and posture at [trust.langchain.com](https://trust.langchain.com). Please note we only enter into BAAs with customers on our Enterprise plan.

### Will you train on the data that I send LangSmith?

We will not train on your data, and you own all rights to your data. See [LangSmith Terms of Service](https://langchain.dev/terms-of-service) for more information.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/pricing-faq.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## from langsmith import aevaluate

**URL:** llms-txt#from-langsmith-import-aevaluate

---

## Functional API overview

**URL:** llms-txt#functional-api-overview

**Contents:**
- Functional API vs. Graph API
- Example
- Entrypoint
  - Definition
  - Executing
  - Resuming
  - Short-term memory
- Task
  - Definition
  - Execution

Source: https://docs.langchain.com/oss/javascript/langgraph/functional-api

The **Functional API** allows you to add LangGraph's key features — [persistence](/oss/javascript/langgraph/persistence), [memory](/oss/javascript/langgraph/add-memory), [human-in-the-loop](/oss/javascript/langgraph/interrupts), and [streaming](/oss/javascript/langgraph/streaming) — to your applications with minimal changes to your existing code.

It is designed to integrate these features into existing code that may use standard language primitives for branching and control flow, such as `if` statements, `for` loops, and function calls. Unlike many data orchestration frameworks that require restructuring code into an explicit pipeline or DAG, the Functional API allows you to incorporate these capabilities without enforcing a rigid execution model.

The Functional API uses two key building blocks:

* **`entrypoint`** – An entrypoint encapsulates workflow logic and manages execution flow, including handling long-running tasks and interrupts.
* **`task`** – Represents a discrete unit of work, such as an API call or data processing step, that can be executed asynchronously within an entrypoint. Tasks return a future-like object that can be awaited or resolved synchronously.

This provides a minimal abstraction for building workflows with state management and streaming.

<Tip>
  For information on how to use the functional API, see [Use Functional API](/oss/javascript/langgraph/use-functional-api).
</Tip>

## Functional API vs. Graph API

For users who prefer a more declarative approach, LangGraph's [Graph API](/oss/javascript/langgraph/graph-api) allows you to define workflows using a Graph paradigm. Both APIs share the same underlying runtime, so you can use them together in the same application.

Here are some key differences:

* **Control flow**: The Functional API does not require thinking about graph structure. You can use standard Python constructs to define workflows. This will usually trim the amount of code you need to write.
* **Short-term memory**: The **GraphAPI** requires declaring a [**State**](/oss/javascript/langgraph/graph-api#state) and may require defining [**reducers**](/oss/javascript/langgraph/graph-api#reducers) to manage updates to the graph state. `@entrypoint` and `@tasks` do not require explicit state management as their state is scoped to the function and is not shared across functions.
* **Checkpointing**: Both APIs generate and use checkpoints. In the **Graph API** a new checkpoint is generated after every [superstep](/oss/javascript/langgraph/graph-api). In the **Functional API**, when tasks are executed, their results are saved to an existing checkpoint associated with the given entrypoint instead of creating a new checkpoint.
* **Visualization**: The Graph API makes it easy to visualize the workflow as a graph which can be useful for debugging, understanding the workflow, and sharing with others. The Functional API does not support visualization as the graph is dynamically generated during runtime.

Below we demonstrate a simple application that writes an essay and [interrupts](/oss/javascript/langgraph/interrupts) to request human review.

<Accordion title="Detailed Explanation">
  This workflow will write an essay about the topic "cat" and then pause to get a review from a human. The workflow can be interrupted for an indefinite amount of time until a review is provided.

When the workflow is resumed, it executes from the very start, but because the result of the `writeEssay` task was already saved, the task result will be loaded from the checkpoint instead of being recomputed.

An essay has been written and is ready for review. Once the review is provided, we can resume the workflow:

The workflow has been completed and the review has been added to the essay.
</Accordion>

The [`entrypoint`](https://langchain-ai.github.io/langgraphjs/reference/functions/langgraph.entrypoint.html) function can be used to create a workflow from a function. It encapsulates workflow logic and manages execution flow, including handling *long-running tasks* and [interrupts](/oss/javascript/langgraph/interrupts).

An **entrypoint** is defined by calling the `entrypoint` function with configuration and a function.

The function **must accept a single positional argument**, which serves as the workflow input. If you need to pass multiple pieces of data, use an object as the input type for the first argument.

Creating an entrypoint with a function produces a workflow instance which helps to manage the execution of the workflow (e.g., handles streaming, resumption, and checkpointing).

You will often want to pass a **checkpointer** to the `entrypoint` function to enable persistence and use features like **human-in-the-loop**.

<Warning>
  **Serialization**
  The **inputs** and **outputs** of entrypoints must be JSON-serializable to support checkpointing. Please see the [serialization](#serialization) section for more details.
</Warning>

Using the [`entrypoint`](#entrypoint) function will return an object that can be executed using the `invoke` and `stream` methods.

<Tabs>
  <Tab title="Invoke">
    
  </Tab>

<Tab title="Stream">
    
  </Tab>
</Tabs>

Resuming an execution after an [interrupt](https://langchain-ai.github.io/langgraphjs/reference/functions/langgraph.interrupt-2.html) can be done by passing a **resume** value to the [`Command`](https://langchain-ai.github.io/langgraphjs/reference/classes/langgraph.Command.html) primitive.

<Tabs>
  <Tab title="Invoke">
    
  </Tab>

<Tab title="Stream">
    
  </Tab>
</Tabs>

**Resuming after an error**

To resume after an error, run the `entrypoint` with `null` and the same **thread id** (config).

This assumes that the underlying **error** has been resolved and execution can proceed successfully.

<Tabs>
  <Tab title="Invoke">
    
  </Tab>

<Tab title="Stream">
    
  </Tab>
</Tabs>

### Short-term memory

When an `entrypoint` is defined with a `checkpointer`, it stores information between successive invocations on the same **thread id** in [checkpoints](/oss/javascript/langgraph/persistence#checkpoints).

This allows accessing the state from the previous invocation using the `getPreviousState` function.

By default, the `getPreviousState` function returns the return value of the previous invocation.

#### `entrypoint.final`

[`entrypoint.final`](https://langchain-ai.github.io/langgraphjs/reference/functions/langgraph.entrypoint.html#final) is a special primitive that can be returned from an entrypoint and allows **decoupling** the value that is **saved in the checkpoint** from the **return value of the entrypoint**.

The first value is the return value of the entrypoint, and the second value is the value that will be saved in the checkpoint.

A **task** represents a discrete unit of work, such as an API call or data processing step. It has two key characteristics:

* **Asynchronous Execution**: Tasks are designed to be executed asynchronously, allowing multiple operations to run concurrently without blocking.
* **Checkpointing**: Task results are saved to a checkpoint, enabling resumption of the workflow from the last saved state. (See [persistence](/oss/javascript/langgraph/persistence) for more details).

Tasks are defined using the `task` function, which wraps a regular function.

<Warning>
  **Serialization**
  The **outputs** of tasks must be JSON-serializable to support checkpointing.
</Warning>

**Tasks** can only be called from within an **entrypoint**, another **task**, or a [state graph node](/oss/javascript/langgraph/graph-api#nodes).

Tasks *cannot* be called directly from the main application code.

When you call a **task**, it returns a Promise that can be awaited.

## When to use a task

**Tasks** are useful in the following scenarios:

* **Checkpointing**: When you need to save the result of a long-running operation to a checkpoint, so you don't need to recompute it when resuming the workflow.
* **Human-in-the-loop**: If you're building a workflow that requires human intervention, you MUST use **tasks** to encapsulate any randomness (e.g., API calls) to ensure that the workflow can be resumed correctly. See the [determinism](#determinism) section for more details.
* **Parallel Execution**: For I/O-bound tasks, **tasks** enable parallel execution, allowing multiple operations to run concurrently without blocking (e.g., calling multiple APIs).
* **Observability**: Wrapping operations in **tasks** provides a way to track the progress of the workflow and monitor the execution of individual operations using [LangSmith](https://docs.smith.langchain.com/).
* **Retryable Work**: When work needs to be retried to handle failures or inconsistencies, **tasks** provide a way to encapsulate and manage the retry logic.

There are two key aspects to serialization in LangGraph:

1. `entrypoint` inputs and outputs must be JSON-serializable.
2. `task` outputs must be JSON-serializable.

These requirements are necessary for enabling checkpointing and workflow resumption. Use primitives like objects, arrays, strings, numbers, and booleans to ensure that your inputs and outputs are serializable.

Serialization ensures that workflow state, such as task results and intermediate values, can be reliably saved and restored. This is critical for enabling human-in-the-loop interactions, fault tolerance, and parallel execution.

Providing non-serializable inputs or outputs will result in a runtime error when a workflow is configured with a checkpointer.

To utilize features like **human-in-the-loop**, any randomness should be encapsulated inside of **tasks**. This guarantees that when execution is halted (e.g., for human in the loop) and then resumed, it will follow the same *sequence of steps*, even if **task** results are non-deterministic.

LangGraph achieves this behavior by persisting **task** and [**subgraph**](/oss/javascript/langgraph/use-subgraphs) results as they execute. A well-designed workflow ensures that resuming execution follows the *same sequence of steps*, allowing previously computed results to be retrieved correctly without having to re-execute them. This is particularly useful for long-running **tasks** or **tasks** with non-deterministic results, as it avoids repeating previously done work and allows resuming from essentially the same.

While different runs of a workflow can produce different results, resuming a **specific** run should always follow the same sequence of recorded steps. This allows LangGraph to efficiently look up **task** and **subgraph** results that were executed prior to the graph being interrupted and avoid recomputing them.

Idempotency ensures that running the same operation multiple times produces the same result. This helps prevent duplicate API calls and redundant processing if a step is rerun due to a failure. Always place API calls inside **tasks** functions for checkpointing, and design them to be idempotent in case of re-execution. Re-execution can occur if a **task** starts, but does not complete successfully. Then, if the workflow is resumed, the **task** will run again. Use idempotency keys or verify existing results to avoid duplication.

### Handling side effects

Encapsulate side effects (e.g., writing to a file, sending an email) in tasks to ensure they are not executed multiple times when resuming a workflow.

<Tabs>
  <Tab title="Incorrect">
    In this example, a side effect (writing to a file) is directly included in the workflow, so it will be executed a second time when resuming the workflow.

<Tab title="Correct">
    In this example, the side effect is encapsulated in a task, ensuring consistent execution upon resumption.

### Non-deterministic control flow

Operations that might give different results each time (like getting current time or random numbers) should be encapsulated in tasks to ensure that on resume, the same result is returned.

* In a task: Get random number (5) → interrupt → resume → (returns 5 again) → ...
* Not in a task: Get random number (5) → interrupt → resume → get new random number (7) → ...

This is especially important when using **human-in-the-loop** workflows with multiple interrupt calls. LangGraph keeps a list of resume values for each task/entrypoint. When an interrupt is encountered, it's matched with the corresponding resume value. This matching is strictly **index-based**, so the order of the resume values should match the order of the interrupts.

If order of execution is not maintained when resuming, one [`interrupt`](https://langchain-ai.github.io/langgraphjs/reference/functions/langgraph.interrupt-2.html) call may be matched with the wrong `resume` value, leading to incorrect results.

Please read the section on [determinism](#determinism) for more details.

<Tabs>
  <Tab title="Incorrect">
    In this example, the workflow uses the current time to determine which task to execute. This is non-deterministic because the result of the workflow depends on the time at which it is executed.

<Tab title="Correct">
    In this example, the workflow uses the input `t0` to determine which task to execute. This is deterministic because the result of the workflow depends only on the input.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/functional-api.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
<Accordion title="Detailed Explanation">
  This workflow will write an essay about the topic "cat" and then pause to get a review from a human. The workflow can be interrupted for an indefinite amount of time until a review is provided.

  When the workflow is resumed, it executes from the very start, but because the result of the `writeEssay` task was already saved, the task result will be loaded from the checkpoint instead of being recomputed.
```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
An essay has been written and is ready for review. Once the review is provided, we can resume the workflow:
```

Example 4 (unknown):
```unknown

```

---

## Generate a completion

**URL:** llms-txt#generate-a-completion

client = openai.Client()
chat_completion = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=messages
)

---

## Generate ClickHouse stats

**URL:** llms-txt#generate-clickhouse-stats

**Contents:**
  - Prerequisites
  - Running the clickhouse stats generation script

Source: https://docs.langchain.com/langsmith/script-generate-clickhouse-stats

As part of troubleshooting your self-hosted instance of LangSmith, the LangChain team may ask you to generate Clickhouse statistics that will help us understand memory and CPU consumption and connection concurrency.

This command will generate a CSV that can be shared with the LangChain team.

Ensure you have the following tools/items ready.

* [https://kubernetes.io/docs/tasks/tools/](https://kubernetes.io/docs/tasks/tools/)

2. Clickhouse database credentials

* Host
   * Port
   * Username
     * If using the bundled version, this is `default`
   * Password
     * If using the bundled version, this is `password`
   * Database name
     * If using the bundled version, this is `default`

3. Connectivity to the Clickhouse database from the machine you will be running the `get_clickhouse_stats` script on.

* If you are using the bundled version, you may need to port forward the clickhouse service to your local machine.
   * Run `kubectl port-forward svc/langsmith-clickhouse 8123:8123` to port forward the clickhouse service to your local machine.

4. The script to generate ClickHouse stats

* You can download the script from [here](https://github.com/langchain-ai/helm/blob/main/charts/langsmith/scripts/get_clickhouse_stats.sh)

### Running the clickhouse stats generation script

Run the following command to run the stats generation script:

For example, if you are using the bundled version with port-forwarding, the command would look like:

and after running this command you should see a file, clickhouse\_stats.csv, has been created with Clickhouse statistics.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/script-generate-clickhouse-stats.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
For example, if you are using the bundled version with port-forwarding, the command would look like:
```

---

## Generate query stats

**URL:** llms-txt#generate-query-stats

**Contents:**
  - Prerequisites
  - Running the query stats generation script

Source: https://docs.langchain.com/langsmith/script-generate-query-stats

As part of troubleshooting your self-hosted instance of LangSmith, the LangChain team may ask you to generate LangSmith query statistics that will help us understand the performance of various queries that drive the LangSmith product experience.

This command will generate a CSV that can be shared with the LangChain team.

Ensure you have the following tools/items ready.

* [https://kubernetes.io/docs/tasks/tools/](https://kubernetes.io/docs/tasks/tools/)

2. Clickhouse database credentials

* Host
   * Port
   * Username
     * If using the bundled version, this is `default`
   * Password
     * If using the bundled version, this is `password`
   * Database name
     * If using the bundled version, this is `default`

3. Connectivity to the Clickhouse database from the machine you will be running the `get_query_stats` script on.

* If you are using the bundled version, you may need to port forward the clickhouse service to your local machine.
   * Run `kubectl port-forward svc/langsmith-clickhouse 8123:8123` to port forward the clickhouse service to your local machine.

4. The script to generate query stats

* You can download the script from [here](https://github.com/langchain-ai/helm/blob/main/charts/langsmith/scripts/get_query_stats.sh)

### Running the query stats generation script

Run the following command to run the stats generation script:

For example, if you are using the bundled version with port-forwarding, the command would look like:

and after running this command you should see a file, query\_stats.csv, has been created with LangSmith query statistics.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/script-generate-query-stats.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
For example, if you are using the bundled version with port-forwarding, the command would look like:
```

---

## Generic / global handler catches calls that aren't handled by more specific handlers

**URL:** llms-txt#generic-/-global-handler-catches-calls-that-aren't-handled-by-more-specific-handlers

@auth.on
async def reject_unhandled_requests(ctx: Auth.types.AuthContext, value: Any) -> False:
    print(f"Request to {ctx.path} by {ctx.user.identity}")
    raise Auth.exceptions.HTTPException(
        status_code=403,
        detail="Forbidden"
    )

---

## get a state snapshot for a specific checkpoint_id

**URL:** llms-txt#get-a-state-snapshot-for-a-specific-checkpoint_id

**Contents:**
  - Get state history
  - Replay
  - Update state
- Memory Store
  - Basic Usage
  - Semantic Search

config = {"configurable": {"thread_id": "1", "checkpoint_id": "1ef663ba-28fe-6528-8002-5a559208592c"}}
graph.get_state(config)

StateSnapshot(
    values={'foo': 'b', 'bar': ['a', 'b']},
    next=(),
    config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28fe-6528-8002-5a559208592c'}},
    metadata={'source': 'loop', 'writes': {'node_b': {'foo': 'b', 'bar': ['b']}}, 'step': 2},
    created_at='2024-08-29T19:19:38.821749+00:00',
    parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f9-6ec4-8001-31981c2c39f8'}}, tasks=()
)
python  theme={null}
config = {"configurable": {"thread_id": "1"}}
list(graph.get_state_history(config))

[
    StateSnapshot(
        values={'foo': 'b', 'bar': ['a', 'b']},
        next=(),
        config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28fe-6528-8002-5a559208592c'}},
        metadata={'source': 'loop', 'writes': {'node_b': {'foo': 'b', 'bar': ['b']}}, 'step': 2},
        created_at='2024-08-29T19:19:38.821749+00:00',
        parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f9-6ec4-8001-31981c2c39f8'}},
        tasks=(),
    ),
    StateSnapshot(
        values={'foo': 'a', 'bar': ['a']},
        next=('node_b',),
        config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f9-6ec4-8001-31981c2c39f8'}},
        metadata={'source': 'loop', 'writes': {'node_a': {'foo': 'a', 'bar': ['a']}}, 'step': 1},
        created_at='2024-08-29T19:19:38.819946+00:00',
        parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f4-6b4a-8000-ca575a13d36a'}},
        tasks=(PregelTask(id='6fb7314f-f114-5413-a1f3-d37dfe98ff44', name='node_b', error=None, interrupts=()),),
    ),
    StateSnapshot(
        values={'foo': '', 'bar': []},
        next=('node_a',),
        config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f4-6b4a-8000-ca575a13d36a'}},
        metadata={'source': 'loop', 'writes': None, 'step': 0},
        created_at='2024-08-29T19:19:38.817813+00:00',
        parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f0-6c66-bfff-6723431e8481'}},
        tasks=(PregelTask(id='f1b14528-5ee5-579c-949b-23ef9bfbed58', name='node_a', error=None, interrupts=()),),
    ),
    StateSnapshot(
        values={'bar': []},
        next=('__start__',),
        config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f0-6c66-bfff-6723431e8481'}},
        metadata={'source': 'input', 'writes': {'foo': ''}, 'step': -1},
        created_at='2024-08-29T19:19:38.816205+00:00',
        parent_config=None,
        tasks=(PregelTask(id='6d27aa2e-d72b-5504-a36f-8620e54a76dd', name='__start__', error=None, interrupts=()),),
    )
]
python  theme={null}
config = {"configurable": {"thread_id": "1", "checkpoint_id": "0c62ca34-ac19-445d-bbb0-5b4984975b2a"}}
graph.invoke(None, config=config)
python  theme={null}
from typing import Annotated
from typing_extensions import TypedDict
from operator import add

class State(TypedDict):
    foo: int
    bar: Annotated[list[str], add]

{"foo": 1, "bar": ["a"]}
python  theme={null}
graph.update_state(config, {"foo": 2, "bar": ["b"]})

{"foo": 2, "bar": ["a", "b"]}
python  theme={null}
from langgraph.store.memory import InMemoryStore
in_memory_store = InMemoryStore()
python  theme={null}
user_id = "1"
namespace_for_memory = (user_id, "memories")
python  theme={null}
memory_id = str(uuid.uuid4())
memory = {"food_preference" : "I like pizza"}
in_memory_store.put(namespace_for_memory, memory_id, memory)
python  theme={null}
memories = in_memory_store.search(namespace_for_memory)
memories[-1].dict()
{'value': {'food_preference': 'I like pizza'},
 'key': '07e0caf4-1631-47b7-b15f-65515d4c1843',
 'namespace': ['1', 'memories'],
 'created_at': '2024-10-02T17:22:31.590602+00:00',
 'updated_at': '2024-10-02T17:22:31.590605+00:00'}
python  theme={null}
from langchain.embeddings import init_embeddings

store = InMemoryStore(
    index={
        "embed": init_embeddings("openai:text-embedding-3-small"),  # Embedding provider
        "dims": 1536,                              # Embedding dimensions
        "fields": ["food_preference", "$"]              # Fields to embed
    }
)
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
In our example, the output of `get_state` will look like this:
```

Example 2 (unknown):
```unknown
### Get state history

You can get the full history of the graph execution for a given thread by calling [`graph.get_state_history(config)`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.CompiledStateGraph.get_state_history). This will return a list of `StateSnapshot` objects associated with the thread ID provided in the config. Importantly, the checkpoints will be ordered chronologically with the most recent checkpoint / `StateSnapshot` being the first in the list.
```

Example 3 (unknown):
```unknown
In our example, the output of [`get_state_history`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.CompiledStateGraph.get_state_history) will look like this:
```

Example 4 (unknown):
```unknown
<img src="https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/get_state.jpg?fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=38ffff52be4d8806b287836295a3c058" alt="State" data-og-width="2692" width="2692" data-og-height="1056" height="1056" data-path="oss/images/get_state.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/get_state.jpg?w=280&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=e932acac5021614d0eb99b90e54be004 280w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/get_state.jpg?w=560&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=2eaf153fd49ba728e1d679c12bb44b6f 560w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/get_state.jpg?w=840&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=0ac091c7dbe8b1f0acff97615a3683ee 840w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/get_state.jpg?w=1100&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=9921a482f1c4f86316fca23a5150b153 1100w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/get_state.jpg?w=1650&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=9412cd906f6d67a9fe1f50a5d4f4c674 1650w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/get_state.jpg?w=2500&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=ccc5118ed85926bda3715c81ce728fcc 2500w" />

### Replay

It's also possible to play-back a prior graph execution. If we `invoke` a graph with a `thread_id` and a `checkpoint_id`, then we will *re-play* the previously executed steps *before* a checkpoint that corresponds to the `checkpoint_id`, and only execute the steps *after* the checkpoint.

* `thread_id` is the ID of a thread.
* `checkpoint_id` is an identifier that refers to a specific checkpoint within a thread.

You must pass these when invoking the graph as part of the `configurable` portion of the config:
```

---

## Get customer information from the API

**URL:** llms-txt#get-customer-information-from-the-api

export LANGSMITH_URL="<your_langsmith_url>"
export response=$(curl -s $LANGSMITH_URL/api/v1/info)
export CUSTOMER_ID=$(echo "$response" | jq -r '.customer_info.customer_id') && echo "Customer ID: $CUSTOMER_ID"
export CUSTOMER_NAME=$(echo "$response" | jq -r '.customer_info.customer_name') && echo "Customer name: $CUSTOMER_NAME"

---

## Get email from command line

**URL:** llms-txt#get-email-from-command-line

email = getpass("Enter your email: ")
base_email = email.split("@")
password = "secure-password"  # CHANGEME
email1 = f"{base_email[0]}+1@{base_email[1]}"
email2 = f"{base_email[0]}+2@{base_email[1]}"

SUPABASE_URL = os.environ.get("SUPABASE_URL")
if not SUPABASE_URL:
    SUPABASE_URL = getpass("Enter your Supabase project URL: ")

---

## Get or create tracer provider

**URL:** llms-txt#get-or-create-tracer-provider

tracer_provider = trace.get_tracer_provider()
if not isinstance(tracer_provider, TracerProvider):
    tracer_provider = TracerProvider()
    trace.set_tracer_provider(tracer_provider)

---

## Get started with Studio

**URL:** llms-txt#get-started-with-studio

**Contents:**
- Deployed graphs
- Local development server
  - Prerequisites
  - Setup
  - (Optional) Attach a debugger
- Next steps

Source: https://docs.langchain.com/langsmith/quick-start-studio

[Studio](/langsmith/studio) in the [LangSmith Deployments UI](https://smith.langchain.com) supports connecting to two types of graphs:

* Graphs deployed on [cloud or self-hosted](#deployed-graphs).
* Graphs running locally with [Agent Server](#local-development-server).

Studio is accessed in the [LangSmith UI](https://smith.langchain.com) from the **Deployments** navigation.

For applications that are [deployed](/langsmith/deployment-quickstart), you can access Studio as part of that deployment. To do so, navigate to the deployment in the UI and select **Studio**.

This will load Studio connected to your live deployment, allowing you to create, read, and update the [threads](/oss/python/langgraph/persistence#threads), [assistants](/langsmith/assistants), and [memory](/oss/python/concepts/memory) in that deployment.

## Local development server

To test your application locally using Studio:

* Follow the [local application quickstart](/langsmith/local-server) first.
* If you don't want data [traced](/langsmith/observability-concepts#traces) to LangSmith, set `LANGSMITH_TRACING=false` in your application's `.env` file. With tracing disabled, no data leaves your local server.

1. Install the [LangGraph CLI](/langsmith/cli):

<Warning>
     **Browser Compatibility**
     Safari blocks `localhost` connections to Studio. To work around this, run the command with `--tunnel` to access Studio via a secure tunnel.
   </Warning>

This will start the Agent Server locally, running in-memory. The server will run in watch mode, listening for and automatically restarting on code changes. Read this [reference](/langsmith/cli#dev) to learn about all the options for starting the API server.

You will see the following logs:

Once running, you will automatically be directed to Studio.

2. For a running server, access the Dbugger with one of the following:

1. Directly navigate to the following URL: `https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024`.
   2. Navigate to **Deployments** in the UI, click the **Studio** button on a deployment, enter `http://127.0.0.1:2024` and click **Connect**.

If running your server at a different host or port, update the `baseUrl` to match.

### (Optional) Attach a debugger

For step-by-step debugging with breakpoints and variable inspection, run the following:

Then attach your preferred debugger:

<Tabs>
  <Tab title="VS Code">
    Add this configuration to `launch.json`:

<Tab title="PyCharm">
    1. Go to Run → Edit Configurations
    2. Click + and select "Python Debug Server"
    3. Set IDE host name: `localhost`
    4. Set port: `5678` (or the port number you chose in the previous step)
    5. Click "OK" and start debugging
  </Tab>
</Tabs>

<Tip>
  For issues getting started, refer to the [troubleshooting guide](/langsmith/troubleshooting-studio).
</Tip>

For more information on how to run Studio, refer to the following guides:

* [Run application](/langsmith/use-studio#run-application)
* [Manage assistants](/langsmith/use-studio#manage-assistants)
* [Manage threads](/langsmith/use-studio#manage-threads)
* [Iterate on prompts](/langsmith/observability-studio)
* [Debug LangSmith traces](/langsmith/observability-studio#debug-langsmith-traces)
* [Add node to dataset](/langsmith/observability-studio#add-node-to-dataset)

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/quick-start-studio.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
</CodeGroup>

   <Warning>
     **Browser Compatibility**
     Safari blocks `localhost` connections to Studio. To work around this, run the command with `--tunnel` to access Studio via a secure tunnel.
   </Warning>

   This will start the Agent Server locally, running in-memory. The server will run in watch mode, listening for and automatically restarting on code changes. Read this [reference](/langsmith/cli#dev) to learn about all the options for starting the API server.

   You will see the following logs:
```

Example 4 (unknown):
```unknown
Once running, you will automatically be directed to Studio.

2. For a running server, access the Dbugger with one of the following:

   1. Directly navigate to the following URL: `https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024`.
   2. Navigate to **Deployments** in the UI, click the **Studio** button on a deployment, enter `http://127.0.0.1:2024` and click **Connect**.

   If running your server at a different host or port, update the `baseUrl` to match.

### (Optional) Attach a debugger

For step-by-step debugging with breakpoints and variable inspection, run the following:

<CodeGroup>
```

---

## Get Tavily API key: https://tavily.com

**URL:** llms-txt#get-tavily-api-key:-https://tavily.com

**Contents:**
  - Define the application

os.environ["TAVILY_API_KEY"] = "YOUR TAVILY API KEY"
python  theme={null}
from langchain.chat_models import init_chat_model
from langchain.agents import create_agent
from langchain_community.tools import DuckDuckGoSearchRun, TavilySearchResults
from langchain_core.rate_limiters import InMemoryRateLimiter

**Examples:**

Example 1 (unknown):
```unknown
### Define the application

For this example lets create a simple Tweet-writing application that has access to some internet search tools:
```

---

## Get the API response and extract customer information

**URL:** llms-txt#get-the-api-response-and-extract-customer-information

export LANGSMITH_URL="<your_langsmith_url>"
response=$(curl -s $LANGSMITH_URL/api/v1/info)

---

## Get the current tracer

**URL:** llms-txt#get-the-current-tracer

**Contents:**
  - Combining with other instrumentors

tracer = trace.get_tracer(__name__)

async def main():
    with tracer.start_as_current_span("semantic_kernel_workflow") as span:
        # Add custom metadata
        span.set_attribute("langsmith.metadata.workflow_type", "code_analysis")
        span.set_attribute("langsmith.metadata.user_id", "developer_123")
        span.set_attribute("langsmith.span.tags", "semantic-kernel,code-analysis")

# Your Semantic Kernel code here
        result = await kernel.invoke(code_analyzer, code=sample_code)
        return result
python  theme={null}
from langsmith.integrations.otel import configure
from openinference.instrumentation.openai import OpenAIInstrumentor
from openinference.instrumentation.dspy import DSPyInstrumentor

**Examples:**

Example 1 (unknown):
```unknown
### Combining with other instrumentors

You can combine Semantic Kernel instrumentation with other instrumentors (e.g., DSPy, AutoGen) by adding them and initializing them as instrumentors:
```

---

## get the latest state snapshot

**URL:** llms-txt#get-the-latest-state-snapshot

config = {"configurable": {"thread_id": "1"}}
graph.get_state(config)

---

## get the "memory" by ID

**URL:** llms-txt#get-the-"memory"-by-id

item = store.get(namespace, "a-memory") # [!code highlight]

---

## Get the tool call

**URL:** llms-txt#get-the-tool-call

**Contents:**
- Prompt chaining
- Parallelization
- Routing
- Orchestrator-worker
  - Creating workers in LangGraph

msg.tool_calls
python Graph API theme={null}
  from typing_extensions import TypedDict
  from langgraph.graph import StateGraph, START, END
  from IPython.display import Image, display

# Graph state
  class State(TypedDict):
      topic: str
      joke: str
      improved_joke: str
      final_joke: str

# Nodes
  def generate_joke(state: State):
      """First LLM call to generate initial joke"""

msg = llm.invoke(f"Write a short joke about {state['topic']}")
      return {"joke": msg.content}

def check_punchline(state: State):
      """Gate function to check if the joke has a punchline"""

# Simple check - does the joke contain "?" or "!"
      if "?" in state["joke"] or "!" in state["joke"]:
          return "Pass"
      return "Fail"

def improve_joke(state: State):
      """Second LLM call to improve the joke"""

msg = llm.invoke(f"Make this joke funnier by adding wordplay: {state['joke']}")
      return {"improved_joke": msg.content}

def polish_joke(state: State):
      """Third LLM call for final polish"""
      msg = llm.invoke(f"Add a surprising twist to this joke: {state['improved_joke']}")
      return {"final_joke": msg.content}

# Build workflow
  workflow = StateGraph(State)

# Add nodes
  workflow.add_node("generate_joke", generate_joke)
  workflow.add_node("improve_joke", improve_joke)
  workflow.add_node("polish_joke", polish_joke)

# Add edges to connect nodes
  workflow.add_edge(START, "generate_joke")
  workflow.add_conditional_edges(
      "generate_joke", check_punchline, {"Fail": "improve_joke", "Pass": END}
  )
  workflow.add_edge("improve_joke", "polish_joke")
  workflow.add_edge("polish_joke", END)

# Compile
  chain = workflow.compile()

# Show workflow
  display(Image(chain.get_graph().draw_mermaid_png()))

# Invoke
  state = chain.invoke({"topic": "cats"})
  print("Initial joke:")
  print(state["joke"])
  print("\n--- --- ---\n")
  if "improved_joke" in state:
      print("Improved joke:")
      print(state["improved_joke"])
      print("\n--- --- ---\n")

print("Final joke:")
      print(state["final_joke"])
  else:
      print("Joke failed quality gate - no punchline detected!")
  python Functional API theme={null}
  from langgraph.func import entrypoint, task

# Tasks
  @task
  def generate_joke(topic: str):
      """First LLM call to generate initial joke"""
      msg = llm.invoke(f"Write a short joke about {topic}")
      return msg.content

def check_punchline(joke: str):
      """Gate function to check if the joke has a punchline"""
      # Simple check - does the joke contain "?" or "!"
      if "?" in joke or "!" in joke:
          return "Fail"

@task
  def improve_joke(joke: str):
      """Second LLM call to improve the joke"""
      msg = llm.invoke(f"Make this joke funnier by adding wordplay: {joke}")
      return msg.content

@task
  def polish_joke(joke: str):
      """Third LLM call for final polish"""
      msg = llm.invoke(f"Add a surprising twist to this joke: {joke}")
      return msg.content

@entrypoint()
  def prompt_chaining_workflow(topic: str):
      original_joke = generate_joke(topic).result()
      if check_punchline(original_joke) == "Pass":
          return original_joke

improved_joke = improve_joke(original_joke).result()
      return polish_joke(improved_joke).result()

# Invoke
  for step in prompt_chaining_workflow.stream("cats", stream_mode="updates"):
      print(step)
      print("\n")
  python Graph API theme={null}
  # Graph state
  class State(TypedDict):
      topic: str
      joke: str
      story: str
      poem: str
      combined_output: str

# Nodes
  def call_llm_1(state: State):
      """First LLM call to generate initial joke"""

msg = llm.invoke(f"Write a joke about {state['topic']}")
      return {"joke": msg.content}

def call_llm_2(state: State):
      """Second LLM call to generate story"""

msg = llm.invoke(f"Write a story about {state['topic']}")
      return {"story": msg.content}

def call_llm_3(state: State):
      """Third LLM call to generate poem"""

msg = llm.invoke(f"Write a poem about {state['topic']}")
      return {"poem": msg.content}

def aggregator(state: State):
      """Combine the joke and story into a single output"""

combined = f"Here's a story, joke, and poem about {state['topic']}!\n\n"
      combined += f"STORY:\n{state['story']}\n\n"
      combined += f"JOKE:\n{state['joke']}\n\n"
      combined += f"POEM:\n{state['poem']}"
      return {"combined_output": combined}

# Build workflow
  parallel_builder = StateGraph(State)

# Add nodes
  parallel_builder.add_node("call_llm_1", call_llm_1)
  parallel_builder.add_node("call_llm_2", call_llm_2)
  parallel_builder.add_node("call_llm_3", call_llm_3)
  parallel_builder.add_node("aggregator", aggregator)

# Add edges to connect nodes
  parallel_builder.add_edge(START, "call_llm_1")
  parallel_builder.add_edge(START, "call_llm_2")
  parallel_builder.add_edge(START, "call_llm_3")
  parallel_builder.add_edge("call_llm_1", "aggregator")
  parallel_builder.add_edge("call_llm_2", "aggregator")
  parallel_builder.add_edge("call_llm_3", "aggregator")
  parallel_builder.add_edge("aggregator", END)
  parallel_workflow = parallel_builder.compile()

# Show workflow
  display(Image(parallel_workflow.get_graph().draw_mermaid_png()))

# Invoke
  state = parallel_workflow.invoke({"topic": "cats"})
  print(state["combined_output"])
  python Functional API theme={null}
  @task
  def call_llm_1(topic: str):
      """First LLM call to generate initial joke"""
      msg = llm.invoke(f"Write a joke about {topic}")
      return msg.content

@task
  def call_llm_2(topic: str):
      """Second LLM call to generate story"""
      msg = llm.invoke(f"Write a story about {topic}")
      return msg.content

@task
  def call_llm_3(topic):
      """Third LLM call to generate poem"""
      msg = llm.invoke(f"Write a poem about {topic}")
      return msg.content

@task
  def aggregator(topic, joke, story, poem):
      """Combine the joke and story into a single output"""

combined = f"Here's a story, joke, and poem about {topic}!\n\n"
      combined += f"STORY:\n{story}\n\n"
      combined += f"JOKE:\n{joke}\n\n"
      combined += f"POEM:\n{poem}"
      return combined

# Build workflow
  @entrypoint()
  def parallel_workflow(topic: str):
      joke_fut = call_llm_1(topic)
      story_fut = call_llm_2(topic)
      poem_fut = call_llm_3(topic)
      return aggregator(
          topic, joke_fut.result(), story_fut.result(), poem_fut.result()
      ).result()

# Invoke
  for step in parallel_workflow.stream("cats", stream_mode="updates"):
      print(step)
      print("\n")
  python Graph API theme={null}
  from typing_extensions import Literal
  from langchain.messages import HumanMessage, SystemMessage

# Schema for structured output to use as routing logic
  class Route(BaseModel):
      step: Literal["poem", "story", "joke"] = Field(
          None, description="The next step in the routing process"
      )

# Augment the LLM with schema for structured output
  router = llm.with_structured_output(Route)

# State
  class State(TypedDict):
      input: str
      decision: str
      output: str

# Nodes
  def llm_call_1(state: State):
      """Write a story"""

result = llm.invoke(state["input"])
      return {"output": result.content}

def llm_call_2(state: State):
      """Write a joke"""

result = llm.invoke(state["input"])
      return {"output": result.content}

def llm_call_3(state: State):
      """Write a poem"""

result = llm.invoke(state["input"])
      return {"output": result.content}

def llm_call_router(state: State):
      """Route the input to the appropriate node"""

# Run the augmented LLM with structured output to serve as routing logic
      decision = router.invoke(
          [
              SystemMessage(
                  content="Route the input to story, joke, or poem based on the user's request."
              ),
              HumanMessage(content=state["input"]),
          ]
      )

return {"decision": decision.step}

# Conditional edge function to route to the appropriate node
  def route_decision(state: State):
      # Return the node name you want to visit next
      if state["decision"] == "story":
          return "llm_call_1"
      elif state["decision"] == "joke":
          return "llm_call_2"
      elif state["decision"] == "poem":
          return "llm_call_3"

# Build workflow
  router_builder = StateGraph(State)

# Add nodes
  router_builder.add_node("llm_call_1", llm_call_1)
  router_builder.add_node("llm_call_2", llm_call_2)
  router_builder.add_node("llm_call_3", llm_call_3)
  router_builder.add_node("llm_call_router", llm_call_router)

# Add edges to connect nodes
  router_builder.add_edge(START, "llm_call_router")
  router_builder.add_conditional_edges(
      "llm_call_router",
      route_decision,
      {  # Name returned by route_decision : Name of next node to visit
          "llm_call_1": "llm_call_1",
          "llm_call_2": "llm_call_2",
          "llm_call_3": "llm_call_3",
      },
  )
  router_builder.add_edge("llm_call_1", END)
  router_builder.add_edge("llm_call_2", END)
  router_builder.add_edge("llm_call_3", END)

# Compile workflow
  router_workflow = router_builder.compile()

# Show the workflow
  display(Image(router_workflow.get_graph().draw_mermaid_png()))

# Invoke
  state = router_workflow.invoke({"input": "Write me a joke about cats"})
  print(state["output"])
  python Functional API theme={null}
  from typing_extensions import Literal
  from pydantic import BaseModel
  from langchain.messages import HumanMessage, SystemMessage

# Schema for structured output to use as routing logic
  class Route(BaseModel):
      step: Literal["poem", "story", "joke"] = Field(
          None, description="The next step in the routing process"
      )

# Augment the LLM with schema for structured output
  router = llm.with_structured_output(Route)

@task
  def llm_call_1(input_: str):
      """Write a story"""
      result = llm.invoke(input_)
      return result.content

@task
  def llm_call_2(input_: str):
      """Write a joke"""
      result = llm.invoke(input_)
      return result.content

@task
  def llm_call_3(input_: str):
      """Write a poem"""
      result = llm.invoke(input_)
      return result.content

def llm_call_router(input_: str):
      """Route the input to the appropriate node"""
      # Run the augmented LLM with structured output to serve as routing logic
      decision = router.invoke(
          [
              SystemMessage(
                  content="Route the input to story, joke, or poem based on the user's request."
              ),
              HumanMessage(content=input_),
          ]
      )
      return decision.step

# Create workflow
  @entrypoint()
  def router_workflow(input_: str):
      next_step = llm_call_router(input_)
      if next_step == "story":
          llm_call = llm_call_1
      elif next_step == "joke":
          llm_call = llm_call_2
      elif next_step == "poem":
          llm_call = llm_call_3

return llm_call(input_).result()

# Invoke
  for step in router_workflow.stream("Write me a joke about cats", stream_mode="updates"):
      print(step)
      print("\n")
  python Graph API theme={null}
  from typing import Annotated, List
  import operator

# Schema for structured output to use in planning
  class Section(BaseModel):
      name: str = Field(
          description="Name for this section of the report.",
      )
      description: str = Field(
          description="Brief overview of the main topics and concepts to be covered in this section.",
      )

class Sections(BaseModel):
      sections: List[Section] = Field(
          description="Sections of the report.",
      )

# Augment the LLM with schema for structured output
  planner = llm.with_structured_output(Sections)
  python Functional API theme={null}
  from typing import List

# Schema for structured output to use in planning
  class Section(BaseModel):
      name: str = Field(
          description="Name for this section of the report.",
      )
      description: str = Field(
          description="Brief overview of the main topics and concepts to be covered in this section.",
      )

class Sections(BaseModel):
      sections: List[Section] = Field(
          description="Sections of the report.",
      )

# Augment the LLM with schema for structured output
  planner = llm.with_structured_output(Sections)

@task
  def orchestrator(topic: str):
      """Orchestrator that generates a plan for the report"""
      # Generate queries
      report_sections = planner.invoke(
          [
              SystemMessage(content="Generate a plan for the report."),
              HumanMessage(content=f"Here is the report topic: {topic}"),
          ]
      )

return report_sections.sections

@task
  def llm_call(section: Section):
      """Worker writes a section of the report"""

# Generate section
      result = llm.invoke(
          [
              SystemMessage(content="Write a report section."),
              HumanMessage(
                  content=f"Here is the section name: {section.name} and description: {section.description}"
              ),
          ]
      )

# Write the updated section to completed sections
      return result.content

@task
  def synthesizer(completed_sections: list[str]):
      """Synthesize full report from sections"""
      final_report = "\n\n---\n\n".join(completed_sections)
      return final_report

@entrypoint()
  def orchestrator_worker(topic: str):
      sections = orchestrator(topic).result()
      section_futures = [llm_call(section) for section in sections]
      final_report = synthesizer(
          [section_fut.result() for section_fut in section_futures]
      ).result()
      return final_report

# Invoke
  report = orchestrator_worker.invoke("Create a report on LLM scaling laws")
  from IPython.display import Markdown
  Markdown(report)
  python  theme={null}
from langgraph.types import Send

**Examples:**

Example 1 (unknown):
```unknown
## Prompt chaining

Prompt chaining is when each LLM call processes the output of the previous call. It's often used for performing well-defined tasks that can be broken down into smaller, verifiable steps. Some examples include:

* Translating documents into different languages
* Verifying generated content for consistency

<img src="https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/prompt_chain.png?fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=762dec147c31b8dc6ebb0857e236fc1f" alt="Prompt chaining" data-og-width="1412" width="1412" data-og-height="444" height="444" data-path="oss/images/prompt_chain.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/prompt_chain.png?w=280&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=fda27cf4f997e350d4ce48be16049c47 280w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/prompt_chain.png?w=560&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=1374b6de11900d394fc73722a3a6040e 560w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/prompt_chain.png?w=840&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=25246c7111a87b5df5a2af24a0181efe 840w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/prompt_chain.png?w=1100&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=0c57da86a49cf966cc090497ade347f1 1100w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/prompt_chain.png?w=1650&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=a1b5c8fc644d7a80c0792b71769c97da 1650w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/prompt_chain.png?w=2500&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=8a3f66f0e365e503a85b30be48bc1a76 2500w" />

<CodeGroup>
```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
</CodeGroup>

## Parallelization

With parallelization, LLMs work simultaneously on a task. This is either done by running multiple independent subtasks at the same time, or running the same task multiple times to check for different outputs. Parallelization is commonly used to:

* Split up subtasks and run them in parallel, which increases speed
* Run tasks multiple times to check for different outputs, which increases confidence

Some examples include:

* Running one subtask that processes a document for keywords, and a second subtask to check for formatting errors
* Running a task multiple times that scores a document for accuracy based on different criteria, like the number of citations, the number of sources used, and the quality of the sources

<img src="https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/parallelization.png?fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=8afe3c427d8cede6fed1e4b2a5107b71" alt="parallelization.png" data-og-width="1020" width="1020" data-og-height="684" height="684" data-path="oss/images/parallelization.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/parallelization.png?w=280&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=88e51062b14d9186a6f0ea246bc48635 280w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/parallelization.png?w=560&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=934941ca52019b7cbce7fbdd31d00f0f 560w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/parallelization.png?w=840&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=30b5c86c545d0e34878ff0a2c367dd0a 840w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/parallelization.png?w=1100&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=6227d2c39f332eaeda23f7db66871dd7 1100w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/parallelization.png?w=1650&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=283f3ee2924a385ab88f2cbfd9c9c48c 1650w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/parallelization.png?w=2500&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=69f6a97716b38998b7b399c3d8ac7d9c 2500w" />

<CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Global limit: max 20 calls per thread, 10 per run

**URL:** llms-txt#global-limit:-max-20-calls-per-thread,-10-per-run

global_limiter = ToolCallLimitMiddleware(
    thread_limit=20,
    run_limit=10,
)

---

## GoogleGenerativeAIEmbeddings

**URL:** llms-txt#googlegenerativeaiembeddings

**Contents:**
- Overview
  - Integration details
- Setup
  - Credentials

Source: https://docs.langchain.com/oss/javascript/integrations/text_embedding/google_generativeai

This will help you get started with Google Generative AI [embedding models](/oss/javascript/integrations/text_embedding) using LangChain. For detailed documentation on `GoogleGenerativeAIEmbeddings` features and configuration options, please refer to the [API reference](https://api.js.langchain.com/classes/langchain_google_genai.GoogleGenerativeAIEmbeddings.html).

### Integration details

| Class                                                                                                                           | Package                                                                | Local | [Py support](https://python.langchain.com/docs/integrations/text_embedding/google_generative_ai/) |                                                Downloads                                                |                                                Version                                               |
| :------------------------------------------------------------------------------------------------------------------------------ | :--------------------------------------------------------------------- | :---: | :-----------------------------------------------------------------------------------------------: | :-----------------------------------------------------------------------------------------------------: | :--------------------------------------------------------------------------------------------------: |
| [`GoogleGenerativeAIEmbeddings`](https://api.js.langchain.com/classes/langchain_google_genai.GoogleGenerativeAIEmbeddings.html) | [`@langchain/google-genai`](https://npmjs.com/@langchain/google-genai) |   ❌   |                                                 ✅                                                 | ![NPM - Downloads](https://img.shields.io/npm/dm/@langchain/google-genai?style=flat-square\&label=%20&) | ![NPM - Version](https://img.shields.io/npm/v/@langchain/google-genai?style=flat-square\&label=%20&) |

To access Google Generative AI embedding models you'll need to sign up for a Google AI account, get an API key, and install the `@langchain/google-genai` integration package.

Get an API key here: [ai.google.dev/tutorials/setup](https://ai.google.dev/tutorials/setup).

Next, set your key as an environment variable named `GOOGLE_API_KEY`:

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```bash  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:
```

---

## Google

**URL:** llms-txt#google

**Contents:**
- Google Generative AI
  - Chat models
  - LLMs
  - Embedding models
- Google Cloud
  - Chat models
  - LLMs
  - Embedding models
  - Document loaders
  - Document transformers

Source: https://docs.langchain.com/oss/python/integrations/providers/google

This page covers all LangChain integrations with [Google Gemini](https://ai.google.dev/gemini-api/docs), [Google Cloud](https://cloud.google.com/), and other Google products (such as Google Maps, YouTube, and [more](#other-google-products)).

Not sure which to use?

<AccordionGroup>
  <Accordion title="Google Generative AI (Gemini API & AI Studio)">
    Access Google Gemini models directly via the [Gemini Developer API](https://ai.google.dev/). This is often the best starting point for individual developers.

[See integrations.](#google-generative-ai)
  </Accordion>

<Accordion title="Google Cloud (Vertex AI & other services)">
    Access Gemini models, Vertex AI Model Garden and a wide range of cloud services (databases, storage, document AI, etc.) via the [Google Cloud Platform](https://cloud.google.com/). Use the `langchain-google-vertexai` package for Vertex AI models and specific packages (e.g., `langchain-google-community`, `langchain-google-cloud-sql-pg`) for other cloud services. This is ideal for developers already using Google Cloud or needing enterprise features like MLOps, specific model tuning or enterprise support.

[See integrations.](#google-cloud)
  </Accordion>
</AccordionGroup>

See Google's guide on [migrating from the Gemini API to Vertex AI](https://ai.google.dev/gemini-api/docs/migrate-to-cloud) for more details on the differences.

<Note>
  Integration packages for Gemini models and the Vertex AI platform are maintained in the [`langchain-google`](https://github.com/langchain-ai/langchain-google) repository.

You can find a host of LangChain integrations with other Google APIs and services in the `langchain-google-community` package (listed on this page) and the [`googleapis`](https://github.com/orgs/googleapis/repositories?q=langchain) Github organization.
</Note>

## Google Generative AI

Access Google Gemini models directly using the [Gemini Developer API](https://ai.google.dev/gemini-api/docs), best suited for rapid development and experimentation.

<Columns cols={1}>
  <Card title="ChatGoogleGenerativeAI" href="/oss/python/integrations/chat/google_generative_ai" cta="Get started" icon="message" arrow>
    Google Gemini chat models via the Gemini Developer API.
  </Card>
</Columns>

<Columns cols={1}>
  <Card title="GoogleGenerativeAI" href="/oss/python/integrations/llms/google_ai" cta="Get started" icon="i-cursor" arrow>
    Access the same Gemini models using the (legacy) LLM text completion interface.
  </Card>
</Columns>

<Columns cols={1}>
  <Card title="GoogleGenerativeAIEmbeddings" href="/oss/python/integrations/text_embedding/google_generative_ai" cta="Get started" icon="layer-group" arrow>
    Gemini embedding models.
  </Card>
</Columns>

Access Gemini models, Vertex AI Model Garden and other Google Cloud services via [Vertex AI](https://docs.cloud.google.com/vertex-ai/generative-ai/docs) and specific cloud integrations.

<Columns cols={2}>
  <Card title="Vertex AI" icon="comments" href="/oss/python/integrations/chat/google_vertex_ai_palm" cta="Get started" arrow>
    Access chat models like Gemini via the Vertex AI platform.
  </Card>

<Card title="Anthropic on Vertex AI Model Garden" icon="comments" href="/oss/python/integrations/llms/google_vertex_ai_palm" cta="Get started" arrow />
</Columns>

<AccordionGroup>
  <Accordion title="Llama on Vertex AI Model Garden">
    
  </Accordion>

<Accordion title="Mistral on Vertex AI Model Garden">
    
  </Accordion>

<Accordion title="Gemma local from Hugging Face">
    Local Gemma model loaded from HuggingFace.

<Accordion title="Gemma local from Kaggle">
    Local Gemma model loaded from Kaggle.

<Accordion title="Gemma on Vertex AI Model Garden">
    
  </Accordion>

<Accordion title="Vertex AI image captioning">
    Implementation of the Image Captioning model as a chat.

<Accordion title="Vertex AI image editor">
    Given an image and a prompt, edit the image. Currently only supports mask-free editing.

<Accordion title="Vertex AI image generator">
    Generates an image from a prompt.

<Accordion title="Vertex AI visual QnA">
    Chat implementation of a visual QnA model.

</Accordion>
</AccordionGroup>

(legacy) string-in, string-out LLM interface.

<Columns cols={2}>
  <Card title="Vertex AI Model Garden" icon="i-cursor" href="/oss/python/integrations/llms/google_vertex_ai_palm#vertex-model-garden" cta="Get started" arrow>
    Access Gemini, and hundreds of OSS models via Vertex AI Model Garden service.
  </Card>
</Columns>

<AccordionGroup>
  <Accordion title="Gemma local from Hugging Face">
    Local Gemma model loaded from HuggingFace.

<Accordion title="Gemma local from Kaggle">
    Local Gemma model loaded from Kaggle.

<Accordion title="Gemma on Vertex AI Model Garden">
    
  </Accordion>

<Accordion title="Vertex AI image captioning">
    Implementation of the Image Captioning model as an LLM.

</Accordion>
</AccordionGroup>

<Columns cols={2}>
  <Card title="Vertex AI" icon="layer-group" href="/oss/python/integrations/text_embedding/google_vertex_ai_palm" cta="Get started" arrow>
    Generate embeddings using models deployed on Vertex AI.
  </Card>
</Columns>

Load documents from various Google Cloud sources.

<Columns cols={2}>
  <Card title="AlloyDB for PostgreSQL" href="/oss/python/integrations/document_loaders/google_alloydb" cta="Get started" arrow>
    Google Cloud AlloyDB is a fully managed PostgreSQL-compatible database service.
  </Card>

<Card title="BigQuery" href="/oss/python/integrations/document_loaders/google_bigquery" cta="Get started" arrow>
    Google Cloud BigQuery is a serverless data warehouse.
  </Card>

<Card title="Bigtable" href="/oss/python/integrations/document_loaders/google_bigtable" cta="Get started" arrow>
    Google Cloud Bigtable is a fully managed NoSQL Big Data database service.
  </Card>

<Card title="Cloud SQL for MySQL" href="/oss/python/integrations/document_loaders/google_cloud_sql_mysql" cta="Get started" arrow>
    Google Cloud SQL for MySQL is a fully-managed MySQL database service.
  </Card>

<Card title="Cloud SQL for SQL Server" href="/oss/python/integrations/document_loaders/google_cloud_sql_mssql" cta="Get started" arrow>
    Google Cloud SQL for SQL Server is a fully-managed SQL Server database service.
  </Card>

<Card title="Cloud SQL for PostgreSQL" href="/oss/python/integrations/document_loaders/google_cloud_sql_pg" cta="Get started" arrow>
    Google Cloud SQL for PostgreSQL is a fully-managed PostgreSQL database service.
  </Card>

<Card title="Cloud Storage (directory)" href="/oss/python/integrations/document_loaders/google_cloud_storage_directory" cta="Get started" arrow>
    Google Cloud Storage is a managed service for storing unstructured data.
  </Card>

<Card title="Cloud Storage (file)" href="/oss/python/integrations/document_loaders/google_cloud_storage_file" cta="Get started" arrow>
    Google Cloud Storage is a managed service for storing unstructured data.
  </Card>

<Card title="El Carro for Oracle Workloads" href="/oss/python/integrations/document_loaders/google_el_carro" cta="Get started" arrow>
    Google El Carro Oracle Operator runs Oracle databases in Kubernetes.
  </Card>

<Card title="Firestore (Native Mode)" href="/oss/python/integrations/document_loaders/google_firestore" cta="Get started" arrow>
    Google Cloud Firestore is a NoSQL document database.
  </Card>

<Card title="Firestore (Datastore Mode)" href="/oss/python/integrations/document_loaders/google_datastore" cta="Get started" arrow>
    Google Cloud Firestore in Datastore mode
  </Card>

<Card title="Memorystore for Redis" href="/oss/python/integrations/document_loaders/google_memorystore_redis" cta="Get started" arrow>
    Google Cloud Memorystore for Redis is a fully managed Redis service.
  </Card>

<Card title="Spanner" href="/oss/python/integrations/document_loaders/google_spanner" cta="Get started" arrow>
    Google Cloud Spanner is a fully managed, globally distributed relational database service.
  </Card>

<Card title="Speech-to-Text" href="/oss/python/integrations/document_loaders/google_speech_to_text" cta="Get started" arrow>
    Google Cloud Speech-to-Text transcribes audio files.
  </Card>
</Columns>

<Card title="Cloud Vision loader">
  Load data using Google Cloud Vision API.

### Document transformers

Transform documents using Google Cloud services.

<Columns cols={2}>
  <Card title="Document AI" href="/oss/python/integrations/document_transformers/google_docai" cta="Get started" arrow>
    Transform unstructured data from documents into structured data, making it easier to understand, analyze, and consume.
  </Card>

<Card title="Google Translate" href="/oss/python/integrations/document_transformers/google_translate" cta="Get started" arrow>
    Translate text and HTML with the Google Cloud Translation API.
  </Card>
</Columns>

Store and search vectors using Google Cloud databases and Vertex AI Vector Search.

<Columns cols={2}>
  <Card title="AlloyDB for PostgreSQL" href="/oss/python/integrations/vectorstores/google_alloydb" cta="Get started" arrow>
    Google Cloud AlloyDB is a fully managed relational database service that offers high performance, seamless integration, and impressive scalability on Google Cloud. AlloyDB is 100% compatible with PostgreSQL.
  </Card>

<Card title="BigQuery Vector Search" href="/oss/python/integrations/vectorstores/google_bigquery_vector_search" cta="Get started" arrow>
    BigQuery vector search lets you use GoogleSQL to do semantic search, using vector indexes for fast but approximate results, or using brute force for exact results.
  </Card>

<Card title="Memorystore for Redis" href="/oss/python/integrations/vectorstores/google_memorystore_redis" cta="Get started" arrow>
    Vector store using Memorystore for Redis
  </Card>

<Card title="Spanner" href="/oss/python/integrations/vectorstores/google_spanner" cta="Get started" arrow>
    Vector store using Cloud Spanner
  </Card>

<Card title="Firestore (Native Mode)" href="/oss/python/integrations/vectorstores/google_firestore" cta="Get started" arrow>
    Vector store using Firestore
  </Card>

<Card title="Cloud SQL for MySQL" href="/oss/python/integrations/vectorstores/google_cloud_sql_mysql" cta="Get started" arrow>
    Vector store using Cloud SQL for MySQL
  </Card>

<Card title="Cloud SQL for PostgreSQL" href="/oss/python/integrations/vectorstores/google_cloud_sql_pg" cta="Get started" arrow>
    Vector store using Cloud SQL for PostgreSQL.
  </Card>

<Card title="Vertex AI Vector Search" href="/oss/python/integrations/vectorstores/google_vertex_ai_vector_search" cta="Get started" arrow>
    Formerly known as Vertex AI Matching Engine, provides a low latency vector database. These vector databases are commonly referred to as vector similarity-matching or an approximate nearest neighbor (ANN) service.
  </Card>

<Card title="With DataStore Backend" href="/oss/python/integrations/vectorstores/google_vertex_ai_vector_search/#optional--you-can-also-create-vectore-and-store-chunks-in-a-datastore" cta="Get started" arrow>
    Vector search using Datastore for document storage.
  </Card>
</Columns>

Retrieve information using Google Cloud services.

<Columns cols={2}>
  <Card title="Vertex AI Search" icon="magnifying-glass" href="/oss/python/integrations/retrievers/google_vertex_ai_search" cta="Get started" arrow>
    Build generative AI powered search engines using Vertex AI Search
  </Card>

<Card title="Document AI Warehouse" icon="warehouse" href="https://cloud.google.com/document-ai-warehouse" cta="Get started" arrow>
    Search, store, and manage documents using Document AI Warehouse.
  </Card>
</Columns>

Integrate agents with various Google Cloud services.

<Columns cols={2}>
  <Card title="Text-to-Speech" icon="volume-high" href="/oss/python/integrations/tools/google_cloud_texttospeech" cta="Get started" arrow>
    Google Cloud Text-to-Speech synthesizes natural-sounding speech with 100+ voices in multiple languages.
  </Card>
</Columns>

Track LLM/Chat model usage.

<AccordionGroup>
  <Accordion title="Vertex AI callback handler">
    Callback Handler that tracks `VertexAI` usage info.

</Accordion>
</AccordionGroup>

Evaluate model outputs using Vertex AI.

<AccordionGroup>
  <Accordion title="VertexPairWiseStringEvaluator">
    Pair-wise evaluation using Vertex AI models.

<Accordion title="VertexStringEvaluator">
    Evaluate a single prediction string using Vertex AI models.

</Accordion>
</AccordionGroup>

## Other Google products

Integrations with various Google services beyond the core Cloud Platform.

<Columns cols={1}>
  <Card title="Google Drive" href="/oss/python/integrations/document_loaders/google_drive" cta="Get started" arrow>
    Google Drive file storage. Currently supports Google Docs.
  </Card>
</Columns>

<Columns cols={1}>
  <Card title="ScaNN (Local Index)" href="/oss/python/integrations/vectorstores/scann" cta="Get started" arrow>
    ScaNN is a method for efficient vector similarity search at scale.
  </Card>
</Columns>

<Columns cols={1}>
  <Card title="Google Drive" href="/oss/python/integrations/retrievers/google_drive" cta="Get started" arrow>
    Retrieve documents from Google Drive.
  </Card>
</Columns>

<Columns cols={2}>
  <Card title="Google Search" href="/oss/python/integrations/tools/google_search" cta="Get started" arrow>
    Perform web searches using Google Custom Search Engine (CSE).
  </Card>

<Card title="Google Drive" href="/oss/python/integrations/tools/google_drive" cta="Get started" arrow>
    Tools for interacting with Google Drive.
  </Card>

<Card title="Google Finance" href="/oss/python/integrations/tools/google_finance" cta="Get started" arrow>
    Query financial data.
  </Card>

<Card title="Google Jobs" href="/oss/python/integrations/tools/google_jobs" cta="Get started" arrow>
    Query job listings.
  </Card>

<Card title="Google Lens" href="/oss/python/integrations/tools/google_lens" cta="Get started" arrow>
    Perform visual searches.
  </Card>

<Card title="Google Places" href="/oss/python/integrations/tools/google_places" cta="Get started" arrow>
    Search for places information.
  </Card>

<Card title="Google Scholar" href="/oss/python/integrations/tools/google_scholar" cta="Get started" arrow>
    Search academic papers.
  </Card>

<Card title="Google Trends" href="/oss/python/integrations/tools/google_trends" cta="Get started" arrow>
    Query Google Trends data.
  </Card>
</Columns>

<Columns cols={1}>
  <Card title="MCP Toolbox" href="/oss/python/integrations/tools/toolbox" cta="Get started" arrow>
    Simple and efficient way to connect to your databases, including those on Google Cloud like Cloud SQL and AlloyDB
  </Card>
</Columns>

Collections of tools for specific Google services.

<Columns cols={2}>
  <Card title="Gmail" icon="envelope" href="/oss/python/integrations/tools/gmail" cta="Get started" arrow>
    Toolkit to create, get, search, and send emails using the Gmail API.
  </Card>
</Columns>

<Columns cols={2}>
  <Card title="Gmail" icon="envelope" href="/oss/python/integrations/chat_loaders/gmail" cta="Get started" arrow>
    Load chat history from Gmail threads.
  </Card>
</Columns>

## 3rd party integrations

Access Google services via unofficial third-party APIs.

<Columns cols={2}>
  <Card title="SearchApi" icon="magnifying-glass" href="/oss/python/integrations/tools/searchapi" cta="Get started" arrow>
    searchapi.io provides API access to Google search results, YouTube, and more.
  </Card>

<Card title="SerpApi" icon="magnifying-glass" href="/oss/python/integrations/tools/serpapi" cta="Get started" arrow>
    SerpApi provides API access to Google search results.
  </Card>

<Card title="Serper.dev" icon="magnifying-glass" href="/oss/python/integrations/tools/google_serper" cta="Get started" arrow>
    serper.dev provides API access to Google search results.
  </Card>
</Columns>

<Columns cols={2}>
  <Card title="Search tool" icon="youtube" href="/oss/python/integrations/tools/youtube" cta="Get started" arrow>
    Search YouTube videos without the official API.
  </Card>

<Card title="Audio loader" icon="youtube" href="/oss/python/integrations/document_loaders/youtube_audio" cta="Get started" arrow>
    Download audio from YouTube videos.
  </Card>

<Card title="Transcripts loader" icon="youtube" href="/oss/python/integrations/document_loaders/youtube_transcript" cta="Get started" arrow>
    Load video transcripts.
  </Card>
</Columns>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/providers/google.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
</Accordion>

  <Accordion title="Mistral on Vertex AI Model Garden">
```

Example 2 (unknown):
```unknown
</Accordion>

  <Accordion title="Gemma local from Hugging Face">
    Local Gemma model loaded from HuggingFace.
```

Example 3 (unknown):
```unknown
</Accordion>

  <Accordion title="Gemma local from Kaggle">
    Local Gemma model loaded from Kaggle.
```

Example 4 (unknown):
```unknown
</Accordion>

  <Accordion title="Gemma on Vertex AI Model Garden">
```

---

## Graph API overview

**URL:** llms-txt#graph-api-overview

**Contents:**
- Graphs
  - StateGraph
  - Compiling your graph
- State
  - Schema
  - Reducers
  - Working with Messages in Graph State
- Nodes
  - `START` Node
  - `END` Node

Source: https://docs.langchain.com/oss/javascript/langgraph/graph-api

At its core, LangGraph models agent workflows as graphs. You define the behavior of your agents using three key components:

1. [`State`](#state): A shared data structure that represents the current snapshot of your application. It can be any data type, but is typically defined using a shared state schema.

2. [`Nodes`](#nodes): Functions that encode the logic of your agents. They receive the current state as input, perform some computation or side-effect, and return an updated state.

3. [`Edges`](#edges): Functions that determine which `Node` to execute next based on the current state. They can be conditional branches or fixed transitions.

By composing `Nodes` and `Edges`, you can create complex, looping workflows that evolve the state over time. The real power, though, comes from how LangGraph manages that state. To emphasize: `Nodes` and `Edges` are nothing more than functions - they can contain an LLM or just good ol' code.

In short: *nodes do the work, edges tell what to do next*.

LangGraph's underlying graph algorithm uses [message passing](https://en.wikipedia.org/wiki/Message_passing) to define a general program. When a Node completes its operation, it sends messages along one or more edges to other node(s). These recipient nodes then execute their functions, pass the resulting messages to the next set of nodes, and the process continues. Inspired by Google's [Pregel](https://research.google/pubs/pregel-a-system-for-large-scale-graph-processing/) system, the program proceeds in discrete "super-steps."

A super-step can be considered a single iteration over the graph nodes. Nodes that run in parallel are part of the same super-step, while nodes that run sequentially belong to separate super-steps. At the start of graph execution, all nodes begin in an `inactive` state. A node becomes `active` when it receives a new message (state) on any of its incoming edges (or "channels"). The active node then runs its function and responds with updates. At the end of each super-step, nodes with no incoming messages vote to `halt` by marking themselves as `inactive`. The graph execution terminates when all nodes are `inactive` and no messages are in transit.

The [`StateGraph`](https://langchain-ai.github.io/langgraphjs/reference/classes/langgraph.StateGraph.html) class is the main graph class to use. This is parameterized by a user defined `State` object.

### Compiling your graph

To build your graph, you first define the [state](#state), you then add [nodes](#nodes) and [edges](#edges), and then you compile it. What exactly is compiling your graph and why is it needed?

Compiling is a pretty simple step. It provides a few basic checks on the structure of your graph (no orphaned nodes, etc). It is also where you can specify runtime args like [checkpointers](/oss/javascript/langgraph/persistence) and breakpoints. You compile your graph by just calling the `.compile` method:

You **MUST** compile your graph before you can use it.

The first thing you do when you define a graph is define the `State` of the graph. The `State` consists of the [schema of the graph](#schema) as well as [`reducer` functions](#reducers) which specify how to apply updates to the state. The schema of the `State` will be the input schema to all `Nodes` and `Edges` in the graph, and can be either a Zod schema or a schema built using `Annotation.Root`. All `Nodes` will emit updates to the `State` which are then applied using the specified `reducer` function.

The main documented way to specify the schema of a graph is by using Zod schemas. However, we also support using the `Annotation` API to define the schema of the graph.

By default, the graph will have the same input and output schemas. If you want to change this, you can also specify explicit input and output schemas directly. This is useful when you have a lot of keys, and some are explicitly for input and others for output.

#### Multiple schemas

Typically, all graph nodes communicate with a single schema. This means that they will read and write to the same state channels. But, there are cases where we want more control over this:

* Internal nodes can pass information that is not required in the graph's input / output.
* We may also want to use different input / output schemas for the graph. The output might, for example, only contain a single relevant output key.

It is possible to have nodes write to private state channels inside the graph for internal node communication. We can simply define a private schema, `PrivateState`.

It is also possible to define explicit input and output schemas for a graph. In these cases, we define an "internal" schema that contains *all* keys relevant to graph operations. But, we also define `input` and `output` schemas that are sub-sets of the "internal" schema to constrain the input and output of the graph. See [this guide](/oss/javascript/langgraph/graph-api#define-input-and-output-schemas) for more detail.

Let's look at an example:

There are two subtle and important points to note here:

1. We pass `state` as the input schema to `node1`. But, we write out to `foo`, a channel in `OverallState`. How can we write out to a state channel that is not included in the input schema? This is because a node *can write to any state channel in the graph state.* The graph state is the union of the state channels defined at initialization, which includes `OverallState` and the filters `InputState` and `OutputState`.

2. We initialize the graph with `StateGraph({ state: OverallState, input: InputState, output: OutputState })`. So, how can we write to `PrivateState` in `node2`? How does the graph gain access to this schema if it was not passed in the `StateGraph` initialization? We can do this because *nodes can also declare additional state channels* as long as the state schema definition exists. In this case, the `PrivateState` schema is defined, so we can add `bar` as a new state channel in the graph and write to it.

Reducers are key to understanding how updates from nodes are applied to the `State`. Each key in the `State` has its own independent reducer function. If no reducer function is explicitly specified then it is assumed that all updates to that key should override it. There are a few different types of reducers, starting with the default type of reducer:

These two examples show how to use the default reducer:

In this example, no reducer functions are specified for any key. Let's assume the input to the graph is:

`{ foo: 1, bar: ["hi"] }`. Let's then assume the first `Node` returns `{ foo: 2 }`. This is treated as an update to the state. Notice that the `Node` does not need to return the whole `State` schema - just an update. After applying this update, the `State` would then be `{ foo: 2, bar: ["hi"] }`. If the second node returns `{ bar: ["bye"] }` then the `State` would then be `{ foo: 2, bar: ["bye"] }`

In this example, we've used [Zod 4 registries](https://zod.dev/metadata) to specify a reducer function for the second key (`bar`). Note that the first key remains unchanged. Let's assume the input to the graph is `{ foo: 1, bar: ["hi"] }`. Let's then assume the first `Node` returns `{ foo: 2 }`. This is treated as an update to the state. Notice that the `Node` does not need to return the whole `State` schema - just an update. After applying this update, the `State` would then be `{ foo: 2, bar: ["hi"] }`. If the second node returns `{ bar: ["bye"] }` then the `State` would then be `{ foo: 2, bar: ["hi", "bye"] }`. Notice here that the `bar` key is updated by adding the two arrays together.

### Working with Messages in Graph State

#### Why use messages?

Most modern LLM providers have a chat model interface that accepts a list of messages as input. LangChain's [`ChatModel`](https://js.langchain.com/docs/concepts/#chat-models) in particular accepts a list of `Message` objects as inputs. These messages come in a variety of forms such as @\[`HumanMessage`] (user input) or [`AIMessage`](https://v03.api.js.langchain.com/classes/_langchain_core.messages_ai_message.AIMessage.html) (LLM response). To read more about what message objects are, please refer to [this](https://js.langchain.com/docs/concepts/#messages) conceptual guide.

#### Using Messages in your Graph

In many cases, it is helpful to store prior conversation history as a list of messages in your graph state. To do so, we can add a key (channel) to the graph state that stores a list of `Message` objects and annotate it with a reducer function (see `messages` key in the example below). The reducer function is vital to telling the graph how to update the list of `Message` objects in the state with each state update (for example, when a node sends an update). If you don't specify a reducer, every state update will overwrite the list of messages with the most recently provided value. If you wanted to simply append messages to the existing list, you could use a function that concatenates arrays as a reducer.

However, you might also want to manually update messages in your graph state (e.g. human-in-the-loop). If you were to use a simple concatenation function, the manual state updates you send to the graph would be appended to the existing list of messages, instead of updating existing messages. To avoid that, you need a reducer that can keep track of message IDs and overwrite existing messages, if updated. To achieve this, you can use the prebuilt `messagesStateReducer` function or `MessagesZodMeta` when state schema is defined with Zod. For brand new messages, it will simply append to existing list, but it will also handle the updates for existing messages correctly.

In addition to keeping track of message IDs, `MessagesZodMeta` will also try to deserialize messages into LangChain `Message` objects whenever a state update is received on the `messages` channel. This allows sending graph inputs / state updates in the following format:

Since the state updates are always deserialized into LangChain `Messages` when using `MessagesZodMeta`, you should use dot notation to access message attributes, like `state.messages[state.messages.length - 1].content`. Below is an example of a graph that uses `MessagesZodMeta`:

`MessagesZodState` is defined with a single `messages` key which is a list of @\[`BaseMessage`] objects and uses the appropriate reducer. Typically, there is more state to track than just messages, so we see people extend this state and add more fields, like:

In LangGraph, nodes are typically functions (sync or async) that accept the following arguments:

1. `state`: The [state](#state) of the graph
2. `config`: A [`RunnableConfig`](https://v03.api.js.langchain.com/interfaces/_langchain_core.runnables.RunnableConfig.html) object that contains configuration information like `thread_id` and tracing information like `tags`

You can add nodes to a graph using the `addNode` method.

Behind the scenes, functions are converted to [RunnableLambda](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableLambda.html)s, which add batch and async support to your function, along with native tracing and debugging.

If you add a node to a graph without specifying a name, it will be given a default name equivalent to the function name.

The [`START`](https://langchain-ai.github.io/langgraphjs/reference/variables/langgraph.START.html) Node is a special node that represents the node that sends user input to the graph. The main purpose for referencing this node is to determine which nodes should be called first.

The `END` Node is a special node that represents a terminal node. This node is referenced when you want to denote which edges have no actions after they are done.

LangGraph supports caching of tasks/nodes based on the input to the node. To use caching:

* Specify a cache when compiling a graph (or specifying an entrypoint)
* Specify a cache policy for nodes. Each cache policy supports:
  * `keyFunc`, which is used to generate a cache key based on the input to a node.
  * `ttl`, the time to live for the cache in seconds. If not specified, the cache will never expire.

Edges define how the logic is routed and how the graph decides to stop. This is a big part of how your agents work and how different nodes communicate with each other. There are a few key types of edges:

* Normal Edges: Go directly from one node to the next.
* Conditional Edges: Call a function to determine which node(s) to go to next.
* Entry Point: Which node to call first when user input arrives.
* Conditional Entry Point: Call a function to determine which node(s) to call first when user input arrives.

A node can have MULTIPLE outgoing edges. If a node has multiple out-going edges, **all** of those destination nodes will be executed in parallel as a part of the next superstep.

If you **always** want to go from node A to node B, you can use the @\[`addEdge`] method directly.

### Conditional Edges

If you want to **optionally** route to one or more edges (or optionally terminate), you can use the @\[`addConditionalEdges`] method. This method accepts the name of a node and a "routing function" to call after that node is executed:

Similar to nodes, the `routingFunction` accepts the current `state` of the graph and returns a value.

By default, the return value `routingFunction` is used as the name of the node (or list of nodes) to send the state to next. All those nodes will be run in parallel as a part of the next superstep.

You can optionally provide an object that maps the `routingFunction`'s output to the name of the next node.

<Tip>
  Use [`Command`](#command) instead of conditional edges if you want to combine state updates and routing in a single function.
</Tip>

The entry point is the first node(s) that are run when the graph starts. You can use the @\[`addEdge`] method from the virtual [`START`](https://langchain-ai.github.io/langgraphjs/reference/variables/langgraph.START.html) node to the first node to execute to specify where to enter the graph.

### Conditional Entry Point

A conditional entry point lets you start at different nodes depending on custom logic. You can use @\[`addConditionalEdges`] from the virtual [`START`](https://langchain-ai.github.io/langgraphjs/reference/variables/langgraph.START.html) node to accomplish this.

You can optionally provide an object that maps the `routingFunction`'s output to the name of the next node.

By default, `Nodes` and `Edges` are defined ahead of time and operate on the same shared state. However, there can be cases where the exact edges are not known ahead of time and/or you may want different versions of `State` to exist at the same time. A common example of this is with map-reduce design patterns. In this design pattern, a first node may generate a list of objects, and you may want to apply some other node to all those objects. The number of objects may be unknown ahead of time (meaning the number of edges may not be known) and the input `State` to the downstream `Node` should be different (one for each generated object).

To support this design pattern, LangGraph supports returning [`Send`](https://langchain-ai.github.io/langgraphjs/reference/classes/langgraph.Send.html) objects from conditional edges. `Send` takes two arguments: first is the name of the node, and second is the state to pass to that node.

It can be useful to combine control flow (edges) and state updates (nodes). For example, you might want to BOTH perform state updates AND decide which node to go to next in the SAME node. LangGraph provides a way to do so by returning a [`Command`](https://langchain-ai.github.io/langgraphjs/reference/classes/langgraph.Command.html) object from node functions:

With [`Command`](https://langchain-ai.github.io/langgraphjs/reference/classes/langgraph.Command.html) you can also achieve dynamic control flow behavior (identical to [conditional edges](#conditional-edges)):

When using [`Command`](https://langchain-ai.github.io/langgraphjs/reference/classes/langgraph.Command.html) in your node functions, you must add the `ends` parameter when adding the node to specify which nodes it can route to:

<Note>
  When returning [`Command`](https://langchain-ai.github.io/langgraphjs/reference/classes/langgraph.Command.html) in your node functions, you must add return type annotations with the list of node names the node is routing to, e.g. `Command[Literal["my_other_node"]]`. This is necessary for the graph rendering and tells LangGraph that `my_node` can navigate to `my_other_node`.
</Note>

Check out this [how-to guide](/oss/javascript/langgraph/use-graph-api#combine-control-flow-and-state-updates-with-command) for an end-to-end example of how to use [`Command`](https://langchain-ai.github.io/langgraphjs/reference/classes/langgraph.Command.html).

### When should I use Command instead of conditional edges?

* Use [`Command`](https://langchain-ai.github.io/langgraphjs/reference/classes/langgraph.Command.html) when you need to **both** update the graph state **and** route to a different node. For example, when implementing [multi-agent handoffs](/oss/javascript/langchain/multi-agent#handoffs) where it's important to route to a different agent and pass some information to that agent.
* Use [conditional edges](#conditional-edges) to route between nodes conditionally without updating the state.

### Navigating to a node in a parent graph

If you are using [subgraphs](/oss/javascript/langgraph/use-subgraphs), you might want to navigate from a node within a subgraph to a different subgraph (i.e. a different node in the parent graph). To do so, you can specify `graph: Command.PARENT` in `Command`:

<Note>
  Setting `graph` to `Command.PARENT` will navigate to the closest parent graph.

When you send updates from a subgraph node to a parent graph node for a key that's shared by both parent and subgraph [state schemas](#schema), you **must** define a [reducer](#reducers) for the key you're updating in the parent graph state.
</Note>

If you are using [subgraphs](/oss/javascript/langgraph/use-subgraphs), you might want to navigate from a node within a subgraph to a different subgraph (i.e. a different node in the parent graph). To do so, you can specify `graph: Command.PARENT` in `Command`:

<Note>
  Setting `graph` to `Command.PARENT` will navigate to the closest parent graph.

When you send updates from a subgraph node to a parent graph node for a key that's shared by both parent and subgraph [state schemas](#schema), you **must** define a [reducer](#reducers) for the key you're updating in the parent graph state.
</Note>

This is particularly useful when implementing [multi-agent handoffs](/oss/javascript/langchain/multi-agent#handoffs).

Check out [this guide](/oss/javascript/langgraph/use-graph-api#navigate-to-a-node-in-a-parent-graph) for detail.

### Using inside tools

A common use case is updating graph state from inside a tool. For example, in a customer support application you might want to look up customer information based on their account number or ID in the beginning of the conversation.

Refer to [this guide](/oss/javascript/langgraph/use-graph-api#use-inside-tools) for detail.

### Human-in-the-loop

[`Command`](https://langchain-ai.github.io/langgraphjs/reference/classes/langgraph.Command.html) is an important part of human-in-the-loop workflows: when using `interrupt()` to collect user input, [`Command`](https://langchain-ai.github.io/langgraphjs/reference/classes/langgraph.Command.html) is then used to supply the input and resume execution via `new Command({ resume: "User input" })`. Check out the [human-in-the-loop conceptual guide](/oss/javascript/langgraph/interrupts) for more information.

LangGraph can easily handle migrations of graph definitions (nodes, edges, and state) even when using a checkpointer to track state.

* For threads at the end of the graph (i.e. not interrupted) you can change the entire topology of the graph (i.e. all nodes and edges, remove, add, rename, etc)
* For threads currently interrupted, we support all topology changes other than renaming / removing nodes (as that thread could now be about to enter a node that no longer exists) -- if this is a blocker please reach out and we can prioritize a solution.
* For modifying state, we have full backwards and forwards compatibility for adding and removing keys
* State keys that are renamed lose their saved state in existing threads
* State keys whose types change in incompatible ways could currently cause issues in threads with state from before the change -- if this is a blocker please reach out and we can prioritize a solution.

When creating a graph, you can specify a `contextSchema` for runtime context passed to nodes. This is useful for passing
information to nodes that is not part of the graph state. For example, you might want to pass dependencies such as model name or a database connection.

You can then pass this configuration into the graph using the `context` property.

You can then access and use this context inside a node or conditional edge:

See [this guide](/oss/javascript/langgraph/use-graph-api#add-runtime-configuration) for a full breakdown on configuration.

The recursion limit sets the maximum number of [super-steps](#graphs) the graph can execute during a single execution. Once the limit is reached, LangGraph will raise `GraphRecursionError`. By default this value is set to 25 steps. The recursion limit can be set on any graph at runtime, and is passed to `invoke`/`stream` via the config object. Importantly, `recursionLimit` is a standalone `config` key and should not be passed inside the `configurable` key as all other user-defined configuration. See the example below:

### Accessing and handling the recursion counter

The current step counter is accessible in `config.metadata.langgraph_step` within any node, allowing for proactive recursion handling before hitting the recursion limit. This enables you to implement graceful degradation strategies within your graph logic.

The step counter is stored in `config.metadata.langgraph_step`. The recursion limit check follows the logic: `step > stop` where `stop = step + recursionLimit + 1`. When the limit is exceeded, LangGraph raises a `GraphRecursionError`.

#### Accessing the current step counter

You can access the current step counter within any node to monitor execution progress.

#### Proactive recursion handling

You can check the step counter and proactively route to a different node before hitting the limit. This allows for graceful degradation within your graph.

#### Proactive vs reactive approaches

There are two main approaches to handling recursion limits: proactive (monitoring within the graph) and reactive (catching errors externally).

The key differences between these approaches are:

| Approach                                  | Detection            | Handling                             | Control Flow                       |
| ----------------------------------------- | -------------------- | ------------------------------------ | ---------------------------------- |
| Proactive (using `langgraph_step`)        | Before limit reached | Inside graph via conditional routing | Graph continues to completion node |
| Reactive (catching `GraphRecursionError`) | After limit exceeded | Outside graph in try/catch           | Graph execution terminated         |

**Proactive advantages:**

* Graceful degradation within the graph
* Can save intermediate state in checkpoints
* Better user experience with partial results
* Graph completes normally (no exception)

**Reactive advantages:**

* Simpler implementation
* No need to modify graph logic
* Centralized error handling

#### Other available metadata

Along with `langgraph_step`, the following metadata is also available in `config.metadata`:

It's often nice to be able to visualize graphs, especially as they get more complex. LangGraph comes with several built-in ways to visualize graphs. See [this how-to guide](/oss/javascript/langgraph/use-graph-api#visualize-your-graph) for more info.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/graph-api.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
You **MUST** compile your graph before you can use it.

## State

The first thing you do when you define a graph is define the `State` of the graph. The `State` consists of the [schema of the graph](#schema) as well as [`reducer` functions](#reducers) which specify how to apply updates to the state. The schema of the `State` will be the input schema to all `Nodes` and `Edges` in the graph, and can be either a Zod schema or a schema built using `Annotation.Root`. All `Nodes` will emit updates to the `State` which are then applied using the specified `reducer` function.

### Schema

The main documented way to specify the schema of a graph is by using Zod schemas. However, we also support using the `Annotation` API to define the schema of the graph.

By default, the graph will have the same input and output schemas. If you want to change this, you can also specify explicit input and output schemas directly. This is useful when you have a lot of keys, and some are explicitly for input and others for output.

#### Multiple schemas

Typically, all graph nodes communicate with a single schema. This means that they will read and write to the same state channels. But, there are cases where we want more control over this:

* Internal nodes can pass information that is not required in the graph's input / output.
* We may also want to use different input / output schemas for the graph. The output might, for example, only contain a single relevant output key.

It is possible to have nodes write to private state channels inside the graph for internal node communication. We can simply define a private schema, `PrivateState`.

It is also possible to define explicit input and output schemas for a graph. In these cases, we define an "internal" schema that contains *all* keys relevant to graph operations. But, we also define `input` and `output` schemas that are sub-sets of the "internal" schema to constrain the input and output of the graph. See [this guide](/oss/javascript/langgraph/graph-api#define-input-and-output-schemas) for more detail.

Let's look at an example:
```

Example 2 (unknown):
```unknown
There are two subtle and important points to note here:

1. We pass `state` as the input schema to `node1`. But, we write out to `foo`, a channel in `OverallState`. How can we write out to a state channel that is not included in the input schema? This is because a node *can write to any state channel in the graph state.* The graph state is the union of the state channels defined at initialization, which includes `OverallState` and the filters `InputState` and `OutputState`.

2. We initialize the graph with `StateGraph({ state: OverallState, input: InputState, output: OutputState })`. So, how can we write to `PrivateState` in `node2`? How does the graph gain access to this schema if it was not passed in the `StateGraph` initialization? We can do this because *nodes can also declare additional state channels* as long as the state schema definition exists. In this case, the `PrivateState` schema is defined, so we can add `bar` as a new state channel in the graph and write to it.

### Reducers

Reducers are key to understanding how updates from nodes are applied to the `State`. Each key in the `State` has its own independent reducer function. If no reducer function is explicitly specified then it is assumed that all updates to that key should override it. There are a few different types of reducers, starting with the default type of reducer:

#### Default Reducer

These two examples show how to use the default reducer:

**Example A:**
```

Example 3 (unknown):
```unknown
In this example, no reducer functions are specified for any key. Let's assume the input to the graph is:

`{ foo: 1, bar: ["hi"] }`. Let's then assume the first `Node` returns `{ foo: 2 }`. This is treated as an update to the state. Notice that the `Node` does not need to return the whole `State` schema - just an update. After applying this update, the `State` would then be `{ foo: 2, bar: ["hi"] }`. If the second node returns `{ bar: ["bye"] }` then the `State` would then be `{ foo: 2, bar: ["bye"] }`

**Example B:**
```

Example 4 (unknown):
```unknown
In this example, we've used [Zod 4 registries](https://zod.dev/metadata) to specify a reducer function for the second key (`bar`). Note that the first key remains unchanged. Let's assume the input to the graph is `{ foo: 1, bar: ["hi"] }`. Let's then assume the first `Node` returns `{ foo: 2 }`. This is treated as an update to the state. Notice that the `Node` does not need to return the whole `State` schema - just an update. After applying this update, the `State` would then be `{ foo: 2, bar: ["hi"] }`. If the second node returns `{ bar: ["bye"] }` then the `State` would then be `{ foo: 2, bar: ["hi", "bye"] }`. Notice here that the `bar` key is updated by adding the two arrays together.

### Working with Messages in Graph State

#### Why use messages?

Most modern LLM providers have a chat model interface that accepts a list of messages as input. LangChain's [`ChatModel`](https://js.langchain.com/docs/concepts/#chat-models) in particular accepts a list of `Message` objects as inputs. These messages come in a variety of forms such as @\[`HumanMessage`] (user input) or [`AIMessage`](https://v03.api.js.langchain.com/classes/_langchain_core.messages_ai_message.AIMessage.html) (LLM response). To read more about what message objects are, please refer to [this](https://js.langchain.com/docs/concepts/#messages) conceptual guide.

#### Using Messages in your Graph

In many cases, it is helpful to store prior conversation history as a list of messages in your graph state. To do so, we can add a key (channel) to the graph state that stores a list of `Message` objects and annotate it with a reducer function (see `messages` key in the example below). The reducer function is vital to telling the graph how to update the list of `Message` objects in the state with each state update (for example, when a node sends an update). If you don't specify a reducer, every state update will overwrite the list of messages with the most recently provided value. If you wanted to simply append messages to the existing list, you could use a function that concatenates arrays as a reducer.

However, you might also want to manually update messages in your graph state (e.g. human-in-the-loop). If you were to use a simple concatenation function, the manual state updates you send to the graph would be appended to the existing list of messages, instead of updating existing messages. To avoid that, you need a reducer that can keep track of message IDs and overwrite existing messages, if updated. To achieve this, you can use the prebuilt `messagesStateReducer` function or `MessagesZodMeta` when state schema is defined with Zod. For brand new messages, it will simply append to existing list, but it will also handle the updates for existing messages correctly.

#### Serialization

In addition to keeping track of message IDs, `MessagesZodMeta` will also try to deserialize messages into LangChain `Message` objects whenever a state update is received on the `messages` channel. This allows sending graph inputs / state updates in the following format:
```

---

## Graph node for executing the refund.

**URL:** llms-txt#graph-node-for-executing-the-refund.

---

## Graph node for extracting user info and routing to lookup/refund/END.

**URL:** llms-txt#graph-node-for-extracting-user-info-and-routing-to-lookup/refund/end.

async def gather_info(state: State) -> Command[Literal["lookup", "refund", END]]:
    info = await info_llm.ainvoke(
        [
            {"role": "system", "content": gather_info_instructions},
            *state["messages"],
        ]
    )
    parsed = info["parsed"]
    if any(parsed[k] for k in ("invoice_id", "invoice_line_ids")):
        goto = "refund"
    elif all(
        parsed[k]
        for k in ("customer_first_name", "customer_last_name", "customer_phone")
    ):
        goto = "lookup"
    else:
        goto = END
    update = {"messages": [info["raw"]], **parsed}
    return Command(update=update, goto=goto)

---

## Graph node for looking up the users purchases

**URL:** llms-txt#graph-node-for-looking-up-the-users-purchases

def lookup(state: State) -> dict:
    args = (
        state[k]
        for k in (
            "customer_first_name",
            "customer_last_name",
            "customer_phone",
            "track_name",
            "album_title",
            "artist_name",
            "purchase_date_iso_8601",
        )
    )
    results = _lookup(*args)
    if not results:
        response = "We did not find any purchases associated with the information you've provided. Are you sure you've entered all of your information correctly?"
        followup = response
    else:
        response = f"Which of the following purchases would you like to be refunded for?\n\n"
        followup = f"Which of the following purchases would you like to be refunded for?\n\n{tabulate(results, headers='keys')}"
    return {
        "messages": [{"role": "assistant", "content": response}],
        "followup": followup,
        "invoice_line_ids": [res["invoice_line_id"] for res in results],
    }

---

## {'graph_output': 'My name is Lance'}

**URL:** llms-txt#{'graph_output':-'my-name-is-lance'}

**Contents:**
  - Reducers
  - Working with Messages in Graph State

python  theme={null}
from typing_extensions import TypedDict

class State(TypedDict):
    foo: int
    bar: list[str]
python  theme={null}
from typing import Annotated
from typing_extensions import TypedDict
from operator import add

class State(TypedDict):
    foo: int
    bar: Annotated[list[str], add]
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
There are two subtle and important points to note here:

1. We pass `state: InputState` as the input schema to `node_1`. But, we write out to `foo`, a channel in `OverallState`. How can we write out to a state channel that is not included in the input schema? This is because a node *can write to any state channel in the graph state.* The graph state is the union of the state channels defined at initialization, which includes `OverallState` and the filters `InputState` and `OutputState`.

2. We initialize the graph with `StateGraph(OverallState,input_schema=InputState,output_schema=OutputState)`. So, how can we write to `PrivateState` in `node_2`? How does the graph gain access to this schema if it was not passed in the `StateGraph` initialization? We can do this because *nodes can also declare additional state channels* as long as the state schema definition exists. In this case, the `PrivateState` schema is defined, so we can add `bar` as a new state channel in the graph and write to it.

### Reducers

Reducers are key to understanding how updates from nodes are applied to the `State`. Each key in the `State` has its own independent reducer function. If no reducer function is explicitly specified then it is assumed that all updates to that key should override it. There are a few different types of reducers, starting with the default type of reducer:

#### Default Reducer

These two examples show how to use the default reducer:

**Example A:**
```

Example 2 (unknown):
```unknown
In this example, no reducer functions are specified for any key. Let's assume the input to the graph is:

`{"foo": 1, "bar": ["hi"]}`. Let's then assume the first `Node` returns `{"foo": 2}`. This is treated as an update to the state. Notice that the `Node` does not need to return the whole `State` schema - just an update. After applying this update, the `State` would then be `{"foo": 2, "bar": ["hi"]}`. If the second node returns `{"bar": ["bye"]}` then the `State` would then be `{"foo": 2, "bar": ["bye"]}`

**Example B:**
```

Example 3 (unknown):
```unknown
In this example, we've used the `Annotated` type to specify a reducer function (`operator.add`) for the second key (`bar`). Note that the first key remains unchanged. Let's assume the input to the graph is `{"foo": 1, "bar": ["hi"]}`. Let's then assume the first `Node` returns `{"foo": 2}`. This is treated as an update to the state. Notice that the `Node` does not need to return the whole `State` schema - just an update. After applying this update, the `State` would then be `{"foo": 2, "bar": ["hi"]}`. If the second node returns `{"bar": ["bye"]}` then the `State` would then be `{"foo": 2, "bar": ["hi", "bye"]}`. Notice here that the `bar` key is updated by adding the two lists together.

#### Overwrite

<Tip>
  In some cases, you may want to bypass a reducer and directly overwrite a state value. LangGraph provides the [`Overwrite`](https://reference.langchain.com/python/langgraph/types/) type for this purpose. [Learn how to use `Overwrite` here](/oss/python/langgraph/use-graph-api#bypass-reducers-with-overwrite).
</Tip>

### Working with Messages in Graph State

#### Why use messages?

Most modern LLM providers have a chat model interface that accepts a list of messages as input. LangChain's [`ChatModel`](https://python.langchain.com/docs/concepts/#chat-models) in particular accepts a list of `Message` objects as inputs. These messages come in a variety of forms such as [`HumanMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.HumanMessage) (user input) or [`AIMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessage) (LLM response). To read more about what message objects are, please refer to [this](https://python.langchain.com/docs/concepts/#messages) conceptual guide.

#### Using Messages in your Graph

In many cases, it is helpful to store prior conversation history as a list of messages in your graph state. To do so, we can add a key (channel) to the graph state that stores a list of `Message` objects and annotate it with a reducer function (see `messages` key in the example below). The reducer function is vital to telling the graph how to update the list of `Message` objects in the state with each state update (for example, when a node sends an update). If you don't specify a reducer, every state update will overwrite the list of messages with the most recently provided value. If you wanted to simply append messages to the existing list, you could use `operator.add` as a reducer.

However, you might also want to manually update messages in your graph state (e.g. human-in-the-loop). If you were to use `operator.add`, the manual state updates you send to the graph would be appended to the existing list of messages, instead of updating existing messages. To avoid that, you need a reducer that can keep track of message IDs and overwrite existing messages, if updated. To achieve this, you can use the prebuilt [`add_messages`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.message.add_messages) function. For brand new messages, it will simply append to existing list, but it will also handle the updates for existing messages correctly.

#### Serialization

In addition to keeping track of message IDs, the [`add_messages`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.message.add_messages) function will also try to deserialize messages into LangChain `Message` objects whenever a state update is received on the `messages` channel. See more information on LangChain serialization/deserialization [here](https://python.langchain.com/docs/how_to/serialization/). This allows sending graph inputs / state updates in the following format:
```

---

## Graph state

**URL:** llms-txt#graph-state

class State(TypedDict):
    topic: str  # Report topic
    sections: list[Section]  # List of report sections
    completed_sections: Annotated[
        list, operator.add
    ]  # All workers write to this key in parallel
    final_report: str  # Final report

---

## Graph state.

**URL:** llms-txt#graph-state.

class State(TypedDict):
    """Agent state."""
    messages: Annotated[list[AnyMessage], add_messages]
    followup: str | None

invoice_id: int | None
    invoice_line_ids: list[int] | None
    customer_first_name: str | None
    customer_last_name: str | None
    customer_phone: str | None
    track_name: str | None
    album_title: str | None
    artist_name: str | None
    purchase_date_iso_8601: str | None

---

## Groq

**URL:** llms-txt#groq

**Contents:**
- Installation and Setup
- Chat models

Source: https://docs.langchain.com/oss/python/integrations/providers/groq

<Warning>
  This page makes reference to [Groq](https://console.groq.com/docs/overview), an AI hardware and software company. For information on how to use Grok models (provided by [xAI](https://docs.x.ai/docs/overview)), see the [xAI provider page](../providers/xai).
</Warning>

> [Groq](https://groq.com) developed the world's first Language Processing Unit™, or `LPU`.
> The `Groq LPU` has a deterministic, single core streaming architecture that sets the standard
> for GenAI inference speed with predictable and repeatable performance for any given workload.
>
> Beyond the architecture, `Groq` software is designed to empower developers like you with
> the tools you need to create innovative, powerful AI applications.
>
> With Groq as your engine, you can:
>
> * Achieve uncompromised low latency and performance for real-time AI and HPC inferences
> * Know the exact performance and compute time for any given workload
> * Take advantage of our cutting-edge technology to stay ahead of the competition

## Installation and Setup

Install the integration package:

Request an [API key](https://console.groq.com/login?utm_source=langchain\&utm_content=provider_page) and set it as an environment variable:

See a [usage example](/oss/python/integrations/chat/groq).

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/providers/groq.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

Request an [API key](https://console.groq.com/login?utm_source=langchain\&utm_content=provider_page) and set it as an environment variable:
```

Example 3 (unknown):
```unknown
## Chat models

See a [usage example](/oss/python/integrations/chat/groq).
```

---

## Guardrails

**URL:** llms-txt#guardrails

**Contents:**
- Built-in guardrails
  - PII detection
  - Human-in-the-loop
- Custom guardrails
  - Before agent guardrails
  - After agent guardrails
  - Combine multiple guardrails
- Additional resources

Source: https://docs.langchain.com/oss/javascript/langchain/guardrails

Implement safety checks and content filtering for your agents

Guardrails help you build safe, compliant AI applications by validating and filtering content at key points in your agent's execution. They can detect sensitive information, enforce content policies, validate outputs, and prevent unsafe behaviors before they cause problems.

Common use cases include:

* Preventing PII leakage
* Detecting and blocking prompt injection attacks
* Blocking inappropriate or harmful content
* Enforcing business rules and compliance requirements
* Validating output quality and accuracy

You can implement guardrails using [middleware](/oss/javascript/langchain/middleware) to intercept execution at strategic points - before the agent starts, after it completes, or around model and tool calls.

<div style={{ display: "flex", justifyContent: "center" }}>
  <img src="https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=eb4404b137edec6f6f0c8ccb8323eaf1" alt="Middleware flow diagram" className="rounded-lg" data-og-width="500" width="500" data-og-height="560" height="560" data-path="oss/images/middleware_final.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?w=280&fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=483413aa87cf93323b0f47c0dd5528e8 280w, https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?w=560&fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=41b7dd647447978ff776edafe5f42499 560w, https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?w=840&fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=e9b14e264f68345de08ae76f032c52d4 840w, https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?w=1100&fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=ec45e1932d1279b1beee4a4b016b473f 1100w, https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?w=1650&fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=3bca5ebf8aa56632b8a9826f7f112e57 1650w, https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?w=2500&fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=437f141d1266f08a95f030c2804691d9 2500w" />
</div>

Guardrails can be implemented using two complementary approaches:

<CardGroup cols={2}>
  <Card title="Deterministic guardrails" icon="list-check">
    Use rule-based logic like regex patterns, keyword matching, or explicit checks. Fast, predictable, and cost-effective, but may miss nuanced violations.
  </Card>

<Card title="Model-based guardrails" icon="brain">
    Use LLMs or classifiers to evaluate content with semantic understanding. Catch subtle issues that rules miss, but are slower and more expensive.
  </Card>
</CardGroup>

LangChain provides both built-in guardrails (e.g., [PII detection](#pii-detection), [human-in-the-loop](#human-in-the-loop)) and a flexible middleware system for building custom guardrails using either approach.

## Built-in guardrails

LangChain provides built-in middleware for detecting and handling Personally Identifiable Information (PII) in conversations. This middleware can detect common PII types like emails, credit cards, IP addresses, and more.

PII detection middleware is helpful for cases such as health care and financial applications with compliance requirements, customer service agents that need to sanitize logs, and generally any application handling sensitive user data.

The PII middleware supports multiple strategies for handling detected PII:

| Strategy | Description                             | Example               |
| -------- | --------------------------------------- | --------------------- |
| `redact` | Replace with `[REDACTED_TYPE]`          | `[REDACTED_EMAIL]`    |
| `mask`   | Partially obscure (e.g., last 4 digits) | `****-****-****-1234` |
| `hash`   | Replace with deterministic hash         | `a8f5f167...`         |
| `block`  | Raise exception when detected           | Error thrown          |

<Accordion title="Built-in PII types and configuration">
  **Built-in PII types:**

* `email` - Email addresses
  * `credit_card` - Credit card numbers (Luhn validated)
  * `ip` - IP addresses
  * `mac_address` - MAC addresses
  * `url` - URLs

**Configuration options:**

| Parameter            | Description                                                            | Default                     |
  | -------------------- | ---------------------------------------------------------------------- | --------------------------- |
  | `piiType`            | Type of PII to detect (built-in or custom)                             | Required                    |
  | `strategy`           | How to handle detected PII (`"block"`, `"redact"`, `"mask"`, `"hash"`) | `"redact"`                  |
  | `detector`           | Custom detector regex pattern                                          | `undefined` (uses built-in) |
  | `applyToInput`       | Check user messages before model call                                  | `true`                      |
  | `applyToOutput`      | Check AI messages after model call                                     | `false`                     |
  | `applyToToolResults` | Check tool result messages after execution                             | `false`                     |
</Accordion>

See the [middleware documentation](/oss/javascript/langchain/middleware#pii-detection) for complete details on PII detection capabilities.

### Human-in-the-loop

LangChain provides built-in middleware for requiring human approval before executing sensitive operations. This is one of the most effective guardrails for high-stakes decisions.

Human-in-the-loop middleware is helpful for cases such as financial transactions and transfers, deleting or modifying production data, sending communications to external parties, and any operation with significant business impact.

<Tip>
  See the [human-in-the-loop documentation](/oss/javascript/langchain/human-in-the-loop) for complete details on implementing approval workflows.
</Tip>

For more sophisticated guardrails, you can create custom middleware that runs before or after the agent executes. This gives you full control over validation logic, content filtering, and safety checks.

### Before agent guardrails

Use "before agent" hooks to validate requests once at the start of each invocation. This is useful for session-level checks like authentication, rate limiting, or blocking inappropriate requests before any processing begins.

### After agent guardrails

Use "after agent" hooks to validate final outputs once before returning to the user. This is useful for model-based safety checks, quality validation, or final compliance scans on the complete agent response.

### Combine multiple guardrails

You can stack multiple guardrails by adding them to the middleware array. They execute in order, allowing you to build layered protection:

## Additional resources

* [Middleware documentation](/oss/javascript/langchain/middleware) - Complete guide to custom middleware
* [Middleware API reference](https://reference.langchain.com/python/langchain/middleware/) - Complete guide to custom middleware
* [Human-in-the-loop](/oss/javascript/langchain/human-in-the-loop) - Add human review for sensitive operations
* [Testing agents](/oss/javascript/langchain/test) - Strategies for testing safety mechanisms

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/guardrails.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
<Accordion title="Built-in PII types and configuration">
  **Built-in PII types:**

  * `email` - Email addresses
  * `credit_card` - Credit card numbers (Luhn validated)
  * `ip` - IP addresses
  * `mac_address` - MAC addresses
  * `url` - URLs

  **Configuration options:**

  | Parameter            | Description                                                            | Default                     |
  | -------------------- | ---------------------------------------------------------------------- | --------------------------- |
  | `piiType`            | Type of PII to detect (built-in or custom)                             | Required                    |
  | `strategy`           | How to handle detected PII (`"block"`, `"redact"`, `"mask"`, `"hash"`) | `"redact"`                  |
  | `detector`           | Custom detector regex pattern                                          | `undefined` (uses built-in) |
  | `applyToInput`       | Check user messages before model call                                  | `true`                      |
  | `applyToOutput`      | Check AI messages after model call                                     | `false`                     |
  | `applyToToolResults` | Check tool result messages after execution                             | `false`                     |
</Accordion>

See the [middleware documentation](/oss/javascript/langchain/middleware#pii-detection) for complete details on PII detection capabilities.

### Human-in-the-loop

LangChain provides built-in middleware for requiring human approval before executing sensitive operations. This is one of the most effective guardrails for high-stakes decisions.

Human-in-the-loop middleware is helpful for cases such as financial transactions and transfers, deleting or modifying production data, sending communications to external parties, and any operation with significant business impact.
```

Example 2 (unknown):
```unknown
<Tip>
  See the [human-in-the-loop documentation](/oss/javascript/langchain/human-in-the-loop) for complete details on implementing approval workflows.
</Tip>

## Custom guardrails

For more sophisticated guardrails, you can create custom middleware that runs before or after the agent executes. This gives you full control over validation logic, content filtering, and safety checks.

### Before agent guardrails

Use "before agent" hooks to validate requests once at the start of each invocation. This is useful for session-level checks like authentication, rate limiting, or blocking inappropriate requests before any processing begins.
```

Example 3 (unknown):
```unknown
### After agent guardrails

Use "after agent" hooks to validate final outputs once before returning to the user. This is useful for model-based safety checks, quality validation, or final compliance scans on the complete agent response.
```

Example 4 (unknown):
```unknown
### Combine multiple guardrails

You can stack multiple guardrails by adding them to the middleware array. They execute in order, allowing you to build layered protection:
```

---

## Helper function to load files as bytes

**URL:** llms-txt#helper-function-to-load-files-as-bytes

def load_file(file_path: str) -> bytes:
    with open(file_path, "rb") as f:
        return f.read()

---

## Here is the user info for user with ID "abc123":

**URL:** llms-txt#here-is-the-user-info-for-user-with-id-"abc123":

---

## How to add custom lifespan events

**URL:** llms-txt#how-to-add-custom-lifespan-events

**Contents:**
- Create app

Source: https://docs.langchain.com/langsmith/custom-lifespan

When deploying agents to LangSmith, you often need to initialize resources like database connections when your server starts up, and ensure they're properly closed when it shuts down. Lifespan events let you hook into your server's startup and shutdown sequence to handle these critical setup and teardown tasks.

This works the same way as [adding custom routes](/langsmith/custom-routes). You just need to provide your own [`Starlette`](https://www.starlette.io/applications/) app (including [`FastAPI`](https://fastapi.tiangolo.com/), [`FastHTML`](https://fastht.ml/) and other compatible apps).

Below is an example using FastAPI.

<Note>
  "Python only"
  We currently only support custom lifespan events in Python deployments with `langgraph-api>=0.0.26`.
</Note>

Starting from an **existing** LangSmith application, add the following lifespan code to your `webapp.py` file. If you are starting from scratch, you can create a new app from a template using the CLI.

Once you have a LangGraph project, add the following app code:

```python {highlight={19}} theme={null}

**Examples:**

Example 1 (unknown):
```unknown
Once you have a LangGraph project, add the following app code:
```

---

## How to add custom middleware

**URL:** llms-txt#how-to-add-custom-middleware

**Contents:**
- Create app

Source: https://docs.langchain.com/langsmith/custom-middleware

When deploying agents to LangSmith, you can add custom middleware to your server to handle concerns like logging request metrics, injecting or checking headers, and enforcing security policies without modifying core server logic. This works the same way as [adding custom routes](/langsmith/custom-routes). You just need to provide your own [`Starlette`](https://www.starlette.io/applications/) app (including [`FastAPI`](https://fastapi.tiangolo.com/), [`FastHTML`](https://fastht.ml/) and other compatible apps).

Adding middleware lets you intercept and modify requests and responses globally across your deployment, whether they're hitting your custom endpoints or the built-in LangSmith APIs.

Below is an example using FastAPI.

<Note>
  "Python only"
  We currently only support custom middleware in Python deployments with `langgraph-api>=0.0.26`.
</Note>

Starting from an **existing** LangSmith application, add the following middleware code to your `webapp.py` file. If you are starting from scratch, you can create a new app from a template using the CLI.

Once you have a LangGraph project, add the following app code:

```python {highlight={5}} theme={null}

**Examples:**

Example 1 (unknown):
```unknown
Once you have a LangGraph project, add the following app code:
```

---

## How to add custom routes

**URL:** llms-txt#how-to-add-custom-routes

**Contents:**
- Create app

Source: https://docs.langchain.com/langsmith/custom-routes

When deploying agents to LangSmith Deployment, your server automatically exposes routes for creating runs and threads, interacting with the long-term memory store, managing configurable assistants, and other core functionality ([see all default API endpoints](/langsmith/server-api-ref)).

You can add custom routes by providing your own [`Starlette`](https://www.starlette.io/applications/) app (including [`FastAPI`](https://fastapi.tiangolo.com/), [`FastHTML`](https://fastht.ml/) and other compatible apps). You make LangSmith aware of this by providing a path to the app in your `langgraph.json` configuration file.

Defining a custom app object lets you add any routes you'd like, so you can do anything from adding a `/login` endpoint to writing an entire full-stack web-app, all deployed in a single Agent Server.

Below is an example using FastAPI.

Starting from an **existing** LangSmith application, add the following custom route code to your `webapp.py` file. If you are starting from scratch, you can create a new app from a template using the CLI.

Once you have a LangGraph project, add the following app code:

```python {highlight={4}} theme={null}

**Examples:**

Example 1 (unknown):
```unknown
Once you have a LangGraph project, add the following app code:
```

---

## How to add semantic search to your agent deployment

**URL:** llms-txt#how-to-add-semantic-search-to-your-agent-deployment

**Contents:**
- Prerequisites
- Steps

Source: https://docs.langchain.com/langsmith/semantic-search

This guide explains how to add semantic search to your deployment's cross-thread [store](/oss/python/langgraph/persistence#memory-store), so that your agent can search for memories and other documents by semantic similarity.

* A deployment (refer to [how to set up an application for deployment](/langsmith/setup-app-requirements-txt)) and details on [hosting options](/langsmith/platform-setup).
* API keys for your embedding provider (in this case, OpenAI).
* `langchain >= 0.3.8` (if you specify using the string format below).

1. Update your `langgraph.json` configuration file to include the store configuration:

* Uses OpenAI's text-embedding-3-small model for generating embeddings
* Sets the embedding dimension to 1536 (matching the model's output)
* Indexes all fields in your stored data (`["$"]` means index everything, or specify specific fields like `["text", "metadata.title"]`)

1. To use the string embedding format above, make sure your dependencies include `langchain >= 0.3.8`:

```toml  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
This configuration:

* Uses OpenAI's text-embedding-3-small model for generating embeddings
* Sets the embedding dimension to 1536 (matching the model's output)
* Indexes all fields in your stored data (`["$"]` means index everything, or specify specific fields like `["text", "metadata.title"]`)

1. To use the string embedding format above, make sure your dependencies include `langchain >= 0.3.8`:
```

---

## How to add TTLs to your application

**URL:** llms-txt#how-to-add-ttls-to-your-application

**Contents:**
- Configuring checkpoint TTL
- Configuring store item TTL
- Combining TTL configurations
- Runtime overrides
- Deployment process

Source: https://docs.langchain.com/langsmith/configure-ttl

<Tip>
  **Prerequisites**
  This guide assumes familiarity with [LangSmith](/langsmith/home), [Persistence](/oss/python/langgraph/persistence), and [Cross-thread persistence](/oss/python/langgraph/persistence#memory-store) concepts.
</Tip>

LangSmith persists both [checkpoints](/oss/python/langgraph/persistence#checkpoints) (thread state) and [cross-thread memories](/oss/python/langgraph/persistence#memory-store) (store items). Configure Time-to-Live (TTL) policies in `langgraph.json` to automatically manage the lifecycle of this data, preventing indefinite accumulation.

## Configuring checkpoint TTL

Checkpoints capture the state of conversation threads. Setting a TTL ensures old checkpoints and threads are automatically deleted.

Add a `checkpointer.ttl` configuration to your `langgraph.json` file:

* `strategy`: Specifies the action taken on expiration. Currently, only `"delete"` is supported, which deletes all checkpoints in the thread upon expiration.
* `sweep_interval_minutes`: Defines how often, in minutes, the system checks for expired checkpoints.
* `default_ttl`: Sets the default lifespan of threads (and corresponding checkpoints) in minutes (e.g., 43200 minutes = 30 days). Applies only to checkpoints created after this configuration is deployed; existing checkpoints/threads are not changed. To clear older data, delete it explicitly.

## Configuring store item TTL

Store items allow cross-thread data persistence. Configuring TTL for store items helps manage memory by removing stale data.

Add a `store.ttl` configuration to your `langgraph.json` file:

* `refresh_on_read`: (Optional, default `true`) If `true`, accessing an item via `get` or `search` resets its expiration timer. If `false`, TTL only refreshes on `put`.
* `sweep_interval_minutes`: (Optional) Defines how often, in minutes, the system checks for expired items. If omitted, no sweeping occurs.
* `default_ttl`: (Optional) Sets the default lifespan of store items in minutes (e.g., 10080 minutes = 7 days). Applies only to items created after this configuration is deployed; existing items are not changed. If you need to clear older items, delete them manually. If omitted, items do not expire by default.

## Combining TTL configurations

You can configure TTLs for both checkpoints and store items in the same `langgraph.json` file to set different policies for each data type. Here is an example:

The default `store.ttl` settings from `langgraph.json` can be overridden at runtime by providing specific TTL values in SDK method calls like `get`, `put`, and `search`.

## Deployment process

After configuring TTLs in `langgraph.json`, deploy or restart your LangGraph application for the changes to take effect. Use `langgraph dev` for local development or `langgraph up` for Docker deployment.

See the [langgraph.json CLI reference](https://langchain-ai.github.io/langgraph/reference/configuration/#configuration-file) for more details on the other configurable options.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/configure-ttl.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
* `strategy`: Specifies the action taken on expiration. Currently, only `"delete"` is supported, which deletes all checkpoints in the thread upon expiration.
* `sweep_interval_minutes`: Defines how often, in minutes, the system checks for expired checkpoints.
* `default_ttl`: Sets the default lifespan of threads (and corresponding checkpoints) in minutes (e.g., 43200 minutes = 30 days). Applies only to checkpoints created after this configuration is deployed; existing checkpoints/threads are not changed. To clear older data, delete it explicitly.

## Configuring store item TTL

Store items allow cross-thread data persistence. Configuring TTL for store items helps manage memory by removing stale data.

Add a `store.ttl` configuration to your `langgraph.json` file:
```

Example 2 (unknown):
```unknown
* `refresh_on_read`: (Optional, default `true`) If `true`, accessing an item via `get` or `search` resets its expiration timer. If `false`, TTL only refreshes on `put`.
* `sweep_interval_minutes`: (Optional) Defines how often, in minutes, the system checks for expired items. If omitted, no sweeping occurs.
* `default_ttl`: (Optional) Sets the default lifespan of store items in minutes (e.g., 10080 minutes = 7 days). Applies only to items created after this configuration is deployed; existing items are not changed. If you need to clear older items, delete them manually. If omitted, items do not expire by default.

## Combining TTL configurations

You can configure TTLs for both checkpoints and store items in the same `langgraph.json` file to set different policies for each data type. Here is an example:
```

---

## How to audit evaluator scores

**URL:** llms-txt#how-to-audit-evaluator-scores

**Contents:**
- In the comparison view
- In the runs table
- In the SDK

Source: https://docs.langchain.com/langsmith/audit-evaluator-scores

LLM-as-a-judge evaluators don't always get it right. Because of this, it is often useful for a human to manually audit the scores left by an evaluator and correct them where necessary. LangSmith allows you to make corrections on evaluator scores in the UI or SDK.

## In the comparison view

In the comparison view, you may click on any feedback tag to bring up the feedback details. From there, click the "edit" icon on the right to bring up the corrections view. You may then type in your desired score in the text box under "Make correction". If you would like, you may also attach an explanation to your correction. This is useful if you are using a [few-shot evaluator](/langsmith/create-few-shot-evaluators) and will be automatically inserted into your few-shot examples in place of the `few_shot_explanation` prompt variable.

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/corrections-comparison-view.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=5b815b771c18f291a9ef1b7defb9feb3" alt="Audit Evaluator Comparison View" data-og-width="3426" width="3426" data-og-height="1878" height="1878" data-path="langsmith/images/corrections-comparison-view.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/corrections-comparison-view.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=4840ceb8c340713fef6a7999c5d9c6cb 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/corrections-comparison-view.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=08b128d085701f17e20fdc6d314253a8 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/corrections-comparison-view.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=7d6300071894c9ff3f1fc80c6954c13d 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/corrections-comparison-view.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=ffe472be6a4b33d741782a1bc3269c60 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/corrections-comparison-view.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=454422187b095a4ad0ec3ad9074d4301 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/corrections-comparison-view.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=93b0585577eb98e6d76db3cba6868473 2500w" />

In the runs table, find the "Feedback" column and click on the feedback tag to bring up the feedback details. Again, click the "edit" icon on the right to bring up the corrections view.

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/corrections-runs-table.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=5e64530681ac9125751af2383b67ba35" alt="Audit Evaluator Runs Table" data-og-width="1734" width="1734" data-og-height="1002" height="1002" data-path="langsmith/images/corrections-runs-table.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/corrections-runs-table.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=46a1a8328ad238d876d3b003a7ab836a 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/corrections-runs-table.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=60183b8a46938ccfe97a694cb941e7e3 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/corrections-runs-table.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=7a2b5008a78d18d283e81eae9a8e23c0 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/corrections-runs-table.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=e58f3c26472e5e78209927d662ab72c1 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/corrections-runs-table.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=eeb374392b18fa613e43564269cd8ff8 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/corrections-runs-table.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=117673d7c311c45f0b954313a35efb32 2500w" />

Corrections can be made via the SDK's `update_feedback` function, with the `correction` dict. You must specify a `score` key which corresponds to a number for it to be rendered in the UI.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/audit-evaluator-scores.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

---

## How to compare experiment results

**URL:** llms-txt#how-to-compare-experiment-results

**Contents:**
- Open the comparison view
- Adjust the table display
- View regressions and improvements
- Update baseline experiment and metric
- Open a trace
- Expand detailed view
- View summary charts
- Use experiment metadata as chart labels

Source: https://docs.langchain.com/langsmith/compare-experiment-results

When you are iterating on your LLM application (such as changing the model or the prompt), you will want to compare the results of different [*experiments*](/langsmith/evaluation-concepts#experiment).

LangSmith supports a comparison view that lets you hone in on key differences, regressions, and improvements between different experiments.

## Open the comparison view

1. To access the experiment comaprison view, navigate to the **Datasets & Experiments** page.
2. Select a dataset, which will open the **Experiments** tab.
3. Select two or more experiments abd then click **Compare**.

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-select.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=67d4d6068012e92b101f900595734977" alt="The Experiments view in the UI with 3 experiments selected and the Compare button highlighted." data-og-width="1626" width="1626" data-og-height="966" height="966" data-path="langsmith/images/compare-select.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-select.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=8e520c4ec316531d45a9f538c3f36f78 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-select.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=0aed7d0b1cb5d70321ea536ce1decea9 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-select.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=71ed0d1faed09fe9bd845e8049327948 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-select.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=b8ccc3f4aa28630b633021b18eb64dd0 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-select.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=6e8b473e44e88e18aa743d1f75b9f6b5 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-select.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=c0784778d3b730d9e8b34400c852462b 2500w" />

## Adjust the table display

You can toggle between different views by clicking **Full** or **Compact** at the top of the **Comparing Experiments** page.

Toggling **Full** will show the full text of the input, output, and reference output for each run. If the reference output is too long to display in the table, you can click on **Expand detailed view** to view the full content.

You can also select and hide individual feedback keys or individual metrics in the **Display** settings dropdown to isolate the information you need in the comparison view.

## View regressions and improvements

In the comparison view, runs that *regressed* on your specified feedback key against your baseline experiment will be highlighted in red, while runs that *improved* will be highlighted in green. At the top of each column, you can find how many runs in that experiment did better and how many did worse than your baseline experiment.

Click on the regressions or improvements buttons on the top of each column to filter to the runs that regressed or improved in that specific experiment.

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/regression-view.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=14f3a9b65dec55c9e4a0f5688d9e8f43" alt="The comparison view comparing 2 experiments with the regressions and improvements highlighted in red and green respectively." data-og-width="1632" width="1632" data-og-height="739" height="739" data-path="langsmith/images/regression-view.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/regression-view.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=c7256046a4a6c5d28f350dd8d26bb7e3 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/regression-view.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=78c6a2cb4baa784abeb3336d5bea94c4 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/regression-view.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=9772371f026939748db446fff60fe19b 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/regression-view.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=d5639c88a1a0214a0955ffbcda23faf0 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/regression-view.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=2e4f232fbef17ff53192d8ac6db3913e 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/regression-view.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=d55c8e280732ba5d9e3866564af29de9 2500w" />

## Update baseline experiment and metric

In order to track regressions, you need to:

1. In the **Baseline** dropdown at the top of the comparison view, select a **Baseline experiment** against which to compare. By default, the newest experiment is selected as the baseline.
2. Select a  **Feedback key** (evaluation metric) you want to focus compare against. One will be assigned by default, but you can adjust as needed.
3. Configure whether a higher score is better for the selected feedback key. This preference will be stored.

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-baseline.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=3df57789664fd92aba18ea2f438934bd" alt="The Baseline dropdown highlighted with a selected experiment and feedback key of &#x22;hallucination&#x22;." data-og-width="1627" width="1627" data-og-height="898" height="898" data-path="langsmith/images/select-baseline.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-baseline.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=173b5ead77441998fc19669be3290aff 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-baseline.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=46f3bd52bad1fe8fc11ea269b37b15f9 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-baseline.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=a7cf328e494a3b674ad60f8a082d6b44 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-baseline.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=ae7ec9831c8c36855d085f48a8d8e6b3 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-baseline.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=4ef21b8740df8f02f7579239e9a9792a 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-baseline.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=c87682dbb3438f9c59a837a740d56979 2500w" />

If the example you're evaluating is from an ingested [run](/langsmith/observability-concepts#runs), you can hover over the output cell and click on the trace icon to open the trace view for that run. This will open up a trace in the side panel.

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/open-source-trace.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=b5feaa0894a645f4642c7422de937c7d" alt="The View trace icon highlighted from an ingested run." data-og-width="1632" width="1632" data-og-height="662" height="662" data-path="langsmith/images/open-source-trace.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/open-source-trace.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=49a42505f2992ab6caeab6f1c52c8e81 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/open-source-trace.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=c05cf153d430c062d42d1bf18ef73d6a 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/open-source-trace.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=734ad1f9a66a03615b259305298480f0 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/open-source-trace.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=3ad016ed31dc9c43eaa74cdffcc66a5e 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/open-source-trace.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=991be6dc57ad88ef21aaba9c90ea24fa 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/open-source-trace.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=c4110db6d5e60b4e8df840c409cb5c38 2500w" />

## Expand detailed view

From any cell, you can click on the expand icon in the hover state to open up a detailed view of all experiment results on that particular example input, along with feedback keys and scores.

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/expanded-view.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=1ff7f02d5ba6ea89902b4de0e37967e7" alt="An example in the Comparing Experiments view of a expanded view of the repetitions." data-og-width="1643" width="1643" data-og-height="926" height="926" data-path="langsmith/images/expanded-view.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/expanded-view.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=4803efab7ae4a4363145857941d50055 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/expanded-view.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=98f67feac8ad53ddb18be06955ab2895 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/expanded-view.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=628927173d27260eb7f76d82e66ff2d4 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/expanded-view.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=6de1f01362d8658d9f673e91deec77b6 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/expanded-view.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=8f68caed39717af5314beb187c3f542f 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/expanded-view.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=00fa65a0580a10973a91d506b7cdbc09 2500w" />

## View summary charts

View summary charts by clicking on the **Charts** tab at the top of the page.

<img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/charts-tab.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=ada485c324964c8df96caa566ad11b1d" alt="The Charts summary page with 8 summary charts for the comparison." data-og-width="1639" width="1639" data-og-height="1147" height="1147" data-path="langsmith/images/charts-tab.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/charts-tab.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=e21f174643b26892cae4280962653263 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/charts-tab.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=40f98d83faf7c13a05a9b410787c9a75 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/charts-tab.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=6a49ddad4c2c02bc14a0d765dc7dc261 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/charts-tab.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=8aec566c2fd07b7d6e12a3e639cf6660 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/charts-tab.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=716edd3ad3626831a88c6db07f0fa566 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/charts-tab.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=08e8ea763e825413a3aff09c061593ca 2500w" />

## Use experiment metadata as chart labels

You can configure the x-axis labels for the charts based on [experiment metadata](/langsmith/filter-experiments-ui#background-add-metadata-to-your-experiments).

Select a metadata key in the **x-axis** dropdown to change the chart labels.

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/metadata-in-charts.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=cb175478369f9a7a2314e44f6becc9e4" alt="x-axis dropdown highlighted with a list of the metadata attached to the experiment." data-og-width="1637" width="1637" data-og-height="1141" height="1141" data-path="langsmith/images/metadata-in-charts.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/metadata-in-charts.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=b34f45a5f0736b6b83a73a00f94212f6 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/metadata-in-charts.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=7084f0126711a63d6f539904f4e9091f 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/metadata-in-charts.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=dee53003f1c958d09f839d24cb54eb63 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/metadata-in-charts.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=7a0323fef549e2aef3fcf22295d40d19 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/metadata-in-charts.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=81c37649a07ffa594bad4fd57cc8fb32 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/metadata-in-charts.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=70d700760e7ba9dd0edce54429d11f95 2500w" />

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/compare-experiment-results.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## How to create and manage datasets programmatically

**URL:** llms-txt#how-to-create-and-manage-datasets-programmatically

**Contents:**
- Create a dataset
  - Create a dataset from list of values
  - Create a dataset from traces
  - Create a dataset from a CSV file
  - Create a dataset from pandas DataFrame (Python only)
- Fetch datasets
  - Query all datasets
  - List datasets by name
  - List datasets by type
- Fetch examples

Source: https://docs.langchain.com/langsmith/manage-datasets-programmatically

You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them.

### Create a dataset from list of values

The most flexible way to make a dataset using the client is by creating examples from a list of inputs and optional outputs. Below is an example.

Note that you can add arbitrary metadata to each example, such as a note or a source. The metadata is stored as a dictionary.

<Check>
  If you have many examples to create, consider using the `create_examples`/`createExamples` method to create multiple examples in a single request. If creating a single example, you can use the `create_example`/`createExample` method.
</Check>

### Create a dataset from traces

To create datasets from the runs (spans) of your traces, you can use the same approach. For **many** more examples of how to fetch and filter runs, see the [export traces](/langsmith/export-traces) guide. Below is an example:

### Create a dataset from a CSV file

In this section, we will demonstrate how you can create a dataset by uploading a CSV file.

First, ensure your CSV file is properly formatted with columns that represent your input and output keys. These keys will be utilized to map your data properly during the upload. You can specify an optional name and description for your dataset. Otherwise, the file name will be used as the dataset name and no description will be provided.

### Create a dataset from pandas DataFrame (Python only)

The python client offers an additional convenience method to upload a dataset from a pandas dataframe.

You can programmatically fetch datasets from LangSmith using the `list_datasets`/`listDatasets` method in the Python and TypeScript SDKs. Below are some common calls.

<Info>
  Initialize the client before running the below code snippets.
</Info>

### Query all datasets

### List datasets by name

If you want to search by the exact name, you can do the following:

If you want to do a case-invariant substring search, try the following:

### List datasets by type

You can filter datasets by type. Below is an example querying for chat datasets.

You can programmatically fetch examples from LangSmith using the `list_examples`/`listExamples` method in the Python and TypeScript SDKs. Below are some common calls.

<Info>
  Initialize the client before running the below code snippets.
</Info>

### List all examples for a dataset

You can filter by dataset ID:

Or you can filter by dataset name (this must exactly match the dataset name you want to query)

### List examples by id

You can also list multiple examples all by ID.

### List examples by metadata

You can also filter examples by metadata. Below is an example querying for examples with a specific metadata key-value pair. Under the hood, we check to see if the example's metadata contains the key-value pair(s) you specify.

For example, if you have an example with metadata `{"foo": "bar", "baz": "qux"}`, both `{foo: bar}` and `{baz: qux}` would match, as would `{foo: bar, baz: qux}`.

### List examples by structured filter

Similar to how you can use the structured filter query language to [fetch runs](/langsmith/export-traces#use-filter-query-language), you can use it to fetch examples.

<Note>
  This is currently only available in v0.1.83 and later of the Python SDK and v0.1.35 and later of the TypeScript SDK.

Additionally, the structured filter query language is only supported for `metadata` fields.
</Note>

You can use the `has` operator to fetch examples with metadata fields that contain specific key/value pairs and the `exists` operator to fetch examples with metadata fields that contain a specific key. Additionally, you can also chain multiple filters together using the `and` operator and negate a filter using the `not` operator.

### Update single example

You can programmatically update examples from LangSmith using the `update_example`/`updateExample` method in the Python and TypeScript SDKs. Below is an example.

### Bulk update examples

You can also programmatically update multiple examples in a single request with the `update_examples`/`updateExamples` method in the Python and TypeScript SDKs. Below is an example.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/manage-datasets-programmatically.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

### Create a dataset from traces

To create datasets from the runs (spans) of your traces, you can use the same approach. For **many** more examples of how to fetch and filter runs, see the [export traces](/langsmith/export-traces) guide. Below is an example:

<CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>

### Create a dataset from a CSV file

In this section, we will demonstrate how you can create a dataset by uploading a CSV file.

First, ensure your CSV file is properly formatted with columns that represent your input and output keys. These keys will be utilized to map your data properly during the upload. You can specify an optional name and description for your dataset. Otherwise, the file name will be used as the dataset name and no description will be provided.

<CodeGroup>
```

---

## How to customize the Dockerfile

**URL:** llms-txt#how-to-customize-the-dockerfile

Source: https://docs.langchain.com/langsmith/custom-docker

Users can add an array of additional lines to add to the Dockerfile following the import from the parent LangGraph image. In order to do this, you simply need to modify your `langgraph.json` file by passing in the commands you want run to the `dockerfile_lines` key. For example, if we wanted to use `Pillow` in our graph you would need to add the following dependencies:

This would install the system packages required to use Pillow if we were working with `jpeg` or `png` image formats.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/custom-docker.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
{
    "dependencies": ["."],
    "graphs": {
        "openai_agent": "./openai_agent.py:agent",
    },
    "env": "./.env",
    "dockerfile_lines": [
        "RUN apt-get update && apt-get install -y libjpeg-dev zlib1g-dev libpng-dev",
        "RUN pip install Pillow"
    ]
}
```

---

## How to define an LLM-as-a-judge evaluator

**URL:** llms-txt#how-to-define-an-llm-as-a-judge-evaluator

**Contents:**
- SDK
  - Pre-built evaluators
  - Create your own LLM-as-a-judge evaluator

Source: https://docs.langchain.com/langsmith/llm-as-judge

<Info>
  * [LLM-as-a-judge evaluator](/langsmith/evaluation-concepts#llm-as-judge)
</Info>

LLM applications can be challenging to evaluate since they often generate conversational text with no single correct answer.

This guide shows you how to define an LLM-as-a-judge evaluator for [offline evaluation](/langsmith/evaluation-concepts#offline-evaluation) using either the LangSmith SDK or the UI. Note: To run evaluations in real-time on your production traces, refer to [setting up online evaluations](/langsmith/online-evaluations#configure-llm-as-judge-evaluators).

### Pre-built evaluators

Pre-built evaluators are a useful starting point for setting up evaluations. Refer to [pre-built evaluators](/langsmith/prebuilt-evaluators) for how to use pre-built evaluators with LangSmith.

### Create your own LLM-as-a-judge evaluator

For complete control of evaluator logic, create your own LLM-as-a-judge evaluator and run it using the LangSmith SDK ([Python](https://docs.smith.langchain.com/reference/python/reference) / [TypeScript](https://docs.smith.langchain.com/reference/js)).

Requires `langsmith>=0.2.0`

```python  theme={null}
from langsmith import evaluate, traceable, wrappers, Client
from openai import OpenAI

---

## How to define a code evaluator

**URL:** llms-txt#how-to-define-a-code-evaluator

**Contents:**
- Basic example
- Evaluator args
- Evaluator output
- Additional examples
- Related

Source: https://docs.langchain.com/langsmith/code-evaluator

<Info>
  * [Evaluators](/langsmith/evaluation-concepts#evaluators)
</Info>

Code evaluators are just functions that take a dataset example and the resulting application output, and return one or more metrics. These functions can be passed directly into [evaluate()](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._runner.evaluate) / [aevaluate()](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._arunner.aevaluate).

code evaluator functions must have specific argument names. They can take any subset of the following arguments:

* `run: Run`: The full [Run](/langsmith/run-data-format) object generated by the application on the given example.
* `example: Example`: The full dataset [Example](/langsmith/example-data-format), including the example inputs, outputs (if available), and metdata (if available).
* `inputs: dict`: A dictionary of the inputs corresponding to a single example in a dataset.
* `outputs: dict`: A dictionary of the outputs generated by the application on the given `inputs`.
* `reference_outputs/referenceOutputs: dict`: A dictionary of the reference outputs associated with the example, if available.

For most use cases you'll only need `inputs`, `outputs`, and `reference_outputs`. `run` and `example` are useful only if you need some extra trace or example metadata outside of the actual inputs and outputs of the application.

When using JS/TS these should all be passed in as part of a single object argument.

Code evaluators are expected to return one of the following types:

* `dict`: dicts of the form `{"score" | "value": ..., "key": ...}` allow you to customize the metric type ("score" for numerical and "value" for categorical) and metric name. This if useful if, for example, you want to log an integer as a categorical metric.

* `int | float | bool`: this is interepreted as an continuous metric that can be averaged, sorted, etc. The function name is used as the name of the metric.
* `str`: this is intepreted as a categorical metric. The function name is used as the name of the metric.
* `list[dict]`: return multiple metrics using a single function.

## Additional examples

Requires `langsmith>=0.2.0`

* [Evaluate aggregate experiment results](/langsmith/summary): Define summary evaluators, which compute metrics for an entire experiment.
* [Run an evaluation comparing two experiments](/langsmith/evaluate-pairwise): Define pairwise evaluators, which compute metrics by comparing two (or more) experiments against each other.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/code-evaluator.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

## Evaluator args

code evaluator functions must have specific argument names. They can take any subset of the following arguments:

* `run: Run`: The full [Run](/langsmith/run-data-format) object generated by the application on the given example.
* `example: Example`: The full dataset [Example](/langsmith/example-data-format), including the example inputs, outputs (if available), and metdata (if available).
* `inputs: dict`: A dictionary of the inputs corresponding to a single example in a dataset.
* `outputs: dict`: A dictionary of the outputs generated by the application on the given `inputs`.
* `reference_outputs/referenceOutputs: dict`: A dictionary of the reference outputs associated with the example, if available.

For most use cases you'll only need `inputs`, `outputs`, and `reference_outputs`. `run` and `example` are useful only if you need some extra trace or example metadata outside of the actual inputs and outputs of the application.

When using JS/TS these should all be passed in as part of a single object argument.

## Evaluator output

Code evaluators are expected to return one of the following types:

Python and JS/TS

* `dict`: dicts of the form `{"score" | "value": ..., "key": ...}` allow you to customize the metric type ("score" for numerical and "value" for categorical) and metric name. This if useful if, for example, you want to log an integer as a categorical metric.

Python only

* `int | float | bool`: this is interepreted as an continuous metric that can be averaged, sorted, etc. The function name is used as the name of the metric.
* `str`: this is intepreted as a categorical metric. The function name is used as the name of the metric.
* `list[dict]`: return multiple metrics using a single function.

## Additional examples

Requires `langsmith>=0.2.0`

<CodeGroup>
```

Example 3 (unknown):
```unknown

```

---

## How to define a summary evaluator

**URL:** llms-txt#how-to-define-a-summary-evaluator

**Contents:**
- Basic example
- Summary evaluator args
- Summary evaluator output

Source: https://docs.langchain.com/langsmith/summary

Some metrics can only be defined on the entire experiment level as opposed to the individual runs of the experiment. For example, you may want to compute the overall pass rate or f1 score of your evaluation target across all examples in the dataset. These are called summary evaluators.

Here, we'll compute the f1-score, which is a combination of precision and recall.

This sort of metric can only be computed over all of the examples in our experiment, so our evaluator takes in a list of outputs, and a list of reference\_outputs.

You can then pass this evaluator to the `evaluate` method as follows:

In the LangSmith UI, you'll the summary evaluator's score displayed with the corresponding key.

<img src="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/summary-eval.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=d0f259baa7d7467bf172ef8197c3bb17" alt="summary_eval.png" data-og-width="1535" width="1535" data-og-height="122" height="122" data-path="langsmith/images/summary-eval.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/summary-eval.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=076d830ea3952a4a598d25a2830232e0 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/summary-eval.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=a5f96c7cf258a92be14f489bc1a05f8c 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/summary-eval.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=4a2a77c6e0ae855a7027888591733e13 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/summary-eval.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=62eee3f7f104ae12e97ba22828a8bb2c 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/summary-eval.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=48d6045a20e1021aceadc98554e39e9e 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/summary-eval.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=d72ea1cfc84b0cb33461eb36ea47c696 2500w" />

## Summary evaluator args

Summary evaluator functions must have specific argument names. They can take any subset of the following arguments:

* `inputs: list[dict]`: A list of the inputs corresponding to a single example in a dataset.
* `outputs: list[dict]`: A list of the dict outputs produced by each experiment on the given inputs.
* `reference_outputs/referenceOutputs: list[dict]`: A list of the reference outputs associated with the example, if available.
* `runs: list[Run]`: A list of the full [Run](/langsmith/run-data-format) objects generated by the two experiments on the given example. Use this if you need access to intermediate steps or metadata about each run.
* `examples: list[Example]`: All of the dataset [Example](/langsmith/example-data-format) objects, including the example inputs, outputs (if available), and metdata (if available).

## Summary evaluator output

Summary evaluators are expected to return one of the following types:

* `dict`: dicts of the form `{"score": ..., "name": ...}` allow you to pass a numeric or boolean score and metric name.

Currently Python only

* `int | float | bool`: this is interepreted as an continuous metric that can be averaged, sorted, etc. The function name is used as the name of the metric.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/summary.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

You can then pass this evaluator to the `evaluate` method as follows:

<CodeGroup>
```

Example 3 (unknown):
```unknown

```

---

## How to define a target function to evaluate

**URL:** llms-txt#how-to-define-a-target-function-to-evaluate

**Contents:**
- Target function signature

Source: https://docs.langchain.com/langsmith/define-target-function

There are three main pieces need to run an evaluation:

1. A [dataset](/langsmith/evaluation-concepts#datasets) of test inputs and expected outputs.
2. A target function which is what you're evaluating.
3. [Evaluators](/langsmith/evaluation-concepts#evaluators) that score your target function's outputs.

This guide shows you how to define the target function depending on the part of your application you are evaluating. See here for [how to create a dataset](/langsmith/manage-datasets-programmatically) and [how to define evaluators](/langsmith/code-evaluator), and here for an [end-to-end example of running an evaluation](/langsmith/evaluate-llm-application).

## Target function signature

In order to evaluate an application in code, we need a way to run the application. When using `evaluate()` ([Python](https://docs.smith.langchain.com/reference/python/client/langsmith.client.Client#langsmith.client.Client.evaluate)/[TypeScript](https://docs.smith.langchain.com/reference/js/functions/evaluation.evaluate))we'll do this by passing in a *target function* argument. This is a function that takes in a dataset [Example's](/langsmith/evaluation-concepts#examples) inputs and returns the application output as a dict. Within this function we can call our application however we'd like. We can also format the output however we'd like. The key is that any evaluator functions we define should work with the output format we return in our target function.

```python  theme={null}
from langsmith import Client

---

## How to evaluate an application's intermediate steps

**URL:** llms-txt#how-to-evaluate-an-application's-intermediate-steps

**Contents:**
- 1. Define your LLM pipeline
- 2. Create a dataset and examples to evaluate the pipeline
- 3. Define your custom evaluators
- 4. Evaluate the pipeline
- Related

Source: https://docs.langchain.com/langsmith/evaluate-on-intermediate-steps

While, in many scenarios, it is sufficient to evaluate the final output of your task, in some cases you might want to evaluate the intermediate steps of your pipeline.

For example, for retrieval-augmented generation (RAG), you might want to

1. Evaluate the retrieval step to ensure that the correct documents are retrieved w\.r.t the input query.
2. Evaluate the generation step to ensure that the correct answer is generated w\.r.t the retrieved documents.

In this guide, we will use a simple, fully-custom evaluator for evaluating criteria 1 and an LLM-based evaluator for evaluating criteria 2 to highlight both scenarios.

In order to evaluate the intermediate steps of your pipeline, your evaluator function should traverse and process the `run`/`rootRun` argument, which is a `Run` object that contains the intermediate steps of your pipeline.

## 1. Define your LLM pipeline

The below RAG pipeline consists of 1) generating a Wikipedia query given the input question, 2) retrieving relevant documents from Wikipedia, and 3) generating an answer given the retrieved documents.

Requires `langsmith>=0.3.13`

This pipeline will produce a trace that looks something like: <img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluation-intermediate-trace.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=3b691ca56f9d60035dcba2c248692fa1" alt="evaluation_intermediate_trace.png" data-og-width="2586" width="2586" data-og-height="1676" height="1676" data-path="langsmith/images/evaluation-intermediate-trace.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluation-intermediate-trace.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=23a9b9abdb3e43e0f6326f0d4293ab7d 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluation-intermediate-trace.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=4b771691bd1afffe9f371a105f7eaebe 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluation-intermediate-trace.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=beb01776de9a5fa663c82d4380bc78cd 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluation-intermediate-trace.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=ec84bd3345df3d2cef38878b902c355b 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluation-intermediate-trace.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=043a7f22da6b158e070c853d67bacd69 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluation-intermediate-trace.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=42a9ea799157fee30a6b243c02615a02 2500w" />

## 2. Create a dataset and examples to evaluate the pipeline

We are building a very simple dataset with a couple of examples to evaluate the pipeline.

Requires `langsmith>=0.3.13`

## 3. Define your custom evaluators

As mentioned above, we will define two evaluators: one that evaluates the relevance of the retrieved documents w\.r.t the input query and another that evaluates the hallucination of the generated answer w\.r.t the retrieved documents. We will be using LangChain LLM wrappers, along with [`with_structured_output`](https://python.langchain.com/v0.1/docs/modules/model_io/chat/structured_output/) to define the evaluator for hallucination.

The key here is that the evaluator function should traverse the `run` / `rootRun` argument to access the intermediate steps of the pipeline. The evaluator can then process the inputs and outputs of the intermediate steps to evaluate according to the desired criteria.

Example uses `langchain` for convenience, this is not required.

## 4. Evaluate the pipeline

Finally, we'll run `evaluate` with the custom evaluators defined above.

The experiment will contain the results of the evaluation, including the scores and comments from the evaluators: <img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluation-intermediate-experiment.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=e926744573c6b9757ba22ff245a3da2c" alt="evaluation_intermediate_experiment.png" data-og-width="2446" width="2446" data-og-height="1244" height="1244" data-path="langsmith/images/evaluation-intermediate-experiment.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluation-intermediate-experiment.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=7b6e321b15a06b2adc7f1cacb8e07a35 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluation-intermediate-experiment.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=c677007bcc1e2af4b3767d6b44fcb327 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluation-intermediate-experiment.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=a39153399b6721b7c51693f5a59cf2b0 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluation-intermediate-experiment.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=1132228eba6761a724ae98d85fcf536c 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluation-intermediate-experiment.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=5d74785384737df0cf67145b397b1934 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluation-intermediate-experiment.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=bef00e4bdc12289d9f1e4b77ed8489cf 2500w" />

* [Evaluate a `langgraph` graph](/langsmith/evaluate-on-intermediate-steps)

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/evaluate-on-intermediate-steps.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

Requires `langsmith>=0.3.13`

<CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>

This pipeline will produce a trace that looks something like: <img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluation-intermediate-trace.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=3b691ca56f9d60035dcba2c248692fa1" alt="evaluation_intermediate_trace.png" data-og-width="2586" width="2586" data-og-height="1676" height="1676" data-path="langsmith/images/evaluation-intermediate-trace.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluation-intermediate-trace.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=23a9b9abdb3e43e0f6326f0d4293ab7d 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluation-intermediate-trace.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=4b771691bd1afffe9f371a105f7eaebe 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluation-intermediate-trace.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=beb01776de9a5fa663c82d4380bc78cd 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluation-intermediate-trace.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=ec84bd3345df3d2cef38878b902c355b 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluation-intermediate-trace.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=043a7f22da6b158e070c853d67bacd69 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluation-intermediate-trace.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=42a9ea799157fee30a6b243c02615a02 2500w" />

## 2. Create a dataset and examples to evaluate the pipeline

We are building a very simple dataset with a couple of examples to evaluate the pipeline.

Requires `langsmith>=0.3.13`

<CodeGroup>
```

---

## How to evaluate an existing experiment (Python only)

**URL:** llms-txt#how-to-evaluate-an-existing-experiment-(python-only)

Source: https://docs.langchain.com/langsmith/evaluate-existing-experiment

Evaluation of existing experiments is currently only supported in the Python SDK.

If you have already run an experiment and want to add additional evaluation metrics, you can apply any evaluators to the experiment using the `evaluate()` / `aevaluate()` methods as before. Just pass in the experiment name / ID instead of a target function:

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/evaluate-existing-experiment.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## How to evaluate an LLM application

**URL:** llms-txt#how-to-evaluate-an-llm-application

**Contents:**
- Define an application
- Create or select a dataset
- Define an evaluator
- Run the evaluation
- Explore the results[​](#explore-the-results "Direct link to Explore the results")
- Reference code[​](#reference-code "Direct link to Reference code")
- Related[​](#related "Direct link to Related")

Source: https://docs.langchain.com/langsmith/evaluate-llm-application

This guide shows you how to run an evaluation on an LLM application using the LangSmith SDK.

<Info>
  [Evaluations](/langsmith/evaluation-concepts#applying-evaluations) | [Evaluators](/langsmith/evaluation-concepts#evaluators) | [Datasets](/langsmith/evaluation-concepts#datasets)
</Info>

In this guide we'll go over how to evaluate an application using the [evaluate()](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._runner.evaluate) method in the LangSmith SDK.

<Check>
  For larger evaluation jobs in Python we recommend using [aevaluate()](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._arunner.aevaluate), the asynchronous version of [evaluate()](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._runner.evaluate). It is still worthwhile to read this guide first, as the two have identical interfaces, before reading the how-to guide on [running an evaluation asynchronously](/langsmith/evaluation-async).

In JS/TS evaluate() is already asynchronous so no separate method is needed.

It is also important to configure the `max_concurrency`/`maxConcurrency` arg when running large jobs. This parallelizes evaluation by effectively splitting the dataset across threads.
</Check>

## Define an application

First we need an application to evaluate. Let's create a simple toxicity classifier for this example.

We've optionally enabled tracing to capture the inputs and outputs of each step in the pipeline. To understand how to annotate your code for tracing, please refer to [this guide](/langsmith/annotate-code).

## Create or select a dataset

We need a [Dataset](/langsmith/evaluation-concepts#datasets) to evaluate our application on. Our dataset will contain labeled [examples](/langsmith/evaluation-concepts#examples) of toxic and non-toxic text.

Requires `langsmith>=0.3.13`

For more details on datasets, refer to the [Manage datasets](/langsmith/manage-datasets) page.

## Define an evaluator

<Check>
  You can also check out LangChain's open source evaluation package [openevals](https://github.com/langchain-ai/openevals) for common pre-built evaluators.
</Check>

[Evaluators](/langsmith/evaluation-concepts#evaluators) are functions for scoring your application's outputs. They take in the example inputs, actual outputs, and, when present, the reference outputs. Since we have labels for this task, our evaluator can directly check if the actual outputs match the reference outputs.

* Python: Requires `langsmith>=0.3.13`
* TypeScript: Requires `langsmith>=0.2.9`

## Run the evaluation

We'll use the [evaluate()](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._runner.evaluate) / [aevaluate()](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._arunner.aevaluate) methods to run the evaluation.

The key arguments are:

* a target function that takes an input dictionary and returns an output dictionary. The `example.inputs` field of each [Example](/langsmith/example-data-format) is what gets passed to the target function. In this case our `toxicity_classifier` is already set up to take in example inputs so we can use it directly.
* `data` - the name OR UUID of the LangSmith dataset to evaluate on, or an iterator of examples
* `evaluators` - a list of evaluators to score the outputs of the function

Python: Requires `langsmith>=0.3.13`

## Explore the results[​](#explore-the-results "Direct link to Explore the results")

Each invocation of `evaluate()` creates an [Experiment](/langsmith/evaluation-concepts#experiments) which can be viewed in the LangSmith UI or queried via the SDK. Evaluation scores are stored against each actual output as feedback.

*If you've annotated your code for tracing, you can open the trace of each row in a side panel view.*

<img src="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-experiment.gif?s=252d96dbd2100a691f1d3b61716fde38" alt="" data-og-width="1132" width="1132" data-og-height="720" height="720" data-path="langsmith/images/view-experiment.gif" data-optimize="true" data-opv="3" />

## Reference code[​](#reference-code "Direct link to Reference code")

<Accordion title="Click to see a consolidated code snippet">
  <CodeGroup>

</CodeGroup>
</Accordion>

## Related[​](#related "Direct link to Related")

* [Run an evaluation asynchronously](/langsmith/evaluation-async)
* [Run an evaluation via the REST API](/langsmith/run-evals-api-only)
* [Run an evaluation from the prompt playground](/langsmith/run-evaluation-from-prompt-playground)

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/evaluate-llm-application.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

We've optionally enabled tracing to capture the inputs and outputs of each step in the pipeline. To understand how to annotate your code for tracing, please refer to [this guide](/langsmith/annotate-code).

## Create or select a dataset

We need a [Dataset](/langsmith/evaluation-concepts#datasets) to evaluate our application on. Our dataset will contain labeled [examples](/langsmith/evaluation-concepts#examples) of toxic and non-toxic text.

Requires `langsmith>=0.3.13`

<CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>

For more details on datasets, refer to the [Manage datasets](/langsmith/manage-datasets) page.

## Define an evaluator

<Check>
  You can also check out LangChain's open source evaluation package [openevals](https://github.com/langchain-ai/openevals) for common pre-built evaluators.
</Check>

[Evaluators](/langsmith/evaluation-concepts#evaluators) are functions for scoring your application's outputs. They take in the example inputs, actual outputs, and, when present, the reference outputs. Since we have labels for this task, our evaluator can directly check if the actual outputs match the reference outputs.

* Python: Requires `langsmith>=0.3.13`
* TypeScript: Requires `langsmith>=0.2.9`

<CodeGroup>
```

---

## How to evaluate a graph

**URL:** llms-txt#how-to-evaluate-a-graph

**Contents:**
- End-to-end evaluations
  - Define a graph

Source: https://docs.langchain.com/langsmith/evaluate-graph

<Info>
  [langgraph](https://langchain-ai.github.io/langgraph/)
</Info>

`langgraph` is a library for building stateful, multi-actor applications with LLMs, used to create agent and multi-agent workflows. Evaluating `langgraph` graphs can be challenging because a single invocation can involve many LLM calls, and which LLM calls are made may depend on the outputs of preceding calls. In this guide we will focus on the mechanics of how to pass graphs and graph nodes to `evaluate()` / `aevaluate()`. For evaluation techniques and best practices when building agents head to the [langgraph docs](https://langchain-ai.github.io/langgraph/tutorials/#evaluation).

## End-to-end evaluations

The most common type of evaluation is an end-to-end one, where we want to evaluate the final graph output for each example input.

Lets construct a simple ReACT agent to start:

```python  theme={null}
from typing import Annotated, Literal, TypedDict
from langchain.chat_models import init_chat_model
from langchain.tools import tool
from langgraph.prebuilt import ToolNode
from langgraph.graph import END, START, StateGraph
from langgraph.graph.message import add_messages

class State(TypedDict):
    # Messages have the type "list". The 'add_messages' function
    # in the annotation defines how this state key should be updated
    # (in this case, it appends messages to the list, rather than overwriting them)
    messages: Annotated[list, add_messages]

---

## How to evaluate a runnable

**URL:** llms-txt#how-to-evaluate-a-runnable

**Contents:**
- Setup
- Evaluate
- Related

Source: https://docs.langchain.com/langsmith/langchain-runnable

<Info>
  * `langchain`: [Python](https://python.langchain.com) and [JS/TS](https://js.langchain.com)
  * Runnable: [Python](https://python.langchain.com/docs/concepts/runnables/) and [JS/TS](https://js.langchain.com/docs/concepts/runnables/)
</Info>

`langchain` [Runnable](https://python.langchain.com/docs/concepts/runnables/) objects (such as chat models, retrievers, chains, etc.) can be passed directly into `evaluate()` / `aevaluate()`.

Let's define a simple chain to evaluate. First, install all the required packages:

To evaluate our chain we can pass it directly to the `evaluate()` / `aevaluate()` method. Note that the input variables of the chain must match the keys of the example inputs. In this case, the example inputs should have the form `{"text": "..."}`.

The runnable is traced appropriately for each output.

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/runnable-eval.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=b9dac41dafb9a1cbb3b90fc508f212f7" alt="Runnable Evaluation" data-og-width="2288" width="2288" data-og-height="1052" height="1052" data-path="langsmith/images/runnable-eval.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/runnable-eval.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=39f7bda57df5d29c72729390065342c2 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/runnable-eval.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=2bbfa58f877541adff85056d2d4910c7 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/runnable-eval.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=198967ebb494d0577fac294f879f348c 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/runnable-eval.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=dd0758a55517d6899d445bd203bc7d03 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/runnable-eval.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=6fa8f6a044a0b978ef727390f18f5ce3 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/runnable-eval.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=40dad8febfdaf0756c90b6326e2c4415 2500w" />

* [How to evaluate a `langgraph` graph](/langsmith/evaluate-on-intermediate-steps)

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/langchain-runnable.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

Now define a chain:

<CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>

## Evaluate

To evaluate our chain we can pass it directly to the `evaluate()` / `aevaluate()` method. Note that the input variables of the chain must match the keys of the example inputs. In this case, the example inputs should have the form `{"text": "..."}`.

<CodeGroup>
```

---

## How to evaluate with repetitions

**URL:** llms-txt#how-to-evaluate-with-repetitions

**Contents:**
- Configuring repetitions on an experiment
- Viewing results of experiments run with repetitions

Source: https://docs.langchain.com/langsmith/repetition

Running multiple repetitions can give a more accurate estimate of the performance of your system since LLM outputs are not deterministic. Outputs can differ from one repetition to the next. Repetitions are a way to reduce noise in systems prone to high variability, such as agents.

## Configuring repetitions on an experiment

Add the optional `num_repetitions` param to the `evaluate` / `aevaluate` function ([Python](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._runner.evaluate), [TypeScript](https://docs.smith.langchain.com/reference/js/interfaces/evaluation.EvaluateOptions#numrepetitions)) to specify how many times to evaluate over each example in your dataset. For instance, if you have 5 examples in the dataset and set `num_repetitions=5`, each example will be run 5 times, for a total of 25 runs.

## Viewing results of experiments run with repetitions

If you've run your experiment with [repetitions](/langsmith/evaluation-concepts#repetitions), there will be arrows in the output results column so you can view outputs in the table. To view each run from the repetition, hover over the output cell and click the expanded view. When you run an experiment with repetitions, LangSmith displays the average for each feedback score in the table. Click on the feedback score to view the feedback scores from individual runs, or to view the standard deviation across repetitions.

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/repetitions.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=60962de04e5533d7718ca60fa9c7dcce" alt="Repetitions" data-og-width="1636" width="1636" data-og-height="959" height="959" data-path="langsmith/images/repetitions.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/repetitions.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=8be83801a53f2544883faf173bc16ef1 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/repetitions.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=7a924559be193efcc2c77dba3fea1231 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/repetitions.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=25cbd580d06bda48419b83401c268c2d 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/repetitions.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=9da3908c81d1c8fd44dde6d3ec7dfe1d 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/repetitions.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=775af0be371e662bea7ba7e29c2f21fd 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/repetitions.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=4d593460688be852a64638f092cba9f3 2500w" />

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/repetition.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

---

## How to evaluate your agent with trajectory evaluations

**URL:** llms-txt#how-to-evaluate-your-agent-with-trajectory-evaluations

**Contents:**
- Installing AgentEvals
- Trajectory match evaluator
  - Strict match
  - Unordered match
  - Subset and superset match
- LLM-as-judge evaluator
  - Without reference trajectory
  - With reference trajectory
- Async support (Python)

Source: https://docs.langchain.com/langsmith/trajectory-evals

Many agent behaviors only emerge when using a real LLM, such as which tool the agent decides to call, how it formats responses, or whether a prompt modification affects the entire execution trajectory. LangChain's [`agentevals`](https://github.com/langchain-ai/agentevals) package provides evaluators specifically designed for testing agent trajectories with live models.

<Note>
  This guide covers the open source [LangChain](/oss/python/langchain/overview) `agentevals` package, which integrates with LangSmith for trajectory evaluation.
</Note>

AgentEvals allows you to evaluate the trajectory of your agent (the exact sequence of messages, including tool calls) by performing a *trajectory match* or by using an *LLM judge*:

<Card title="Trajectory match" icon="equals" arrow="true" href="#trajectory-match-evaluator">
  Hard-code a reference trajectory for a given input and validate the run via a step-by-step comparison.

Ideal for testing well-defined workflows where you know the expected behavior. Use when you have specific expectations about which tools should be called and in what order. This approach is deterministic, fast, and cost-effective since it doesn't require additional LLM calls.
</Card>

<Card title="LLM-as-judge" icon="gavel" arrow="true" href="#llm-as-judge-evaluator">
  Use a LLM to qualitatively validate your agent's execution trajectory. The "judge" LLM reviews the agent's decisions against a prompt rubric (which can include a reference trajectory).

More flexible and can assess nuanced aspects like efficiency and appropriateness, but requires an LLM call and is less deterministic. Use when you want to evaluate the overall quality and reasonableness of the agent's trajectory without strict tool call or ordering requirements.
</Card>

## Installing AgentEvals

Or, clone the [AgentEvals repository](https://github.com/langchain-ai/agentevals) directly.

## Trajectory match evaluator

AgentEvals offers the `create_trajectory_match_evaluator` function in Python and `createTrajectoryMatchEvaluator` in TypeScript to match your agent's trajectory against a reference trajectory.

You can use the following modes:

| Mode                                     | Description                                               | Use Case                                                              |
| ---------------------------------------- | --------------------------------------------------------- | --------------------------------------------------------------------- |
| [`strict`](#strict-match)                | Exact match of messages and tool calls in the same order  | Testing specific sequences (e.g., policy lookup before authorization) |
| [`unordered`](#unordered-match)          | Same tool calls allowed in any order                      | Verifying information retrieval when order doesn't matter             |
| [`subset`](#subset-and-superset-match)   | Agent calls only tools from reference (no extras)         | Ensuring agent doesn't exceed expected scope                          |
| [`superset`](#subset-and-superset-match) | Agent calls at least the reference tools (extras allowed) | Verifying minimum required actions are taken                          |

The `strict` mode ensures trajectories contain identical messages in the same order with the same tool calls, though it allows for differences in message content. This is useful when you need to enforce a specific sequence of operations, such as requiring a policy lookup before authorizing an action.

The `unordered` mode allows the same tool calls in any order, which is helpful when you want to verify that the correct set of tools are being invoked but don't care about the sequence. For example, an agent might need to check both weather and events for a city, but the order doesn't matter.

### Subset and superset match

The `superset` and `subset` modes focus on which tools are called rather than the order of tool calls, allowing you to control how strictly the agent's tool calls must align with the reference.

* Use `superset` mode when you want to verify that a few key tools are called in the execution, but you're okay with the agent calling additional tools. The agent's trajectory must include at least all the tool calls in the reference trajectory, and may include additional tool calls beyond the reference.
* Use `subset` mode to ensure agent efficiency by verifying that the agent did not call any irrelevant or unnecessary tools beyond those in the reference. The agent's trajectory must include only tool calls that appear in the reference trajectory.

The following example demonstrates `superset` mode, where the reference trajectory only requires the `get_weather` tool, but the agent can call additional tools:

<Info>
  You can also customize how the evaluator considers equality between tool calls in the actual trajectory vs. the reference by setting the `tool_args_match_mode` (Python) or `toolArgsMatchMode` (TypeScript) property, as well as the `tool_args_match_overrides` (Python) or `toolArgsMatchOverrides` (TypeScript) property. By default, only tool calls with the same arguments to the same tool are considered equal. Visit the [repository](https://github.com/langchain-ai/agentevals?tab=readme-ov-file#tool-args-match-modes) for more details.
</Info>

## LLM-as-judge evaluator

<Note>
  This section covers the trajectory-specific LLM-as-a-judge evaluator from the `agentevals` package. For general-purpose LLM-as-a-judge evaluators in LangSmith, refer to the [LLM-as-a-judge evaluator](/langsmith/llm-as-judge).
</Note>

You can also use an LLM to evaluate the agent's execution path. Unlike the trajectory match evaluators, it doesn't require a reference trajectory, but one can be provided if available.

### Without reference trajectory

### With reference trajectory

If you have a reference trajectory, you can add an extra variable to your prompt and pass in the reference trajectory. Below, we use the prebuilt `TRAJECTORY_ACCURACY_PROMPT_WITH_REFERENCE` prompt and configure the `reference_outputs` variable:

<Info>
  For more configurability over how the LLM evaluates the trajectory, visit the [repository](https://github.com/langchain-ai/agentevals?tab=readme-ov-file#trajectory-llm-as-judge).
</Info>

## Async support (Python)

All `agentevals` evaluators support Python asyncio. For evaluators that use factory functions, async versions are available by adding `async` after `create_` in the function name.

Here's an example using the async judge and evaluator:

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/trajectory-evals.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

Or, clone the [AgentEvals repository](https://github.com/langchain-ai/agentevals) directly.

## Trajectory match evaluator

AgentEvals offers the `create_trajectory_match_evaluator` function in Python and `createTrajectoryMatchEvaluator` in TypeScript to match your agent's trajectory against a reference trajectory.

You can use the following modes:

| Mode                                     | Description                                               | Use Case                                                              |
| ---------------------------------------- | --------------------------------------------------------- | --------------------------------------------------------------------- |
| [`strict`](#strict-match)                | Exact match of messages and tool calls in the same order  | Testing specific sequences (e.g., policy lookup before authorization) |
| [`unordered`](#unordered-match)          | Same tool calls allowed in any order                      | Verifying information retrieval when order doesn't matter             |
| [`subset`](#subset-and-superset-match)   | Agent calls only tools from reference (no extras)         | Ensuring agent doesn't exceed expected scope                          |
| [`superset`](#subset-and-superset-match) | Agent calls at least the reference tools (extras allowed) | Verifying minimum required actions are taken                          |

### Strict match

The `strict` mode ensures trajectories contain identical messages in the same order with the same tool calls, though it allows for differences in message content. This is useful when you need to enforce a specific sequence of operations, such as requiring a policy lookup before authorizing an action.

<CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>

### Unordered match

The `unordered` mode allows the same tool calls in any order, which is helpful when you want to verify that the correct set of tools are being invoked but don't care about the sequence. For example, an agent might need to check both weather and events for a city, but the order doesn't matter.

<CodeGroup>
```

---

## How to fetch performance metrics for an experiment

**URL:** llms-txt#how-to-fetch-performance-metrics-for-an-experiment

Source: https://docs.langchain.com/langsmith/fetch-perf-metrics-experiment

<Check>
  Tracing projects and experiments use the same underlying data structure in our backend, which is called a "session."

You might see these terms interchangeably in our documentation, but they all refer to the same underlying data structure.

We are working on unifying the terminology across our documentation and APIs.
</Check>

When you run an experiment using `evaluate` with the Python or TypeScript SDK, you can fetch the performance metrics for the experiment using the `read_project`/`readProject` methods.

The payload for experiment details includes the following values:

From here, you can extract performance metrics such as:

* `latency_p50`: The 50th percentile latency in seconds.
* `latency_p99`: The 99th percentile latency in seconds.
* `total_tokens`: The total number of tokens used.
* `prompt_tokens`: The number of prompt tokens used.
* `completion_tokens`: The number of completion tokens used.
* `total_cost`: The total cost of the experiment.
* `prompt_cost`: The cost of the prompt tokens.
* `completion_cost`: The cost of the completion tokens.
* `feedback_stats`: The feedback statistics for the experiment.
* `error_rate`: The error rate for the experiment.
* `first_token_p50`: The 50th percentile latency for the time to generate the first token (if using streaming).
* `first_token_p99`: The 99th percentile latency for the time to generate the first token (if using streaming).

Here is an example of how you can fetch the performance metrics for an experiment using the Python and TypeScript SDKs.

First, as a prerequisite, we will create a trivial dataset. Here, we only demonstrate this in Python, but you can do the same in TypeScript. Please view the [how-to guide](/langsmith/evaluate-llm-application) on evaluation for more details.

```python  theme={null}
from langsmith import Client

**Examples:**

Example 1 (unknown):
```unknown
From here, you can extract performance metrics such as:

* `latency_p50`: The 50th percentile latency in seconds.
* `latency_p99`: The 99th percentile latency in seconds.
* `total_tokens`: The total number of tokens used.
* `prompt_tokens`: The number of prompt tokens used.
* `completion_tokens`: The number of completion tokens used.
* `total_cost`: The total cost of the experiment.
* `prompt_cost`: The cost of the prompt tokens.
* `completion_cost`: The cost of the completion tokens.
* `feedback_stats`: The feedback statistics for the experiment.
* `error_rate`: The error rate for the experiment.
* `first_token_p50`: The 50th percentile latency for the time to generate the first token (if using streaming).
* `first_token_p99`: The 99th percentile latency for the time to generate the first token (if using streaming).

Here is an example of how you can fetch the performance metrics for an experiment using the Python and TypeScript SDKs.

First, as a prerequisite, we will create a trivial dataset. Here, we only demonstrate this in Python, but you can do the same in TypeScript. Please view the [how-to guide](/langsmith/evaluate-llm-application) on evaluation for more details.
```

---

## How to filter experiments in the UI

**URL:** llms-txt#how-to-filter-experiments-in-the-ui

**Contents:**
- Background: add metadata to your experiments
- Filter experiments in the UI

Source: https://docs.langchain.com/langsmith/filter-experiments-ui

LangSmith lets you filter your previous experiments by feedback scores and metadata to make it easy to find only the experiments you care about.

## Background: add metadata to your experiments

When you run an experiment in the SDK, you can attach metadata to make it easier to filter in UI. This is helpful if you know what axes you want to drill down into when running experiments.

In our example, we are going to attach metadata to our experiment around the model used, the model provider, and a known ID of the prompt:

## Filter experiments in the UI

In the UI, we see all experiments that have been run by default.

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-all-experiments.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=0cb5b29f405286dadb8b6491709eb789" alt="" data-og-width="2900" width="2900" data-og-height="1370" height="1370" data-path="langsmith/images/filter-all-experiments.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-all-experiments.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=45a66945b4ca67d1dc3c29890b5d5feb 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-all-experiments.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=1d8fdbdf637f750dcef092a49b45e0ae 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-all-experiments.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=7177578edd6e6bcfef63d7a778ded723 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-all-experiments.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=f0c6bdae3afabe2b179a74450adbc508 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-all-experiments.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=8dd72a9e30b093a58819d79f40f69b65 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-all-experiments.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=e65610094b48a0f81ac75a49be573382 2500w" />

If we, say, have a preference for openai models, we can easily filter down and see scores within just openai models first:

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-openai.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=e3a0c42e4eb37cae68d367dec75d0df1" alt="" data-og-width="2910" width="2910" data-og-height="1130" height="1130" data-path="langsmith/images/filter-openai.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-openai.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=96ef35ffdaf371b2819da5a3f4bdf7aa 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-openai.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=b44c6d30e0e03560d26719facf1dfc0c 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-openai.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=a737dceb8e7a2c89b9d0dfc008352afd 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-openai.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=cd3adf4b2d249bde3646810d3783b931 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-openai.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=7dc39864577acc20bb9229de313cbe33 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-openai.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=7d5392a8453b2f416f795834eb02cc6e 2500w" />

We can stack filters, allowing us to filter out low scores on correctness to make sure we only compare relevant experiments:

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-feedback.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=c0e223bbe637a03a4c251896a5662f52" alt="" data-og-width="2912" width="2912" data-og-height="826" height="826" data-path="langsmith/images/filter-feedback.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-feedback.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=476871f1d9569046084b380b3fa50ba3 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-feedback.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=c8f24775e8f736a2cfd6b1092106a200 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-feedback.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=c014f713cb651fa61d13dc8fe11055e2 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-feedback.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=f8c9ad41ba9ba73ee43df8c0c740b53e 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-feedback.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=4e8105316812331bf3d865c15028e3e7 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-feedback.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=45c2cd7461fde3670a814ae0915a3584 2500w" />

Finally, we can clear and reset filters. For example, if we see there is clear there's a winner with the `singleminded` prompt, we can change filtering settings to see if any other model providers' models work as well with it:

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-singleminded.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=7a30d214745fcddc022e3c715267dfd2" alt="" data-og-width="2904" width="2904" data-og-height="832" height="832" data-path="langsmith/images/filter-singleminded.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-singleminded.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=bcf8d6a876b76137cc5682308f9e2f74 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-singleminded.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=4c1e77dc12de1b65183d26877104a384 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-singleminded.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=3aa8585e58f88e5e7eac344ae386808f 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-singleminded.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=613558759a127d7003d3ae653e83298b 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-singleminded.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=2ba32a223d7012904c0eb63fe7c1fdb7 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-singleminded.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=c643c61125d409c706ea607b1504740c 2500w" />

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/filter-experiments-ui.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## How to handle model rate limits

**URL:** llms-txt#how-to-handle-model-rate-limits

**Contents:**
- Using `langchain` RateLimiters (Python only)
- Retrying with exponential backoff
- Limiting `max_concurrency`

Source: https://docs.langchain.com/langsmith/rate-limiting

A common issue when running large evaluation jobs is running into third-party API rate limits, usually from model providers. There are a few ways to deal with rate limits.

## Using `langchain` RateLimiters (Python only)

If you're using `langchain` Python chat models in your application or evaluators, you can add rate limiters to your model(s) that will add client-side control of the frequency with which requests are sent to the model provider API to avoid rate limit errors.

See the [langchain](https://python.langchain.com/docs/how_to/chat_model_rate_limiting/) documentation for more on how to configure rate limiters.

## Retrying with exponential backoff

A very common way to deal with rate limit errors is retrying with exponential backoff. Retrying with exponential backoff means repeatedly retrying failed requests with an (exponentially) increasing wait time between each retry. This continues until either the request succeeds or a maximum number of requests is made.

#### With `langchain`

If you're using `langchain` components you can add retries to all model calls with the `.with_retry(...)` / `.withRetry()` method:

See the `langchain` [Python](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.chat_models.BaseChatModel.html#langchain_core.language_models.chat_models.BaseChatModel.with_retry) and [JS](https://v03.api.js.langchain.com/classes/_langchain_core.language_models_chat_models.BaseChatModel.html#withRetry) API references for more.

#### Without `langchain`

If you're not using `langchain` you can use other libraries like `tenacity` (Python) or `backoff` (Python) to implement retries with exponential backoff, or you can implement it from scratch. See some examples of how to do this in the [OpenAI docs](https://platform.openai.com/docs/guides/rate-limits#retrying-with-exponential-backoff).

## Limiting `max_concurrency`

Limiting the number of concurrent calls you're making to your application and evaluators is another way to decrease the frequency of model calls you're making, and in that way avoid rate limit errors. `max_concurrency` can be set directly on the [evaluate()](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._runner.evaluate) / [aevaluate()](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._arunner.aevaluate) functions. This parallelizes evaluation by effectively splitting the dataset across threads.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/rate-limiting.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
See the [langchain](https://python.langchain.com/docs/how_to/chat_model_rate_limiting/) documentation for more on how to configure rate limiters.

## Retrying with exponential backoff

A very common way to deal with rate limit errors is retrying with exponential backoff. Retrying with exponential backoff means repeatedly retrying failed requests with an (exponentially) increasing wait time between each retry. This continues until either the request succeeds or a maximum number of requests is made.

#### With `langchain`

If you're using `langchain` components you can add retries to all model calls with the `.with_retry(...)` / `.withRetry()` method:

<CodeGroup>
```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
</CodeGroup>

See the `langchain` [Python](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.chat_models.BaseChatModel.html#langchain_core.language_models.chat_models.BaseChatModel.with_retry) and [JS](https://v03.api.js.langchain.com/classes/_langchain_core.language_models_chat_models.BaseChatModel.html#withRetry) API references for more.

#### Without `langchain`

If you're not using `langchain` you can use other libraries like `tenacity` (Python) or `backoff` (Python) to implement retries with exponential backoff, or you can implement it from scratch. See some examples of how to do this in the [OpenAI docs](https://platform.openai.com/docs/guides/rate-limits#retrying-with-exponential-backoff).

## Limiting `max_concurrency`

Limiting the number of concurrent calls you're making to your application and evaluators is another way to decrease the frequency of model calls you're making, and in that way avoid rate limit errors. `max_concurrency` can be set directly on the [evaluate()](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._runner.evaluate) / [aevaluate()](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._arunner.aevaluate) functions. This parallelizes evaluation by effectively splitting the dataset across threads.

<CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## How to implement generative user interfaces with LangGraph

**URL:** llms-txt#how-to-implement-generative-user-interfaces-with-langgraph

**Contents:**
- Tutorial
  - 1. Define and configure UI components
  - 2. Send the UI components in your graph
  - 3. Handle UI elements in your React application
- How-to guides
  - Provide custom components on the client side
  - Show loading UI when components are loading
  - Customise the namespace of UI components.
  - Access and interact with the thread state from the UI component
  - Pass additional context to the client components

Source: https://docs.langchain.com/langsmith/generative-ui-react

<Info>
  **Prerequisites**

* [LangSmith](/langsmith/home)
  * [Agent Server](/langsmith/agent-server)
  * [`useStream()` React Hook](/langsmith/use-stream-react)
</Info>

Generative user interfaces (Generative UI) allows agents to go beyond text and generate rich user interfaces. This enables creating more interactive and context-aware applications where the UI adapts based on the conversation flow and AI responses.

<img src="https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/generative-ui-sample.jpg?fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=105943c6c28853fad0a9bc3b4af3a999" alt="Agent Chat showing a prompt about booking/lodging and a generated set of hotel listing cards (images, titles, prices, locations) rendered inline as UI components." data-og-width="1814" width="1814" data-og-height="898" height="898" data-path="langsmith/images/generative-ui-sample.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/generative-ui-sample.jpg?w=280&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=0fd526a7132d33ab6f72002d68a66dec 280w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/generative-ui-sample.jpg?w=560&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=0c9ffe86700a7b8404f1fdf51b906aa1 560w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/generative-ui-sample.jpg?w=840&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=50652e58566db8171ead4aef57d78fa6 840w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/generative-ui-sample.jpg?w=1100&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=a764d790719e8233313fabe4cee93958 1100w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/generative-ui-sample.jpg?w=1650&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=a02d8d6ecace7eee6df55e3a391c09e2 1650w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/generative-ui-sample.jpg?w=2500&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=b0709ca94bd9533f5ef5a80da1d60bf6 2500w" />

LangSmith supports colocating your React components with your graph code. This allows you to focus on building specific UI components for your graph while easily plugging into existing chat interfaces such as [Agent Chat](https://agentchat.vercel.app) and loading the code only when actually needed.

### 1. Define and configure UI components

First, create your first UI component. For each component you need to provide an unique identifier that will be used to reference the component in your graph code.

Next, define your UI components in your `langgraph.json` configuration:

The `ui` section points to the UI components that will be used by graphs. By default, we recommend using the same key as the graph name, but you can split out the components however you like, see [Customise the namespace of UI components](#customise-the-namespace-of-ui-components) for more details.

LangSmith will automatically bundle your UI components code and styles and serve them as external assets that can be loaded by the `LoadExternalComponent` component. Some dependencies such as `react` and `react-dom` will be automatically excluded from the bundle.

CSS and Tailwind 4.x is also supported out of the box, so you can freely use Tailwind classes as well as `shadcn/ui` in your UI components.

<Tabs>
  <Tab title="src/agent/ui.tsx">
    
  </Tab>

<Tab title="src/agent/styles.css">
    
  </Tab>
</Tabs>

### 2. Send the UI components in your graph

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="JS">
    Use the `typedUi` utility to emit UI elements from your agent nodes:

### 3. Handle UI elements in your React application

On the client side, you can use `useStream()` and `LoadExternalComponent` to display the UI elements.

Behind the scenes, `LoadExternalComponent` will fetch the JS and CSS for the UI components from LangSmith and render them in a shadow DOM, thus ensuring style isolation from the rest of your application.

### Provide custom components on the client side

If you already have the components loaded in your client application, you can provide a map of such components to be rendered directly without fetching the UI code from LangSmith.

### Show loading UI when components are loading

You can provide a fallback UI to be rendered when the components are loading.

### Customise the namespace of UI components.

By default `LoadExternalComponent` will use the `assistantId` from `useStream()` hook to fetch the code for UI components. You can customise this by providing a `namespace` prop to the `LoadExternalComponent` component.

<Tabs>
  <Tab title="src/app/page.tsx">
    
  </Tab>

<Tab title="langgraph.json">
    
  </Tab>
</Tabs>

### Access and interact with the thread state from the UI component

You can access the thread state inside the UI component by using the `useStreamContext` hook.

### Pass additional context to the client components

You can pass additional context to the client components by providing a `meta` prop to the `LoadExternalComponent` component.

Then, you can access the `meta` prop in the UI component by using the `useStreamContext` hook.

### Streaming UI messages from the server

You can stream UI messages before the node execution is finished by using the `onCustomEvent` callback of the `useStream()` hook. This is especially useful when updating the UI component as the LLM is generating the response.

Then you can push updates to the UI component by calling `ui.push()` / `push_ui_message()` with the same ID as the UI message you wish to update.

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="JS">
    
  </Tab>

<Tab title="ui.tsx">
    
  </Tab>
</Tabs>

### Remove UI messages from state

Similar to how messages can be removed from the state by appending a RemoveMessage you can remove an UI message from the state by calling `remove_ui_message` / `ui.delete` with the ID of the UI message.

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="JS">
    
  </Tab>
</Tabs>

* [JS/TS SDK Reference](https://langchain-ai.github.io/langgraphjs/reference/modules/sdk.html)

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/generative-ui-react.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
Next, define your UI components in your `langgraph.json` configuration:
```

Example 2 (unknown):
```unknown
The `ui` section points to the UI components that will be used by graphs. By default, we recommend using the same key as the graph name, but you can split out the components however you like, see [Customise the namespace of UI components](#customise-the-namespace-of-ui-components) for more details.

LangSmith will automatically bundle your UI components code and styles and serve them as external assets that can be loaded by the `LoadExternalComponent` component. Some dependencies such as `react` and `react-dom` will be automatically excluded from the bundle.

CSS and Tailwind 4.x is also supported out of the box, so you can freely use Tailwind classes as well as `shadcn/ui` in your UI components.

<Tabs>
  <Tab title="src/agent/ui.tsx">
```

Example 3 (unknown):
```unknown
</Tab>

  <Tab title="src/agent/styles.css">
```

Example 4 (unknown):
```unknown
</Tab>
</Tabs>

### 2. Send the UI components in your graph

<Tabs>
  <Tab title="Python">
```

---

## How to improve your evaluator with few-shot examples

**URL:** llms-txt#how-to-improve-your-evaluator-with-few-shot-examples

**Contents:**
- How few-shot examples work
- Configure your evaluator
  - 1. Configure variable mapping
  - 2. Specify the number of few-shot examples to use
- Make corrections
- View your corrections dataset

Source: https://docs.langchain.com/langsmith/create-few-shot-evaluators

Using LLM-as-a-judge evaluators can be very helpful when you can't evaluate your system programmatically. However, their effectiveness depends on their quality and how well they align with human reviewer feedback. LangSmith provides the ability to improve the alignment of LLM-as-a-judge evaluator to human preferences using few-shot examples.

Human corrections are automatically inserted into your evaluator prompt using few-shot examples. Few-shot examples is a technique inspired by [few-shot prompting](https://www.promptingguide.ai/techniques/fewshot) that guides the models output with a few high-quality examples.

This guide covers how to set up few-shot examples as part of your LLM-as-a-judge evaluator and apply corrections to feedback scores.

## How few-shot examples work

* Few-shot examples are added to your evaluator prompt using the `{{Few-shot examples}}` variable
* Creating an evaluator with few-shot examples, will automatically create a dataset for you, which will be auto-populated with few-shot examples once you start making corrections
* At runtime, these examples will inserted into the evaluator to serve as a guide for its outputs - this will help the evaluator to better align with human preferences

## Configure your evaluator

<Note>
  Few-shot examples are not currently supported in LLM-as-a-judge evaluators that use the prompt hub and are only compatible with prompts that use mustache formatting.
</Note>

Before enabling few-shot examples, set up your LLM-as-a-judge evaluator. If you haven't done this yet, follow the steps in the [LLM-as-a-judge evaluator guide](/langsmith/llm-as-judge).

### 1. Configure variable mapping

Each few-shot example is formatted according to the variable mapping specified in the configuration. The variable mapping for few-shot examples, should contain the same variables as your main prompt, plus a `few_shot_explanation` and a `score` variable which should have the same name as your feedback key.

For example, if your main prompt has variables `question` and `response`, and your evaluator outputs a `correctness` score, then your few-shot prompt should have the vartiables `question`, `response`, `few_shot_explanation`, and `correctness`.

### 2. Specify the number of few-shot examples to use

You may also specify the number of few-shot examples to use. The default is 5. If your examples are very long, you may want to set this number lower to save tokens - whereas if your examples tend to be short, you can set a higher number in order to give your evaluator more examples to learn from. If you have more examples in your dataset than this number, we will randomly choose them for you.

<Info>
  [Audit evaluator scores](/langsmith/audit-evaluator-scores)
</Info>

As you start logging traces or running experiments, you will likely disagree with some of the scores that your evaluator has given. When you [make corrections to these scores](/langsmith/audit-evaluator-scores), you will begin seeing examples populated inside your corrections dataset. As you make corrections, make sure to attach explanations - these will get populated into your evaluator prompt in place of the `few_shot_explanation` variable.

The inputs to the few-shot examples will be the relevant fields from the inputs, outputs, and reference (if this an offline evaluator) of your chain/dataset. The outputs will be the corrected evaluator score and the explanations that you created when you left the corrections. Feel free to edit these to your liking. Here is an example of a few-shot example in a corrections dataset:

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-example.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=8c7bfcc6cc4ab86c18240c3cbf2ea44c" alt="Few-shot example" data-og-width="1572" width="1572" data-og-height="790" height="790" data-path="langsmith/images/few-shot-example.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-example.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=91f4e17fd853ba23c1b04934144dfa77 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-example.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=e0a5e2a026e4166c341900dd49316f35 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-example.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=88aec2ef5c37c16c67e0eefecd3fbc0a 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-example.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=bfbbd35cf503ce2f3dbf743fab8fb75b 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-example.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=bf0765bfeabc8d34ef49626dca0135ae 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-example.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=f9d2cf9437ee0e160a511903bd88238a 2500w" />

Note that the corrections may take a minute or two to be populated into your few-shot dataset. Once they are there, future runs of your evaluator will include them in the prompt!

## View your corrections dataset

In order to view your corrections dataset:

* **Online evaluators**: Select your run rule and click **Edit Rule**
* **Offline evaluators**: Select your evaluator and click **Edit Evaluator**

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/edit-evaluator.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=03453ef08f1c272d5d9aaf71d1fb7301" alt="Edit Evaluator" data-og-width="800" width="800" data-og-height="284" height="284" data-path="langsmith/images/edit-evaluator.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/edit-evaluator.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=d495c791e6c8ae9d241085795d4b67b5 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/edit-evaluator.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=b1ddde8054744862494e4d3f02a460b0 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/edit-evaluator.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=9838e073d1d7e61c6b79d8f35ba1a1b3 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/edit-evaluator.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=67322be4166f4479a466427a9b270ca1 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/edit-evaluator.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=4cd490404d1616498fed810b3ce75a21 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/edit-evaluator.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=4c8a87ad332661a0b8b472dd34f1f4ab 2500w" />

Head to your dataset of corrections linked in the the **Improve evaluator accuracy using few-shot examples** section. You can view and update your few-shot examples in the dataset.

<img src="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-few-shot-ds.png?fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=3215f3f24a08186fd76c6dbad18a3cf5" alt="View few-shot dataset" data-og-width="1470" width="1470" data-og-height="478" height="478" data-path="langsmith/images/view-few-shot-ds.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-few-shot-ds.png?w=280&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=ad702a532a8f083c71056baff4370f30 280w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-few-shot-ds.png?w=560&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=d45ebd4263adc9c10598fad633167ca3 560w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-few-shot-ds.png?w=840&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=fe060c8d000a41566949ff35d6c62135 840w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-few-shot-ds.png?w=1100&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=f735943da46a1e57328b86246f5da25f 1100w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-few-shot-ds.png?w=1650&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=9861b9651d5d63a07662e7aa1bc68491 1650w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-few-shot-ds.png?w=2500&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=6084c3697ffd582e30301540906a5698 2500w" />

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/create-few-shot-evaluators.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## How to integrate LangGraph into your React application

**URL:** llms-txt#how-to-integrate-langgraph-into-your-react-application

**Contents:**
- Installation
- Example
- Customizing Your UI
  - Loading States
  - Resume a stream after page refresh
  - Thread Management
  - Messages Handling
  - Interrupts
  - Branching
  - Optimistic Updates

Source: https://docs.langchain.com/langsmith/use-stream-react

<Info>
  **Prerequisites**

* [LangSmith](/langsmith/home)
  * [Agent Server](/langsmith/agent-server)
</Info>

The [`useStream()`](https://langchain-ai.github.io/langgraphjs/reference/modules/sdk.html) React hook provides a seamless way to integrate LangGraph into your React applications. It handles all the complexities of streaming, state management, and branching logic, letting you focus on building great chat experiences.

* Messages streaming: Handle a stream of message chunks to form a complete message
* Automatic state management for messages, interrupts, loading states, and errors
* Conversation branching: Create alternate conversation paths from any point in the chat history
* UI-agnostic design: bring your own components and styling

Let's explore how to use [`useStream()`](https://langchain-ai.github.io/langgraphjs/reference/modules/sdk.html) in your React application.

The [`useStream()`](https://langchain-ai.github.io/langgraphjs/reference/modules/sdk.html) provides a solid foundation for creating bespoke chat experiences. For pre-built chat components and interfaces, we also recommend checking out [CopilotKit](https://docs.copilotkit.ai/coagents/quickstart/langgraph) and [assistant-ui](https://www.assistant-ui.com/docs/runtimes/langgraph).

## Customizing Your UI

The [`useStream()`](https://langchain-ai.github.io/langgraphjs/reference/modules/sdk.html) hook takes care of all the complex state management behind the scenes, providing you with simple interfaces to build your UI. Here's what you get out of the box:

* Thread state management
* Loading and error states
* Interrupts
* Message handling and updates
* Branching support

Here are some examples on how to use these features effectively:

The `isLoading` property tells you when a stream is active, enabling you to:

* Show a loading indicator
* Disable input fields during processing
* Display a cancel button

### Resume a stream after page refresh

The [`useStream()`](https://langchain-ai.github.io/langgraphjs/reference/modules/sdk.html) hook can automatically resume an ongoing run upon mounting by setting `reconnectOnMount: true`. This is useful for continuing a stream after a page refresh, ensuring no messages and events generated during the downtime are lost.

By default the ID of the created run is stored in `window.sessionStorage`, which can be swapped by passing a custom storage in `reconnectOnMount` instead. The storage is used to persist the in-flight run ID for a thread (under `lg:stream:${threadId}` key).

You can also manually manage the resuming process by using the run callbacks to persist the run metadata and the `joinStream` function to resume the stream. Make sure to pass `streamResumable: true` when creating the run; otherwise some events might be lost.

### Thread Management

Keep track of conversations with built-in thread management. You can access the current thread ID and get notified when new threads are created:

We recommend storing the `threadId` in your URL's query parameters to let users resume conversations after page refreshes.

### Messages Handling

The [`useStream()`](https://langchain-ai.github.io/langgraphjs/reference/modules/sdk.html) hook will keep track of the message chunks received from the server and concatenate them together to form a complete message. The completed message chunks can be retrieved via the `messages` property.

By default, the `messagesKey` is set to `messages`, where it will append the new messages chunks to `values["messages"]`. If you store messages in a different key, you can change the value of `messagesKey`.

Under the hood, the [`useStream()`](https://langchain-ai.github.io/langgraphjs/reference/modules/sdk.html) hook will use the `streamMode: "messages-tuple"` to receive a stream of messages (i.e. individual LLM tokens) from any LangChain chat model invocations inside your graph nodes. Learn more about messages streaming in the [streaming](/langsmith/streaming#messages) guide.

The [`useStream()`](https://langchain-ai.github.io/langgraphjs/reference/modules/sdk.html) hook exposes the `interrupt` property, which will be filled with the last interrupt from the thread. You can use interrupts to:

* Render a confirmation UI before executing a node
* Wait for human input, allowing agent to ask the user with clarifying questions

Learn more about interrupts in the [How to handle interrupts](/oss/python/langgraph/interrupts#pause-using-interrupt) guide.

For each message, you can use `getMessagesMetadata()` to get the first checkpoint from which the message has been first seen. You can then create a new run from the checkpoint preceding the first seen checkpoint to create a new branch in a thread.

A branch can be created in following ways:

1. Edit a previous user message.
2. Request a regeneration of a previous assistant message.

For advanced use cases you can use the `experimental_branchTree` property to get the tree representation of the thread, which can be used to render branching controls for non-message based graphs.

### Optimistic Updates

You can optimistically update the client state before performing a network request to the agent, allowing you to provide immediate feedback to the user, such as showing the user message immediately before the agent has seen the request.

### Cached Thread Display

Use the `initialValues` option to display cached thread data immediately while the history is being loaded from the server. This improves user experience by showing cached data instantly when navigating to existing threads.

### Optimistic Thread Creation

Use the `threadId` option in `submit` function to enable optimistic UI patterns where you need to know the thread ID before the thread is actually created.

The [`useStream()`](https://langchain-ai.github.io/langgraphjs/reference/modules/sdk.html) hook is friendly for apps written in TypeScript and you can specify types for the state to get better type safety and IDE support.

You can also optionally specify types for different scenarios, such as:

* `ConfigurableType`: Type for the `config.configurable` property (default: `Record<string, unknown>`)
* `InterruptType`: Type for the interrupt value - i.e. contents of `interrupt(...)` function (default: `unknown`)
* `CustomEventType`: Type for the custom events (default: `unknown`)
* `UpdateType`: Type for the submit function (default: `Partial<State>`)

If you're using LangGraph.js, you can also reuse your graph's annotation types. However, make sure to only import the types of the annotation schema in order to avoid importing the entire LangGraph.js runtime (i.e. via `import type { ... }` directive).

The [`useStream()`](https://langchain-ai.github.io/langgraphjs/reference/modules/sdk.html) hook provides several callback options to help you respond to different events:

* `onError`: Called when an error occurs.
* `onFinish`: Called when the stream is finished.
* `onUpdateEvent`: Called when an update event is received.
* `onCustomEvent`: Called when a custom event is received. See the [streaming](/oss/python/langgraph/streaming#stream-custom-data) guide to learn how to stream custom events.
* `onMetadataEvent`: Called when a metadata event is received, which contains the Run ID and Thread ID.

* [JS/TS SDK Reference](https://langchain-ai.github.io/langgraphjs/reference/modules/sdk.html)

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/use-stream-react.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
## Example
```

Example 2 (unknown):
```unknown
## Customizing Your UI

The [`useStream()`](https://langchain-ai.github.io/langgraphjs/reference/modules/sdk.html) hook takes care of all the complex state management behind the scenes, providing you with simple interfaces to build your UI. Here's what you get out of the box:

* Thread state management
* Loading and error states
* Interrupts
* Message handling and updates
* Branching support

Here are some examples on how to use these features effectively:

### Loading States

The `isLoading` property tells you when a stream is active, enabling you to:

* Show a loading indicator
* Disable input fields during processing
* Display a cancel button
```

Example 3 (unknown):
```unknown
### Resume a stream after page refresh

The [`useStream()`](https://langchain-ai.github.io/langgraphjs/reference/modules/sdk.html) hook can automatically resume an ongoing run upon mounting by setting `reconnectOnMount: true`. This is useful for continuing a stream after a page refresh, ensuring no messages and events generated during the downtime are lost.
```

Example 4 (unknown):
```unknown
By default the ID of the created run is stored in `window.sessionStorage`, which can be swapped by passing a custom storage in `reconnectOnMount` instead. The storage is used to persist the in-flight run ID for a thread (under `lg:stream:${threadId}` key).
```

---

## How to integrate LangGraph with AutoGen, CrewAI, and other frameworks

**URL:** llms-txt#how-to-integrate-langgraph-with-autogen,-crewai,-and-other-frameworks

**Contents:**
- Prerequisites
- Setup
- 1. Define AutoGen agent
- 2. Create the graph

Source: https://docs.langchain.com/langsmith/autogen-integration

This guide shows how to integrate AutoGen agents with LangGraph to leverage features like persistence, streaming, and memory, and then deploy the integrated solution to LangSmith for scalable production use. In this guide we show how to build a LangGraph chatbot that integrates with AutoGen, but you can follow the same approach with other frameworks.

Integrating AutoGen with LangGraph provides several benefits:

* Enhanced features: Add [persistence](/oss/python/langgraph/persistence), [streaming](/langsmith/streaming), [short and long-term memory](/oss/python/concepts/memory) and more to your AutoGen agents.
* Multi-agent systems: Build [multi-agent systems](/oss/python/langchain/multi-agent) where individual agents are built with different frameworks.
* Production deployment: Deploy your integrated solution to [LangSmith](/langsmith/home) for scalable production use.

* Python 3.9+
* Autogen: `pip install autogen`
* LangGraph: `pip install langgraph`
* OpenAI API key

Set your your environment:

## 1. Define AutoGen agent

Create an AutoGen agent that can execute code. This example is adapted from AutoGen's [official tutorials](https://github.com/microsoft/autogen/blob/0.2/notebook/agentchat_web_info.ipynb):

## 2. Create the graph

We will now create a LangGraph chatbot graph that calls AutoGen agent.

```python  theme={null}
from langchain_core.messages import convert_to_openai_messages
from langgraph.graph import StateGraph, MessagesState, START
from langgraph.checkpoint.memory import MemorySaver

def call_autogen_agent(state: MessagesState):
    # Convert LangGraph messages to OpenAI format for AutoGen
    messages = convert_to_openai_messages(state["messages"])

# Get the last user message
    last_message = messages[-1]

# Pass previous message history as context (excluding the last message)
    carryover = messages[:-1] if len(messages) > 1 else []

# Initiate chat with AutoGen
    response = user_proxy.initiate_chat(
        autogen_agent,
        message=last_message,
        carryover=carryover
    )

# Extract the final response from the agent
    final_content = response.chat_history[-1]["content"]

# Return the response in LangGraph format
    return {"messages": {"role": "assistant", "content": final_content}}

**Examples:**

Example 1 (unknown):
```unknown
## 1. Define AutoGen agent

Create an AutoGen agent that can execute code. This example is adapted from AutoGen's [official tutorials](https://github.com/microsoft/autogen/blob/0.2/notebook/agentchat_web_info.ipynb):
```

Example 2 (unknown):
```unknown
## 2. Create the graph

We will now create a LangGraph chatbot graph that calls AutoGen agent.
```

---

## How to interact with a deployment using RemoteGraph

**URL:** llms-txt#how-to-interact-with-a-deployment-using-remotegraph

**Contents:**
- Prerequisites
- Initialize the graph
  - Use a URL
  - Use a client
- Invoke the graph
  - Asynchronously
  - Synchronously
- Persist state at the thread level
- Use as a subgraph

Source: https://docs.langchain.com/langsmith/use-remote-graph

[`RemoteGraph`](https://langchain-ai.github.io/langgraph/reference/remote_graph) is a client-side interface that allows you to interact with your [deployment](/langsmith/deployments) as if it were a local graph. It provides API parity with [`CompiledGraph`](/oss/python/langgraph/graph-api#compiling-your-graph), which means that you can use the same methods (`invoke()`, `stream()`, `get_state()`, etc.) in your development and production environments. This page describes how to initialize a `RemoteGraph` and interact with it.

`RemoteGraph` is useful for the following:

* Separation of development and deployment: Build and test a graph locally with `CompiledGraph`, deploy it to LangSmith, and then [use `RemoteGraph`](#initialize-the-graph) to call it in production while working with the same API interface.
* Thread-level persistence: [Persist and fetch the state](#persist-state-at-the-thread-level) of a conversation across calls with a thread ID.
* Subgraph embedding: Compose modular graphs for a multi-agent workflow by embedding a `RemoteGraph` as a [subgraph](#use-as-a-subgraph) within another graph.
* Reusable workflows: Use deployed graphs as nodes or [tools](https://langchain-ai.github.io/langgraph/reference/remote_graph/#langgraph.pregel.remote.RemoteGraph.as_tool), so that you can reuse and expose complex logic.

<Warning>
  **Important: Avoid calling the same deployment**

`RemoteGraph` is designed to call graphs on other deployments. Do not use `RemoteGraph` to call itself or another graph on the same deployment, as this can lead to deadlocks and resource exhaustion. Instead, use local graph composition or [subgraphs](/oss/python/langgraph/use-subgraphs) for graphs within the same deployment.
</Warning>

Before getting started with `RemoteGraph`, make sure you have:

* Access to [LangSmith](/langsmith/home), where your graphs are developed and managed.
* A running [Agent Server](/langsmith/agent-server), which hosts your deployed graphs for remote interaction.

## Initialize the graph

When initializing a `RemoteGraph`, you must always specify:

* `name`: The name of the graph you want to interact with **or** an assistant ID. If you specify a graph name, the default assistant will be used. If you specify an assistant ID, that specific assistant will be used. The graph name is the same name you use in the `langgraph.json` configuration file for your deployment.
* `api_key`: A valid [LangSmith API key](/langsmith/create-account-api-key). You can set as an environment variable (`LANGSMITH_API_KEY`) or pass directly in the `api_key` argument. You can also provide the API key in the `client` / `sync_client` arguments, if `LangGraphClient` / `SyncLangGraphClient` was initialized with the `api_key` argument.

Additionally, you have to provide one of the following:

* [`url`](#use-a-url): The URL of the deployment you want to interact with. If you pass the `url` argument, both sync and async clients will be created using the provided URL, headers (if provided), and default configuration values (e.g., timeout).
* [`client`](#use-a-client): A `LangGraphClient` instance for interacting with the deployment asynchronously (e.g., using `.astream()`, `.ainvoke()`, `.aget_state()`, `.aupdate_state()`).
* `sync_client`: A `SyncLangGraphClient` instance for interacting with the deployment synchronously (e.g., using `.stream()`, `.invoke()`, `.get_state()`, `.update_state()`).

<Note>
  If you pass both `client` or `sync_client` as well as the `url` argument, they will take precedence over the `url` argument. If none of the `client` / `sync_client` / `url` arguments are provided, `RemoteGraph` will raise a `ValueError` at runtime.
</Note>

`RemoteGraph` implements the same Runnable interface as `CompiledGraph`, so you can use it in the same way as a compiled graph. It supports the full set of standard methods, including `.invoke()`, `.stream()`, `.get_state()`, and `.update_state()`, as well as their asynchronous variants.

<Note>
  To use the graph asynchronously, you must provide either the `url` or `client` when initializing the `RemoteGraph`.
</Note>

<Note>
  To use the graph synchronously, you must provide either the `url` or `sync_client` when initializing the `RemoteGraph`.
</Note>

<CodeGroup>
  
</CodeGroup>

## Persist state at the thread level

By default, graph runs (for example, calls made with `.invoke()` or `.stream()`) are stateless, which means that intermediate checkpoints and the final state are not persisted after a run.

If you want to preserve the outputs of a run—for example, to support human-in-the-loop workflows—you can create a thread and pass its ID through the `config` argument. This works the same way as with a regular compiled graph:

<Note>
  If you need to use a `checkpointer` with a graph that has a `RemoteGraph` subgraph node, make sure to use UUIDs as thread IDs.
</Note>

A graph can also call out to multiple `RemoteGraph` instances as [*subgraph*](/oss/python/langgraph/use-subgraphs) nodes. This allows for modular, scalable workflows where different responsibilities are split across separate graphs.

`RemoteGraph` exposes the same interface as a regular `CompiledGraph`, so you can use it directly as a subgraph inside another graph. For example:

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/use-remote-graph.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

### Use a client

<CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>

## Invoke the graph

`RemoteGraph` implements the same Runnable interface as `CompiledGraph`, so you can use it in the same way as a compiled graph. It supports the full set of standard methods, including `.invoke()`, `.stream()`, `.get_state()`, and `.update_state()`, as well as their asynchronous variants.

### Asynchronously

<Note>
  To use the graph asynchronously, you must provide either the `url` or `client` when initializing the `RemoteGraph`.
</Note>

<CodeGroup>
```

---

## How to kick off background runs

**URL:** llms-txt#how-to-kick-off-background-runs

**Contents:**
- Setup
- Check runs on thread
- Start runs on thread

Source: https://docs.langchain.com/langsmith/background-run

This guide covers how to kick off background runs for your agent.
This can be useful for long running jobs.

First let's set up our client and thread:

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

## Check runs on thread

If we list the current runs on this thread, we will see that it's empty:

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

## Start runs on thread

Now let's kick off a run:

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

The first time we poll it, we can see `status=pending`:

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

Now we can join the run, wait for it to finish and check that status again:

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

Perfect! The run succeeded as we would expect. We can double check that the run worked as expected by printing out the final state:

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

We can also just print the content of the last AIMessage:

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/background-run.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
</Tab>

  <Tab title="Javascript">
```

Example 2 (unknown):
```unknown
</Tab>

  <Tab title="CURL">
```

Example 3 (unknown):
```unknown
</Tab>
</Tabs>

Output:
```

Example 4 (unknown):
```unknown
## Check runs on thread

If we list the current runs on this thread, we will see that it's empty:

<Tabs>
  <Tab title="Python">
```

---

## How to return categorical vs numerical metrics

**URL:** llms-txt#how-to-return-categorical-vs-numerical-metrics

**Contents:**
- Related

Source: https://docs.langchain.com/langsmith/metric-type

LangSmith supports both categorical and numerical metrics, and you can return either when writing a custom evaluator.

For an evaluator result to be logged as a numerical metric, it must returned as:

* (Python only) an `int`, `float`, or `bool`
* a dict of the form `{"key": "metric_name", "score": int | float | bool}`

For an evaluator result to be logged as a categorical metric, it must be returned as:

* (Python only) a `str`
* a dict of the form `{"key": "metric_name", "value": str | int | float | bool}`

Here are some examples:

* Python: Requires `langsmith>=0.2.0`
* TypeScript: Support for multiple scores is available in `langsmith@0.1.32` and higher

* [Return multiple metrics in one evaluator](/langsmith/multiple-scores)

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/metric-type.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

---

## How to return multiple scores in one evaluator

**URL:** llms-txt#how-to-return-multiple-scores-in-one-evaluator

**Contents:**
- Related

Source: https://docs.langchain.com/langsmith/multiple-scores

Sometimes it is useful for a custom evaluator or summary evaluator to return multiple metrics. For example, if you have multiple metrics being generated by an LLM judge, you can save time and money by making a single LLM call that generates multiple metrics instead of making multiple LLM calls.

To return multiple scores using the Python SDK, simply return a list of dictionaries/objects of the following form:

To do so with the JS/TS SDK, return an object with a 'results' key and then a list of the above form

Each of these dictionaries can contain any or all of the [feedback fields](/langsmith/feedback-data-format); check out the linked document for more information.

* Python: Requires `langsmith>=0.2.0`
* TypeScript: Support for multiple scores is available in `langsmith@0.1.32` and higher

Rows from the resulting experiment will display each of the scores.

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiple-scores.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=7f0a67189b7202a46d5e093cce9ea283" alt="multiple_scores.png" data-og-width="1622" width="1622" data-og-height="1020" height="1020" data-path="langsmith/images/multiple-scores.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiple-scores.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=cb2f322c66eadd3ef0eef99a5c11063f 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiple-scores.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=05a52f0d760f2e079c6701e83c48fb73 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiple-scores.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=147f1a6046e9dabac499f68ec6abf68f 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiple-scores.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=0a807e000d2e2d7cd18b6af3e83e8a59 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiple-scores.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=f46442dafae69f9039fe1c9def5a1a01 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiple-scores.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=c182c52b39a42dbcab618fc8b2a0c525 2500w" />

* [Return categorical vs numerical metrics](/langsmith/metric-type)

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/multiple-scores.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
To do so with the JS/TS SDK, return an object with a 'results' key and then a list of the above form
```

Example 2 (unknown):
```unknown
Each of these dictionaries can contain any or all of the [feedback fields](/langsmith/feedback-data-format); check out the linked document for more information.

Example:

* Python: Requires `langsmith>=0.2.0`
* TypeScript: Support for multiple scores is available in `langsmith@0.1.32` and higher

<CodeGroup>
```

Example 3 (unknown):
```unknown

```

---

## How to run an evaluation asynchronously

**URL:** llms-txt#how-to-run-an-evaluation-asynchronously

**Contents:**
- Use `aevaluate()`

Source: https://docs.langchain.com/langsmith/evaluation-async

<Info>
  [Evaluations](/langsmith/evaluation-concepts#applying-evaluations) | [Evaluators](/langsmith/evaluation-concepts#evaluators) | [Datasets](/langsmith/evaluation-concepts#datasets) | [Experiments](/langsmith/evaluation-concepts#experiments)
</Info>

We can run evaluations asynchronously via the SDK using [aevaluate()](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._arunner.aevaluate), which accepts all of the same arguments as [evaluate()](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._runner.evaluate) but expects the application function to be asynchronous. You can learn more about how to use the `evaluate()` function [here](/langsmith/evaluate-llm-application).

<Info>
  This guide is only relevant when using the Python SDK. In JS/TS the `evaluate()` function is already async. You can see how to use it [here](/langsmith/evaluate-llm-application).
</Info>

Requires `langsmith>=0.3.13`

```python  theme={null}
from langsmith import wrappers, Client
from openai import AsyncOpenAI

---

## How to run an evaluation locally (Python only)

**URL:** llms-txt#how-to-run-an-evaluation-locally-(python-only)

**Contents:**
- Example

Source: https://docs.langchain.com/langsmith/local

Sometimes it is helpful to run an evaluation locally without uploading any results to LangSmith. For example, if you're quickly iterating on a prompt and want to smoke test it on a few examples, or if you're validating that your target and evaluator functions are defined correctly, you may not want to record these evaluations.

You can do this by using the LangSmith Python SDK and passing `upload_results=False` to `evaluate()` / `aevaluate()`.

This will run you application and evaluators exactly as it always does and return the same output, but nothing will be recorded to LangSmith. This includes not just the experiment results but also the application and evaluator traces.

Let's take a look at an example:

Requires `langsmith>=0.2.0`. Example also uses `pandas`.

```python  theme={null}
from langsmith import Client

---

## How to run a pairwise evaluation

**URL:** llms-txt#how-to-run-a-pairwise-evaluation

**Contents:**
- Prerequisites
- `evaluate()` comparative args
- Define a pairwise evaluator
  - Evaluator args
  - Evaluator output
- Run a pairwise evaluation
- View pairwise experiments

Source: https://docs.langchain.com/langsmith/evaluate-pairwise

<Info>
  Concept: [Pairwise evaluations](/langsmith/evaluation-concepts#pairwise)
</Info>

LangSmith supports evaluating **existing** experiments in a comparative manner. Instead of evaluating one output at a time, you can score the output from multiple experiments against each other. In this guide, you'll use [`evaluate()`](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._runner.evaluate) with two existing experiments to [define an evaluator](#define-a-pairwise-evaluator) and [run a pairwise evaluation](#run-a-pairwise-evaluation). Finally, you'll use the LangSmith UI to [view the pairwise experiments](#view-pairwise-experiments).

* If you haven't already created experiments to compare, check out the [quick start](/langsmith/evaluation-quickstart) or the [how-to guide](/langsmith/evaluate-llm-application) to get started with evaluations.
* This guide requires `langsmith` Python version `>=0.2.0` or JS version `>=0.2.9`.

<Info>
  You can also use [`evaluate_comparative()`](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._runner.evaluate_comparative) with more than two existing experiments.
</Info>

## `evaluate()` comparative args

At its simplest, `evaluate` / `aevaluate` function takes the following arguments:

| Argument     | Description                                                                                                                        |
| ------------ | ---------------------------------------------------------------------------------------------------------------------------------- |
| `target`     | A list of the two **existing experiments** you would like to evaluate against each other. These can be uuids or experiment names.  |
| `evaluators` | A list of the pairwise evaluators that you would like to attach to this evaluation. See the section below for how to define these. |

Along with these, you can also pass in the following optional args:

| Argument                                 | Description                                                                                                                                                                                                                                                                                                                                                                    |
| ---------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| `randomize_order` / `randomizeOrder`     | An optional boolean indicating whether the order of the outputs should be randomized for each evaluation. This is a strategy for minimizing positional bias in your prompt: often, the LLM will be biased towards one of the responses based on the order. This should mainly be addressed via prompt engineering, but this is another optional mitigation. Defaults to False. |
| `experiment_prefix` / `experimentPrefix` | A prefix to be attached to the beginning of the pairwise experiment name. Defaults to None.                                                                                                                                                                                                                                                                                    |
| `description`                            | A description of the pairwise experiment. Defaults to None.                                                                                                                                                                                                                                                                                                                    |
| `max_concurrency` / `maxConcurrency`     | The maximum number of concurrent evaluations to run. Defaults to 5.                                                                                                                                                                                                                                                                                                            |
| `client`                                 | The LangSmith client to use. Defaults to None.                                                                                                                                                                                                                                                                                                                                 |
| `metadata`                               | Metadata to attach to your pairwise experiment. Defaults to None.                                                                                                                                                                                                                                                                                                              |
| `load_nested` / `loadNested`             | Whether to load all child runs for the experiment. When False, only the root trace will be passed to your evaluator. Defaults to False.                                                                                                                                                                                                                                        |

## Define a pairwise evaluator

Pairwise evaluators are just functions with an expected signature.

Custom evaluator functions must have specific argument names. They can take any subset of the following arguments:

* `inputs: dict`: A dictionary of the inputs corresponding to a single example in a dataset.
* `outputs: list[dict]`: A two-item list of the dict outputs produced by each experiment on the given inputs.
* `reference_outputs` / `referenceOutputs: dict`: A dictionary of the reference outputs associated with the example, if available.
* `runs: list[Run]`: A two-item list of the full [Run](/langsmith/run-data-format) objects generated by the two experiments on the given example. Use this if you need access to intermediate steps or metadata about each run.
* `example: Example`: The full dataset [Example](/langsmith/example-data-format), including the example inputs, outputs (if available), and metadata (if available).

For most use cases you'll only need `inputs`, `outputs`, and `reference_outputs` / `referenceOutputs`. `runs` and `example` are useful only if you need some extra trace or example metadata outside of the actual inputs and outputs of the application.

Custom evaluators are expected to return one of the following types:

* `dict`: dictionary with keys:

* `key`, which represents the feedback key that will be logged
  * `scores`, which is a mapping from run ID to score for that run.
  * `comment`, which is a string. Most commonly used for model reasoning.

Currently Python only

* `list[int | float | bool]`: a two-item list of scores. The list is assumed to have the same order as the `runs` / `outputs` evaluator args. The evaluator function name is used for the feedback key.

Note that you should choose a feedback key that is distinct from standard feedbacks on your run. We recommend prefixing pairwise feedback keys with `pairwise_` or `ranked_`.

## Run a pairwise evaluation

The following example uses [a prompt](https://smith.langchain.com/hub/langchain-ai/pairwise-evaluation-2) which asks the LLM to decide which is better between two AI assistant responses. It uses structured output to parse the AI's response: 0, 1, or 2.

<Info>
  In the Python example below, we are pulling [this structured prompt](https://smith.langchain.com/hub/langchain-ai/pairwise-evaluation-2) from the [LangChain Hub](/langsmith/manage-prompts#public-prompt-hub) and using it with a LangChain chat model wrapper.

**Usage of LangChain is totally optional.** To illustrate this point, the TypeScript example uses the OpenAI SDK directly.
</Info>

* Python: Requires `langsmith>=0.2.0`
* TypeScript: Requires `langsmith>=0.2.9`

## View pairwise experiments

Navigate to the "Pairwise Experiments" tab from the dataset page:

<img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/pairwise-from-dataset.png?fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=dddf35fd971055d0d94ae4184c91dea3" alt="Pairwise Experiments Tab" data-og-width="3454" width="3454" data-og-height="1912" height="1912" data-path="langsmith/images/pairwise-from-dataset.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/pairwise-from-dataset.png?w=280&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=4c1677867b832da9c3b4338a210570f8 280w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/pairwise-from-dataset.png?w=560&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=80d4795bc999156850eb8092e8267c9f 560w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/pairwise-from-dataset.png?w=840&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=ed2e5fb624828fb649bf33473e7dc797 840w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/pairwise-from-dataset.png?w=1100&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=05c2248284b4efb2f5a9f38cffef0b9b 1100w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/pairwise-from-dataset.png?w=1650&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=3014131b2f5ae730aa354afaa7312316 1650w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/pairwise-from-dataset.png?w=2500&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=38c4f707158930cf7d1d155db4021362 2500w" />

Click on a pairwise experiment that you would like to inspect, and you will be brought to the Comparison View:

<img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/pairwise-comparison-view.png?fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=8afa7467faf707c0bb5ede23b007beda" alt="Pairwise Comparison View" data-og-width="3430" width="3430" data-og-height="1886" height="1886" data-path="langsmith/images/pairwise-comparison-view.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/pairwise-comparison-view.png?w=280&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=9a837cee527a1bf5dda5a77b8ce16ba6 280w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/pairwise-comparison-view.png?w=560&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=ebb39f8f2fb7a542d2273cfc64c5b4f4 560w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/pairwise-comparison-view.png?w=840&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=2f2de8c570a3e6401ba0220da343b3e0 840w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/pairwise-comparison-view.png?w=1100&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=cf612b4c6938e856b78c7476f8cc6304 1100w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/pairwise-comparison-view.png?w=1650&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=ff05d71cd12f19d0403e6a1e3e64609a 1650w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/pairwise-comparison-view.png?w=2500&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=5674d6403a3070935830983b9e36ac2f 2500w" />

You may filter to runs where the first experiment was better or vice versa by clicking the thumbs up/thumbs down buttons in the table header:

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-pairwise.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=677c48099cee9848d2119c154c7b0d88" alt="Pairwise Filtering" data-og-width="3454" width="3454" data-og-height="1914" height="1914" data-path="langsmith/images/filter-pairwise.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-pairwise.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=1ceff9156ccfdb48f246f41c7e0d16ab 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-pairwise.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=745f3dc2bed9e3e2d8333df0ff57a43e 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-pairwise.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=e81f10f544953ee39366866c1f4a5d71 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-pairwise.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=3f4af39e4da50d0ad081d03aaf7b238e 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-pairwise.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=63688564868c7a0989c429c6e740e014 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-pairwise.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=527537f492665790aa8380e0d75e7fb3 2500w" />

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/evaluate-pairwise.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

---

## How to run evaluations with pytest (beta)

**URL:** llms-txt#how-to-run-evaluations-with-pytest-(beta)

**Contents:**
- Installation
- Define and run tests
- Log inputs, outputs, and reference outputs
- Log feedback
- Trace intermediate calls
- Grouping tests into a test suite
- Naming experiments
- Caching
- pytest features
  - Parametrize with `pytest.mark.parametrize`

Source: https://docs.langchain.com/langsmith/pytest

The LangSmith pytest plugin lets Python developers define their datasets and evaluations as pytest test cases. Compared to the standard evaluation flow, this is useful when:

* Each example requires different evaluation logic
* You want to assert binary expectations, and both track these assertions in LangSmith and raise assertion errors locally (e.g. in CI pipelines)
* You want pytest-like terminal outputs
* You already use pytest to test your app and want to add LangSmith tracking

<Warning>
  The pytest integration is in beta and is subject to change in upcoming releases.
</Warning>

<Info>
  The JS/TS SDK has an analogous [Vitest/Jest integration](/langsmith/vitest-jest).
</Info>

This functionality requires Python SDK version `langsmith>=0.3.4`.

For extra features like [rich terminal outputs](#rich-outputs) and [test caching](#caching) install:

## Define and run tests

The pytest integration lets you define datasets and evaluators as test cases.

To track a test in LangSmith add the `@pytest.mark.langsmith` decorator. Every decorated test case will be synced to a dataset example. When you run the test suite, the dataset will be updated and a new experiment will be created with one result for each test case.

<CodeGroup>
  
</CodeGroup>

When you run this test it will have a default `pass` boolean feedback key based on the test case passing / failing. It will also track any inputs, outputs, and reference (expected) outputs that you log.

Use `pytest` as you normally would to run the tests:

In most cases we recommend setting a test suite name:

Each time you run this test suite, LangSmith:

* creates a [dataset](/langsmith/evaluation-concepts#datasets) for each test file. If a dataset for this test file already exists it will be updated
* creates an [experiment](/langsmith/evaluation-concepts#experiment) in each created/updated dataset
* creates an experiment row for each test case, with the inputs, outputs, reference outputs and feedback you've logged
* collects the pass/fail rate under the `pass` feedback key for each test case

Here's what a test suite dataset looks like:

<img src="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/simple-pytest-dataset.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=f40d29e4260eebc87838ea7be78bd08d" alt="Dataset" data-og-width="1078" width="1078" data-og-height="437" height="437" data-path="langsmith/images/simple-pytest-dataset.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/simple-pytest-dataset.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=ac625f42dd28d99e4fedaf193421d7f5 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/simple-pytest-dataset.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=169fc45042f5c75d61e5c9a0dc9117cf 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/simple-pytest-dataset.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=b63c8752c2356d297cc44969da407673 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/simple-pytest-dataset.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=f9a727e2fed9369844b4e6de59d72c0f 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/simple-pytest-dataset.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=e5d14240d3d0df568adb1655a28dbc58 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/simple-pytest-dataset.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=9adad58f251fc1dba96c9b6f4092a11f 2500w" />

And what an experiment against that test suite looks like:

<img src="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/simple-pytest.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=09776ac389e88f7f5058f4f7cf44dc72" alt="Experiment" data-og-width="1077" width="1077" data-og-height="444" height="444" data-path="langsmith/images/simple-pytest.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/simple-pytest.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=250e774ba91ea54112577a40466fdf51 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/simple-pytest.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=e77b0e75d32f47f9b9ab86df83f0ac99 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/simple-pytest.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=1b1bd6106aaed9412a2e56b52598e412 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/simple-pytest.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=65630e87475c1c8570e0d47d18fd1629 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/simple-pytest.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=a4507629b61206359c2546afb17ac77a 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/simple-pytest.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=a5ee150a76f3e40e704c7de4b2d74fdf 2500w" />

## Log inputs, outputs, and reference outputs

Every time we run a test we're syncing it to a dataset example and tracing it as a run. There's a few different ways that we can trace the example inputs and reference outputs and the run outputs. The simplest is to use the `log_inputs`, `log_outputs`, and `log_reference_outputs` methods. You can run these any time in a test to update the example and run for that test:

Running this test will create/update an example with name "test\_foo", inputs `{"a": 1, "b": 2}`, reference outputs `{"foo": "bar"}` and trace a run with outputs `{"foo": "baz"}`.

**NOTE**: If you run `log_inputs`, `log_outputs`, or `log_reference_outputs` twice, the previous values will be overwritten.

Another way to define example inputs and reference outputs is via pytest fixtures/parametrizations. By default any arguments to your test function will be logged as inputs on the corresponding example. If certain arguments are meant to represet reference outputs, you can specify that they should be logged as such using `@pytest.mark.langsmith(output_keys=["name_of_ref_output_arg"])`:

This will create/sync an example with name "test\_cd", inputs `{"c": 5}` and reference outputs `{"d": 6}`, and run output `{"d": 10}`.

By default LangSmith collects the pass/fail rate under the `pass` feedback key for each test case. You can add additional feedback with `log_feedback`.

Note the use of the `trace_feedback()` context manager. This makes it so that the LLM-as-judge call is traced separately from the rest of the test case. Instead of showing up in the main test case run it will instead show up in the trace for the `correct` feedback key.

**NOTE**: Make sure that the `log_feedback` call associated with the feedback trace occurs inside the `trace_feedback` context. This way we'll be able to associate the feedback with the trace, and when seeing the feedback in the UI you'll be able to click on it to see the trace that generated it.

## Trace intermediate calls

LangSmith will automatically trace any traceable intermediate calls that happen in the course of test case execution.

## Grouping tests into a test suite

By default, all tests within a given file will be grouped as a single "test suite" with a corresponding dataset. You can configure which test suite a test belongs to by passing the `test_suite_name` parameter to `@pytest.mark.langsmith` for case-by-case grouping, or you can set the `LANGSMITH_TEST_SUITE` env var to group all tests from an execution into a single test suite:

We generally recommend setting `LANGSMITH_TEST_SUITE` to get a consolidated view of all of your results.

## Naming experiments

You can name an experiment using the `LANGSMITH_EXPERIMENT` env var:

LLMs on every commit in CI can get expensive. To save time and resources, LangSmith lets you cache HTTP requests to disk. To enable caching, install with `langsmith[pytest]` and set an env var: `LANGSMITH_TEST_CACHE=/my/cache/path`:

All requests will be cached to `tests/cassettes` and loaded from there on subsequent runs. If you check this in to your repository, your CI will be able to use the cache as well.

In `langsmith>=0.4.10`, you may selectively enable caching for requests to individual URLs or hostnames like this:

`@pytest.mark.langsmith` is designed to stay out of your way and works well with familiar `pytest` features.

### Parametrize with `pytest.mark.parametrize`

You can use the `parametrize` decorator as before. This will create a new test case for each parametrized instance of the test.

**Note:** as the parametrized list grows, you may consider using `evaluate()` instead. This parallelizes the evaluation and makes it easier to control individual experiments and the corresponding dataset.

### Parallelize with `pytest-xdist`

You can use [pytest-xdist](https://pytest-xdist.readthedocs.io/en/stable/) as you normally would to parallelize test execution:

### Async tests with `pytest-asyncio`

`@pytest.mark.langsmith` works with sync or async tests, so you can run async tests exactly as before.

### Watch mode with `pytest-watch`

Use watch mode to quickly iterate on your tests. We *highly* recommend ony using this with test caching (see below) enabled to avoid unnecessary LLM calls:

If you'd like to see a rich display of the LangSmith results of your test run you can specify `--langsmith-output`:

**Note:** This flag used to be `--output=langsmith` in `langsmith<=0.3.3` but was updated to avoid collisions with other pytest plugins.

You'll get a nice table per test suite that updates live as the results are uploaded to LangSmith:

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rich-pytest-outputs.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=10712bc97e37900ca83cb70df1c9357d" alt="Rich pytest outputs" data-og-width="1340" width="1340" data-og-height="548" height="548" data-path="langsmith/images/rich-pytest-outputs.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rich-pytest-outputs.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=b689a6512f89045fdda11112f344c0aa 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rich-pytest-outputs.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=be71e1c2abb5e14616ab8f9c4cf05912 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rich-pytest-outputs.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=4d7beb30f4e5f72bc30def219831184d 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rich-pytest-outputs.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=85478a1e6830a5bc01b35eda991e86c7 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rich-pytest-outputs.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=2980269c433c3a30c0da34e57a6232ba 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rich-pytest-outputs.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=fc52452a3b17850b27d4922b392185f8 2500w" />

Some important notes for using this feature:

* Make sure you've installed `pip install -U "langsmith[pytest]"`
* Rich outputs do not currently work with `pytest-xdist`

**NOTE**: The custom output removes all the standard pytest outputs. If you're trying to debug some unexpected behavior it's often better to show the regular pytest outputs so to get full error traces.

If you want to run the tests without syncing the results to LangSmith, you can set `LANGSMITH_TEST_TRACKING=false` in your environment.

The tests will run as normal, but the experiment logs will not be sent to LangSmith.

LangSmith provides an [expect](https://docs.smith.langchain.com/reference/python/_expect/langsmith._expect._Expect#langsmith._expect._Expect) utility to help define expectations about your LLM output. For example:

This will log the binary "expectation" score to the experiment results, additionally `assert`ing that the expectation is met possibly triggering a test failure.

`expect` also provides "fuzzy match" methods. For example:

This test case will be assigned 4 scores:

1. The `embedding_distance` between the prediction and the expectation
2. The binary `expectation` score (1 if cosine distance is less than 0.5, 0 if not)
3. The `edit_distance` between the prediction and the expectation
4. The overall test pass/fail score (binary)

The `expect` utility is modeled off of [Jest](https://jestjs.io/docs/expect)'s expect API, with some off-the-shelf functionality to make it easier to grade your LLMs.

#### `@test` / `@unit` decorator

The legacy method for marking test cases is using the `@test` or `@unit` decorators:

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/pytest.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

## Define and run tests

The pytest integration lets you define datasets and evaluators as test cases.

To track a test in LangSmith add the `@pytest.mark.langsmith` decorator. Every decorated test case will be synced to a dataset example. When you run the test suite, the dataset will be updated and a new experiment will be created with one result for each test case.

<CodeGroup>
```

Example 3 (unknown):
```unknown
</CodeGroup>

When you run this test it will have a default `pass` boolean feedback key based on the test case passing / failing. It will also track any inputs, outputs, and reference (expected) outputs that you log.

Use `pytest` as you normally would to run the tests:
```

Example 4 (unknown):
```unknown
In most cases we recommend setting a test suite name:
```

---

## How to run evaluations with Vitest/Jest (beta)

**URL:** llms-txt#how-to-run-evaluations-with-vitest/jest-(beta)

**Contents:**
- Setup
  - Vitest
  - Jest
- Define and run evals
- Trace feedback
- Running multiple examples against a test case
- Log outputs
- Trace intermediate calls
- Focusing or skipping tests
- Configuring test suites

Source: https://docs.langchain.com/langsmith/vitest-jest

LangSmith provides integrations with Vitest and Jest that allow JavaScript and TypeScript developers define their datasets and evaluate using familiar syntax.

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/jest-vitest-reporter-output.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=94fd2a6f61c9dc386002fadbab7024a8" alt="Jest/Vitest reporter output" data-og-width="2200" width="2200" data-og-height="564" height="564" data-path="langsmith/images/jest-vitest-reporter-output.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/jest-vitest-reporter-output.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=cf56669ba6d6ab79ed6237424f163fa7 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/jest-vitest-reporter-output.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=af88b4a6c4d31520b783336f311f56fc 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/jest-vitest-reporter-output.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=32ee63fc2b8923236850f9b2a1fb1775 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/jest-vitest-reporter-output.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=e1f396ae3e1b68e358efc599f700e0c3 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/jest-vitest-reporter-output.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=77b9a394525825d5f9395cdb22a5c8b4 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/jest-vitest-reporter-output.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=2d22bf302eedc50c70280ce2d8bc7d79 2500w" />

Compared to the `evaluate()` evaluation flow, this is useful when:

* Each example requires different evaluation logic
* You want to assert binary expectations, and both track these assertions in LangSmith and raise assertion errors locally (e.g. in CI pipelines)
* You want to take advantage of mocks, watch mode, local results, or other features of the Vitest/Jest ecosystems

<Info>
  Requires JS/TS SDK version `langsmith>=0.3.1`.
</Info>

<Warning>
  The Vitest/Jest integrations are in beta and are subject to change in upcoming releases.
</Warning>

<Info>
  The Python SDK has an analogous [pytest integration](/langsmith/pytest).
</Info>

Set up the integrations as follows. Note that while you can add LangSmith evals alongside your other unit tests (as standard `*.test.ts` files) using your existing test config files, the below examples will also set up a separate test config file and command to run your evals. It will assume you end your test files with `.eval.ts`.

This ensures that the custom test reporter and other LangSmith touchpoints do not modify your existing test outputs.

Install the required development dependencies if you have not already:

The examples below also require `openai` (and of course `langsmith`!) as a dependency:

Then create a separate `ls.vitest.config.ts` file with the following base config:

* `include` ensures that only files ending with some variation of `eval.ts` in your project are run
* `reporters` is responsible for nicely formatting your output as shown above
* `setupFiles` runs `dotenv` to load environment variables before running your evals

<Warning>
  JSDom environments are not supported at this time. You should either omit the `"environment"` field from your config or set it to `"node"`.
</Warning>

Finally, add the following to the `scripts` field in your `package.json` to run Vitest with the config you just created:

Note that the above script disables Vitest's default watch mode for running evals since many evaluators may include longer running LLM calls.

Install the required development dependencies if you have not already:

The examples below also require `openai` (and of course `langsmith`!) as a dependency:

<Info>
  The setup instructions below are for basic JS files and CJS. To add support for TypeScript and ESM, see Jest's official docs or use [Vitest](#vitest).
</Info>

Then create a separate config file named `ls.jest.config.cjs`:

* `testMatch` ensures that only files ending with some variation of `eval.js` in your project are run
* `reporters` is responsible for nicely formatting your output as shown above
* `setupFiles` runs `dotenv` to load environment variables before running your evals

<Warning>
  JSDom environments are not supported at this time. You should either omit the `"testEnvironment"` field from your config or set it to `"node"`.
</Warning>

Finally, add the following to the `scripts` field in your `package.json` to run Jest with the config you just created:

## Define and run evals

You can now define evals as tests using familiar Vitest/Jest syntax, with a few caveats:

* You should import `describe` and `test` from the `langsmith/jest` or `langsmith/vitest` entrypoint
* You must wrap your test cases in a `describe` block
* When declaring tests, the signature is slightly different - there is an extra argument containing example inputs and expected outputs

Try it out by creating a file named `sql.eval.ts` (or `sql.eval.js` if you are using Jest without TypeScript) and pasting the below contents into it:

You can think of each `ls.test()` case as corresponding to a dataset example, and `ls.describe()` as defining a LangSmith dataset. If you have LangSmith [tracing environment variables](/#3-set-up-your-environment) set when you run the test suite, the SDK does the following:

* creates a [dataset](/langsmith/evaluation-concepts#datasets) with the same name as the name passed to `ls.describe()` in LangSmith if it does not exist
* creates an example in the dataset for each input and expected output passed into a test case if a matching one does not already exist
* creates a new [experiment](/langsmith/evaluation-concepts#experiment) with one result for each test case
* collects the pass/fail rate under the `pass` feedback key for each test case

When you run this test it will have a default `pass` boolean feedback key based on the test case passing / failing. It will also track any outputs that you log with the `ls.logOutputs()` or return from the test function as "actual" result values from your app for the experiment.

Create a `.env` file with your `OPENAI_API_KEY` and LangSmith credentials if you don't already have one:

Now use the `eval` script we set up in the previous step to run the test:

And your declared test should run!

Once it finishes, if you've set your LangSmith environment variables, you should see a link directing you to an experiment created in LangSmith alongside the test results.

Here's what an experiment against that test suite looks like:

<img src="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/simple-vitest.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=9cde688950dd2fc454a8514b02ed7268" alt="Experiment" data-og-width="2752" width="2752" data-og-height="902" height="902" data-path="langsmith/images/simple-vitest.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/simple-vitest.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=e583f4ee7179018b026ce9c037a05702 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/simple-vitest.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=3d87bf9ced639e2deb375f0638b1912e 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/simple-vitest.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=e1f92efbbffa880300575043180eb107 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/simple-vitest.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=90a9e603ea1b613b6a95f4a686cb954b 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/simple-vitest.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=335d4ba669f1a75d6c8171ce2d7cbb99 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/simple-vitest.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=9330a3bc998e80851ce3a162272b037d 2500w" />

By default LangSmith collects the pass/fail rate under the `pass` feedback key for each test case. You can add additional feedback with either `ls.logFeedback()` or `wrapEvaluator()`. To do so, try the following as your `sql.eval.ts` file (or `sql.eval.js` if you are using Jest without TypeScript):

Note the use of `ls.wrapEvaluator()` around the `myEvaluator` function. This makes it so that the LLM-as-judge call is traced separately from the rest of the test case to avoid clutter, and conveniently creates feedback if the return value from the wrapped function matches `{ key: string; score: number | boolean }`. In this case, instead of showing up in the main test case run, the evaluator trace will instead show up in a trace associated with the `correctness` feedback key.

You can see the evaluator runs in LangSmith by clicking their corresponding feedback chips in the UI.

## Running multiple examples against a test case

You can run the same test case over multiple examples and parameterize your tests using `ls.test.each()`. This is useful when you want to evaluate your app the same way against different inputs:

If you have tracking enabled, each example in the local dataset will be synced to the one created in LangSmith.

Every time we run a test we're syncing it to a dataset example and tracing it as a run. To trace final outputs for the run, you can use `ls.logOutputs()` like this:

The logged outputs will appear in your reporter summary and in LangSmith.

You can also directly return a value from your test function:

However keep in mind if you do this that if your test fails to complete due to a failed assertion or other error, your output will not appear.

## Trace intermediate calls

LangSmith will automatically trace any traceable intermediate calls that happen in the course of test case execution.

## Focusing or skipping tests

You can chain the Vitest/Jest `.skip` and `.only` methods on `ls.test()` and `ls.describe()`:

## Configuring test suites

You can configure test suites with values like metadata or a custom client by passing an extra argument to `ls.describe()` for the full suite or by passing a `config` field into `ls.test()` for individual tests:

The test suite will also automatically extract environment variables from `process.env.ENVIRONMENT`, `process.env.NODE_ENV` and `process.env.LANGSMITH_ENVIRONMENT` and set them as metadata on created experiments. You can then filter experiments by metadata in LangSmith's UI.

See [the API refs](https://docs.smith.langchain.com/reference/js/functions/vitest.describe) for a full list of configuration options.

If you want to run the tests without syncing the results to LangSmith, you can set omit your LangSmith tracing environment variables or set `LANGSMITH_TEST_TRACKING=false` in your environment.

The tests will run as normal, but the experiment logs will not be sent to LangSmith.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/vitest-jest.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
</CodeGroup>

The examples below also require `openai` (and of course `langsmith`!) as a dependency:

<CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## How to run multiple agents on the same thread

**URL:** llms-txt#how-to-run-multiple-agents-on-the-same-thread

**Contents:**
- Setup
- Run assistants on thread
  - Run OpenAI assistant
  - Run default assistant

Source: https://docs.langchain.com/langsmith/same-thread

In LangSmith Deployment, a thread is not explicitly associated with a particular agent.
This means that you can run multiple agents on the same thread, which allows a different agent to continue from an initial agent's progress.

In this example, we will create two agents and then call them both on the same thread.
You'll see that the second agent will respond using information from the [checkpoint](/oss/python/langgraph/graph-api#checkpointer-state) generated in the thread by the first agent as context.

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

We can see that these agents are different:

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

## Run assistants on thread

### Run OpenAI assistant

We can now run the OpenAI assistant on the thread first.

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

### Run default assistant

Now, we can run it on the default assistant and see that this second assistant is aware of the initial question, and can answer the question, "and you?":

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/same-thread.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
</Tab>

  <Tab title="Javascript">
```

Example 2 (unknown):
```unknown
</Tab>

  <Tab title="CURL">
```

Example 3 (unknown):
```unknown
</Tab>
</Tabs>

We can see that these agents are different:

<Tabs>
  <Tab title="Python">
```

Example 4 (unknown):
```unknown
</Tab>

  <Tab title="Javascript">
```

---

## How to set up an application with pyproject.toml

**URL:** llms-txt#how-to-set-up-an-application-with-pyproject.toml

**Contents:**
- Specify dependencies
- Specify environment variables
- Define graphs

Source: https://docs.langchain.com/langsmith/setup-pyproject

An application must be configured with a [configuration file](/langsmith/cli#configuration-file) in order to be deployed to LangSmith (or to be self-hosted). This how-to guide discusses the basic steps to set up an application for deployment using `pyproject.toml` to define your package's dependencies.

This example is based on [this repository](https://github.com/langchain-ai/langgraph-example-pyproject), which uses the LangGraph framework.

The final repository structure will look something like this:

<Tip>
  LangSmith Deployment supports deploying a [LangGraph](/oss/python/langgraph/overview) *graph*. However, the implementation of a *node* of a graph can contain arbitrary Python code. This means any framework can be implemented within a node and deployed on LangSmith Deployment. This lets you keep your core application logic outside LangGraph while still using LangSmith for [deployment](/langsmith/deployments), scaling, and [observability](/langsmith/observability).
</Tip>

You can also set up with:

* `requirements.txt`: for dependency management, check out [this how-to guide](/langsmith/setup-app-requirements-txt) on using `requirements.txt` for LangSmith.
* a monorepo: To deploy a graph located inside a monorepo, take a look at [this repository](https://github.com/langchain-ai/langgraph-example-monorepo) for an example of how to do so.

After each step, an example file directory is provided to demonstrate how code can be organized.

## Specify dependencies

Dependencies can optionally be specified in one of the following files: `pyproject.toml`, `setup.py`, or `requirements.txt`. If none of these files is created, then dependencies can be specified later in the [configuration file](#create-the-configuration-file).

The dependencies below will be included in the image, you can also use them in your code, as long as with a compatible version range:

Example `pyproject.toml` file:

Example file directory:

## Specify environment variables

Environment variables can optionally be specified in a file (e.g. `.env`). See the [Environment Variables reference](/langsmith/env-var) to configure additional variables for a deployment.

Example file directory:

<Tip>
  By default, LangSmith follows the `uv`/`pip` behavior of **not** installing prerelease versions unless explicitly allowed. If want to use prereleases, you have the following options:

* With `pyproject.toml`: add `allow-prereleases = true` to your `[tool.uv]` section.
  * With `requirements.txt` or `setup.py`: you must explicitly specify every prerelease dependency, including transitive ones. For example, if you declare `a==0.0.1a1` and `a` depends on `b==0.0.1a1`, then you must also explicitly include `b==0.0.1a1` in your dependencies.
</Tip>

Implement your graphs. Graphs can be defined in a single file or multiple files. Make note of the variable names of each [CompiledStateGraph](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.CompiledStateGraph) to be included in the application. The variable names will be used later when creating the [configuration file](/langsmith/cli#configuration-file).

Example `agent.py` file, which shows how to import from other modules you define (code for the modules is not shown here, please see [this repository](https://github.com/langchain-ai/langgraph-example-pyproject) to see their implementation):

```python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
<Tip>
  LangSmith Deployment supports deploying a [LangGraph](/oss/python/langgraph/overview) *graph*. However, the implementation of a *node* of a graph can contain arbitrary Python code. This means any framework can be implemented within a node and deployed on LangSmith Deployment. This lets you keep your core application logic outside LangGraph while still using LangSmith for [deployment](/langsmith/deployments), scaling, and [observability](/langsmith/observability).
</Tip>

You can also set up with:

* `requirements.txt`: for dependency management, check out [this how-to guide](/langsmith/setup-app-requirements-txt) on using `requirements.txt` for LangSmith.
* a monorepo: To deploy a graph located inside a monorepo, take a look at [this repository](https://github.com/langchain-ai/langgraph-example-monorepo) for an example of how to do so.

After each step, an example file directory is provided to demonstrate how code can be organized.

## Specify dependencies

Dependencies can optionally be specified in one of the following files: `pyproject.toml`, `setup.py`, or `requirements.txt`. If none of these files is created, then dependencies can be specified later in the [configuration file](#create-the-configuration-file).

The dependencies below will be included in the image, you can also use them in your code, as long as with a compatible version range:
```

Example 2 (unknown):
```unknown
Example `pyproject.toml` file:
```

Example 3 (unknown):
```unknown
Example file directory:
```

Example 4 (unknown):
```unknown
## Specify environment variables

Environment variables can optionally be specified in a file (e.g. `.env`). See the [Environment Variables reference](/langsmith/env-var) to configure additional variables for a deployment.

Example `.env` file:
```

---

## How to set up an application with requirements.txt

**URL:** llms-txt#how-to-set-up-an-application-with-requirements.txt

**Contents:**
- Specify dependencies
- Specify environment variables
- Define graphs

Source: https://docs.langchain.com/langsmith/setup-app-requirements-txt

An application must be configured with a [configuration file](/langsmith/cli#configuration-file) in order to be deployed to LangSmith (or to be self-hosted). This how-to guide discusses the basic steps to set up an application for deployment using `requirements.txt` to specify project dependencies.

This example is based on [this repository](https://github.com/langchain-ai/langgraph-example), which uses the LangGraph framework.

The final repository structure will look something like this:

<Tip>
  LangSmith Deployment supports deploying a [LangGraph](/oss/python/langgraph/overview) *graph*. However, the implementation of a *node* of a graph can contain arbitrary Python code. This means any framework can be implemented within a node and deployed on LangSmith Deployment. This lets you keep your core application logic outside LangGraph while still using LangSmith for [deployment](/langsmith/deployments), scaling, and [observability](/langsmith/observability).
</Tip>

You can also set up with:

* `pyproject.toml`: If you prefer using poetry for dependency management, check out [this how-to guide](/langsmith/setup-app-requirements-txt) on using `pyproject.toml` for LangSmith.
* a monorepo: If you are interested in deploying a graph located inside a monorepo, take a look at [this repository](https://github.com/langchain-ai/langgraph-example-monorepo) for an example of how to do so.

After each step, an example file directory is provided to demonstrate how code can be organized.

## Specify dependencies

Dependencies can optionally be specified in one of the following files: `pyproject.toml`, `setup.py`, or `requirements.txt`. If none of these files is created, then dependencies can be specified later in the [configuration file](#create-the-configuration-file).

The dependencies below will be included in the image, you can also use them in your code, as long as with a compatible version range:

Example `requirements.txt` file:

Example file directory:

## Specify environment variables

Environment variables can optionally be specified in a file (e.g. `.env`). See the [Environment Variables reference](/langsmith/env-var) to configure additional variables for a deployment.

Example file directory:

<Tip>
  By default, LangSmith follows the `uv`/`pip` behavior of **not** installing prerelease versions unless explicitly allowed. If want to use prereleases, you have the following options:

* With `pyproject.toml`: add `allow-prereleases = true` to your `[tool.uv]` section.
  * With `requirements.txt` or `setup.py`: you must explicitly specify every prerelease dependency, including transitive ones. For example, if you declare `a==0.0.1a1` and `a` depends on `b==0.0.1a1`, then you must also explicitly include `b==0.0.1a1` in your dependencies.
</Tip>

Implement your graphs. Graphs can be defined in a single file or multiple files. Make note of the variable names of each [CompiledStateGraph](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.CompiledStateGraph) to be included in the application. The variable names will be used later when creating the [LangGraph configuration file](/langsmith/cli#configuration-file).

Example `agent.py` file, which shows how to import from other modules you define (code for the modules is not shown here, please see [this repository](https://github.com/langchain-ai/langgraph-example) to see their implementation):

```python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
<Tip>
  LangSmith Deployment supports deploying a [LangGraph](/oss/python/langgraph/overview) *graph*. However, the implementation of a *node* of a graph can contain arbitrary Python code. This means any framework can be implemented within a node and deployed on LangSmith Deployment. This lets you keep your core application logic outside LangGraph while still using LangSmith for [deployment](/langsmith/deployments), scaling, and [observability](/langsmith/observability).
</Tip>

You can also set up with:

* `pyproject.toml`: If you prefer using poetry for dependency management, check out [this how-to guide](/langsmith/setup-app-requirements-txt) on using `pyproject.toml` for LangSmith.
* a monorepo: If you are interested in deploying a graph located inside a monorepo, take a look at [this repository](https://github.com/langchain-ai/langgraph-example-monorepo) for an example of how to do so.

After each step, an example file directory is provided to demonstrate how code can be organized.

## Specify dependencies

Dependencies can optionally be specified in one of the following files: `pyproject.toml`, `setup.py`, or `requirements.txt`. If none of these files is created, then dependencies can be specified later in the [configuration file](#create-the-configuration-file).

The dependencies below will be included in the image, you can also use them in your code, as long as with a compatible version range:
```

Example 2 (unknown):
```unknown
Example `requirements.txt` file:
```

Example 3 (unknown):
```unknown
Example file directory:
```

Example 4 (unknown):
```unknown
## Specify environment variables

Environment variables can optionally be specified in a file (e.g. `.env`). See the [Environment Variables reference](/langsmith/env-var) to configure additional variables for a deployment.

Example `.env` file:
```

---

## How to set up a JavaScript application

**URL:** llms-txt#how-to-set-up-a-javascript-application

**Contents:**
- Specify dependencies
- Specify environment variables
- Define graphs
- Create the API config
- Next

Source: https://docs.langchain.com/langsmith/setup-javascript

An application must be configured with a [configuration file](/langsmith/cli#configuration-file) in order to be deployed to LangSmith (or to be self-hosted). This how-to guide discusses the basic steps to set up a JavaScript application for deployment using `package.json` to specify project dependencies.

This walkthrough is based on [this repository](https://github.com/langchain-ai/langgraphjs-studio-starter), which you can play around with to learn more about how to set up your application for deployment.

The final repository structure will look something like this:

<Tip>
  LangSmith Deployment supports deploying a [LangGraph](/oss/python/langgraph/overview) *graph*. However, the implementation of a *node* of a graph can contain arbitrary Python code. This means any framework can be implemented within a node and deployed on LangSmith Deployment. This lets you keep your core application logic outside LangGraph while still using LangSmith for [deployment](/langsmith/deployments), scaling, and [observability](/langsmith/observability).
</Tip>

After each step, an example file directory is provided to demonstrate how code can be organized.

## Specify dependencies

Dependencies can be specified in a `package.json`. If none of these files is created, then dependencies can be specified later in the [configuration file](#create-the-api-config).

Example `package.json` file:

When deploying your app, the dependencies will be installed using the package manager of your choice, provided they adhere to the compatible version ranges listed below:

Example file directory:

## Specify environment variables

Environment variables can optionally be specified in a file (e.g. `.env`). See the [Environment Variables reference](/langsmith/env-var) to configure additional variables for a deployment.

Example file directory:

Implement your graphs. Graphs can be defined in a single file or multiple files. Make note of the variable names of each compiled graph to be included in the application. The variable names will be used later when creating the [configuration file](/langsmith/cli#configuration-file).

Here is an example `agent.ts`:

Example file directory:

## Create the API config

Create a [configuration file](/langsmith/cli#configuration-file) called `langgraph.json`. See the [configuration file reference](/langsmith/cli#configuration-file) for detailed explanations of each key in the JSON object of the configuration file.

Example `langgraph.json` file:

Note that the variable name of the `CompiledGraph` appears at the end of the value of each subkey in the top-level `graphs` key (i.e. `:<variable_name>`).

<Info>
  **Configuration Location**
  The configuration file must be placed in a directory that is at the same level or higher than the TypeScript files that contain compiled graphs and associated dependencies.
</Info>

After you setup your project and place it in a GitHub repository, it's time to [deploy your app](/langsmith/deployment-quickstart).

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/setup-javascript.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
<Tip>
  LangSmith Deployment supports deploying a [LangGraph](/oss/python/langgraph/overview) *graph*. However, the implementation of a *node* of a graph can contain arbitrary Python code. This means any framework can be implemented within a node and deployed on LangSmith Deployment. This lets you keep your core application logic outside LangGraph while still using LangSmith for [deployment](/langsmith/deployments), scaling, and [observability](/langsmith/observability).
</Tip>

After each step, an example file directory is provided to demonstrate how code can be organized.

## Specify dependencies

Dependencies can be specified in a `package.json`. If none of these files is created, then dependencies can be specified later in the [configuration file](#create-the-api-config).

Example `package.json` file:
```

Example 2 (unknown):
```unknown
When deploying your app, the dependencies will be installed using the package manager of your choice, provided they adhere to the compatible version ranges listed below:
```

Example 3 (unknown):
```unknown
Example file directory:
```

Example 4 (unknown):
```unknown
## Specify environment variables

Environment variables can optionally be specified in a file (e.g. `.env`). See the [Environment Variables reference](/langsmith/env-var) to configure additional variables for a deployment.

Example `.env` file:
```

---

## How to simulate multi-turn interactions

**URL:** llms-txt#how-to-simulate-multi-turn-interactions

**Contents:**
- Setup
- Running a simulation
- Running in LangSmith experiments
  - Using `pytest` or `Vitest/Jest`
  - Using `evaluate`
- Modifying the simulated user persona
- Next steps

Source: https://docs.langchain.com/langsmith/multi-turn-simulation

<Info>
  * [Multi-turn interactions](/langsmith/evaluation-concepts#multi-turn-interactions)
  * [Evaluators](/langsmith/evaluation-concepts#evaluators)
  * [LLM-as-judge](/langsmith/evaluation-concepts#llm-as-judge)
  * [OpenEvals](https://github.com/langchain-ai/openevals)
</Info>

AI applications with conversational interfaces, like chatbots, operate over multiple interactions with a user, also called conversation *turns*. When evaluating the performance of such applications, core concepts such as [building a dataset](/langsmith/evaluation-concepts#datasets) and defining [evaluators](/langsmith/evaluation-concepts#evaluators) and metrics to judge your app outputs remain useful. However, you may also find it useful to run a *simulation* between your app and a user, then evaluate this dynamically created trajectory.

Some advantages of doing this are:

* Ease of getting started vs. an evaluation over a full dataset of pre-existing trajectories
* End-to-end coverage from an initial query until a successful or unsuccessful resolution
* The ability to detect repetitive behavior or context loss over several iterations of your app

The downside is that because you are broadening your evaluation surface area to contain multiple turns, there is less consistency than evaluating a single output from your app given a static input from a dataset.

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-trace.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=c903f388600ab575e70edb92209c6b2e" alt="" data-og-width="2952" width="2952" data-og-height="1790" height="1790" data-path="langsmith/images/multi-turn-trace.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-trace.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=a5113736dc83834150a2b414619626b2 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-trace.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=f0bfc1f1764c80efcdebfcc07149ef8a 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-trace.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=53e53c0f05f5638b5e17576c0f37d195 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-trace.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=dc05ed0dcfe9d6da5b8872df189f6ed9 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-trace.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=7fbd598ec4a113bb143d0a0a1ca68a91 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-trace.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=a9dcabf49b84ba2ad8aac4159b5b0657 2500w" />

This guide will show you how to simulate multi-turn interactions and evaluate them using the open-source [`openevals`](https://github.com/langchain-ai/openevals) package, which contains prebuilt evaluators and other convenient resources for evaluating your AI apps. It will also use OpenAI models, though you can use other providers as well.

First, ensure you have the required dependencies installed:

<Info>
  If you are using `yarn` as your package manager, you will also need to manually install `@langchain/core` as a peer dependency of `openevals`. This is not required for LangSmith evals in general.
</Info>

And set up your environment variables:

## Running a simulation

There are two primary components you'll need to get started:

* `app`: Your application, or a function wrapping it. Must accept a single chat message (dict with "role" and "content" keys) as an input arg and a `thread_id` as a kwarg. Should accept other kwargs as more may be added in future releases. Returns a chat message as output with at least role and content keys.
* `user`: The simulated user. In this guide, we will use an imported prebuilt function named `create_llm_simulated_user` which uses an LLM to generate user responses, though you can [create your own too](https://github.com/langchain-ai/openevals?tab=readme-ov-file#custom-simulated-users).

The simulator in `openevals` passes a single chat message to your `app` from the `user` for each turn. Therefore you should statefully track the current history internally based on `thread_id` if needed.

Here's an example that simulates a multi-turn customer support interaction. This guide uses a simple chat app that wraps a single call to the OpenAI chat completions API, however this is where you would call your application or agent. In this example, our simulated user is playing the role of a particularly aggressive customer:

The response looks like this:

The simulation first generates an initial query from the simulated `user`, then passes response chat messages back and forth until it reaches `max_turns` (you can alternatively pass a `stopping_condition` that takes the current trajectory and returns `True` or `False` - [see the OpenEvals README for more information](https://github.com/langchain-ai/openevals?tab=readme-ov-file#multiturn-simulation)). The return value is the final list of chat messages that make up the converation's **trajectory**.

<Info>
  There are several ways to configure the simulated user, such as having it return fixed responses for the first turns of your simulation, as well as the simulation as a whole. For full details, check out [the OpenEvals README](https://github.com/langchain-ai/openevals?tab=readme-ov-file#multiturn-simulation).
</Info>

The final trace will look something [like this](https://smith.langchain.com/public/648ca37d-1c4d-4f7b-9b6a-89e35dc5d4f0/r) with responses from your `app` and `user` interleaved:

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-trace.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=c903f388600ab575e70edb92209c6b2e" alt="" data-og-width="2952" width="2952" data-og-height="1790" height="1790" data-path="langsmith/images/multi-turn-trace.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-trace.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=a5113736dc83834150a2b414619626b2 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-trace.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=f0bfc1f1764c80efcdebfcc07149ef8a 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-trace.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=53e53c0f05f5638b5e17576c0f37d195 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-trace.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=dc05ed0dcfe9d6da5b8872df189f6ed9 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-trace.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=7fbd598ec4a113bb143d0a0a1ca68a91 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-trace.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=a9dcabf49b84ba2ad8aac4159b5b0657 2500w" />

Congrats! You just ran your first multi-turn simulation. Next, we'll cover how to run it in a LangSmith experiment.

## Running in LangSmith experiments

You can use the results of multi-turn simulations as part of a LangSmith experiment to track performance and progress over time. For these sections, it helps to be familiar with at least one of LangSmith's [`pytest`](/langsmith/pytest) (Python-only), [`Vitest`/`Jest`](/langsmith/vitest-jest) (JS only), or [`evaluate`](/langsmith/evaluate-llm-application) runners.

### Using `pytest` or `Vitest/Jest`

<Check>
  See the following guides to learn how to set up evals using LangSmith's integrations with test frameworks:

* [`pytest`](https://docs.smith.langchain.com/langsmith/pytest)
  * [`Vitest` or `Jest`](https://docs.smith.langchain.com/langsmith/vitest-jest)
</Check>

If you are using one of the [LangSmith test framework integrations](/langsmith/pytest), you can pass in an array of OpenEvals evaluators as a `trajectory_evaluators` param when running the simulation. These evaluators will run at the end of the simulation, taking the final list of chat messages as an `outputs` kwarg. Your passed `trajectory_evaluator` must therefore accept this kwarg.

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-vitest.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=04c56e67e7bb9f01cb905d8a184d62d5" alt="" data-og-width="3448" width="3448" data-og-height="1128" height="1128" data-path="langsmith/images/multi-turn-vitest.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-vitest.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=41ac969c6ceb99ac0976ab3027b00e89 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-vitest.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=c10eece031225173dc0ded446e3e2e3c 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-vitest.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=90bda951b2cfa02bde0c8ad204a7dac7 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-vitest.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=2ca95c911f68412eb09e2f8a0a6b42e4 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-vitest.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=12be96d7f216cd8ce664c01f61f45288 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-vitest.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=1faa3a34b3daf67705e2afeca748e353 2500w" />

LangSmith will automatically detect and log the feedback returned from the passed `trajectory_evaluators`, adding it to the experiment. Note also that the test case uses the `fixed_responses` param on the simulated user to start the conversation with a specific input, which you can log and make part of your stored dataset.

You may also find it convenient to have the simulated user's system prompt to be part of your logged dataset as well.

You can also use the [`evaluate`](/langsmith/evaluate-llm-application) runner to evaluate simulated multi-turn interactions. This will be a little bit different from the `pytest`/`Vitest`/`Jest` example in the following ways:

* The simulation should be part of your `target` function, and your target function should return the final trajectory.
  * This will make the trajectory the `outputs` that LangSmith will pass to your evaluators.
* Instead of using the `trajectory_evaluators` param, you should pass your evaluators as a param into the `evaluate()` method.
* You will need an existing dataset of inputs and (optionally) reference trajectories.

## Modifying the simulated user persona

The above examples run using the same simulated user persona for all input examples, defined by the `system` parameter passed into `create_llm_simulated_user`. If you would like to use a different persona for specific items in your dataset, you can update your dataset examples to also contain an extra field with the desired `system` prompt, then pass that field in when creating your simulated user like this:

You've just seen some techniques for simulating multi-turn interactions and running them in LangSmith evals.

Here are some topics you might want to explore next:

* [Trace multiturn conversations across different traces](/langsmith/threads)
* [Use multiple messages in the playground UI](/langsmith/multiple-messages)
* [Return multiple metrics in one evaluator](/langsmith/multiple-scores)

You can also explore the [OpenEvals readme](https://github.com/langchain-ai/openevals) for more on prebuilt evaluators.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/multi-turn-simulation.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

<Info>
  If you are using `yarn` as your package manager, you will also need to manually install `@langchain/core` as a peer dependency of `openevals`. This is not required for LangSmith evals in general.
</Info>

And set up your environment variables:
```

Example 3 (unknown):
```unknown
## Running a simulation

There are two primary components you'll need to get started:

* `app`: Your application, or a function wrapping it. Must accept a single chat message (dict with "role" and "content" keys) as an input arg and a `thread_id` as a kwarg. Should accept other kwargs as more may be added in future releases. Returns a chat message as output with at least role and content keys.
* `user`: The simulated user. In this guide, we will use an imported prebuilt function named `create_llm_simulated_user` which uses an LLM to generate user responses, though you can [create your own too](https://github.com/langchain-ai/openevals?tab=readme-ov-file#custom-simulated-users).

The simulator in `openevals` passes a single chat message to your `app` from the `user` for each turn. Therefore you should statefully track the current history internally based on `thread_id` if needed.

Here's an example that simulates a multi-turn customer support interaction. This guide uses a simple chat app that wraps a single call to the OpenAI chat completions API, however this is where you would call your application or agent. In this example, our simulated user is playing the role of a particularly aggressive customer:

<CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## How to sync prompts with GitHub

**URL:** llms-txt#how-to-sync-prompts-with-github

**Contents:**
- Prerequisites
- Understanding LangSmith "Prompt Commits" and webhooks
- Implementing a FastAPI server for webhook reception
- Configuring the webhook in LangSmith
- The workflow in action
- Beyond a simple commit

Source: https://docs.langchain.com/langsmith/prompt-commit

LangSmith provides a collaborative interface to create, test, and iterate on prompts.

While you can [dynamically fetch prompts](/langsmith/manage-prompts-programmatically#pull-a-prompt) from LangSmith into your application at runtime, you may prefer to sync prompts with your own database or version control system. To support this workflow, LangSmith allows you to receive notifications of prompt updates via webhooks.

**Why sync prompts with GitHub?**

* **Version Control:** Keep your prompts versioned alongside your application code in a familiar system.
* **CI/CD Integration:** Trigger automated staging or production deployments when critical prompts change.

<img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-excalidraw.png?fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=a7fd1ae2a70f91c14298803a48785f89" alt="Prompt Webhook Diagram" data-og-width="1336" width="1336" data-og-height="343" height="343" data-path="langsmith/images/prompt-excalidraw.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-excalidraw.png?w=280&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=02f868ec42337b43a533f23effa76417 280w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-excalidraw.png?w=560&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=68806f6d9d1e5b8dbaf49d98294190da 560w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-excalidraw.png?w=840&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=133aeb6d3a880b989c2246632b8c0d5d 840w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-excalidraw.png?w=1100&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=fc7091cce5fec3407e005f798db85544 1100w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-excalidraw.png?w=1650&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=d789978f57574ec4f0c75beac19fc10c 1650w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-excalidraw.png?w=2500&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=d6362e5787b7fd1a5db69b3d704912bb 2500w" />

Before we begin, ensure you have the following set up:

1. **GitHub Account:** A standard GitHub account.

2. **GitHub Repository:** Create a new (or choose an existing) repository where your LangSmith prompt manifests will be stored. This could be the same repository as your application code or a dedicated one for prompts.

3. **GitHub Personal Access Token (PAT):**

* LangSmith webhooks don't directly interact with GitHub—they call an intermediary server that *you* create.
   * This server requires a GitHub PAT to authenticate and make commits to your repository.
   * Must include the `repo` scope (`public_repo` is sufficient for public repositories).
   * Go to **GitHub > Settings > Developer settings > Personal access tokens > Tokens (classic)**.
   * Click **Generate new token (classic)**.
   * Name it (e.g., "LangSmith Prompt Sync"), set an expiration, and select the required scopes.
   * Click **Generate token** and **copy it immediately** — it won't be shown again.
   * Store the token securely and provide it as an environment variable to your server.

## Understanding LangSmith "Prompt Commits" and webhooks

In LangSmith, when you save changes to a prompt, you're essentially creating a new version or a "Prompt Commit." These commits are what can trigger webhooks.

The webhook will send a JSON payload containing the new **prompt manifest**.

<Accordion title="Sample Webhook Payload">
  
</Accordion>

<Note>
  It's important to understand that LangSmith webhooks for prompt commits are generally triggered at the **workspace level**. This means if *any* prompt within your LangSmith workspace is modified and a "prompt commit" is saved, the webhook will fire and send the updated manifest of the prompt. The payloads are identifiable by prompt id. Your receiving server should be designed with this in mind.
</Note>

## Implementing a FastAPI server for webhook reception

To effectively process webhook notifications from LangSmith when prompts are updated, an intermediary server application is necessary. This server will act as the receiver for HTTP POST requests sent by LangSmith. For demonstration purposes in this guide, we will outline the creation of a simple FastAPI application to fulfill this role.

This publicly accessible server will be responsible for:

1. **Receiving Webhook Requests:** Listening for incoming HTTP POST requests.
2. **Parsing Payloads:** Extracting and interpreting the JSON-formatted prompt manifest from the request body.
3. **Committing to GitHub:** Programmatically creating a new commit in your specified GitHub repository, containing the updated prompt manifest. This ensures your prompts remain version-controlled and synchronized with changes made in LangSmith.

For deployment, platforms like [Render.com](https://render.com/) (offering a suitable free tier), Vercel, Fly.io, or other cloud providers (AWS, GCP, Azure) can be utilized to host the FastAPI application and obtain a public URL.

The server's core functionality will include an endpoint for webhook reception, logic for parsing the manifest, and integration with the GitHub API (using a Personal Access Token for authentication) to manage commits.

<Accordion title="Minimal FastAPI Server Code ()">
  `main.py`

This server will listen for incoming webhooks from LangSmith and commit the received prompt manifest to your GitHub repository.

**Key aspects of this server:**

* **Configuration (`.env`):** It expects a `.env` file with your `GITHUB_TOKEN`, `GITHUB_REPO_OWNER`, and `GITHUB_REPO_NAME`. You can also customize `GITHUB_FILE_PATH` (default: `LangSmith_prompt_manifest.json`) and `GITHUB_BRANCH` (default: `main`).
  * **GitHub Interaction:** The `commit_manifest_to_github` function handles the logic of fetching the current file's SHA (to update it) and then committing the new manifest content.
  * **Webhook Endpoint (`/webhook/commit`):** This is the URL path your LangSmith webhook will target.
  * **Error Handling:** Basic error handling for GitHub API interactions is included.

**Deploy this server to your chosen platform (e.g., Render) and note down its public URL (e.g., `https://prompt-commit-webhook.onrender.com`).**
</Accordion>

## Configuring the webhook in LangSmith

Once your FastAPI server is deployed and you have its public URL, you can configure the webhook in LangSmith:

1. Navigate to your LangSmith workspace.

2. Go to the **Prompts** section. Here you'll see a list of your prompts.

<img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-commit-main.png?fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=7e61c83cdd67749970d8f0e401066d60" alt="LangSmith Prompts section" data-og-width="2996" width="2996" data-og-height="852" height="852" data-path="langsmith/images/prompt-commit-main.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-commit-main.png?w=280&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=f492027577eacd4131954de447fa77f2 280w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-commit-main.png?w=560&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=b5f7ea835f9cddd0724a209869c2512e 560w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-commit-main.png?w=840&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=9aceebb242173d56a79e4974eb0fc7cd 840w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-commit-main.png?w=1100&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=2ec5c08e0a4682b84a202ebb52bf981d 1100w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-commit-main.png?w=1650&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=7dce8f5f164d40ab0f47dbf7e308382e 1650w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-commit-main.png?w=2500&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=03f8d9d0d9368ec370a71f7605f019a2 2500w" />

3. On the top right of the Prompts page, click the **+ Webhook** button.

4. You'll be presented with a form to configure your webhook:

<img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-commit-webhook.png?fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=775cc6392de007e894c42400117d113e" alt="LangSmith Webhook configuration modal" data-og-width="3008" width="3008" data-og-height="1454" height="1454" data-path="langsmith/images/prompt-commit-webhook.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-commit-webhook.png?w=280&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=a1d037dbc657bc0758b444d94d458c91 280w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-commit-webhook.png?w=560&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=708b8a9d27bd584fdd3a4a8df17be36d 560w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-commit-webhook.png?w=840&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=080f345e1d97aeac9b0d15a0d762fd42 840w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-commit-webhook.png?w=1100&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=387781e1444e51ff64cc2b0f7f02cd26 1100w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-commit-webhook.png?w=1650&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=c855c18d16698e49737ea6f581162970 1650w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-commit-webhook.png?w=2500&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=38e417a519b578f880d59a6c23efb102 2500w" />

* **Webhook URL:** Enter the full public URL of your deployed FastAPI server's endpoint. For our example server, this would be `https://prompt-commit-webhook.onrender.com/webhook/commit`.
   * **Headers (Optional):**
     * You can add custom headers that LangSmith will send with each webhook request.

5. **Test the Webhook:** LangSmith provides a "Send Test Notification" button. Use this to send a sample payload to your server. Check your server logs (e.g., on Render) to ensure it receives the request and processes it successfully (or to debug any issues).

6. **Save** the webhook configuration.

## The workflow in action

<img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-sequence-diagram.png?fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=823988be6300f39a6e9de784b34a2a77" alt="Workflow Diagram showing: User saves prompt in LangSmith, LangSmith sends webhook to FastAPI Server, which interacts with GitHub to update files" data-og-width="2922" width="2922" data-og-height="1014" height="1014" data-path="langsmith/images/prompt-sequence-diagram.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-sequence-diagram.png?w=280&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=13a26887fccdca822c175912ed7fbd3b 280w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-sequence-diagram.png?w=560&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=07e46cc25aa6d0ebe76b14ce9057936b 560w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-sequence-diagram.png?w=840&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=887758bfcf886c3062177706b7f22fb1 840w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-sequence-diagram.png?w=1100&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=a5aa30f606eac545f30a715eccfd80a1 1100w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-sequence-diagram.png?w=1650&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=32cae9f09ec30759550214f3c3e490ff 1650w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-sequence-diagram.png?w=2500&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=bced33a052aa88056de397389f9f7d64 2500w" />

Now, with everything set up, here's what happens:

1. **Prompt Modification:** A user (developer or non-technical team member) modifies a prompt in the LangSmith UI and saves it, creating a new "prompt commit."

2. **Webhook Trigger:** LangSmith detects this new prompt commit and triggers the configured webhook.

3. **HTTP Request:** LangSmith sends an HTTP POST request to the public URL of your FastAPI server (e.g., `https://prompt-commit-webhook.onrender.com/webhook/commit`). The body of this request contains the JSON prompt manifest for the entire workspace.

4. **Server Receives Payload:** Your FastAPI server's endpoint receives the request.

5. **GitHub Commit:** The server parses the JSON manifest from the request body. It then uses the configured GitHub Personal Access Token, repository owner, repository name, file path, and branch to:

* Check if the manifest file already exists in the repository on the specified branch to get its SHA (this is necessary for updating an existing file).
   * Create a new commit with the latest prompt manifest, either creating the file or updating it if it already exists. The commit message will indicate that it's an update from LangSmith.

6. **Confirmation:** You should see the new commit appear in your GitHub repository.

<img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-commit-github.png?fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=213d6364ce20e4acf4e3eb7fe8c1b13d" alt="Manifest commited to Github" data-og-width="2982" width="2982" data-og-height="1270" height="1270" data-path="langsmith/images/prompt-commit-github.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-commit-github.png?w=280&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=f03e1a469e28196f66bc1f92993e6045 280w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-commit-github.png?w=560&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=645f3cf20850a03a89a2e56a53d95719 560w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-commit-github.png?w=840&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=418f7716a8521e3d57a4381d4ebd4d08 840w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-commit-github.png?w=1100&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=34af751c0c20b176c715f0c6f86654f1 1100w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-commit-github.png?w=1650&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=13309faa18bde23df5e6df56521823fb 1650w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-commit-github.png?w=2500&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=f17d618b6d8e237aa8be023cccea4fd3 2500w" />

You've now successfully synced your LangSmith prompts with GitHub!

## Beyond a simple commit

Our example FastAPI server performs a direct commit of the entire prompt manifest. However, this is just the starting point. You can extend the server's functionality to perform more sophisticated actions:

* **Granular Commits:** Parse the manifest and commit changes to individual prompt files if you prefer a more granular structure in your repository.
* **Trigger CI/CD:** Instead of (or in addition to) committing, have the server trigger a CI/CD pipeline (e.g., Jenkins, GitHub Actions, GitLab CI) to deploy a staging environment, run tests, or build new application versions.
* **Update Databases/Caches:** If your application loads prompts from a database or cache, update these stores directly.
* **Notifications:** Send notifications to Slack, email, or other communication channels about prompt changes.
* **Selective Processing:** Based on metadata within the LangSmith payload (if available, e.g., which specific prompt changed or by whom), you could apply different logic.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/prompt-commit.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
</Accordion>

<Note>
  It's important to understand that LangSmith webhooks for prompt commits are generally triggered at the **workspace level**. This means if *any* prompt within your LangSmith workspace is modified and a "prompt commit" is saved, the webhook will fire and send the updated manifest of the prompt. The payloads are identifiable by prompt id. Your receiving server should be designed with this in mind.
</Note>

## Implementing a FastAPI server for webhook reception

To effectively process webhook notifications from LangSmith when prompts are updated, an intermediary server application is necessary. This server will act as the receiver for HTTP POST requests sent by LangSmith. For demonstration purposes in this guide, we will outline the creation of a simple FastAPI application to fulfill this role.

This publicly accessible server will be responsible for:

1. **Receiving Webhook Requests:** Listening for incoming HTTP POST requests.
2. **Parsing Payloads:** Extracting and interpreting the JSON-formatted prompt manifest from the request body.
3. **Committing to GitHub:** Programmatically creating a new commit in your specified GitHub repository, containing the updated prompt manifest. This ensures your prompts remain version-controlled and synchronized with changes made in LangSmith.

For deployment, platforms like [Render.com](https://render.com/) (offering a suitable free tier), Vercel, Fly.io, or other cloud providers (AWS, GCP, Azure) can be utilized to host the FastAPI application and obtain a public URL.

The server's core functionality will include an endpoint for webhook reception, logic for parsing the manifest, and integration with the GitHub API (using a Personal Access Token for authentication) to manage commits.

<Accordion title="Minimal FastAPI Server Code ()">
  `main.py`

  This server will listen for incoming webhooks from LangSmith and commit the received prompt manifest to your GitHub repository.
```

---

## How to upload experiments run outside of LangSmith with the REST API

**URL:** llms-txt#how-to-upload-experiments-run-outside-of-langsmith-with-the-rest-api

**Contents:**
- Request body schema
- Considerations
- Example request
- View the experiment in the UI

Source: https://docs.langchain.com/langsmith/upload-existing-experiments

Some users prefer to manage their datasets and run their experiments outside of LangSmith, but want to use the LangSmith UI to view the results. This is supported via our endpoint.

This guide will show you how to upload evals using the REST API, using the `requests` library in Python as an example. However, the same principles apply to any language.

## Request body schema

Uploading an experiment requires specifying the relevant high-level information for your experiment and dataset, along with the individual data for your examples and runs within the experiment. Each object in the `results` represents a "row" in the experiment - a single dataset example, along with an associated run. Note that `dataset_id` and `dataset_name` refer to your dataset identifier in your external system and will be used to group external experiments together in a single dataset. They should not refer to an existing dataset in LangSmith (unless that dataset was created via this endpoint).

You may use the following schema to upload experiments to the `/datasets/upload-experiment` endpoint:

The response JSON will be a dict with keys `experiment` and `dataset`, each of which is an object that contains relevant information about the experiment and dataset that was created.

You may upload multiple experiments to the same dataset by providing the same dataset\_id or dataset\_name between multiple calls. Your experiments will be grouped together under a single dataset, and you will be able to [use the comparison view to compare results between experiments](/langsmith/compare-experiment-results).

Ensure that the start and end times of your individual rows are all between the start and end time of your experiment.

You must provide either a dataset\_id or a dataset\_name. If you only provide an ID and the dataset does not yet exist, we will generate a name for you, and vice versa if you only provide a name.

You may not upload experiments to a dataset that was not created via this endpoint. Uploading experiments is only supported for externally-managed datasets.

Below is an example of a simple call to the `/datasets/upload-experiment`. This is a basic example that just uses the most important fields as an illustration.

Below is the response received:

Note that the latency and feedback stats in the experiment results are null because the runs haven't had a chance to be persisted yet, which may take a few seconds. If you save the experiment id and query again in a few seconds, you will see all the stats (although tokens/cost will still be null, because we don't ask for this information in the request body).

## View the experiment in the UI

Now, login to the UI and click on your newly-created dataset! You should see a single experiment: <img src="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/uploaded-dataset.png?fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=797dd62e7cd3f833cd13bafcedfa5607" alt="Uploaded experiments table" data-og-width="3454" width="3454" data-og-height="1914" height="1914" data-path="langsmith/images/uploaded-dataset.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/uploaded-dataset.png?w=280&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=b1209e0ffca0c29ca3e0d7e42f0e8ac8 280w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/uploaded-dataset.png?w=560&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=a0dc70688d773066f6844e49b0654c5d 560w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/uploaded-dataset.png?w=840&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=abbe1374d734e276503394edb09aab40 840w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/uploaded-dataset.png?w=1100&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=22595d2f890ce7918da47809c2ce18cd 1100w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/uploaded-dataset.png?w=1650&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=303c085b3c5e8a5c9e49f5deb54852f0 1650w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/uploaded-dataset.png?w=2500&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=b5ee82414e1af652674022087d5dc131 2500w" />

Your examples will have been uploaded: <img src="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/uploaded-dataset-examples.png?fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=093061568ece423d5a2c4cb2b5df2721" alt="Uploaded examples" data-og-width="3454" width="3454" data-og-height="1912" height="1912" data-path="langsmith/images/uploaded-dataset-examples.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/uploaded-dataset-examples.png?w=280&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=2112ea321eb2b791f29b2817e4ecfa70 280w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/uploaded-dataset-examples.png?w=560&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=8fa00a59f3ebe49c8dc413fcb8cdfff6 560w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/uploaded-dataset-examples.png?w=840&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=3a34d7d6ab89007cfb849b3b1e9fed6e 840w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/uploaded-dataset-examples.png?w=1100&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=4949985fdd2517513234cbec7fa70274 1100w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/uploaded-dataset-examples.png?w=1650&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=bdbc201e179f1ba78da86efe792837c9 1650w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/uploaded-dataset-examples.png?w=2500&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=fafd5f13e4a508ecc26f370f85a143d2 2500w" />

Clicking on your experiment will bring you to the comparison view: <img src="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/uploaded-experiment.png?fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=d66c9ea1cfbb1acf4f591b11f54a71da" alt="Uploaded experiment comparison view" data-og-width="3452" width="3452" data-og-height="1912" height="1912" data-path="langsmith/images/uploaded-experiment.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/uploaded-experiment.png?w=280&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=9839c5164dffd92bb302b1858e6f36e5 280w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/uploaded-experiment.png?w=560&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=a013685347c8915b42925027212052e5 560w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/uploaded-experiment.png?w=840&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=513f29d729a87e70f1b1c0bc0f42c4b5 840w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/uploaded-experiment.png?w=1100&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=eb4eda99fb12b2ebee2c1f4529fa63a8 1100w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/uploaded-experiment.png?w=1650&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=b79aabcad20cca0bfc19cc363c6704d0 1650w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/uploaded-experiment.png?w=2500&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=b78901795d6b7886a0c251a9abba8d52 2500w" />

As you upload more experiments to your dataset, you will be able to compare the results and easily identify regressions in the comparison view.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/upload-existing-experiments.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
The response JSON will be a dict with keys `experiment` and `dataset`, each of which is an object that contains relevant information about the experiment and dataset that was created.

## Considerations

You may upload multiple experiments to the same dataset by providing the same dataset\_id or dataset\_name between multiple calls. Your experiments will be grouped together under a single dataset, and you will be able to [use the comparison view to compare results between experiments](/langsmith/compare-experiment-results).

Ensure that the start and end times of your individual rows are all between the start and end time of your experiment.

You must provide either a dataset\_id or a dataset\_name. If you only provide an ID and the dataset does not yet exist, we will generate a name for you, and vice versa if you only provide a name.

You may not upload experiments to a dataset that was not created via this endpoint. Uploading experiments is only supported for externally-managed datasets.

## Example request

Below is an example of a simple call to the `/datasets/upload-experiment`. This is a basic example that just uses the most important fields as an illustration.
```

Example 2 (unknown):
```unknown
Below is the response received:
```

---

## How to use prebuilt evaluators

**URL:** llms-txt#how-to-use-prebuilt-evaluators

**Contents:**
- Setup
- Running an evaluator

Source: https://docs.langchain.com/langsmith/prebuilt-evaluators

LangSmith integrates with the open-source openevals package to provide a suite of prebuilt evaluators that you can use as starting points for evaluation.

<Note>
  This how-to guide will demonstrate how to set up and run one type of evaluator (LLM-as-a-judge). For a complete list of prebuilt evaluators with usage examples, refer to the [openevals](https://github.com/langchain-ai/openevals) and [agentevals](https://github.com/langchain-ai/agentevals) repos.
</Note>

You'll need to install the `openevals` package to use the pre-built LLM-as-a-judge evaluator.

You'll also need to set your OpenAI API key as an environment variable, though you can choose different providers too:

We'll also use LangSmith's [pytest](/langsmith/pytest) integration for Python and [Vitest/Jest](/langsmith/vitest-jest) for TypeScript to run our evals. `openevals` also integrates seamlessly with the [`evaluate`](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._runner.evaluate) method as well. See the [appropriate guides](/langsmith/pytest) for setup instructions.

## Running an evaluator

The general flow is simple: import the evaluator or factory function from `openevals`, then run it within your test file with inputs, outputs, and reference outputs. LangSmith will automatically log the evaluator's results as feedback.

Note that not all evaluators will require each parameter (the exact match evaluator only requires outputs and reference outputs, for example). Additionally, if your LLM-as-a-judge prompt requires additional variables, passing them in as kwargs will format them into the prompt.

Set up your test file like this:

The `feedback_key`/`feedbackKey` parameter will be used as the name of the feedback in your experiment.

Running the eval in your terminal will result in something like the following:

<img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prebuilt-eval-result.png?fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=c2351acb065520c3cef3c374bd762982" alt="Prebuilt evaluator terminal result" data-og-width="2114" width="2114" data-og-height="614" height="614" data-path="langsmith/images/prebuilt-eval-result.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prebuilt-eval-result.png?w=280&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=5a091195ae1351d5b16b2ebe53632e1e 280w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prebuilt-eval-result.png?w=560&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=1e7488bb77662f71e60f01b9fa9609d6 560w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prebuilt-eval-result.png?w=840&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=7e491cd83accabc3a56153a6c12d84fe 840w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prebuilt-eval-result.png?w=1100&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=2fbc03b560b082ae5f6de8d17d4ae626 1100w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prebuilt-eval-result.png?w=1650&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=20f6023215721383019659a0b99f3de5 1650w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prebuilt-eval-result.png?w=2500&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=af97fb8ec7343f536704719294560dd0 2500w" />

You can also pass prebuilt evaluators directly into the `evaluate` method if you have already created a dataset in LangSmith. If using Python, this requires `langsmith>=0.3.11`:

For a complete list of available evaluators, see the [openevals](https://github.com/langchain-ai/openevals) and [agentevals](https://github.com/langchain-ai/agentevals) repos.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/prebuilt-evaluators.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

You'll also need to set your OpenAI API key as an environment variable, though you can choose different providers too:
```

Example 3 (unknown):
```unknown
We'll also use LangSmith's [pytest](/langsmith/pytest) integration for Python and [Vitest/Jest](/langsmith/vitest-jest) for TypeScript to run our evals. `openevals` also integrates seamlessly with the [`evaluate`](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._runner.evaluate) method as well. See the [appropriate guides](/langsmith/pytest) for setup instructions.

## Running an evaluator

The general flow is simple: import the evaluator or factory function from `openevals`, then run it within your test file with inputs, outputs, and reference outputs. LangSmith will automatically log the evaluator's results as feedback.

Note that not all evaluators will require each parameter (the exact match evaluator only requires outputs and reference outputs, for example). Additionally, if your LLM-as-a-judge prompt requires additional variables, passing them in as kwargs will format them into the prompt.

Set up your test file like this:

<CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## How to use Studio

**URL:** llms-txt#how-to-use-studio

**Contents:**
- Run application
- Manage assistants
- Manage threads
- Next steps

Source: https://docs.langchain.com/langsmith/use-studio

This page describes the core workflows you’ll use in Studio. It explains how to run your application, manage assistant configurations, and work with conversation threads. Each section includes steps in both graph mode (full-featured view of your graph’s execution) and chat mode (lightweight conversational interface):

* [Run application](#run-application): Execute your application or agent and observe its behavior.
* [Manage assistants](#manage-assistants): Create, edit, and select the assistant configuration used by your application.
* [Manage threads](#manage-threads): View and organize the threads, including forking or editing past runs for debugging.

<Tabs>
  <Tab title="Graph">
    ### Specify input

1. Define the input to your graph in the **Input** section on the left side of the page, below the graph interface. Studio will attempt to render a form for your input based on the graph's defined [state schema](/oss/python/langgraph/graph-api/#schema). To disable this, click the **View Raw** button, which will present you with a JSON editor.
    2. Click the up or down arrows at the top of the **Input** section to toggle through and use previously submitted inputs.

To specify the [assistant](/langsmith/assistants) that is used for the run:

1. Click the **Settings** button in the bottom left corner. If an assistant is currently selected the button will also list the assistant name. If no assistant is selected it will say **Manage Assistants**.
    2. Select the assistant to run.
    3. Click the **Active** toggle at the top of the modal to activate it.

For more information, refer to [Manage assistants](#manage-assistants).

Click the dropdown next to **Submit** and click the toggle to enable or disable streaming.

To run your graph with breakpoints:

1. Click **Interrupt**.
    2. Select a node and whether to pause before or after that node has executed.
    3. Click **Continue** in the thread log to resume execution.

For more information on breakpoints, refer to [Human-in-the-loop](/oss/python/langchain/human-in-the-loop).

To submit the run with the specified input and run settings:

1. Click the **Submit** button. This will add a [run](/langsmith/assistants#execution) to the existing selected [thread](/oss/python/langgraph/persistence#threads). If no thread is currently selected, a new one will be created.
    2. To cancel the ongoing run, click the **Cancel** button.
  </Tab>

<Tab title="Chat">
    Specify the input to your chat application in the bottom of the conversation panel.

1. Click the **Send message** button to submit the input as a Human message and have the response streamed back.

To cancel the ongoing run:

1. Click **Cancel**.
    2. Click the **Show tool calls** toggle to hide or show tool calls in the conversation.
  </Tab>
</Tabs>

Studio lets you view, edit, and update your assistants, and allows you to run your graph using these assistant configurations.

For more conceptual details, refer to the [Assistants overview](/langsmith/assistants/).

<Tabs>
  <Tab title="Graph">
    To view your assistants:

1. Click **Manage Assistants** in the bottom left corner. This opens a modal for you to view all the assistants for the selected graph.
    2. Specify the assistant and its version you would like to mark as **Active**. LangSmith will use this assistant when runs are submitted.

The **Default configuration** option will be active, which reflects the default configuration defined in your graph. Edits made to this configuration will be used to update the run-time configuration, but will not update or create a new assistant unless you click **Create new assistant**.
  </Tab>

<Tab title="Chat">
    Chat mode enables you to switch through the different assistants in your graph via the dropdown selector at the top of the page. To create, edit, or delete assistants, use Graph mode.
  </Tab>
</Tabs>

Studio provides tools to view all [threads](/oss/python/langgraph/persistence#threads) saved on the server and edit their state. You can create new threads, switch between threads, and modify past states both in graph mode and chat mode.

<Tabs>
  <Tab title="Graph">
    ### View threads

1. In the top of the right-hand pane, select the dropdown menu to view existing threads.
    2. Select the desired thread, and the thread history will populate in the right-hand side of the page.
    3. To create a new thread, click **+ New Thread** and [submit a run](#run-application).
    4. To view more granular information in the thread, drag the slider at the top of the page to the right. To view less information, drag the slider to the left. Additionally, collapse or expand individual turns, nodes, and keys of the state.
    5. Switch between `Pretty` and `JSON` mode for different rendering formats.

### Edit thread history

To edit the state of the thread:

1. Select <Icon icon="pencil" /> **Edit node state** next to the desired node.
    2. Edit the node's output as desired and click **Fork** to confirm. This will create a new forked run from the checkpoint of the selected node.

If you instead want to re-run the thread from a given checkpoint without editing the state, click **Re-run from here**. This will again create a new forked run from the selected checkpoint. This is useful for re-running with changes that are not specific to the state, such as the selected assistant.
  </Tab>

<Tab title="Chat">
    1. View all threads in the right-hand pane of the page.
    2. Select the desired thread and the thread history will populate in the center panel.
    3. To create a new thread, click **+** and submit a run.

To edit a human message in the thread:

1. Click <Icon icon="pencil" /> **Edit node state** below the human message.
    2. Edit the message as desired and submit. This will create a new fork of the conversation history.
    3. To re-generate an AI message, click the retry icon below the AI message.
  </Tab>
</Tabs>

Refer to the following guides for more detail on tasks you can complete in Studio:

* [Iterate on prompts](/langsmith/observability-studio)
* [Run experiments over datasets](/langsmith/observability-studio#run-experiments-over-a-dataset)

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/use-studio.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## How to use the REST API

**URL:** llms-txt#how-to-use-the-rest-api

**Contents:**
- Create a dataset

Source: https://docs.langchain.com/langsmith/run-evals-api-only

It is highly recommended to run evals with either the Python or TypeScript SDKs. The SDKs have many optimizations and features that enhance the performance and reliability of your evals.
However, if you are unable to use the SDKs, either because you are using a different language or because you are running in a restricted environment, you can use the REST API directly.

This guide will show you how to run evals using the REST API, using the `requests` library in Python as an example. However, the same principles apply to any language.

Before diving into this content, it might be helpful to read the following:

* [Evaluate LLM applications](/langsmith/evaluate-llm-application)
* [LangSmith API Reference](https://api.smith.langchain.com/redoc)

Here, we are using the python SDK for convenience. You can also use the API directly use the UI, see [this guide](/langsmith/manage-datasets-in-application) for more information.

```python  theme={null}
import os
import requests

from datetime import datetime
from langsmith import Client
from openai import OpenAI
from uuid import uuid4

client = Client()
oa_client = OpenAI()

---

## How we are sampling runs to include in our dataset

**URL:** llms-txt#how-we-are-sampling-runs-to-include-in-our-dataset

**Contents:**
  - Convert runs to experiment

end_time = datetime.now(tz=timezone.utc)
start_time = end_time - timedelta(days=1)
run_filter = f'and(gt(start_time, "{start_time.isoformat()}"), lt(end_time, "{end_time.isoformat()}"))'
prod_runs = list(
    client.list_runs(
        project_name=project_name,
        is_root=True,
        filter=run_filter,
    )
)
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
### Convert runs to experiment

`convert_runs_to_test` is a function which takes some runs and does the following:

1. The inputs, and optionally the outputs, are saved to a dataset as Examples.
2. The inputs and outputs are stored as an experiment, as if you had run the `evaluate` function and received those outputs.
```

---

## Hugging Face

**URL:** llms-txt#hugging-face

**Contents:**
- Chat models
  - ChatHuggingFace
- LLMs
  - HuggingFaceEndpoint
  - HuggingFacePipeline
- Embedding Models
  - HuggingFaceEmbeddings
  - HuggingFaceEndpointEmbeddings
  - HuggingFaceInferenceAPIEmbeddings
  - HuggingFaceInstructEmbeddings

Source: https://docs.langchain.com/oss/python/integrations/providers/huggingface

This page covers all LangChain integrations with [Hugging Face Hub](https://huggingface.co/) and libraries like [transformers](https://huggingface.co/docs/transformers/index), [sentence transformers](https://sbert.net/), and [datasets](https://huggingface.co/docs/datasets/index).

We can use the `Hugging Face` LLM classes or directly use the `ChatHuggingFace` class.

See a [usage example](/oss/python/integrations/chat/huggingface).

### HuggingFaceEndpoint

We can use the `HuggingFaceEndpoint` class to run open source models via serverless [Inference Providers](https://huggingface.co/docs/inference-providers) or via dedicated [Inference Endpoints](https://huggingface.co/inference-endpoints/dedicated).

See a [usage example](/oss/python/integrations/llms/huggingface_endpoint).

### HuggingFacePipeline

We can use the `HuggingFacePipeline` class to run open source models locally.

See a [usage example](/oss/python/integrations/llms/huggingface_pipelines).

### HuggingFaceEmbeddings

We can use the `HuggingFaceEmbeddings` class to run open source embedding models locally.

See a [usage example](/oss/python/integrations/text_embedding/huggingfacehub).

### HuggingFaceEndpointEmbeddings

We can use the `HuggingFaceEndpointEmbeddings` class to run open source embedding models via a dedicated [Inference Endpoint](https://huggingface.co/inference-endpoints/dedicated).

See a [usage example](/oss/python/integrations/text_embedding/huggingfacehub).

### HuggingFaceInferenceAPIEmbeddings

We can use the `HuggingFaceInferenceAPIEmbeddings` class to run open source embedding models via [Inference Providers](https://huggingface.co/docs/inference-providers).

See a [usage example](/oss/python/integrations/text_embedding/huggingfacehub).

### HuggingFaceInstructEmbeddings

We can use the `HuggingFaceInstructEmbeddings` class to run open source embedding models locally.

See a [usage example](/oss/python/integrations/text_embedding/instruct_embeddings).

### HuggingFaceBgeEmbeddings

> [BGE models on the HuggingFace](https://huggingface.co/BAAI/bge-large-en-v1.5) are one of [the best open-source embedding models](https://huggingface.co/spaces/mteb/leaderboard).
> BGE model is created by the [Beijing Academy of Artificial Intelligence (BAAI)](https://en.wikipedia.org/wiki/Beijing_Academy_of_Artificial_Intelligence). `BAAI` is a private non-profit organization engaged in AI research and development.

See a [usage example](/oss/python/integrations/text_embedding/bge_huggingface).

### Hugging Face dataset

> [Hugging Face Hub](https://huggingface.co/docs/hub/index) is home to over 75,000
> [datasets](https://huggingface.co/docs/hub/index#datasets) in more than 100 languages
> that can be used for a broad range of tasks across NLP, Computer Vision, and Audio.
> They used for a diverse range of tasks such as translation, automatic speech
> recognition, and image classification.

We need to install `datasets` python package.

See a [usage example](/oss/python/integrations/document_loaders/hugging_face_dataset).

### Hugging Face model loader

> Load model information from `Hugging Face Hub`, including README content.
>
> This loader interfaces with the `Hugging Face Models API` to fetch
> and load model metadata and README files.
> The API allows you to search and filter models based on
> specific criteria such as model tags, authors, and more.

It uses the Hugging Face models to generate image captions.

We need to install several python packages.

See a [usage example](/oss/python/integrations/document_loaders/image_captions).

### Hugging Face Hub Tools

> [Hugging Face Tools](https://huggingface.co/docs/transformers/v4.29.0/en/custom_tools)
> support text I/O and are loaded using the `load_huggingface_tool` function.

We need to install several python packages.

See a [usage example](/oss/python/integrations/tools/huggingface_tools).

### Hugging Face Text-to-Speech Model Inference.

> It is a wrapper around `OpenAI Text-to-Speech API`.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/providers/huggingface.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
## LLMs

### HuggingFaceEndpoint

We can use the `HuggingFaceEndpoint` class to run open source models via serverless [Inference Providers](https://huggingface.co/docs/inference-providers) or via dedicated [Inference Endpoints](https://huggingface.co/inference-endpoints/dedicated).

See a [usage example](/oss/python/integrations/llms/huggingface_endpoint).
```

Example 2 (unknown):
```unknown
### HuggingFacePipeline

We can use the `HuggingFacePipeline` class to run open source models locally.

See a [usage example](/oss/python/integrations/llms/huggingface_pipelines).
```

Example 3 (unknown):
```unknown
## Embedding Models

### HuggingFaceEmbeddings

We can use the `HuggingFaceEmbeddings` class to run open source embedding models locally.

See a [usage example](/oss/python/integrations/text_embedding/huggingfacehub).
```

Example 4 (unknown):
```unknown
### HuggingFaceEndpointEmbeddings

We can use the `HuggingFaceEndpointEmbeddings` class to run open source embedding models via a dedicated [Inference Endpoint](https://huggingface.co/inference-endpoints/dedicated).

See a [usage example](/oss/python/integrations/text_embedding/huggingfacehub).
```

---

## Human-in-the-loop

**URL:** llms-txt#human-in-the-loop

**Contents:**
- Interrupt decision types
- Configuring interrupts
- Responding to interrupts
  - Decision types
- Execution lifecycle
- Custom HITL logic

Source: https://docs.langchain.com/oss/javascript/langchain/human-in-the-loop

The Human-in-the-Loop (HITL) middleware lets you add human oversight to agent tool calls.
When a model proposes an action that might require review — for example, writing to a file or executing SQL — the middleware can pause execution and wait for a decision.

It does this by checking each tool call against a configurable policy. If intervention is needed, the middleware issues an [interrupt](https://langchain-ai.github.io/langgraphjs/reference/functions/langgraph.interrupt-2.html) that halts execution. The graph state is saved using LangGraph’s [persistence layer](/oss/javascript/langgraph/persistence), so execution can pause safely and resume later.

A human decision then determines what happens next: the action can be approved as-is (`approve`), modified before running (`edit`), or rejected with feedback (`reject`).

## Interrupt decision types

The middleware defines three built-in ways a human can respond to an interrupt:

| Decision Type | Description                                                               | Example Use Case                                    |
| ------------- | ------------------------------------------------------------------------- | --------------------------------------------------- |
| ✅ `approve`   | The action is approved as-is and executed without changes.                | Send an email draft exactly as written              |
| ✏️ `edit`     | The tool call is executed with modifications.                             | Change the recipient before sending an email        |
| ❌ `reject`    | The tool call is rejected, with an explanation added to the conversation. | Reject an email draft and explain how to rewrite it |

The available decision types for each tool depend on the policy you configure in `interrupt_on`.
When multiple tool calls are paused at the same time, each action requires a separate decision.
Decisions must be provided in the same order as the actions appear in the interrupt request.

<Tip>
  When **editing** tool arguments, make changes conservatively. Significant modifications to the original arguments may cause the model to re-evaluate its approach and potentially execute the tool multiple times or take unexpected actions.
</Tip>

## Configuring interrupts

To use HITL, add the middleware to the agent’s `middleware` list when creating the agent.

You configure it with a mapping of tool actions to the decision types that are allowed for each action. The middleware will interrupt execution when a tool call matches an action in the mapping.

<Info>
  You must configure a checkpointer to persist the graph state across interrupts.
  In production, use a persistent checkpointer like @\[`AsyncPostgresSaver`]. For testing or prototyping, use @\[`InMemorySaver`].

When invoking the agent, pass a `config` that includes the **thread ID** to associate execution with a conversation thread.
  See the [LangGraph interrupts documentation](/oss/javascript/langgraph/interrupts) for details.
</Info>

## Responding to interrupts

When you invoke the agent, it runs until it either completes or an interrupt is raised. An interrupt is triggered when a tool call matches the policy you configured in `interrupt_on`. In that case, the invocation result will include an `__interrupt__` field with the actions that require review. You can then present those actions to a reviewer and resume execution once decisions are provided.

<Tabs>
  <Tab title="✅ approve">
    Use `approve` to approve the tool call as-is and execute it without changes.

<Tab title="✏️ edit">
    Use `edit` to modify the tool call before execution.
    Provide the edited action with the new tool name and arguments.

<Tip>
      When **editing** tool arguments, make changes conservatively. Significant modifications to the original arguments may cause the model to re-evaluate its approach and potentially execute the tool multiple times or take unexpected actions.
    </Tip>
  </Tab>

<Tab title="❌ reject">
    Use `reject` to reject the tool call and provide feedback instead of execution.

The `message` is added to the conversation as feedback to help the agent understand why the action was rejected and what it should do instead.

### Multiple decisions

When multiple actions are under review, provide a decision for each action in the same order as they appear in the interrupt:

## Execution lifecycle

The middleware defines an `after_model` hook that runs after the model generates a response but before any tool calls are executed:

1. The agent invokes the model to generate a response.
2. The middleware inspects the response for tool calls.
3. If any calls require human input, the middleware builds a `HITLRequest` with `action_requests` and `review_configs` and calls [interrupt](https://langchain-ai.github.io/langgraphjs/reference/functions/langgraph.interrupt-2.html).
4. The agent waits for human decisions.
5. Based on the `HITLResponse` decisions, the middleware executes approved or edited calls, synthesizes @\[ToolMessage]'s for rejected calls, and resumes execution.

For more specialized workflows, you can build custom HITL logic directly using the [interrupt](https://langchain-ai.github.io/langgraphjs/reference/functions/langgraph.interrupt-2.html) primitive and [middleware](/oss/javascript/langchain/middleware) abstraction.

Review the [execution lifecycle](#execution-lifecycle) above to understand how to integrate interrupts into the agent's operation.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/human-in-the-loop.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
<Info>
  You must configure a checkpointer to persist the graph state across interrupts.
  In production, use a persistent checkpointer like @\[`AsyncPostgresSaver`]. For testing or prototyping, use @\[`InMemorySaver`].

  When invoking the agent, pass a `config` that includes the **thread ID** to associate execution with a conversation thread.
  See the [LangGraph interrupts documentation](/oss/javascript/langgraph/interrupts) for details.
</Info>

## Responding to interrupts

When you invoke the agent, it runs until it either completes or an interrupt is raised. An interrupt is triggered when a tool call matches the policy you configured in `interrupt_on`. In that case, the invocation result will include an `__interrupt__` field with the actions that require review. You can then present those actions to a reviewer and resume execution once decisions are provided.
```

Example 2 (unknown):
```unknown
### Decision types

<Tabs>
  <Tab title="✅ approve">
    Use `approve` to approve the tool call as-is and execute it without changes.
```

Example 3 (unknown):
```unknown
</Tab>

  <Tab title="✏️ edit">
    Use `edit` to modify the tool call before execution.
    Provide the edited action with the new tool name and arguments.
```

Example 4 (unknown):
```unknown
<Tip>
      When **editing** tool arguments, make changes conservatively. Significant modifications to the original arguments may cause the model to re-evaluate its approach and potentially execute the tool multiple times or take unexpected actions.
    </Tip>
  </Tab>

  <Tab title="❌ reject">
    Use `reject` to reject the tool call and provide feedback instead of execution.
```

---

## Human-in-the-loop leverages LangGraph's persistence layer.

**URL:** llms-txt#human-in-the-loop-leverages-langgraph's-persistence-layer.

---

## Human-in-the-loop requires a thread ID for persistence

**URL:** llms-txt#human-in-the-loop-requires-a-thread-id-for-persistence

config = {"configurable": {"thread_id": "some_id"}}

---

## Human-in-the-loop using server API

**URL:** llms-txt#human-in-the-loop-using-server-api

**Contents:**
- Dynamic interrupts
- Static interrupts
- Learn more

Source: https://docs.langchain.com/langsmith/add-human-in-the-loop

To review, edit, and approve tool calls in an agent or workflow, use LangGraph's [human-in-the-loop](/oss/python/langgraph/interrupts) features.

## Dynamic interrupts

<Tabs>
  <Tab title="Python">

1. The graph is invoked with some initial state.
    2. When the graph hits the interrupt, it returns an interrupt object with the payload and metadata.
       3\. The graph is resumed with a `Command(resume=...)`, injecting the human's input and continuing execution.
  </Tab>

<Tab title="JavaScript">

1. The graph is invoked with some initial state.
    2. When the graph hits the interrupt, it returns an interrupt object with the payload and metadata.
    3. The graph is resumed with a `{ resume: ... }` command object, injecting the human's input and continuing execution.
  </Tab>

<Tab title="cURL">
    Create a thread:

Run the graph until the interrupt is hit.:

<Accordion title="Extended example: using `interrupt`">
  This is an example graph you can run in the Agent Server.
  See [LangSmith quickstart](/langsmith/deployment-quickstart) for more details.

1. `interrupt(...)` pauses execution at `human_node`, surfacing the given payload to a human.
  2. Any JSON serializable value can be passed to the [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) function. Here, a dict containing the text to revise.
  3. Once resumed, the return value of `interrupt(...)` is the human-provided input, which is used to update the state.

Once you have a running Agent Server, you can interact with it using
  [LangGraph SDK](/langsmith/langgraph-python-sdk)

<Tabs>
    <Tab title="Python">

1. The graph is invoked with some initial state.
      2. When the graph hits the interrupt, it returns an interrupt object with the payload and metadata.
         3\. The graph is resumed with a `Command(resume=...)`, injecting the human's input and continuing execution.
    </Tab>

<Tab title="JavaScript">

1. The graph is invoked with some initial state.
      2. When the graph hits the interrupt, it returns an interrupt object with the payload and metadata.
      3. The graph is resumed with a `{ resume: ... }` command object, injecting the human's input and continuing execution.
    </Tab>

<Tab title="cURL">
      Create a thread:

Run the graph until the interrupt is hit:

</Tab>
  </Tabs>
</Accordion>

Static interrupts (also known as static breakpoints) are triggered either before or after a node executes.

<Warning>
  Static interrupts are **not** recommended for human-in-the-loop workflows. They are best used for debugging and testing.
</Warning>

You can set static interrupts by specifying `interrupt_before` and `interrupt_after` at compile time:

1. The breakpoints are set during `compile` time.
2. `interrupt_before` specifies the nodes where execution should pause before the node is executed.
3. `interrupt_after` specifies the nodes where execution should pause after the node is executed.

Alternatively, you can set static interrupts at run time:

<Tabs>
  <Tab title="Python">

1. `client.runs.wait` is called with the `interrupt_before` and `interrupt_after` parameters. This is a run-time configuration and can be changed for every invocation.
    2. `interrupt_before` specifies the nodes where execution should pause before the node is executed.
    3. `interrupt_after` specifies the nodes where execution should pause after the node is executed.
  </Tab>

<Tab title="JavaScript">

1. `client.runs.wait` is called with the `interruptBefore` and `interruptAfter` parameters. This is a run-time configuration and can be changed for every invocation.
    2. `interruptBefore` specifies the nodes where execution should pause before the node is executed.
    3. `interruptAfter` specifies the nodes where execution should pause after the node is executed.
  </Tab>

<Tab title="cURL">
    
  </Tab>
</Tabs>

The following example shows how to add static interrupts:

<Tabs>
  <Tab title="Python">

1. The graph is run until the first breakpoint is hit.
    2. The graph is resumed by passing in `None` for the input. This will run the graph until the next breakpoint is hit.
  </Tab>

<Tab title="JavaScript">

1. The graph is run until the first breakpoint is hit.
    2. The graph is resumed by passing in `null` for the input. This will run the graph until the next breakpoint is hit.
  </Tab>

<Tab title="cURL">
    Create a thread:

Run the graph until the breakpoint:

* [Human-in-the-loop conceptual guide](/oss/python/langgraph/interrupts): learn more about LangGraph human-in-the-loop features.
* [Common patterns](/oss/python/langgraph/interrupts#common-patterns): learn how to implement patterns like approving/rejecting actions, requesting user input, tool call review, and validating human input.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/add-human-in-the-loop.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
1. The graph is invoked with some initial state.
    2. When the graph hits the interrupt, it returns an interrupt object with the payload and metadata.
       3\. The graph is resumed with a `Command(resume=...)`, injecting the human's input and continuing execution.
  </Tab>

  <Tab title="JavaScript">
```

Example 2 (unknown):
```unknown
1. The graph is invoked with some initial state.
    2. When the graph hits the interrupt, it returns an interrupt object with the payload and metadata.
    3. The graph is resumed with a `{ resume: ... }` command object, injecting the human's input and continuing execution.
  </Tab>

  <Tab title="cURL">
    Create a thread:
```

Example 3 (unknown):
```unknown
Run the graph until the interrupt is hit.:
```

Example 4 (unknown):
```unknown
Resume the graph:
```

---

## Hybrid

**URL:** llms-txt#hybrid

**Contents:**
  - Workflow
  - Architecture
  - Compute Platforms
  - Egress to LangSmith and the control plane
- Listeners
  - Kubernetes cluster organization
  - LangSmith workspace organization
- Use Cases
  - Each LangSmith workspace → separate Kubernetes cluster
  - Separate clusters, with shared “dev” cluster

Source: https://docs.langchain.com/langsmith/hybrid

<Info>
  **Important**
  The hybrid option requires an [Enterprise](https://langchain.com/pricing) plan.
</Info>

The **hybrid** model splits LangSmith infrastructure between LangChain's cloud and yours:

* **Control plane** (LangSmith UI, APIs, and orchestration) runs in LangChain's cloud, managed by LangChain.
* **Data plane** (your <Tooltip tip="The server that runs your applications.">Agent Servers</Tooltip> and agent workloads) runs in your cloud, managed by you.

This combines the convenience of a managed interface with the flexibility of running workloads in your own environment.

<Note>
  Learn more about the [control plane](/langsmith/control-plane), [data plane](/langsmith/data-plane), and [Agent Server](/langsmith/agent-server) architecture concepts.
</Note>

| Component                                                                                                | Responsibilities                                                                                                                           | Where it runs     | Who manages it |
| -------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------ | ----------------- | -------------- |
| <Tooltip tip="The LangSmith UI and APIs for managing deployments.">Control plane</Tooltip>               | <ul><li>UI for creating deployments and revisions</li><li>APIs for managing deployments</li><li>Observability data storage</li></ul>       | LangChain's cloud | LangChain      |
| <Tooltip tip="The runtime environment where your Agent Servers and agents execute.">Data plane</Tooltip> | <ul><li>Listener to sync with control plane</li><li>Agent Servers (your agents)</li><li>Backing services (Postgres, Redis, etc.)</li></ul> | Your cloud        | You            |

When running LangSmith in a hybrid model, you authenticate with a [LangSmith API key](/langsmith/create-account-api-key).

1. Use the `langgraph-cli` or [Studio](/langsmith/studio) to test your graph locally.
2. Build a Docker image using the `langgraph build` command.
3. Deploy your Agent Server from the [control plane UI](/langsmith/control-plane#control-plane-ui).

<Note>
  Supported Compute Platforms: [Kubernetes](https://kubernetes.io/).<br />
  For setup, refer to the [Hybrid setup guide](/langsmith/deploy-hybrid).
</Note>

<img className="block dark:hidden" src="https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/hybrid-with-deployment-light.png?fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=86d548632d33be3644bad7213287ac78" alt="Hybrid deployment: LangChain-hosted control plane (LangSmith UI/APIs) manages deployments. Your cloud runs a listener, Agent Server instances, and backing stores (Postgres/Redis) on Kubernetes." data-og-width="1784" width="1784" data-og-height="1782" height="1782" data-path="langsmith/images/hybrid-with-deployment-light.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/hybrid-with-deployment-light.png?w=280&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=2fe7b82524e32a2ce1e3726ad3bce553 280w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/hybrid-with-deployment-light.png?w=560&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=807a35d47b9c8e740a96f0a8aa4389a1 560w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/hybrid-with-deployment-light.png?w=840&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=84333efa9a9e83305b93f4b6e770b2f8 840w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/hybrid-with-deployment-light.png?w=1100&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=1d8bd0547f7814cad914b1ddc6dbfa48 1100w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/hybrid-with-deployment-light.png?w=1650&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=09f181972952ab4362b3ac70b7934d59 1650w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/hybrid-with-deployment-light.png?w=2500&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=e2d292d67dbf1fdb68758fac293c0cc7 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/hybrid-with-deployment-dark.png?fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=829f0ef40c315c493ef8e30857e9abf5" alt="Hybrid deployment: LangChain-hosted control plane (LangSmith UI/APIs) manages deployments. Your cloud runs a listener, Agent Server instances, and backing stores (Postgres/Redis) on Kubernetes." data-og-width="1784" width="1784" data-og-height="1782" height="1782" data-path="langsmith/images/hybrid-with-deployment-dark.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/hybrid-with-deployment-dark.png?w=280&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=bdb7a126e3914a07ed1ff72b66e50e9a 280w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/hybrid-with-deployment-dark.png?w=560&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=14f4f01c71edca5ce1594f3f2145f0e4 560w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/hybrid-with-deployment-dark.png?w=840&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=04f8b60076c4ff6263af77da5a65ccc1 840w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/hybrid-with-deployment-dark.png?w=1100&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=50fe58a42273562591bf695d5cdbfe57 1100w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/hybrid-with-deployment-dark.png?w=1650&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=20025a5634783e2eb1d2ba177724ccc6 1650w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/hybrid-with-deployment-dark.png?w=2500&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=79e4e542803c26c38e5fffaf2bc961bf 2500w" />

### Compute Platforms

* **Kubernetes**: Hybrid supports running the data plane on any Kubernetes cluster.

<Tip>
  For setup in Kubernetes, refer to the [Hybrid setup guide](/langsmith/deploy-hybrid)
</Tip>

### Egress to LangSmith and the control plane

In the hybrid deployment model, your self-hosted data plane will send network requests to the control plane to poll for changes that need to be implemented in the data plane. Traces from data plane deployments also get sent to the LangSmith instance integrated with the control plane. This traffic to the control plane is encrypted, over HTTPS. The data plane authenticates with the control plane with a LangSmith API key.

In order to enable this egress, you may need to update internal firewall rules or cloud resources (such as Security Groups) to [allow certain IP addresses](/langsmith/cloud#ingress-into-langchain-saas).

<Warning>
  AWS/Azure PrivateLink or GCP Private Service Connect is currently not supported. This traffic will go over the internet.
</Warning>

In the hybrid option, one or more ["listener" applications](/langsmith/data-plane#listener-application) can run depending on how your LangSmith workspaces and Kubernetes clusters are organized.

### Kubernetes cluster organization

* One or more listeners can run in a Kubernetes cluster.
* A listener can deploy into one or more namespaces in that cluster.
* Cluster owners are responsible for planning listener layout and Agent Server deployments.

### LangSmith workspace organization

* A workspace can be associated with one or more listeners.
* A workspace can only deploy to Kubernetes clusters where all of its listeners are deployed.

Here are some common listener configurations (not strict requirements):

### Each LangSmith workspace → separate Kubernetes cluster

* Cluster `alpha` runs workspace `A`
* Cluster `beta` runs workspace `B`

### Separate clusters, with shared “dev” cluster

* Cluster `alpha` runs workspace `A`
* Cluster `beta` runs workspace `B`
* Cluster `dev` runs workspaces `A` and `B`
* Both workspaces have two listeners; cluster `dev` has two listener deployments

### One cluster, one namespace per workspace

* Cluster `alpha`, namespace `1` runs workspace `A`
* Cluster `alpha`, namespace `2` runs workspace `B`

### One cluster, single namespace for multiple workspaces

* Cluster `alpha` runs workspace `A`
* Cluster `alpha` runs workspace `B`

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/hybrid.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## if configured with a subdomain / path prefix:

**URL:** llms-txt#if-configured-with-a-subdomain-/-path-prefix:

**Contents:**
  - Process the API response with jq

curl http://<langsmith_url/prefix/api/v1/info
json  theme={null}
{
  "version": "0.11.4",
  "license_expiration_time": "2026-08-18T19:14:34Z",
  "customer_info": {
    "customer_id": "<id>",
    "customer_name": "<name>"
  }
}
bash  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
This will return a JSON response containing your customer information:
```

Example 2 (unknown):
```unknown
Extract the `customer_id` and `customer_name` from this response to use as input for the export scripts.

### Process the API response with jq

You can use [jq](https://jqlang.org/download) to parse the JSON response and set bash variables for use in your scripts:
```

---

## If desired, specify custom instructions

**URL:** llms-txt#if-desired,-specify-custom-instructions

**Contents:**
  - RAG chains
- Next steps

prompt = (
    "You have access to a tool that retrieves context from a blog post. "
    "Use the tool to help answer user queries."
)
agent = create_agent(model, tools, system_prompt=prompt)
python  theme={null}
query = (
    "What is the standard method for Task Decomposition?\n\n"
    "Once you get the answer, look up common extensions of that method."
)

for event in agent.stream(
    {"messages": [{"role": "user", "content": query}]},
    stream_mode="values",
):
    event["messages"][-1].pretty_print()

================================ Human Message =================================

What is the standard method for Task Decomposition?

Once you get the answer, look up common extensions of that method.
================================== Ai Message ==================================
Tool Calls:
  retrieve_context (call_d6AVxICMPQYwAKj9lgH4E337)
 Call ID: call_d6AVxICMPQYwAKj9lgH4E337
  Args:
    query: standard method for Task Decomposition
================================= Tool Message =================================
Name: retrieve_context

Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}
Content: Task decomposition can be done...

Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}
Content: Component One: Planning...
================================== Ai Message ==================================
Tool Calls:
  retrieve_context (call_0dbMOw7266jvETbXWn4JqWpR)
 Call ID: call_0dbMOw7266jvETbXWn4JqWpR
  Args:
    query: common extensions of the standard method for Task Decomposition
================================= Tool Message =================================
Name: retrieve_context

Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}
Content: Task decomposition can be done...

Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}
Content: Component One: Planning...
================================== Ai Message ==================================

The standard method for Task Decomposition often used is the Chain of Thought (CoT)...
python  theme={null}
from langchain.agents.middleware import dynamic_prompt, ModelRequest

@dynamic_prompt
def prompt_with_context(request: ModelRequest) -> str:
    """Inject context into state messages."""
    last_query = request.state["messages"][-1].text
    retrieved_docs = vector_store.similarity_search(last_query)

docs_content = "\n\n".join(doc.page_content for doc in retrieved_docs)

system_message = (
        "You are a helpful assistant. Use the following context in your response:"
        f"\n\n{docs_content}"
    )

return system_message

agent = create_agent(model, tools=[], middleware=[prompt_with_context])
python  theme={null}
query = "What is task decomposition?"
for step in agent.stream(
    {"messages": [{"role": "user", "content": query}]},
    stream_mode="values",
):
    step["messages"][-1].pretty_print()

================================ Human Message =================================

What is task decomposition?
================================== Ai Message ==================================

Task decomposition is...
python  theme={null}
  from typing import Any
  from langchain_core.documents import Document
  from langchain.agents.middleware import AgentMiddleware, AgentState

class State(AgentState):
      context: list[Document]

class RetrieveDocumentsMiddleware(AgentMiddleware[State]):
      state_schema = State

def before_model(self, state: AgentState) -> dict[str, Any] | None:
          last_message = state["messages"][-1]
          retrieved_docs = vector_store.similarity_search(last_message.text)

docs_content = "\n\n".join(doc.page_content for doc in retrieved_docs)

augmented_message_content = (
              f"{last_message.text}\n\n"
              "Use the following context to answer the query:\n"
              f"{docs_content}"
          )
          return {
              "messages": [last_message.model_copy(update={"content": augmented_message_content})],
              "context": retrieved_docs,
          }

agent = create_agent(
      model,
      tools=[],
      middleware=[RetrieveDocumentsMiddleware()],
  )
  ```
</Accordion>

Now that we've implemented a simple RAG application via [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent), we can easily incorporate new features and go deeper:

* [Stream](/oss/python/langchain/streaming) tokens and other information for responsive user experiences
* Add [conversational memory](/oss/python/langchain/short-term-memory) to support multi-turn interactions
* Add [long-term memory](/oss/python/langchain/long-term-memory) to support memory across conversational threads
* Add [structured responses](/oss/python/langchain/structured-output)
* Deploy your application with [LangSmith Deployments](/langsmith/deployments)

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/rag.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
Let's test this out. We construct a question that would typically require an iterative sequence of retrieval steps to answer:
```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
Note that the agent:

1. Generates a query to search for a standard method for task decomposition;
2. Receiving the answer, generates a second query to search for common extensions of it;
3. Having received all necessary context, answers the question.

We can see the full sequence of steps, along with latency and other metadata, in the [LangSmith trace](https://smith.langchain.com/public/7b42d478-33d2-4631-90a4-7cb731681e88/r).

<Tip>
  You can add a deeper level of control and customization using the [LangGraph](/oss/python/langgraph/overview) framework directly— for example, you can add steps to grade document relevance and rewrite search queries. Check out LangGraph's [Agentic RAG tutorial](/oss/python/langgraph/agentic-rag) for more advanced formulations.
</Tip>

### RAG chains

In the above [agentic RAG](#rag-agents) formulation we allow the LLM to use its discretion in generating a [tool call](/oss/python/langchain/models#tool-calling) to help answer user queries. This is a good general-purpose solution, but comes with some trade-offs:

| ✅ Benefits                                                                                                                                                 | ⚠️ Drawbacks                                                                                                                                |
| ---------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------- |
| **Search only when needed** – The LLM can handle greetings, follow-ups, and simple queries without triggering unnecessary searches.                        | **Two inference calls** – When a search is performed, it requires one call to generate the query and another to produce the final response. |
| **Contextual search queries** – By treating search as a tool with a `query` input, the LLM crafts its own queries that incorporate conversational context. | **Reduced control** – The LLM may skip searches when they are actually needed, or issue extra searches when unnecessary.                    |
| **Multiple searches allowed** – The LLM can execute several searches in support of a single user query.                                                    |                                                                                                                                             |

Another common approach is a two-step chain, in which we always run a search (potentially using the raw user query) and incorporate the result as context for a single LLM query. This results in a single inference call per query, buying reduced latency at the expense of flexibility.

In this approach we no longer call the model in a loop, but instead make a single pass.

We can implement this chain by removing tools from the agent and instead incorporating the retrieval step into a custom prompt:
```

Example 4 (unknown):
```unknown
Let's try this out:
```

---

## If "env" is set to "test", then we don't actually delete any rows from our database.

**URL:** llms-txt#if-"env"-is-set-to-"test",-then-we-don't-actually-delete-any-rows-from-our-database.

---

## If prod tag points to commit a1b2c3d4, this is equivalent to:

**URL:** llms-txt#if-prod-tag-points-to-commit-a1b2c3d4,-this-is-equivalent-to:

**Contents:**
- Trigger a webhook on prompt commit
  - Configure a webhook
  - Trigger the webhook
- Public prompt hub

prompt = client.pull_prompt("joke-generator:a1b2c3d4")
```

For more information on how to use prompts in code, refer to [Managing prompts programmatically](/langsmith/manage-prompts-programmatically).

## Trigger a webhook on prompt commit

You can configure a webhook to be triggered whenever a commit is made to a prompt.

Some common use cases of this include:

* Triggering a CI/CD pipeline when prompts are updated.
* Synchronizing prompts with a GitHub repository.
* Notifying team members about prompt modifications.

### Configure a webhook

Navigate to the **Prompts** section in the left-hand sidebar or from the application homepage. In the top right corner, click on the `+ Webhook` button.

Add a webhook URL and any required headers.

<Note>
  You can only configure one webhook per workspace. If you want to configure multiple per workspace or set up a different webhook for each prompt, let us know in the [LangChain Forum](https://forum.langchain.com/).
</Note>

To test out your webhook, click the **Send test notification** button. This will send a test notification to the webhook URL you provided with a sample payload.

The sample payload is a JSON object with the following fields:

* `prompt_id`: The ID of the prompt that was committed.
* `prompt_name`: The name of the prompt that was committed.
* `commit_hash`: The commit hash of the prompt.
* `created_at`: The date of the commit.
* `created_by`: The author of the commit.
* `manifest`: The manifest of the prompt.

### Trigger the webhook

Commit to a prompt to trigger the webhook you've configured.

#### Use the Playground

If you do this in the Playground, you'll be prompted to deselect the webhooks you'd like to avoid triggering.

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/commit-prompt-playground.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=84f487c929ab7894bbd1e2c8922b6a9e" alt="" data-og-width="736" width="736" data-og-height="540" height="540" data-path="langsmith/images/commit-prompt-playground.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/commit-prompt-playground.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=68e38eb869ab7f97b5e53bb793a0ba24 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/commit-prompt-playground.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=84bbdbb22d53ca87b9e49b164d4dd190 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/commit-prompt-playground.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=f679941d9c42a52f30c858a00174749d 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/commit-prompt-playground.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=c8ba79844ab070cd1a7b443677bbef4a 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/commit-prompt-playground.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=d011a2ebe87104d006e71470204d158d 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/commit-prompt-playground.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=cdfcfddfdad692fd623b502efdd6589d 2500w" />

If you commit via the API, you can specify to skip triggering the webhook by setting the `skip_webhooks` parameter to `true` or to an array of webhook ids to ignore. Refer to the [API docs](https://api.smith.langchain.com/redoc#tag/commits/operation/create_commit_api_v1_commits__owner___repo__post) for more information.

LangSmith's public prompt hub is a collection of prompts that have been created by the LangChain community that you can use for reference.

<Note>
  Note that prompts are user-generated and unverified. LangChain does not review or endorse public prompts, use these at your own risk. Use of Prompt Hub is subject to our [Terms of Service](https://www.langchain.com/terms-of-service).
</Note>

Navigate to the **Prompts** section of the left-hand sidebar and click on **Browse all Public Prompts in the LangChain Hub**.

Here you'll find all of the publicly listed prompts in the LangChain Hub. You can search for prompts by name, handle, use cases, descriptions, or models. You can fork prompts to your personal organization, view the prompt's details, and run the prompt in the Playground. You can [pull any public prompt into your code](/langsmith/manage-prompts-programmatically) using the SDK.

To view prompts tied to your workspace, visit the **Prompts** tab in the sidebar.

<img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompts-tab.png?fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=d689013c2158309249c547086e145783" alt="" data-og-width="3012" width="3012" data-og-height="1704" height="1704" data-path="langsmith/images/prompts-tab.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompts-tab.png?w=280&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=ad811ae37a565d22851d6162d2f45e07 280w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompts-tab.png?w=560&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=d48f24e8fcc7d5813e7093f117c21cac 560w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompts-tab.png?w=840&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=b93e12cdf73897ab5373eb5019ac58cf 840w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompts-tab.png?w=1100&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=cb361802a4bb5d823dda53605b647567 1100w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompts-tab.png?w=1650&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=8450e66dcc99a90ea84b1fc66bdfc6ad 1650w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompts-tab.png?w=2500&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=5de8982f6602902e9fa239ba3f4157d4 2500w" />

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/manage-prompts.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## If you have a higher tiered Tavily API plan you can increase this

**URL:** llms-txt#if-you-have-a-higher-tiered-tavily-api-plan-you-can-increase-this

rate_limiter = InMemoryRateLimiter(requests_per_second=0.08)

---

## If you have pandas installed can easily explore results as df:

**URL:** llms-txt#if-you-have-pandas-installed-can-easily-explore-results-as-df:

---

## Image block

**URL:** llms-txt#image-block

**Contents:**
  - Serialize standard content
- Simplified package
  - Namespace
  - `langchain-classic`
- Breaking changes
  - Dropped Python 3.9 support
  - Updated return type for chat models
  - Default message format for OpenAI Responses API

image_block = {
    "type": "image",
    "url": "https://example.com/image.png",
    "mime_type": "image/png",
}
bash Environment variable theme={null}
  export LC_OUTPUT_VERSION=v1
  python Initialization parameter theme={null}
  from langchain.chat_models import init_chat_model

model = init_chat_model(
      "gpt-5-nano",
      output_version="v1",
  )
  python v1 (new) theme={null}
  # Chains
  from langchain_classic.chains import LLMChain

# Retrievers
  from langchain_classic.retrievers import ...

# Indexing
  from langchain_classic.indexes import ...

# Hub
  from langchain_classic import hub
  python v0 (old) theme={null}
  # Chains
  from langchain.chains import LLMChain

# Retrievers
  from langchain.retrievers import ...

# Indexing
  from langchain.indexes import ...

# Hub
  from langchain import hub
  bash  theme={null}
uv pip install langchain-classic
python v1 (new) theme={null}
  def bind_tools(
          ...
      ) -> Runnable[LanguageModelInput, AIMessage]:
  python v0 (old) theme={null}
  def bind_tools(
          ...
      ) -> Runnable[LanguageModelInput, BaseMessage]:
  python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
See the content blocks [reference](/oss/python/langchain/messages#content-block-reference) for more details.

### Serialize standard content

Standard content blocks are **not serialized** into the `content` attribute by default. If you need to access standard content blocks in the `content` attribute (e.g., when sending messages to a client), you can opt-in to serializing them into `content`.

<CodeGroup>
```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
</CodeGroup>

<Note>
  Learn more: [Messages](/oss/python/langchain/messages#message-content), [Standard content blocks](/oss/python/langchain/messages#standard-content-blocks), and [Multimodal](/oss/python/langchain/messages#multimodal).
</Note>

***

## Simplified package

The `langchain` package namespace has been significantly reduced in v1 to focus on essential building blocks for agents. The streamlined package makes it easier to discover and use the core functionality.

### Namespace

| Module                                                                                | What's available                                                                                                                                                                                                                                                          | Notes                             |
| ------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------- |
| [`langchain.agents`](https://reference.langchain.com/python/langchain/agents)         | [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent), [`AgentState`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.AgentState)                                                            | Core agent creation functionality |
| [`langchain.messages`](https://reference.langchain.com/python/langchain/messages)     | Message types, [content blocks](https://reference.langchain.com/python/langchain/messages/#langchain.messages.ContentBlock), [`trim_messages`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.trim_messages)                               | Re-exported from `langchain-core` |
| [`langchain.tools`](https://reference.langchain.com/python/langchain/tools)           | [`@tool`](https://reference.langchain.com/python/langchain/tools/#langchain.tools.tool), [`BaseTool`](https://reference.langchain.com/python/langchain/tools/#langchain.tools.BaseTool), injection helpers                                                                | Re-exported from `langchain-core` |
| [`langchain.chat_models`](https://reference.langchain.com/python/langchain/models)    | [`init_chat_model`](https://reference.langchain.com/python/langchain/models/#langchain.chat_models.init_chat_model), [`BaseChatModel`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel)   | Unified model initialization      |
| [`langchain.embeddings`](https://reference.langchain.com/python/langchain/embeddings) | [`init_embeddings`](https://reference.langchain.com/python/langchain_core/embeddings/#langchain_core.embeddings.embeddings.Embeddings), [`Embeddings`](https://reference.langchain.com/python/langchain_core/embeddings/#langchain_core.embeddings.embeddings.Embeddings) | Embedding models                  |

### `langchain-classic`

If you were using any of the following from the `langchain` package, you'll need to install [`langchain-classic`](https://pypi.org/project/langchain-classic/) and update your imports:

* Legacy chains (`LLMChain`, `ConversationChain`, etc.)
* Retrievers (e.g. `MultiQueryRetriever` or anything from the previous `langchain.retrievers` module)
* The indexing API
* The hub module (for managing prompts programmatically)
* Embeddings modules (e.g. `CacheBackedEmbeddings` and community embeddings)
* [`langchain-community`](https://pypi.org/project/langchain-community) re-exports
* Other deprecated functionality

<CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Immediate termination with "end" behavior

**URL:** llms-txt#immediate-termination-with-"end"-behavior

critical_tool_limiter = ToolCallLimitMiddleware(
    tool_name="delete_records",
    run_limit=1,
    exit_behavior="end",
)

---

## Implement a CI/CD pipeline using LangSmith Deployments and Evaluation

**URL:** llms-txt#implement-a-ci/cd-pipeline-using-langsmith-deployments-and-evaluation

**Contents:**
- Overview
- Pipeline architecture
  - Trigger sources
  - Testing layers
- GitHub Actions workflow
  - Prerequisites
- Deployment options
  - Prerequisites for manual deployment
  - Local development and testing

Source: https://docs.langchain.com/langsmith/cicd-pipeline-example

This guide demonstrates how to implement a comprehensive CI/CD pipeline for AI agent applications deployed in LangSmith Deployments. In this example, you'll use the [LangGraph](/oss/python/langgraph/overview) open source framework for orchestrating and building the agent, [LangSmith](/langsmith/home) for observability and evaluations. This pipeline is based on the [cicd-pipeline-example repository](https://github.com/langchain-ai/cicd-pipeline-example).

The CI/CD pipeline provides:

* <Icon icon="check-circle" /> **Automated testing**: Unit, integration, and end-to-end tests.
* <Icon icon="chart-line" /> **Offline evaluations**: Performance assessment using [AgentEvals](https://github.com/langchain-ai/agentevals), [OpenEvals](https://github.com/langchain-ai/openevals) and [LangSmith](https://docs.langchain.com/langsmith/home).
* <Icon icon="rocket" /> **Preview and production deployments**: Automated staging and quality-gated production releases using the Control Plane API.
* <Icon icon="eye" /> **Monitoring**: Continuous evaluation and alerting.

## Pipeline architecture

The CI/CD pipeline consists of several key components that work together to ensure code quality and reliable deployments:

There are multiple ways you can trigger this pipeline, either during development or if your application is already live. The pipeline can be triggered by:

* <Icon icon="code-branch" /> **Code changes**: Pushes to main/development branches where you can modify the LangGraph architecture, try different models, update agent logic, or make any code improvements.
* <Icon icon="edit" /> **PromptHub updates**: Changes to prompt templates stored in LangSmith PromptHub—whenever there's a new prompt commit, the system triggers a webhook to run the pipeline.
* <Icon icon="exclamation-triangle" /> **Online evaluation alerts**: Performance degradation notifications from live deployments
* <Icon icon="webhook" /> **LangSmith traces webhooks**: Automated triggers based on trace analysis and performance metrics.
* <Icon icon="play" /> **Manual trigger**: Manual initiation of the pipeline for testing or emergency deployments.

Compared to traditional software, testing AI agent applications also requires assessing response quality, so it is important to test each part of the workflow. The pipeline implements multiple testing layers:

1. <Icon icon="puzzle-piece" /> **Unit tests**: Individual node and utility function testing.
2. <Icon icon="link" /> **Integration tests**: Component interaction testing.
3. <Icon icon="route" /> **End-to-end tests**: Full graph execution testing.
4. <Icon icon="brain" /> **Offline evaluations**: Performance assessment with real-world scenarios including end-to-end evaluations, single-step evaluations, agent trajectory analysis, and multi-turn simulations.
5. <Icon icon="server" /> **LangGraph dev server tests**: Use the [langgraph-cli](/langsmith/cli) tool for spinning up (inside the GitHub Action) a local server to run the LangGraph agent. This polls the `/ok` server API endpoint until it is available and for 30 seconds, after that it throws an error.

## GitHub Actions workflow

The CI/CD pipeline uses GitHub Actions with the [Control Plane API](/langsmith/api-ref-control-plane) and [LangSmith API](https://api.smith.langchain.com/redoc) to automate deployment. A helper script manages API interactions and deployments: [https://github.com/langchain-ai/cicd-pipeline-example/blob/main/.github/scripts/langgraph\_api.py](https://github.com/langchain-ai/cicd-pipeline-example/blob/main/.github/scripts/langgraph_api.py).

The workflow includes:

* **New agent deployment**: When a new PR is opened and tests pass, a new preview deployment is created in LangSmith Deployments using the [Control Plane API](/langsmith/api-ref-control-plane). This allows you to test the agent in a staging environment before promoting to production.

* **Agent deployment revision**: A revision happens when an existing deployment with the same ID is found, or when the PR is merged into main. In the case of merging to main, the preview deployment is deleted and a production deployment is created. This ensures that any updates to the agent are properly deployed and integrated into the production infrastructure.

<img src="https://mintcdn.com/langchain-5e9cc07a/-UAx6PdOIJpPyTy2/langsmith/images/cicd-new-lgp-revision.png?fit=max&auto=format&n=-UAx6PdOIJpPyTy2&q=85&s=3ef7d51a322b8b5e2f9c2c70579fcc97" alt="Agent Deployment Revision Workflow" data-og-width="1022" width="1022" data-og-height="196" height="196" data-path="langsmith/images/cicd-new-lgp-revision.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/-UAx6PdOIJpPyTy2/langsmith/images/cicd-new-lgp-revision.png?w=280&fit=max&auto=format&n=-UAx6PdOIJpPyTy2&q=85&s=a3d06c339e84a1af99450d23e8bd617f 280w, https://mintcdn.com/langchain-5e9cc07a/-UAx6PdOIJpPyTy2/langsmith/images/cicd-new-lgp-revision.png?w=560&fit=max&auto=format&n=-UAx6PdOIJpPyTy2&q=85&s=30589c8727af3ecb1d97881fd6692554 560w, https://mintcdn.com/langchain-5e9cc07a/-UAx6PdOIJpPyTy2/langsmith/images/cicd-new-lgp-revision.png?w=840&fit=max&auto=format&n=-UAx6PdOIJpPyTy2&q=85&s=c05ab515ea0901fb2d076dee256ad108 840w, https://mintcdn.com/langchain-5e9cc07a/-UAx6PdOIJpPyTy2/langsmith/images/cicd-new-lgp-revision.png?w=1100&fit=max&auto=format&n=-UAx6PdOIJpPyTy2&q=85&s=b939ad6842110227f70cc0526468d21d 1100w, https://mintcdn.com/langchain-5e9cc07a/-UAx6PdOIJpPyTy2/langsmith/images/cicd-new-lgp-revision.png?w=1650&fit=max&auto=format&n=-UAx6PdOIJpPyTy2&q=85&s=0559d5b2a85414e954a72377b2eed9ec 1650w, https://mintcdn.com/langchain-5e9cc07a/-UAx6PdOIJpPyTy2/langsmith/images/cicd-new-lgp-revision.png?w=2500&fit=max&auto=format&n=-UAx6PdOIJpPyTy2&q=85&s=b8b96047a8b37f31b78d793cd7d18f45 2500w" />

* **Testing and evaluation workflow**: In addition to the more traditional testing phases (unit tests, integration tests, end-to-end tests, etc.), the pipeline includes [offline evaluations](/langsmith/evaluation-concepts#offline-evaluation) and [Agent dev server testing](/langsmith/local-server) because you want to test the quality of your agent. These evaluations provide comprehensive assessment of the agent's performance using real-world scenarios and data.

<img src="https://mintcdn.com/langchain-5e9cc07a/MrTet_AXQVddxOlO/langsmith/images/cicd-test-with-results.png?fit=max&auto=format&n=MrTet_AXQVddxOlO&q=85&s=477c3f5ec3d9bb9dfc354b9a57860636" alt="Test with Results Workflow" data-og-width="2050" width="2050" data-og-height="996" height="996" data-path="langsmith/images/cicd-test-with-results.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/MrTet_AXQVddxOlO/langsmith/images/cicd-test-with-results.png?w=280&fit=max&auto=format&n=MrTet_AXQVddxOlO&q=85&s=7c5885b5f85c1c408fda449c5a0c706a 280w, https://mintcdn.com/langchain-5e9cc07a/MrTet_AXQVddxOlO/langsmith/images/cicd-test-with-results.png?w=560&fit=max&auto=format&n=MrTet_AXQVddxOlO&q=85&s=3b9a25332a9f6b56edfc9fbbfec248c1 560w, https://mintcdn.com/langchain-5e9cc07a/MrTet_AXQVddxOlO/langsmith/images/cicd-test-with-results.png?w=840&fit=max&auto=format&n=MrTet_AXQVddxOlO&q=85&s=380cb346fffbaf13365b37c6fa955c05 840w, https://mintcdn.com/langchain-5e9cc07a/MrTet_AXQVddxOlO/langsmith/images/cicd-test-with-results.png?w=1100&fit=max&auto=format&n=MrTet_AXQVddxOlO&q=85&s=8994d1e816e725865f90a2ac6601f7a4 1100w, https://mintcdn.com/langchain-5e9cc07a/MrTet_AXQVddxOlO/langsmith/images/cicd-test-with-results.png?w=1650&fit=max&auto=format&n=MrTet_AXQVddxOlO&q=85&s=42b752f1e5f0043dd6998ae372e83874 1650w, https://mintcdn.com/langchain-5e9cc07a/MrTet_AXQVddxOlO/langsmith/images/cicd-test-with-results.png?w=2500&fit=max&auto=format&n=MrTet_AXQVddxOlO&q=85&s=043be8ed1ef59cea171f30146790a877 2500w" />

<AccordionGroup>
    <Accordion title="Final Response Evaluation" icon="check-circle">
      Evaluates the final output of your agent against expected results. This is the most common type of evaluation that checks if the agent's final response meets quality standards and answers the user's question correctly.
    </Accordion>

<Accordion title="Single Step Evaluation" icon="step-forward">
      Tests individual steps or nodes within your LangGraph workflow. This allows you to validate specific components of your agent's logic in isolation, ensuring each step functions correctly before testing the full pipeline.
    </Accordion>

<Accordion title="Agent Trajectory Evaluation" icon="route">
      Analyzes the complete path your agent takes through the graph, including all intermediate steps and decision points. This helps identify bottlenecks, unnecessary steps, or suboptimal routing in your agent's workflow. It also evaluates whether your agent invoked the right tools in the right order or at the right time.
    </Accordion>

<Accordion title="Multi-Turn Evaluation" icon="comments">
      Tests conversational flows where the agent maintains context across multiple interactions. This is crucial for agents that handle follow-up questions, clarifications, or extended dialogues with users.
    </Accordion>
  </AccordionGroup>

See the [LangGraph testing documentation](/oss/python/langgraph/test) for specific testing approaches and the [evaluation approaches guide](/langsmith/evaluation-approaches) for a comprehensive overview of offline evaluations.

Before setting up the CI/CD pipeline, ensure you have:

* <Icon icon="robot" /> An AI agent application (in this case built using [LangGraph](/oss/python/langgraph/overview))
* <Icon icon="user" /> A [LangSmith account](https://smith.langchain.com/)
* <Icon icon="key" /> A [LangSmith API key](/langsmith/create-account-api-key) needed to deploy agents and retrieve experiment results
* <Icon icon="cog" /> Project-specific environment variables configured in your repository secrets (e.g., LLM model API keys, vector store credentials, database connections)

<Note>
  While this example uses GitHub, the CI/CD pipeline works with other Git hosting platforms including GitLab, Bitbucket, and others.
</Note>

## Deployment options

LangSmith supports multiple deployment methods, depending on how your [LangSmith instance is hosted](/langsmith/platform-setup):

* <Icon icon="cloud" /> **Cloud LangSmith**: Direct GitHub integration or Docker image deployment.
* <Icon icon="server" /> **Self-Hosted/Hybrid**: Container registry-based deployments.

The deployment flow starts by modifying your agent implementation. At minimum, you must have a [`langgraph.json`](/langsmith/application-structure) and dependency file in your project (`requirements.txt` or `pyproject.toml`). Use the `langgraph dev` CLI tool to check for errors—fix any errors; otherwise, the deployment will succeed when deployed to LangSmith Deployments.

### Prerequisites for manual deployment

Before deploying your agent, ensure you have:

1. <Icon icon="project-diagram" /> **LangGraph graph**: Your agent implementation (e.g., `./agents/simple_text2sql.py:agent`).
2. <Icon icon="box" /> **Dependencies**: Either `requirements.txt` or `pyproject.toml` with all required packages.
3. <Icon icon="cog" /> **Configuration**: `langgraph.json` file specifying:
   * Path to your agent graph
   * Dependencies location
   * Environment variables
   * Python version

Example `langgraph.json`:

### Local development and testing

<img src="https://mintcdn.com/langchain-5e9cc07a/-UAx6PdOIJpPyTy2/langsmith/images/cicd-studio-cli.png?fit=max&auto=format&n=-UAx6PdOIJpPyTy2&q=85&s=425460d3401221ab441e21fc706c9cf1" alt="Studio CLI Interface" data-og-width="2972" width="2972" data-og-height="1354" height="1354" data-path="langsmith/images/cicd-studio-cli.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/-UAx6PdOIJpPyTy2/langsmith/images/cicd-studio-cli.png?w=280&fit=max&auto=format&n=-UAx6PdOIJpPyTy2&q=85&s=35e64359dba47f4db4962148073cfadb 280w, https://mintcdn.com/langchain-5e9cc07a/-UAx6PdOIJpPyTy2/langsmith/images/cicd-studio-cli.png?w=560&fit=max&auto=format&n=-UAx6PdOIJpPyTy2&q=85&s=c12eb479d5c46921633c56bdead978bc 560w, https://mintcdn.com/langchain-5e9cc07a/-UAx6PdOIJpPyTy2/langsmith/images/cicd-studio-cli.png?w=840&fit=max&auto=format&n=-UAx6PdOIJpPyTy2&q=85&s=b36efc12f81027b7364cea82a4600fc3 840w, https://mintcdn.com/langchain-5e9cc07a/-UAx6PdOIJpPyTy2/langsmith/images/cicd-studio-cli.png?w=1100&fit=max&auto=format&n=-UAx6PdOIJpPyTy2&q=85&s=131c3fa2e989fbb8ebc4748a5790dc36 1100w, https://mintcdn.com/langchain-5e9cc07a/-UAx6PdOIJpPyTy2/langsmith/images/cicd-studio-cli.png?w=1650&fit=max&auto=format&n=-UAx6PdOIJpPyTy2&q=85&s=afa56b4e5ca02495ef5e7cb69d8e1329 1650w, https://mintcdn.com/langchain-5e9cc07a/-UAx6PdOIJpPyTy2/langsmith/images/cicd-studio-cli.png?w=2500&fit=max&auto=format&n=-UAx6PdOIJpPyTy2&q=85&s=774ec3dcf76a4b0e61989cd12e41e0c3 2500w" />

First, test your agent locally using [Studio](/langsmith/studio):

```bash  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
### Trigger sources

There are multiple ways you can trigger this pipeline, either during development or if your application is already live. The pipeline can be triggered by:

* <Icon icon="code-branch" /> **Code changes**: Pushes to main/development branches where you can modify the LangGraph architecture, try different models, update agent logic, or make any code improvements.
* <Icon icon="edit" /> **PromptHub updates**: Changes to prompt templates stored in LangSmith PromptHub—whenever there's a new prompt commit, the system triggers a webhook to run the pipeline.
* <Icon icon="exclamation-triangle" /> **Online evaluation alerts**: Performance degradation notifications from live deployments
* <Icon icon="webhook" /> **LangSmith traces webhooks**: Automated triggers based on trace analysis and performance metrics.
* <Icon icon="play" /> **Manual trigger**: Manual initiation of the pipeline for testing or emergency deployments.

### Testing layers

Compared to traditional software, testing AI agent applications also requires assessing response quality, so it is important to test each part of the workflow. The pipeline implements multiple testing layers:

1. <Icon icon="puzzle-piece" /> **Unit tests**: Individual node and utility function testing.
2. <Icon icon="link" /> **Integration tests**: Component interaction testing.
3. <Icon icon="route" /> **End-to-end tests**: Full graph execution testing.
4. <Icon icon="brain" /> **Offline evaluations**: Performance assessment with real-world scenarios including end-to-end evaluations, single-step evaluations, agent trajectory analysis, and multi-turn simulations.
5. <Icon icon="server" /> **LangGraph dev server tests**: Use the [langgraph-cli](/langsmith/cli) tool for spinning up (inside the GitHub Action) a local server to run the LangGraph agent. This polls the `/ok` server API endpoint until it is available and for 30 seconds, after that it throws an error.

## GitHub Actions workflow

The CI/CD pipeline uses GitHub Actions with the [Control Plane API](/langsmith/api-ref-control-plane) and [LangSmith API](https://api.smith.langchain.com/redoc) to automate deployment. A helper script manages API interactions and deployments: [https://github.com/langchain-ai/cicd-pipeline-example/blob/main/.github/scripts/langgraph\_api.py](https://github.com/langchain-ai/cicd-pipeline-example/blob/main/.github/scripts/langgraph_api.py).

The workflow includes:

* **New agent deployment**: When a new PR is opened and tests pass, a new preview deployment is created in LangSmith Deployments using the [Control Plane API](/langsmith/api-ref-control-plane). This allows you to test the agent in a staging environment before promoting to production.

* **Agent deployment revision**: A revision happens when an existing deployment with the same ID is found, or when the PR is merged into main. In the case of merging to main, the preview deployment is deleted and a production deployment is created. This ensures that any updates to the agent are properly deployed and integrated into the production infrastructure.

  <img src="https://mintcdn.com/langchain-5e9cc07a/-UAx6PdOIJpPyTy2/langsmith/images/cicd-new-lgp-revision.png?fit=max&auto=format&n=-UAx6PdOIJpPyTy2&q=85&s=3ef7d51a322b8b5e2f9c2c70579fcc97" alt="Agent Deployment Revision Workflow" data-og-width="1022" width="1022" data-og-height="196" height="196" data-path="langsmith/images/cicd-new-lgp-revision.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/-UAx6PdOIJpPyTy2/langsmith/images/cicd-new-lgp-revision.png?w=280&fit=max&auto=format&n=-UAx6PdOIJpPyTy2&q=85&s=a3d06c339e84a1af99450d23e8bd617f 280w, https://mintcdn.com/langchain-5e9cc07a/-UAx6PdOIJpPyTy2/langsmith/images/cicd-new-lgp-revision.png?w=560&fit=max&auto=format&n=-UAx6PdOIJpPyTy2&q=85&s=30589c8727af3ecb1d97881fd6692554 560w, https://mintcdn.com/langchain-5e9cc07a/-UAx6PdOIJpPyTy2/langsmith/images/cicd-new-lgp-revision.png?w=840&fit=max&auto=format&n=-UAx6PdOIJpPyTy2&q=85&s=c05ab515ea0901fb2d076dee256ad108 840w, https://mintcdn.com/langchain-5e9cc07a/-UAx6PdOIJpPyTy2/langsmith/images/cicd-new-lgp-revision.png?w=1100&fit=max&auto=format&n=-UAx6PdOIJpPyTy2&q=85&s=b939ad6842110227f70cc0526468d21d 1100w, https://mintcdn.com/langchain-5e9cc07a/-UAx6PdOIJpPyTy2/langsmith/images/cicd-new-lgp-revision.png?w=1650&fit=max&auto=format&n=-UAx6PdOIJpPyTy2&q=85&s=0559d5b2a85414e954a72377b2eed9ec 1650w, https://mintcdn.com/langchain-5e9cc07a/-UAx6PdOIJpPyTy2/langsmith/images/cicd-new-lgp-revision.png?w=2500&fit=max&auto=format&n=-UAx6PdOIJpPyTy2&q=85&s=b8b96047a8b37f31b78d793cd7d18f45 2500w" />

* **Testing and evaluation workflow**: In addition to the more traditional testing phases (unit tests, integration tests, end-to-end tests, etc.), the pipeline includes [offline evaluations](/langsmith/evaluation-concepts#offline-evaluation) and [Agent dev server testing](/langsmith/local-server) because you want to test the quality of your agent. These evaluations provide comprehensive assessment of the agent's performance using real-world scenarios and data.

  <img src="https://mintcdn.com/langchain-5e9cc07a/MrTet_AXQVddxOlO/langsmith/images/cicd-test-with-results.png?fit=max&auto=format&n=MrTet_AXQVddxOlO&q=85&s=477c3f5ec3d9bb9dfc354b9a57860636" alt="Test with Results Workflow" data-og-width="2050" width="2050" data-og-height="996" height="996" data-path="langsmith/images/cicd-test-with-results.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/MrTet_AXQVddxOlO/langsmith/images/cicd-test-with-results.png?w=280&fit=max&auto=format&n=MrTet_AXQVddxOlO&q=85&s=7c5885b5f85c1c408fda449c5a0c706a 280w, https://mintcdn.com/langchain-5e9cc07a/MrTet_AXQVddxOlO/langsmith/images/cicd-test-with-results.png?w=560&fit=max&auto=format&n=MrTet_AXQVddxOlO&q=85&s=3b9a25332a9f6b56edfc9fbbfec248c1 560w, https://mintcdn.com/langchain-5e9cc07a/MrTet_AXQVddxOlO/langsmith/images/cicd-test-with-results.png?w=840&fit=max&auto=format&n=MrTet_AXQVddxOlO&q=85&s=380cb346fffbaf13365b37c6fa955c05 840w, https://mintcdn.com/langchain-5e9cc07a/MrTet_AXQVddxOlO/langsmith/images/cicd-test-with-results.png?w=1100&fit=max&auto=format&n=MrTet_AXQVddxOlO&q=85&s=8994d1e816e725865f90a2ac6601f7a4 1100w, https://mintcdn.com/langchain-5e9cc07a/MrTet_AXQVddxOlO/langsmith/images/cicd-test-with-results.png?w=1650&fit=max&auto=format&n=MrTet_AXQVddxOlO&q=85&s=42b752f1e5f0043dd6998ae372e83874 1650w, https://mintcdn.com/langchain-5e9cc07a/MrTet_AXQVddxOlO/langsmith/images/cicd-test-with-results.png?w=2500&fit=max&auto=format&n=MrTet_AXQVddxOlO&q=85&s=043be8ed1ef59cea171f30146790a877 2500w" />

  <AccordionGroup>
    <Accordion title="Final Response Evaluation" icon="check-circle">
      Evaluates the final output of your agent against expected results. This is the most common type of evaluation that checks if the agent's final response meets quality standards and answers the user's question correctly.
    </Accordion>

    <Accordion title="Single Step Evaluation" icon="step-forward">
      Tests individual steps or nodes within your LangGraph workflow. This allows you to validate specific components of your agent's logic in isolation, ensuring each step functions correctly before testing the full pipeline.
    </Accordion>

    <Accordion title="Agent Trajectory Evaluation" icon="route">
      Analyzes the complete path your agent takes through the graph, including all intermediate steps and decision points. This helps identify bottlenecks, unnecessary steps, or suboptimal routing in your agent's workflow. It also evaluates whether your agent invoked the right tools in the right order or at the right time.
    </Accordion>

    <Accordion title="Multi-Turn Evaluation" icon="comments">
      Tests conversational flows where the agent maintains context across multiple interactions. This is crucial for agents that handle follow-up questions, clarifications, or extended dialogues with users.
    </Accordion>
  </AccordionGroup>

  See the [LangGraph testing documentation](/oss/python/langgraph/test) for specific testing approaches and the [evaluation approaches guide](/langsmith/evaluation-approaches) for a comprehensive overview of offline evaluations.

### Prerequisites

Before setting up the CI/CD pipeline, ensure you have:

* <Icon icon="robot" /> An AI agent application (in this case built using [LangGraph](/oss/python/langgraph/overview))
* <Icon icon="user" /> A [LangSmith account](https://smith.langchain.com/)
* <Icon icon="key" /> A [LangSmith API key](/langsmith/create-account-api-key) needed to deploy agents and retrieve experiment results
* <Icon icon="cog" /> Project-specific environment variables configured in your repository secrets (e.g., LLM model API keys, vector store credentials, database connections)

<Note>
  While this example uses GitHub, the CI/CD pipeline works with other Git hosting platforms including GitLab, Bitbucket, and others.
</Note>

## Deployment options

LangSmith supports multiple deployment methods, depending on how your [LangSmith instance is hosted](/langsmith/platform-setup):

* <Icon icon="cloud" /> **Cloud LangSmith**: Direct GitHub integration or Docker image deployment.
* <Icon icon="server" /> **Self-Hosted/Hybrid**: Container registry-based deployments.

The deployment flow starts by modifying your agent implementation. At minimum, you must have a [`langgraph.json`](/langsmith/application-structure) and dependency file in your project (`requirements.txt` or `pyproject.toml`). Use the `langgraph dev` CLI tool to check for errors—fix any errors; otherwise, the deployment will succeed when deployed to LangSmith Deployments.
```

Example 2 (unknown):
```unknown
### Prerequisites for manual deployment

Before deploying your agent, ensure you have:

1. <Icon icon="project-diagram" /> **LangGraph graph**: Your agent implementation (e.g., `./agents/simple_text2sql.py:agent`).
2. <Icon icon="box" /> **Dependencies**: Either `requirements.txt` or `pyproject.toml` with all required packages.
3. <Icon icon="cog" /> **Configuration**: `langgraph.json` file specifying:
   * Path to your agent graph
   * Dependencies location
   * Environment variables
   * Python version

Example `langgraph.json`:
```

Example 3 (unknown):
```unknown
### Local development and testing

<img src="https://mintcdn.com/langchain-5e9cc07a/-UAx6PdOIJpPyTy2/langsmith/images/cicd-studio-cli.png?fit=max&auto=format&n=-UAx6PdOIJpPyTy2&q=85&s=425460d3401221ab441e21fc706c9cf1" alt="Studio CLI Interface" data-og-width="2972" width="2972" data-og-height="1354" height="1354" data-path="langsmith/images/cicd-studio-cli.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/-UAx6PdOIJpPyTy2/langsmith/images/cicd-studio-cli.png?w=280&fit=max&auto=format&n=-UAx6PdOIJpPyTy2&q=85&s=35e64359dba47f4db4962148073cfadb 280w, https://mintcdn.com/langchain-5e9cc07a/-UAx6PdOIJpPyTy2/langsmith/images/cicd-studio-cli.png?w=560&fit=max&auto=format&n=-UAx6PdOIJpPyTy2&q=85&s=c12eb479d5c46921633c56bdead978bc 560w, https://mintcdn.com/langchain-5e9cc07a/-UAx6PdOIJpPyTy2/langsmith/images/cicd-studio-cli.png?w=840&fit=max&auto=format&n=-UAx6PdOIJpPyTy2&q=85&s=b36efc12f81027b7364cea82a4600fc3 840w, https://mintcdn.com/langchain-5e9cc07a/-UAx6PdOIJpPyTy2/langsmith/images/cicd-studio-cli.png?w=1100&fit=max&auto=format&n=-UAx6PdOIJpPyTy2&q=85&s=131c3fa2e989fbb8ebc4748a5790dc36 1100w, https://mintcdn.com/langchain-5e9cc07a/-UAx6PdOIJpPyTy2/langsmith/images/cicd-studio-cli.png?w=1650&fit=max&auto=format&n=-UAx6PdOIJpPyTy2&q=85&s=afa56b4e5ca02495ef5e7cb69d8e1329 1650w, https://mintcdn.com/langchain-5e9cc07a/-UAx6PdOIJpPyTy2/langsmith/images/cicd-studio-cli.png?w=2500&fit=max&auto=format&n=-UAx6PdOIJpPyTy2&q=85&s=774ec3dcf76a4b0e61989cd12e41e0c3 2500w" />

First, test your agent locally using [Studio](/langsmith/studio):
```

---

## Implement a LangChain integration

**URL:** llms-txt#implement-a-langchain-integration

Source: https://docs.langchain.com/oss/javascript/contributing/implement-langchain

Integration packages are Python packages that users can install for use in their projects. They implement one or more components that adhere to the LangChain interface standards.

LangChain components are subclasses of base classes in [`langchain-core`](https://github.com/langchain-ai/langchain/tree/master/libs/core). Examples include [chat models](/oss/javascript/integrations/chat), [tools](/oss/javascript/integrations/tools), [retrievers](/oss/javascript/integrations/retrievers), and more.

Your integration package will typically implement a subclass of at least one of these components. Expand the tabs below to see details on each.

<Tabs>
  <Tab title="Chat Models">
    Chat models are subclasses of the @\[`BaseChatModel`] class. They implement methods for generating chat completions, handling message formatting, and managing model parameters.

<Warning>
      The chat model integration guide is currently WIP. In the meantime, read the [chat model conceptual guide](/oss/javascript/langchain/models) for details on how LangChain chat models function.
    </Warning>
  </Tab>

<Tab title="Tools">
    Tools are used in 2 main ways:

1. To define an "input schema" or "args schema" to pass to a chat model's tool calling feature along with a text request, such that the chat model can generate a "tool call", or parameters to call the tool with.
    2. To take a "tool call" as generated above, and take some action and return a response that can be passed back to the chat model as a ToolMessage.

The Tools class must inherit from the @\[`BaseTool`] base class. This interface has 3 properties and 2 methods that should be implemented in a subclass.

<Warning>
      The tools integration guide is currently WIP. In the meantime, read the [tools conceptual guide](/oss/javascript/langchain/tools) for details on how LangChain tools function.
    </Warning>
  </Tab>

<Tab title="Retrievers">
    Retrievers are used to retrieve documents from APIs, databases, or other sources based on a query. The Retriever class must inherit from the BaseRetriever base class.

<Warning>
      The retriever integration guide is currently WIP. In the meantime, read the [retriever conceptual guide](/oss/javascript/integrations/retrievers) for details on how LangChain retrievers function.
    </Warning>
  </Tab>

<Tab title="Vector Stores">
    All vector stores must inherit from the @\[`VectorStore`] base class. This interface consists of methods for writing, deleting and searching for documents in the vector store.

See the [vector store integration guide](/oss/javascript/integrations/vectorstores) for details on implementing a vector store integration.

<Warning>
      The vector store integration guide is currently WIP. In the meantime, read the [vector store conceptual guide](/oss/javascript/integrations/vectorstores) for details on how LangChain vector stores function.
    </Warning>
  </Tab>

<Tab title="Embeddings">
    Embedding models are subclasses of the [`Embeddings`](https://v03.api.js.langchain.com/classes/_langchain_core.embeddings.Embeddings.html) class.

<Warning>
      The embedding model integration guide is currently WIP. In the meantime, read the [embedding model conceptual guide](/oss/javascript/integrations/text_embedding) for details on how LangChain embedding models function.
    </Warning>
  </Tab>
</Tabs>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/contributing/implement-langchain.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Implement a LangGraph integration

**URL:** llms-txt#implement-a-langgraph-integration

Source: https://docs.langchain.com/oss/javascript/contributing/implement-langgraph

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/contributing/implement-langgraph.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Implement distributed tracing

**URL:** llms-txt#implement-distributed-tracing

**Contents:**
- Distributed tracing in Python

Source: https://docs.langchain.com/langsmith/distributed-tracing

Sometimes, you need to trace a request across multiple services.

LangSmith supports distributed tracing out of the box, linking runs within a trace across services using context propagation headers (`langsmith-trace` and optional `baggage` for metadata/tags).

Example client-server setup:

* Trace starts on client
* Continues on server

## Distributed tracing in Python

```python  theme={null}

---

## Improve LLM-as-judge evaluators using human feedback

**URL:** llms-txt#improve-llm-as-judge-evaluators-using-human-feedback

**Contents:**
- How it works
- Prerequisites
  - Offline evaluations
  - Online evaluations
- Getting started
- 1. Select experiments or runs
- 2. Label examples
- 3. Test your evaluator prompt against the labeled examples
- 4. Repeat to improve evaluator alignment
  - Tips for improving evaluator alignment

Source: https://docs.langchain.com/langsmith/improve-judge-evaluator-feedback

<Check>
  Before working through this page, it might be helpful to read the following:

* [Evaluation concepts](/langsmith/evaluation-concepts#evaluators)
  * [Creating LLM-as-a-judge evaluators](/langsmith/llm-as-judge)
</Check>

Reliable [*LLM-as-a-judge evaluators*](/langsmith/evaluation-concepts#llm-as-judge) are critical for making informed decisions about your AI applications (e.g., prompt, model, architecture changes). Defining the evaluator prompt correctly can be difficult, but it directly affects the trustworthiness of your evaluations.

This guide describes how to align your LLM-as-a-judge evaluator using human feedback to improve your evaluator's quality and help you build reliable AI applications.

LangSmith's **Align Evaluator** feature has a series of steps that help you align your LLM-as-a-judge evaluator with human expert feedback. You can use this feature to align evaluators that run on a dataset for [offline evaluations](/langsmith/evaluation-concepts#offline-evaluation) or for [online evaluations](/langsmith/evaluation-concepts#online-evaluation). In either case, the steps are similar:

1. **Select experiments or runs** that contain outputs from your application.
2. Add the selected experiments or runs to an **annotation queue** where a human expert can label the data.
3. **Test your LLM-as-a-judge evaluator prompt** against the labeled examples. Check the cases where your evaluator result is not aligned with the labeled data. This indicates areas where your evaluator prompt needs improvement.
4. **Refine and repeat** to improve evaluator alignment. Update your LLM-as-a-judge evaluator prompt and test again.

You'll need the following before starting this guide for [offline evaluations](#offline-evaluations) or [online evaluations](#online-evaluations):

### Offline evaluations

* A [dataset](/langsmith/evaluation-concepts#datasets) with at least one [experiment](/langsmith/evaluation-concepts#experiment).
* You'll need to upload or create datasets via the [SDK](/langsmith/manage-datasets-programmatically#create-a-dataset) or the [UI](/langsmith/manage-datasets-in-application#set-up-your-dataset) and run an experiment via the [SDK](/langsmith/evaluate-llm-application#run-the-evaluation) or the [Playground](/langsmith/run-evaluation-from-prompt-playground#5-run-your-evaluation).

### Online evaluations

* An application that’s already sending traces to LangSmith.
* Configure this with one of the [tracing integrations](/langsmith/observability-concepts#integrations) to start.

You can enter the alignment flow for both new and existing evaluators in datasets and tracing projects.

|                                              | Dataset Evaluators                                                                                                                                                                                     | Tracing Project Evaluators                                                                                                                                                                         |
| -------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Create an aligned evaluator from scratch** | 1. **Datasets & Experiments** and select your dataset<br />2. Click **+ Evaluator** > **Create from labeled data**<br />3. Enter a descriptive feedback key name (e.g. `correctness`, `hallucination`) | 1. **Projects** and select your project<br />2. Click **+ New** > **Evaluator** > **Create from labeled data**<br />3. Enter a descriptive feedback‑key name (e.g. `correctness`, `hallucination`) |
| **Align an existing evaluator**              | 1. **Datasets & Experiments** > select your dataset > **Evaluators** tab<br />2. In the **Align Evaluator with experiment data** box, click **Select Experiments**                                     | 1. **Projects** > select your project > **Evaluators** tab<br />2. In the **Align Evaluator with experiment data** box, click **Select Experiments**                                               |

## 1. Select experiments or runs

Select one or more experiments (or runs) to send for human labeling. This will add runs to an [annotation queue](/langsmith/evaluation-concepts#annotation-queues).

<img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-to-evaluator-queue.gif?s=3c457faaaf2c7f31425ba510a518260e" alt="Add to evaluator queue" data-og-width="1976" width="1976" data-og-height="1080" height="1080" data-path="langsmith/images/add-to-evaluator-queue.gif" data-optimize="true" data-opv="3" />

To add any new experiments/runs to an existing annotation queue, head to the **Evaluators** tab, select the evaluator you are aligning and click **Add to Queue.**

<Check>
  Datasets should be representative of inputs and outputs you expect to see in production.

While you don’t need to cover every possible scenario, it’s important to include examples across the full range of expected use cases. For example, if you're building a sports bot that answers questions about baseball, basketball, and football, your dataset should include at least one labeled example from each sport.
</Check>

Label examples in the annotation queue by adding a feedback score. Once you've labeled an example, click **Add to Reference Dataset**.

<Check>
  If you have a large number of examples in your experiments, you don't need to label every example to get started. We recommend starting with at least 20 examples, you can always add more later. We recommend that the examples that you label are diverse (balanced in both 0 and 1 labels) to ensure that you're building a well rounded evaluator prompt.
</Check>

## 3. Test your evaluator prompt against the labeled examples

Once you have labeled examples, the next step is iterating on your evaluator prompt to mimic the labeled data as well as possible. This iteration is done in the **Evaluator Playground**.

To go to the evaluator playground: Click the **View evaluator** button on the top right of the evaluator queue. This will take you to the detail page of the evaluator you are aligning. Click the **Evaluator Playground** button to access the playground.

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluator-pg.gif?s=c8108ce3abc95ce756fcbd1b3726916d" alt="Evaluator Playground" data-og-width="1916" width="1916" data-og-height="1080" height="1080" data-path="langsmith/images/evaluator-pg.gif" data-optimize="true" data-opv="3" />

In the evaluator playground you can create or edit your evaluator prompt and click **Start Alignment** to run it over the set of labeled examples that you created in Step 2. After running your evaluator, you'll see how its generated scores compare to your human labels. The alignment score is the percentage of examples where the evaluator's judgment matches that of the human expert.

<img src="https://mintlify.s3.us-west-1.amazonaws.com/langchain-5e9cc07a/langsmith/images/alignment-evaluator-pg.gif" alt="Evaluator Playground" />

## 4. Repeat to improve evaluator alignment

Iterate by updating your prompt and testing again to improve evaluator alignment.

<Check>
  Updates to your evaluator prompt are **not saved by default**. We reccomend saving your evaluator prompt regularly, and especially after you see your alignment score improve.

The evaluator playground will show the alignment score for the most recently saved version of your evaluator prompt for comparison when you're iterating on your prompt.
</Check>

Improving the alignment score of your evaluator isn't an exact science but there are a few strategies that are helpful in increasing the alignment score.

### Tips for improving evaluator alignment

**1. Investigate misaligned examples**

Digging into misaligned examples and trying to group them into common failure modes is a great first step for improving your evaluator alignment.

Once you have identified the common failure modes, add instructions to your evaluator prompt so the LLM knows about them. For example, you could explain that "MFA stands for "multi-factor authentication" if you notice it not understanding that specific acronym. Or you could tell it that "a good response will always contain at least 3 potential hotels to book" if it is confused on what good/bad means in your evaluator's context.

**2. Inspect the reasoning behind the LLM score**

To understand why the LLM scored an example the way it did, you can enable reasoning for your LLM-as-a-judge evaluator. Reasoning is helpful to understand the LLM's thought process and can help you identify common failure modes to incorporate into your evaluator prompt as well..

In order to see the reasoning in the evaluator playground, hover over the LLM score.

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/enable-reasoning.gif?s=d6ed09e4149a63723ef9305544b8b652" alt="Enable reasoning" data-og-width="1520" width="1520" data-og-height="1080" height="1080" data-path="langsmith/images/enable-reasoning.gif" data-optimize="true" data-opv="3" />

This will show the reasoning behind the LLM's score in the evaluator playground.

**3. Add more labeled examples and validate performance**

To avoid overfitting to the labeled examples, it's important to add more labeled examples and test performance, especially if you started off with a small number of examples.

<iframe className="w-full aspect-video rounded-xl" src="https://www.youtube.com/embed/-9o94oj4x0A?si=wfv9cN3L4DalMD2e" title="YouTube video player" frameBorder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowFullScreen />

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/improve-judge-evaluator-feedback.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Include multimodal content in a prompt

**URL:** llms-txt#include-multimodal-content-in-a-prompt

**Contents:**
- Inline content
- Template variables
- Populate the template variable
- Run an evaluation

Source: https://docs.langchain.com/langsmith/multimodal-content

Some applications are based around multimodal content, like a chatbot that can answer questions about a PDF or image. In these cases, you'll want to include multimodal content in your prompt and test the model's ability to answer questions about the content.

The LangSmith Playground supports two methods for incorporating multimodal content in your prompts:

1. Inline content: Embed static files (images, PDFs, audio) directly in your prompt. This is ideal when you want to consistently include the same multimodal content across all uses of the prompt. For example, you might include a reference image that helps ground the model's responses.

2. Template variables: Create dynamic placeholders for attachments that can be populated with different content each time. This approach offers more flexibility, allowing you to:

* Test how the model handles different inputs
   * Create reusable prompts that work with varying content

<Note>
  Not all models support multimodal content. Before using multimodal features in the playground, make sure your selected model supports the file types you want to use.
</Note>

Click the file icon in the message where you want to add multimodal content. Under the `Upload content` tab, you can upload a file and include it inline in the prompt.

<img src="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/upload-inline-multimodal-content.png?fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=b93d2a6731d26d3ff58a3d0d6d909159" alt="" data-og-width="410" width="410" data-og-height="339" height="339" data-path="langsmith/images/upload-inline-multimodal-content.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/upload-inline-multimodal-content.png?w=280&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=ae329136b8de8d5f4b12ecab49651063 280w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/upload-inline-multimodal-content.png?w=560&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=f1644a04d5a33e803a15bc9794ecb528 560w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/upload-inline-multimodal-content.png?w=840&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=275ffe46010c6b24012ed049a17a59fb 840w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/upload-inline-multimodal-content.png?w=1100&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=9591d26a3b27c0462f7674f84c2a525c 1100w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/upload-inline-multimodal-content.png?w=1650&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=f4439dc91e005750232b40efbb19954d 1650w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/upload-inline-multimodal-content.png?w=2500&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=1c429f8f78846a14e0c9a379b9a24355 2500w" />

## Template variables

Click the file icon in the message where you want to add multimodal content. Under the `Template variables` tab, you can create a template variable for a specific attachment type. Currently, only images, PDFs, and audio files (.wav, .mp3) are supported.

<img src="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/template-variable-multimodal-content.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=e69596dd9fe7d16252c7054bf9efcdf0" alt="" data-og-width="391" width="391" data-og-height="303" height="303" data-path="langsmith/images/template-variable-multimodal-content.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/template-variable-multimodal-content.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=1efb9ed1c9a4a64be7174b90ffbfe664 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/template-variable-multimodal-content.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=1375989ffa599f506d308441e46907f9 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/template-variable-multimodal-content.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=f50c62b520314c18e433f77952ff79a2 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/template-variable-multimodal-content.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=e38e8283352a13b54e96a381a2449396 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/template-variable-multimodal-content.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=331ff1a4664debd8d7b08a6ad645d9c6 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/template-variable-multimodal-content.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=ffa740fa2a3f4f52173cd073843d697f 2500w" />

## Populate the template variable

Once you've added a template variable, you can provide content for it using the panel on the right side of the screen. Simply click the `+` button to upload or select content that will be used to populate the template variable.

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/manual-prompt-multimodal.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=5983e91ca9f596918c9068f8d7450d8d" alt="" data-og-width="1466" width="1466" data-og-height="482" height="482" data-path="langsmith/images/manual-prompt-multimodal.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/manual-prompt-multimodal.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=ccb36221eb97f54196de13df4f9749e7 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/manual-prompt-multimodal.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=6b4554ac3156e805655eba6afd2ce771 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/manual-prompt-multimodal.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=a3e85646401f9aa31e1d79257344c158 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/manual-prompt-multimodal.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=9727ce9dec69152f6e49e3d8ff575300 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/manual-prompt-multimodal.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=f73957416f51c78a0e29c449f64ff1e2 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/manual-prompt-multimodal.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=ab8421f512b312b56ac515b970d337f4 2500w" />

After testing out your prompt manually, you can [run an evaluation](/langsmith/evaluate-with-attachments?mode=ui) to see how the prompt performs over a golden dataset of examples.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/multimodal-content.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Indicate that a chat model supports image inputs

**URL:** llms-txt#indicate-that-a-chat-model-supports-image-inputs

**Contents:**
- Running tests

class TestChatParrotLinkStandard(ChatModelIntegrationTests):
    # ... other required properties

@property
    def supports_image_inputs(self) -> bool:
        return True  # (The default is False)
bash  theme={null}
make test
make integration_test
bash  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
<Note>
  You should organize tests in these subdirectories relative to the root of your package:

  * `tests/unit_tests` for unit tests
  * `tests/integration_tests` for integration tests
</Note>

To see the complete list of configurable capabilities and their defaults, visit the [API reference](https://reference.langchain.com/python/langchain_tests) for standard tests.

Here are some example implementations of standard tests from popular integrations:

<Tabs>
  <Tab title="Unit tests">
    <Columns cols={3}>
      <Card title="ChatOpenAI" href="https://github.com/langchain-ai/langchain/blob/master/libs/partners/openai/tests/unit_tests/chat_models/test_base_standard.py" arrow>Unit tests</Card>
      <Card title="ChatAnthropic" href="https://github.com/langchain-ai/langchain/blob/master/libs/partners/anthropic/tests/unit_tests/test_standard.py" arrow>Unit tests</Card>
      <Card title="ChatGenAI" href="https://github.com/langchain-ai/langchain-google/blob/main/libs/genai/tests/unit_tests/test_standard.py" arrow>Unit tests</Card>
    </Columns>
  </Tab>

  <Tab title="Integration tests">
    <Columns cols={3}>
      <Card title="ChatOpenAI" href="https://github.com/langchain-ai/langchain/blob/master/libs/partners/openai/tests/integration_tests/chat_models/test_base_standard.py" arrow>Integration tests</Card>
      <Card title="ChatAnthropic" href="https://github.com/langchain-ai/langchain/blob/master/libs/partners/anthropic/tests/integration_tests/test_standard.py" arrow>Integration tests</Card>
      <Card title="ChatGenAI" href="https://github.com/langchain-ai/langchain-google/blob/main/libs/genai/tests/integration_tests/test_standard.py" arrow>Integration tests</Card>
    </Columns>
  </Tab>
</Tabs>

***

## Running tests

If bootstrapping an integration from a template, a `Makefile` is provided that includes targets for running unit and integration tests:
```

Example 2 (unknown):
```unknown
Otherwise, if you follow the recommended directory structure, you can run tests with:
```

---

## Initialize an in-memory checkpointer for persistence

**URL:** llms-txt#initialize-an-in-memory-checkpointer-for-persistence

checkpointer = InMemorySaver()

@task
def slow_task():
    """
    Simulates a slow-running task by introducing a 1-second delay.
    """
    time.sleep(1)
    return "Ran slow task."

@entrypoint(checkpointer=checkpointer)
def main(inputs, writer: StreamWriter):
    """
    Main workflow function that runs the slow_task and get_info tasks sequentially.

Parameters:
    - inputs: Dictionary containing workflow input values.
    - writer: StreamWriter for streaming custom data.

The workflow first executes `slow_task` and then attempts to execute `get_info`,
    which will fail on the first invocation.
    """
    slow_task_result = slow_task().result()  # Blocking call to slow_task
    get_info().result()  # Exception will be raised here on the first attempt
    return slow_task_result

---

## Initialize multiple instrumentors

**URL:** llms-txt#initialize-multiple-instrumentors

OpenAIInstrumentor().instrument()
DSPyInstrumentor().instrument()

---

## Initialize the LangSmith Client so we can use to get the dataset

**URL:** llms-txt#initialize-the-langsmith-client-so-we-can-use-to-get-the-dataset

---

## initialize the langsmith client with the anonymization functions

**URL:** llms-txt#initialize-the-langsmith-client-with-the-anonymization-functions

langsmith_client = Client(
  hide_inputs=comprehend_anonymize, hide_outputs=comprehend_anonymize
)

---

## Initial run - hits the interrupt and pauses

**URL:** llms-txt#initial-run---hits-the-interrupt-and-pauses

---

## InMemoryStore saves data to an in-memory dictionary. Use a DB-backed store in production use.

**URL:** llms-txt#inmemorystore-saves-data-to-an-in-memory-dictionary.-use-a-db-backed-store-in-production-use.

store = InMemoryStore(index={"embed": embed, "dims": 2}) # [!code highlight]
user_id = "my-user"
application_context = "chitchat"
namespace = (user_id, application_context) # [!code highlight]
store.put( # [!code highlight]
    namespace,
    "a-memory",
    {
        "rules": [
            "User likes short, direct language",
            "User only speaks English & python",
        ],
        "my-key": "my-value",
    },
)

---

## InMemoryStore saves data to an in-memory dictionary. Use a DB-backed store in production.

**URL:** llms-txt#inmemorystore-saves-data-to-an-in-memory-dictionary.-use-a-db-backed-store-in-production.

store = InMemoryStore() # [!code highlight]

@dataclass
class Context:
    user_id: str

---

## 'inputs' will come from your dataset.

**URL:** llms-txt#'inputs'-will-come-from-your-dataset.

---

## Install LangChain

**URL:** llms-txt#install-langchain

Source: https://docs.langchain.com/oss/javascript/langchain/install

To install the LangChain package:

LangChain provides integrations to hundreds of LLMs and thousands of other integrations. These live in independent provider packages. For example:

<Tip>
  See the [Integrations tab](/oss/javascript/integrations/providers/overview) for a full list of available integrations.
</Tip>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/install.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>

LangChain provides integrations to hundreds of LLMs and thousands of other integrations. These live in independent provider packages. For example:

<CodeGroup>
```

---

## Install LangGraph

**URL:** llms-txt#install-langgraph

Source: https://docs.langchain.com/oss/javascript/langgraph/install

To install the base LangGraph package:

To use LangGraph you will usually want to access LLMs and define tools.
You can do this however you see fit.

One way to do this (which we will use in the docs) is to use [LangChain](/oss/javascript/langchain/overview).

Install LangChain with:

To work with specific LLM provider packages, you will need install them separately.

Refer to the [integrations](/oss/javascript/integrations/providers/overview) page for provider-specific installation instructions.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/install.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>

To use LangGraph you will usually want to access LLMs and define tools.
You can do this however you see fit.

One way to do this (which we will use in the docs) is to use [LangChain](/oss/javascript/langchain/overview).

Install LangChain with:

<CodeGroup>
```

---

## Instructions for extracting the user/purchase info from the conversation.

**URL:** llms-txt#instructions-for-extracting-the-user/purchase-info-from-the-conversation.

gather_info_instructions = """You are managing an online music store that sells song tracks. \
Customers can buy multiple tracks at a time and these purchases are recorded in a database as \
an Invoice per purchase and an associated set of Invoice Lines for each purchased track.

Your task is to help customers who would like a refund for one or more of the tracks they've \
purchased. In order for you to be able refund them, the customer must specify the Invoice ID \
to get a refund on all the tracks they bought in a single transaction, or one or more Invoice \
Line IDs if they would like refunds on individual tracks.

Often a user will not know the specific Invoice ID(s) or Invoice Line ID(s) for which they \
would like a refund. In this case you can help them look up their invoices by asking them to \
specify:
- Required: Their first name, last name, and phone number.
- Optionally: The track name, artist name, album name, or purchase date.

If the customer has not specified the required information (either Invoice/Invoice Line IDs \
or first name, last name, phone) then please ask them to specify it."""

---

## Instructions for routing.

**URL:** llms-txt#instructions-for-routing.

route_instructions = """You are managing an online music store that sells song tracks. \
You can help customers in two types of ways: (1) answering general questions about \
tracks sold at your store, (2) helping them get a refund on a purhcase they made at your store.

Based on the following conversation, determine if the user is currently seeking general \
information about song tracks or if they are trying to refund a specific purchase.

Return 'refund' if they are trying to get a refund and 'question_answering' if they are \
asking a general music question. Do NOT return anything else. Do NOT try to respond to \
the user.
"""

---

## Instrument AutoGen and OpenAI

**URL:** llms-txt#instrument-autogen-and-openai

AutogenInstrumentor().instrument()
OpenAIInstrumentor().instrument()

---

## Instrument AutoGen and OpenAI calls

**URL:** llms-txt#instrument-autogen-and-openai-calls

**Contents:**
  - 3. Create and run your AutoGen application

AutogenInstrumentor().instrument()
OpenAIInstrumentor().instrument()
python  theme={null}
import autogen
from openinference.instrumentation.autogen import AutogenInstrumentor
from openinference.instrumentation.openai import OpenAIInstrumentor
from langsmith.integrations.otel import configure
import os
import dotenv

**Examples:**

Example 1 (unknown):
```unknown
<Note>
  You do not need to set any OpenTelemetry environment variables or configure exporters manually—`configure()` handles everything automatically.
</Note>

### 3. Create and run your AutoGen application

Once configured, your AutoGen application will automatically send traces to LangSmith:
```

---

## Instrument CrewAI and OpenAI

**URL:** llms-txt#instrument-crewai-and-openai

CrewAIInstrumentor().instrument()
OpenAIInstrumentor().instrument()

---

## Instrument Google ADK directly

**URL:** llms-txt#instrument-google-adk-directly

**Contents:**
  - 3. Create and run your ADK agent

GoogleADKInstrumentor().instrument()
python  theme={null}
import asyncio
from langsmith.integrations.otel import configure
from google.adk import Runner
from google.adk.agents import LlmAgent
from google.adk.sessions import InMemorySessionService
from google.genai import types

**Examples:**

Example 1 (unknown):
```unknown
<Note>
  You do not need to set any OpenTelemetry environment variables or configure exporters manually—`configure()` handles everything automatically.
</Note>

### 3. Create and run your ADK agent

Once configured, your Google ADK application will automatically send traces to LangSmith:

This example includes a minimal app that sets up an agent, session, and runner, then sends a message and streams events.
```

---

## Instrument OpenAI calls

**URL:** llms-txt#instrument-openai-calls

OpenAIInstrumentor().instrument()

---

## Integrations

**URL:** llms-txt#integrations

**Contents:**
- Open source frameworks
- LLM providers
- Agent frameworks
- Other

Source: https://docs.langchain.com/langsmith/integrations

[LangSmith](https://smith.langchain.com/) provides support for [LangChain](/oss/python/langchain/overview) and [LangGraph](/oss/python/langgraph/overview) as well as integrations with a growing set of popular [LLM providers](#llm-providers) and [agent frameworks](#agent-frameworks). For setup and usage, refer to the guide pages in the navigation bar.

## Open source frameworks

<Columns cols={2}>
  <Card title="LangChain" icon="link" href="/langsmith/trace-with-langchain" horizontal />

<Card title="LangGraph" icon="circle-nodes" href="/langsmith/trace-with-langgraph" horizontal />
</Columns>

<Columns cols={2}>
  <Card title="OpenAI" icon="https://mintcdn.com/langchain-5e9cc07a/DtQtEdiVyCTfcVOD/langsmith/images/OpenAI-black-monoblossom.svg?fit=max&auto=format&n=DtQtEdiVyCTfcVOD&q=85&s=eb1007c5cd89bedb14c30571c78b596d" href="/langsmith/trace-openai" horizontal data-og-width="721" width="721" data-og-height="721" height="721" data-path="langsmith/images/OpenAI-black-monoblossom.svg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/DtQtEdiVyCTfcVOD/langsmith/images/OpenAI-black-monoblossom.svg?w=280&fit=max&auto=format&n=DtQtEdiVyCTfcVOD&q=85&s=3b67552cf8b645eee7489b8a799b27b8 280w, https://mintcdn.com/langchain-5e9cc07a/DtQtEdiVyCTfcVOD/langsmith/images/OpenAI-black-monoblossom.svg?w=560&fit=max&auto=format&n=DtQtEdiVyCTfcVOD&q=85&s=6f2159a8ca6c363a4820f70b4e9cadc3 560w, https://mintcdn.com/langchain-5e9cc07a/DtQtEdiVyCTfcVOD/langsmith/images/OpenAI-black-monoblossom.svg?w=840&fit=max&auto=format&n=DtQtEdiVyCTfcVOD&q=85&s=c117950d7b8f3e82480d35351eebd3c6 840w, https://mintcdn.com/langchain-5e9cc07a/DtQtEdiVyCTfcVOD/langsmith/images/OpenAI-black-monoblossom.svg?w=1100&fit=max&auto=format&n=DtQtEdiVyCTfcVOD&q=85&s=995e9d18a5d467709806aa276a863751 1100w, https://mintcdn.com/langchain-5e9cc07a/DtQtEdiVyCTfcVOD/langsmith/images/OpenAI-black-monoblossom.svg?w=1650&fit=max&auto=format&n=DtQtEdiVyCTfcVOD&q=85&s=796fffe51871332bda23631a643e758c 1650w, https://mintcdn.com/langchain-5e9cc07a/DtQtEdiVyCTfcVOD/langsmith/images/OpenAI-black-monoblossom.svg?w=2500&fit=max&auto=format&n=DtQtEdiVyCTfcVOD&q=85&s=fe796d7e6173f8a8fe0c157e65e0cece 2500w" />

<Card title="Anthropic" icon="https://mintcdn.com/langchain-5e9cc07a/NMCyHPS_X1n3SCfv/langsmith/images/anthropic-icon.svg?fit=max&auto=format&n=NMCyHPS_X1n3SCfv&q=85&s=72154d71964e6774d27d4594c9f8d5df" href="/langsmith/trace-anthropic" horizontal data-og-width="92" width="92" data-og-height="65" height="65" data-path="langsmith/images/anthropic-icon.svg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/NMCyHPS_X1n3SCfv/langsmith/images/anthropic-icon.svg?w=280&fit=max&auto=format&n=NMCyHPS_X1n3SCfv&q=85&s=4e5f3b475b14149ec23c9d4b8fb3c061 280w, https://mintcdn.com/langchain-5e9cc07a/NMCyHPS_X1n3SCfv/langsmith/images/anthropic-icon.svg?w=560&fit=max&auto=format&n=NMCyHPS_X1n3SCfv&q=85&s=55fe1a7afe87d1e8708aa1f66b77cf2b 560w, https://mintcdn.com/langchain-5e9cc07a/NMCyHPS_X1n3SCfv/langsmith/images/anthropic-icon.svg?w=840&fit=max&auto=format&n=NMCyHPS_X1n3SCfv&q=85&s=b8a7017607f0237e292f1ab0055f811b 840w, https://mintcdn.com/langchain-5e9cc07a/NMCyHPS_X1n3SCfv/langsmith/images/anthropic-icon.svg?w=1100&fit=max&auto=format&n=NMCyHPS_X1n3SCfv&q=85&s=c553d9eae479f9895b9dee6ea23f4a93 1100w, https://mintcdn.com/langchain-5e9cc07a/NMCyHPS_X1n3SCfv/langsmith/images/anthropic-icon.svg?w=1650&fit=max&auto=format&n=NMCyHPS_X1n3SCfv&q=85&s=6dcf0839bad72678c844bff5643d2051 1650w, https://mintcdn.com/langchain-5e9cc07a/NMCyHPS_X1n3SCfv/langsmith/images/anthropic-icon.svg?w=2500&fit=max&auto=format&n=NMCyHPS_X1n3SCfv&q=85&s=72b7e5f4d275be75b50c1c50f9311751 2500w" />

<Card title="Google Gemini" icon="https://mintcdn.com/langchain-5e9cc07a/5R5TUrRCLnd2oAV2/langsmith/images/google-gemini-icon.svg?fit=max&auto=format&n=5R5TUrRCLnd2oAV2&q=85&s=bdcc784af442d2f7b2610fc273f085cc" horizontal data-og-width="16" width="16" data-og-height="16" height="16" data-path="langsmith/images/google-gemini-icon.svg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/5R5TUrRCLnd2oAV2/langsmith/images/google-gemini-icon.svg?w=280&fit=max&auto=format&n=5R5TUrRCLnd2oAV2&q=85&s=fcea47cc0a5df5efc7f2654d09c3d1a7 280w, https://mintcdn.com/langchain-5e9cc07a/5R5TUrRCLnd2oAV2/langsmith/images/google-gemini-icon.svg?w=560&fit=max&auto=format&n=5R5TUrRCLnd2oAV2&q=85&s=48f8899c42f9c40c83acbfab13915d8c 560w, https://mintcdn.com/langchain-5e9cc07a/5R5TUrRCLnd2oAV2/langsmith/images/google-gemini-icon.svg?w=840&fit=max&auto=format&n=5R5TUrRCLnd2oAV2&q=85&s=61df8e4997b5b4082cbf7085dfecf4f9 840w, https://mintcdn.com/langchain-5e9cc07a/5R5TUrRCLnd2oAV2/langsmith/images/google-gemini-icon.svg?w=1100&fit=max&auto=format&n=5R5TUrRCLnd2oAV2&q=85&s=1bf1fcafac76f49788d8a05a51903280 1100w, https://mintcdn.com/langchain-5e9cc07a/5R5TUrRCLnd2oAV2/langsmith/images/google-gemini-icon.svg?w=1650&fit=max&auto=format&n=5R5TUrRCLnd2oAV2&q=85&s=929adec10c07e770cb094e13976542b8 1650w, https://mintcdn.com/langchain-5e9cc07a/5R5TUrRCLnd2oAV2/langsmith/images/google-gemini-icon.svg?w=2500&fit=max&auto=format&n=5R5TUrRCLnd2oAV2&q=85&s=b8374b4c5d5621562fc8171909b5b59a 2500w" />

<Card title="Amazon Bedrock" icon="https://mintcdn.com/langchain-5e9cc07a/NMCyHPS_X1n3SCfv/langsmith/images/bedrock-icon.png?fit=max&auto=format&n=NMCyHPS_X1n3SCfv&q=85&s=35de36f88ad936c67da79c0c7cbfa2d9" horizontal data-og-width="300" width="300" data-og-height="300" height="300" data-path="langsmith/images/bedrock-icon.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/NMCyHPS_X1n3SCfv/langsmith/images/bedrock-icon.png?w=280&fit=max&auto=format&n=NMCyHPS_X1n3SCfv&q=85&s=d2dc8b8d5109877c3da9f2d49d66fca9 280w, https://mintcdn.com/langchain-5e9cc07a/NMCyHPS_X1n3SCfv/langsmith/images/bedrock-icon.png?w=560&fit=max&auto=format&n=NMCyHPS_X1n3SCfv&q=85&s=76c66b6a29ca3cf0522416b2d7f5913f 560w, https://mintcdn.com/langchain-5e9cc07a/NMCyHPS_X1n3SCfv/langsmith/images/bedrock-icon.png?w=840&fit=max&auto=format&n=NMCyHPS_X1n3SCfv&q=85&s=40461c08e09bb6b71fc666a0bfda0920 840w, https://mintcdn.com/langchain-5e9cc07a/NMCyHPS_X1n3SCfv/langsmith/images/bedrock-icon.png?w=1100&fit=max&auto=format&n=NMCyHPS_X1n3SCfv&q=85&s=d7fc2a61b5dd7429da7e5e6c686a3353 1100w, https://mintcdn.com/langchain-5e9cc07a/NMCyHPS_X1n3SCfv/langsmith/images/bedrock-icon.png?w=1650&fit=max&auto=format&n=NMCyHPS_X1n3SCfv&q=85&s=f6f08f6c001978bb0e249e9902edd9b2 1650w, https://mintcdn.com/langchain-5e9cc07a/NMCyHPS_X1n3SCfv/langsmith/images/bedrock-icon.png?w=2500&fit=max&auto=format&n=NMCyHPS_X1n3SCfv&q=85&s=af8e021e33194ad04a709d733e5926ef 2500w" />

<Card title="DeepSeek" icon="https://mintcdn.com/langchain-5e9cc07a/Pr90PDf9m6t7bJ5a/langsmith/images/deepseek-icon.svg?fit=max&auto=format&n=Pr90PDf9m6t7bJ5a&q=85&s=614bc77c507a6dc8e3ad6f05d6607f7b" horizontal data-og-width="16" width="16" data-og-height="16" height="16" data-path="langsmith/images/deepseek-icon.svg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Pr90PDf9m6t7bJ5a/langsmith/images/deepseek-icon.svg?w=280&fit=max&auto=format&n=Pr90PDf9m6t7bJ5a&q=85&s=35f4e5466b37b039e1d76fdcc88cbe08 280w, https://mintcdn.com/langchain-5e9cc07a/Pr90PDf9m6t7bJ5a/langsmith/images/deepseek-icon.svg?w=560&fit=max&auto=format&n=Pr90PDf9m6t7bJ5a&q=85&s=7824062c77536af5a878a556feb29b32 560w, https://mintcdn.com/langchain-5e9cc07a/Pr90PDf9m6t7bJ5a/langsmith/images/deepseek-icon.svg?w=840&fit=max&auto=format&n=Pr90PDf9m6t7bJ5a&q=85&s=7a3f3312072f49520ea419009a176259 840w, https://mintcdn.com/langchain-5e9cc07a/Pr90PDf9m6t7bJ5a/langsmith/images/deepseek-icon.svg?w=1100&fit=max&auto=format&n=Pr90PDf9m6t7bJ5a&q=85&s=fa400b206a27d743f165df55c0237e04 1100w, https://mintcdn.com/langchain-5e9cc07a/Pr90PDf9m6t7bJ5a/langsmith/images/deepseek-icon.svg?w=1650&fit=max&auto=format&n=Pr90PDf9m6t7bJ5a&q=85&s=c6779d0c17f714bdef4a1701b980ba74 1650w, https://mintcdn.com/langchain-5e9cc07a/Pr90PDf9m6t7bJ5a/langsmith/images/deepseek-icon.svg?w=2500&fit=max&auto=format&n=Pr90PDf9m6t7bJ5a&q=85&s=07dfff76b9a53a04f15c34811b21a7aa 2500w" />

<Card title="Mistral" icon="https://mintcdn.com/langchain-5e9cc07a/5R5TUrRCLnd2oAV2/langsmith/images/mistral-icon.svg?fit=max&auto=format&n=5R5TUrRCLnd2oAV2&q=85&s=6e13bc597cfcc563eb9ca83e4453a827" horizontal data-og-width="16" width="16" data-og-height="16" height="16" data-path="langsmith/images/mistral-icon.svg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/5R5TUrRCLnd2oAV2/langsmith/images/mistral-icon.svg?w=280&fit=max&auto=format&n=5R5TUrRCLnd2oAV2&q=85&s=0baabadda81dc3e1dbaf5cbe8ba63f11 280w, https://mintcdn.com/langchain-5e9cc07a/5R5TUrRCLnd2oAV2/langsmith/images/mistral-icon.svg?w=560&fit=max&auto=format&n=5R5TUrRCLnd2oAV2&q=85&s=46b0e892bddfdaeb73636e567f1643f6 560w, https://mintcdn.com/langchain-5e9cc07a/5R5TUrRCLnd2oAV2/langsmith/images/mistral-icon.svg?w=840&fit=max&auto=format&n=5R5TUrRCLnd2oAV2&q=85&s=2d7d407801568768c7fdf3d6c25fb837 840w, https://mintcdn.com/langchain-5e9cc07a/5R5TUrRCLnd2oAV2/langsmith/images/mistral-icon.svg?w=1100&fit=max&auto=format&n=5R5TUrRCLnd2oAV2&q=85&s=c6b2a74fac5cfb711a118a8e575c881d 1100w, https://mintcdn.com/langchain-5e9cc07a/5R5TUrRCLnd2oAV2/langsmith/images/mistral-icon.svg?w=1650&fit=max&auto=format&n=5R5TUrRCLnd2oAV2&q=85&s=55537c89b675caafb837bfbec603e227 1650w, https://mintcdn.com/langchain-5e9cc07a/5R5TUrRCLnd2oAV2/langsmith/images/mistral-icon.svg?w=2500&fit=max&auto=format&n=5R5TUrRCLnd2oAV2&q=85&s=01f7fb37e87e7433890567b567ec87ae 2500w" />
</Columns>

<Columns cols={3}>
  <Card title="AutoGen" icon="https://mintcdn.com/langchain-5e9cc07a/NMCyHPS_X1n3SCfv/langsmith/images/ag-icon.svg?fit=max&auto=format&n=NMCyHPS_X1n3SCfv&q=85&s=d26f92a1a92dfa32c5a6a808fcd53437" href="/langsmith/trace-with-autogen" horizontal data-og-width="96" width="96" data-og-height="85" height="85" data-path="langsmith/images/ag-icon.svg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/NMCyHPS_X1n3SCfv/langsmith/images/ag-icon.svg?w=280&fit=max&auto=format&n=NMCyHPS_X1n3SCfv&q=85&s=4e8f12de28e277c20509b9ab401a7ddf 280w, https://mintcdn.com/langchain-5e9cc07a/NMCyHPS_X1n3SCfv/langsmith/images/ag-icon.svg?w=560&fit=max&auto=format&n=NMCyHPS_X1n3SCfv&q=85&s=b219b84fdd296aea935ece82b15d668d 560w, https://mintcdn.com/langchain-5e9cc07a/NMCyHPS_X1n3SCfv/langsmith/images/ag-icon.svg?w=840&fit=max&auto=format&n=NMCyHPS_X1n3SCfv&q=85&s=1029702ca2d93f383a5a1eaf4f967ee4 840w, https://mintcdn.com/langchain-5e9cc07a/NMCyHPS_X1n3SCfv/langsmith/images/ag-icon.svg?w=1100&fit=max&auto=format&n=NMCyHPS_X1n3SCfv&q=85&s=c4d23601202a7494e0b84a2f63493423 1100w, https://mintcdn.com/langchain-5e9cc07a/NMCyHPS_X1n3SCfv/langsmith/images/ag-icon.svg?w=1650&fit=max&auto=format&n=NMCyHPS_X1n3SCfv&q=85&s=2539d4f1be8cf5232b741f12567c6eb5 1650w, https://mintcdn.com/langchain-5e9cc07a/NMCyHPS_X1n3SCfv/langsmith/images/ag-icon.svg?w=2500&fit=max&auto=format&n=NMCyHPS_X1n3SCfv&q=85&s=445dd69beb31df66cc166e2e4bd8409e 2500w" />

<Card title="Claude Agent SDK" icon="https://mintcdn.com/langchain-5e9cc07a/VTUyw-EOhx2PnFCY/langsmith/images/claude.svg?fit=max&auto=format&n=VTUyw-EOhx2PnFCY&q=85&s=8b7cf6ad6a521b6db5193db084dc3d4c" href="/langsmith/trace-claude-agent-sdk" horizontal data-og-width="16" width="16" data-og-height="16" height="16" data-path="langsmith/images/claude.svg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/VTUyw-EOhx2PnFCY/langsmith/images/claude.svg?w=280&fit=max&auto=format&n=VTUyw-EOhx2PnFCY&q=85&s=d1b1b8aa9a95dc34cadecdaef46340f9 280w, https://mintcdn.com/langchain-5e9cc07a/VTUyw-EOhx2PnFCY/langsmith/images/claude.svg?w=560&fit=max&auto=format&n=VTUyw-EOhx2PnFCY&q=85&s=e7be993b69b5fe674a4b48abeb26987c 560w, https://mintcdn.com/langchain-5e9cc07a/VTUyw-EOhx2PnFCY/langsmith/images/claude.svg?w=840&fit=max&auto=format&n=VTUyw-EOhx2PnFCY&q=85&s=e45adf3b7d609c8efefc8fd32d275187 840w, https://mintcdn.com/langchain-5e9cc07a/VTUyw-EOhx2PnFCY/langsmith/images/claude.svg?w=1100&fit=max&auto=format&n=VTUyw-EOhx2PnFCY&q=85&s=bf02a0bcddf0efe0a6cd0a783542cd07 1100w, https://mintcdn.com/langchain-5e9cc07a/VTUyw-EOhx2PnFCY/langsmith/images/claude.svg?w=1650&fit=max&auto=format&n=VTUyw-EOhx2PnFCY&q=85&s=0c6ef1d5e61245384829d0bbd2fd184c 1650w, https://mintcdn.com/langchain-5e9cc07a/VTUyw-EOhx2PnFCY/langsmith/images/claude.svg?w=2500&fit=max&auto=format&n=VTUyw-EOhx2PnFCY&q=85&s=338a1548df0989b5ea847eda08449092 2500w" />

<Card title="CrewAI" icon="https://mintcdn.com/langchain-5e9cc07a/VTUyw-EOhx2PnFCY/langsmith/images/crewai-icon.svg?fit=max&auto=format&n=VTUyw-EOhx2PnFCY&q=85&s=b55086b8158f6986b44a1d47fc4ca412" href="/langsmith/trace-with-crewai" horizontal data-og-width="71" width="71" data-og-height="80" height="80" data-path="langsmith/images/crewai-icon.svg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/VTUyw-EOhx2PnFCY/langsmith/images/crewai-icon.svg?w=280&fit=max&auto=format&n=VTUyw-EOhx2PnFCY&q=85&s=68057657dd0c49947ee350ea475eb4c3 280w, https://mintcdn.com/langchain-5e9cc07a/VTUyw-EOhx2PnFCY/langsmith/images/crewai-icon.svg?w=560&fit=max&auto=format&n=VTUyw-EOhx2PnFCY&q=85&s=5d6bf21f3acc8309bef36860b37e2540 560w, https://mintcdn.com/langchain-5e9cc07a/VTUyw-EOhx2PnFCY/langsmith/images/crewai-icon.svg?w=840&fit=max&auto=format&n=VTUyw-EOhx2PnFCY&q=85&s=7ddf6ed932c8b14d18a96ce60f7850d4 840w, https://mintcdn.com/langchain-5e9cc07a/VTUyw-EOhx2PnFCY/langsmith/images/crewai-icon.svg?w=1100&fit=max&auto=format&n=VTUyw-EOhx2PnFCY&q=85&s=5244a0bdb832ce0079346499384cf068 1100w, https://mintcdn.com/langchain-5e9cc07a/VTUyw-EOhx2PnFCY/langsmith/images/crewai-icon.svg?w=1650&fit=max&auto=format&n=VTUyw-EOhx2PnFCY&q=85&s=a8598a68fbe318e93a1044037087a774 1650w, https://mintcdn.com/langchain-5e9cc07a/VTUyw-EOhx2PnFCY/langsmith/images/crewai-icon.svg?w=2500&fit=max&auto=format&n=VTUyw-EOhx2PnFCY&q=85&s=df5b641d14dbc589331b5ef8eb94c320 2500w" />

<Card title="Google ADK" icon="https://mintcdn.com/langchain-5e9cc07a/NMCyHPS_X1n3SCfv/langsmith/images/agent-development-kit.png?fit=max&auto=format&n=NMCyHPS_X1n3SCfv&q=85&s=afc5c9f25aa944f4592e7aebb04b467d" href="/langsmith/trace-with-google-adk" horizontal data-og-width="512" width="512" data-og-height="512" height="512" data-path="langsmith/images/agent-development-kit.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/NMCyHPS_X1n3SCfv/langsmith/images/agent-development-kit.png?w=280&fit=max&auto=format&n=NMCyHPS_X1n3SCfv&q=85&s=98fef8d62e5df4535f12a7d05a3fa714 280w, https://mintcdn.com/langchain-5e9cc07a/NMCyHPS_X1n3SCfv/langsmith/images/agent-development-kit.png?w=560&fit=max&auto=format&n=NMCyHPS_X1n3SCfv&q=85&s=00a2575125e1039dcd077cd9f2486122 560w, https://mintcdn.com/langchain-5e9cc07a/NMCyHPS_X1n3SCfv/langsmith/images/agent-development-kit.png?w=840&fit=max&auto=format&n=NMCyHPS_X1n3SCfv&q=85&s=43a8197c23165e7ff8d70741a86ff9a1 840w, https://mintcdn.com/langchain-5e9cc07a/NMCyHPS_X1n3SCfv/langsmith/images/agent-development-kit.png?w=1100&fit=max&auto=format&n=NMCyHPS_X1n3SCfv&q=85&s=3a90a117eafe3b158b2a5ee61ac7f31c 1100w, https://mintcdn.com/langchain-5e9cc07a/NMCyHPS_X1n3SCfv/langsmith/images/agent-development-kit.png?w=1650&fit=max&auto=format&n=NMCyHPS_X1n3SCfv&q=85&s=6589d87d6258e47fc5fe983b41c87e9a 1650w, https://mintcdn.com/langchain-5e9cc07a/NMCyHPS_X1n3SCfv/langsmith/images/agent-development-kit.png?w=2500&fit=max&auto=format&n=NMCyHPS_X1n3SCfv&q=85&s=156374e785bd74a10435cfee4f6b7898 2500w" />

<Card title="OpenAI Agents" icon="https://mintcdn.com/langchain-5e9cc07a/DtQtEdiVyCTfcVOD/langsmith/images/OpenAI-black-monoblossom.svg?fit=max&auto=format&n=DtQtEdiVyCTfcVOD&q=85&s=eb1007c5cd89bedb14c30571c78b596d" href="/langsmith/trace-with-openai-agents-sdk" horizontal data-og-width="721" width="721" data-og-height="721" height="721" data-path="langsmith/images/OpenAI-black-monoblossom.svg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/DtQtEdiVyCTfcVOD/langsmith/images/OpenAI-black-monoblossom.svg?w=280&fit=max&auto=format&n=DtQtEdiVyCTfcVOD&q=85&s=3b67552cf8b645eee7489b8a799b27b8 280w, https://mintcdn.com/langchain-5e9cc07a/DtQtEdiVyCTfcVOD/langsmith/images/OpenAI-black-monoblossom.svg?w=560&fit=max&auto=format&n=DtQtEdiVyCTfcVOD&q=85&s=6f2159a8ca6c363a4820f70b4e9cadc3 560w, https://mintcdn.com/langchain-5e9cc07a/DtQtEdiVyCTfcVOD/langsmith/images/OpenAI-black-monoblossom.svg?w=840&fit=max&auto=format&n=DtQtEdiVyCTfcVOD&q=85&s=c117950d7b8f3e82480d35351eebd3c6 840w, https://mintcdn.com/langchain-5e9cc07a/DtQtEdiVyCTfcVOD/langsmith/images/OpenAI-black-monoblossom.svg?w=1100&fit=max&auto=format&n=DtQtEdiVyCTfcVOD&q=85&s=995e9d18a5d467709806aa276a863751 1100w, https://mintcdn.com/langchain-5e9cc07a/DtQtEdiVyCTfcVOD/langsmith/images/OpenAI-black-monoblossom.svg?w=1650&fit=max&auto=format&n=DtQtEdiVyCTfcVOD&q=85&s=796fffe51871332bda23631a643e758c 1650w, https://mintcdn.com/langchain-5e9cc07a/DtQtEdiVyCTfcVOD/langsmith/images/OpenAI-black-monoblossom.svg?w=2500&fit=max&auto=format&n=DtQtEdiVyCTfcVOD&q=85&s=fe796d7e6173f8a8fe0c157e65e0cece 2500w" />

<Card title="OpenTelemetry" icon="https://mintcdn.com/langchain-5e9cc07a/kL_P9zpLIoeJVEMB/langsmith/images/opentelemetry-icon.svg?fit=max&auto=format&n=kL_P9zpLIoeJVEMB&q=85&s=b5ed93d3d8f091346beab9ff553258ee" href="/langsmith/trace-with-opentelemetry" horizontal data-og-width="128" width="128" data-og-height="128" height="128" data-path="langsmith/images/opentelemetry-icon.svg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/kL_P9zpLIoeJVEMB/langsmith/images/opentelemetry-icon.svg?w=280&fit=max&auto=format&n=kL_P9zpLIoeJVEMB&q=85&s=44998c37fe8fad8eb1f2a2088b221945 280w, https://mintcdn.com/langchain-5e9cc07a/kL_P9zpLIoeJVEMB/langsmith/images/opentelemetry-icon.svg?w=560&fit=max&auto=format&n=kL_P9zpLIoeJVEMB&q=85&s=bb0c96765c31224729d4f679106014f9 560w, https://mintcdn.com/langchain-5e9cc07a/kL_P9zpLIoeJVEMB/langsmith/images/opentelemetry-icon.svg?w=840&fit=max&auto=format&n=kL_P9zpLIoeJVEMB&q=85&s=d59ba8154295a8e2dcc9051c4a5f62fa 840w, https://mintcdn.com/langchain-5e9cc07a/kL_P9zpLIoeJVEMB/langsmith/images/opentelemetry-icon.svg?w=1100&fit=max&auto=format&n=kL_P9zpLIoeJVEMB&q=85&s=d28401de0456b594aed4b16875ed4e69 1100w, https://mintcdn.com/langchain-5e9cc07a/kL_P9zpLIoeJVEMB/langsmith/images/opentelemetry-icon.svg?w=1650&fit=max&auto=format&n=kL_P9zpLIoeJVEMB&q=85&s=7b7a1ee555d0061c63002d26f8869790 1650w, https://mintcdn.com/langchain-5e9cc07a/kL_P9zpLIoeJVEMB/langsmith/images/opentelemetry-icon.svg?w=2500&fit=max&auto=format&n=kL_P9zpLIoeJVEMB&q=85&s=1ef12004d35d8dca6e17cbd126c5d898 2500w" />

<Card title="Semantic Kernel" icon="https://mintcdn.com/langchain-5e9cc07a/5R5TUrRCLnd2oAV2/langsmith/images/microsoft-icon.svg?fit=max&auto=format&n=5R5TUrRCLnd2oAV2&q=85&s=f72a7081983314666065154f0b36f984" href="/langsmith/trace-with-semantic-kernel" horizontal data-og-width="21" width="21" data-og-height="21" height="21" data-path="langsmith/images/microsoft-icon.svg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/5R5TUrRCLnd2oAV2/langsmith/images/microsoft-icon.svg?w=280&fit=max&auto=format&n=5R5TUrRCLnd2oAV2&q=85&s=7954aa62f6f71331985bb12f2b959691 280w, https://mintcdn.com/langchain-5e9cc07a/5R5TUrRCLnd2oAV2/langsmith/images/microsoft-icon.svg?w=560&fit=max&auto=format&n=5R5TUrRCLnd2oAV2&q=85&s=c1bb6e693858fe3d00f32ce9824e5931 560w, https://mintcdn.com/langchain-5e9cc07a/5R5TUrRCLnd2oAV2/langsmith/images/microsoft-icon.svg?w=840&fit=max&auto=format&n=5R5TUrRCLnd2oAV2&q=85&s=54fae5276cf6249124b2ba7aa05c656b 840w, https://mintcdn.com/langchain-5e9cc07a/5R5TUrRCLnd2oAV2/langsmith/images/microsoft-icon.svg?w=1100&fit=max&auto=format&n=5R5TUrRCLnd2oAV2&q=85&s=c2e50dbdbc1c1e6403fd417bd28dcaca 1100w, https://mintcdn.com/langchain-5e9cc07a/5R5TUrRCLnd2oAV2/langsmith/images/microsoft-icon.svg?w=1650&fit=max&auto=format&n=5R5TUrRCLnd2oAV2&q=85&s=04ef067654fb39ea096193f682101bee 1650w, https://mintcdn.com/langchain-5e9cc07a/5R5TUrRCLnd2oAV2/langsmith/images/microsoft-icon.svg?w=2500&fit=max&auto=format&n=5R5TUrRCLnd2oAV2&q=85&s=c2dcdb24fa62442e3357609adb25d58a 2500w" />

<Card title="Vercel AI SDK" icon="https://mintcdn.com/langchain-5e9cc07a/igkac3bTlN5hpgNy/langsmith/images/vercel-icon-light.svg?fit=max&auto=format&n=igkac3bTlN5hpgNy&q=85&s=ae7683dcafe139ae42a32a1be055a3e9" href="/langsmith/trace-with-vercel-ai-sdk" horizontal data-og-width="1155" width="1155" data-og-height="1000" height="1000" data-path="langsmith/images/vercel-icon-light.svg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/igkac3bTlN5hpgNy/langsmith/images/vercel-icon-light.svg?w=280&fit=max&auto=format&n=igkac3bTlN5hpgNy&q=85&s=8e8ef1361119ca4e14f7491045f7b31e 280w, https://mintcdn.com/langchain-5e9cc07a/igkac3bTlN5hpgNy/langsmith/images/vercel-icon-light.svg?w=560&fit=max&auto=format&n=igkac3bTlN5hpgNy&q=85&s=f1ae1cd65bd92e744fc8f91939ba92b0 560w, https://mintcdn.com/langchain-5e9cc07a/igkac3bTlN5hpgNy/langsmith/images/vercel-icon-light.svg?w=840&fit=max&auto=format&n=igkac3bTlN5hpgNy&q=85&s=61386ab1bf31cb46e58354b042cc168e 840w, https://mintcdn.com/langchain-5e9cc07a/igkac3bTlN5hpgNy/langsmith/images/vercel-icon-light.svg?w=1100&fit=max&auto=format&n=igkac3bTlN5hpgNy&q=85&s=cb2863062de268967b2df6ac64e3c474 1100w, https://mintcdn.com/langchain-5e9cc07a/igkac3bTlN5hpgNy/langsmith/images/vercel-icon-light.svg?w=1650&fit=max&auto=format&n=igkac3bTlN5hpgNy&q=85&s=b29d63c33b0487bce425379cba9686d0 1650w, https://mintcdn.com/langchain-5e9cc07a/igkac3bTlN5hpgNy/langsmith/images/vercel-icon-light.svg?w=2500&fit=max&auto=format&n=igkac3bTlN5hpgNy&q=85&s=a024203e334a6697fe475603068f2720 2500w" />
</Columns>

<Columns cols={2}>
  <Card title="Instructor" icon="https://mintcdn.com/langchain-5e9cc07a/5R5TUrRCLnd2oAV2/langsmith/images/instructor-icon.svg?fit=max&auto=format&n=5R5TUrRCLnd2oAV2&q=85&s=6efff2ac45f2641e6bc4ea7572eb4b9b" href="/langsmith/trace-with-instructor" horizontal data-og-width="1155" width="1155" data-og-height="1000" height="1000" data-path="langsmith/images/instructor-icon.svg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/5R5TUrRCLnd2oAV2/langsmith/images/instructor-icon.svg?w=280&fit=max&auto=format&n=5R5TUrRCLnd2oAV2&q=85&s=1c98fc15f5d8cc0a7545a8858cf5ea09 280w, https://mintcdn.com/langchain-5e9cc07a/5R5TUrRCLnd2oAV2/langsmith/images/instructor-icon.svg?w=560&fit=max&auto=format&n=5R5TUrRCLnd2oAV2&q=85&s=6e0bc6f7336da553adf4b9fa50cacc82 560w, https://mintcdn.com/langchain-5e9cc07a/5R5TUrRCLnd2oAV2/langsmith/images/instructor-icon.svg?w=840&fit=max&auto=format&n=5R5TUrRCLnd2oAV2&q=85&s=51b0e99b8a7f70ec1822a022d203a2c8 840w, https://mintcdn.com/langchain-5e9cc07a/5R5TUrRCLnd2oAV2/langsmith/images/instructor-icon.svg?w=1100&fit=max&auto=format&n=5R5TUrRCLnd2oAV2&q=85&s=e627ad88a0a042eb04531b9165e88b86 1100w, https://mintcdn.com/langchain-5e9cc07a/5R5TUrRCLnd2oAV2/langsmith/images/instructor-icon.svg?w=1650&fit=max&auto=format&n=5R5TUrRCLnd2oAV2&q=85&s=3336699e192f8c67c3cd5d364810057a 1650w, https://mintcdn.com/langchain-5e9cc07a/5R5TUrRCLnd2oAV2/langsmith/images/instructor-icon.svg?w=2500&fit=max&auto=format&n=5R5TUrRCLnd2oAV2&q=85&s=b166f3891adfff8600137bf219c79704 2500w" />

<Card title="Claude Code" icon="https://mintcdn.com/langchain-5e9cc07a/VTUyw-EOhx2PnFCY/langsmith/images/claude.svg?fit=max&auto=format&n=VTUyw-EOhx2PnFCY&q=85&s=8b7cf6ad6a521b6db5193db084dc3d4c" href="/langsmith/trace-claude-code" horizontal data-og-width="16" width="16" data-og-height="16" height="16" data-path="langsmith/images/claude.svg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/VTUyw-EOhx2PnFCY/langsmith/images/claude.svg?w=280&fit=max&auto=format&n=VTUyw-EOhx2PnFCY&q=85&s=d1b1b8aa9a95dc34cadecdaef46340f9 280w, https://mintcdn.com/langchain-5e9cc07a/VTUyw-EOhx2PnFCY/langsmith/images/claude.svg?w=560&fit=max&auto=format&n=VTUyw-EOhx2PnFCY&q=85&s=e7be993b69b5fe674a4b48abeb26987c 560w, https://mintcdn.com/langchain-5e9cc07a/VTUyw-EOhx2PnFCY/langsmith/images/claude.svg?w=840&fit=max&auto=format&n=VTUyw-EOhx2PnFCY&q=85&s=e45adf3b7d609c8efefc8fd32d275187 840w, https://mintcdn.com/langchain-5e9cc07a/VTUyw-EOhx2PnFCY/langsmith/images/claude.svg?w=1100&fit=max&auto=format&n=VTUyw-EOhx2PnFCY&q=85&s=bf02a0bcddf0efe0a6cd0a783542cd07 1100w, https://mintcdn.com/langchain-5e9cc07a/VTUyw-EOhx2PnFCY/langsmith/images/claude.svg?w=1650&fit=max&auto=format&n=VTUyw-EOhx2PnFCY&q=85&s=0c6ef1d5e61245384829d0bbd2fd184c 1650w, https://mintcdn.com/langchain-5e9cc07a/VTUyw-EOhx2PnFCY/langsmith/images/claude.svg?w=2500&fit=max&auto=format&n=VTUyw-EOhx2PnFCY&q=85&s=338a1548df0989b5ea847eda08449092 2500w" />
</Columns>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/integrations.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Integration Packages

**URL:** llms-txt#integration-packages

**Contents:**
- Popular providers
- All providers

Source: https://docs.langchain.com/oss/javascript/integrations/providers/overview

LangChain integrates with a wide variety of chat & embedding models, tools & toolkits, document loaders, vector stores, and more.

These providers have standalone `langchain-provider` packages for improved versioning, dependency management, and testing.

| Provider                                                                                 | Package                                                                                          | Downloads                                                                  | Latest                                                              |
| :--------------------------------------------------------------------------------------- | :----------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------- | :------------------------------------------------------------------ |
| [Anthropic](/oss/javascript/integrations/providers/anthropic)                            | [`@langchain/anthropic`](https://www.npmjs.com/package/@langchain/anthropic)                     | ![Downloads](https://img.shields.io/npm/dm/@langchain/anthropic)           | ![NPM](https://img.shields.io/npm/v/@langchain/anthropic)           |
| [Azure CosmosDB](/oss/javascript/integrations/vectorstores/azure_cosmosdb_nosql)         | [`@langchain/azure-cosmosdb`](https://www.npmjs.com/package/@langchain/azure-cosmosdb)           | ![Downloads](https://img.shields.io/npm/dm/@langchain/azure-cosmosdb)      | ![NPM](https://img.shields.io/npm/v/@langchain/azure-cosmosdb)      |
| [Cerebras](/oss/javascript/integrations/chat/cerebras)                                   | [`@langchain/cerebras`](https://www.npmjs.com/package/@langchain/cerebras)                       | ![Downloads](https://img.shields.io/npm/dm/@langchain/cerebras)            | ![NPM](https://img.shields.io/npm/v/@langchain/cerebras)            |
| Cloudflare                                                                               | [`@langchain/cloudflare`](https://www.npmjs.com/package/@langchain/cloudflare)                   | ![Downloads](https://img.shields.io/npm/dm/@langchain/cloudflare)          | ![NPM](https://img.shields.io/npm/v/@langchain/cloudflare)          |
| [Cohere](/oss/javascript/integrations/chat/cohere)                                       | [`@langchain/cohere`](https://www.npmjs.com/package/@langchain/cohere)                           | ![Downloads](https://img.shields.io/npm/dm/@langchain/cohere)              | ![NPM](https://img.shields.io/npm/v/@langchain/cohere)              |
| [Exa](/oss/javascript/integrations/retrievers/exa)                                       | [`langchain-exa`](https://www.npmjs.com/package/@langchain/exa)                                  | ![Downloads](https://img.shields.io/npm/dm/@langchain/exa)                 | ![NPM](https://img.shields.io/npm/v/@langchain/exa)                 |
| [Google GenAI](/oss/javascript/integrations/chat/google_generative_ai)                   | [`@langchain/google-genai`](https://www.npmjs.com/package/@langchain/google-genai)               | ![Downloads](https://img.shields.io/npm/dm/@langchain/google-genai)        | ![NPM](https://img.shields.io/npm/v/@langchain/google-genai)        |
| [Google VertexAI](/oss/javascript/integrations/chat/google_vertex_ai)                    | [`@langchain/google-vertexai`](https://www.npmjs.com/package/@langchain/google-vertexai)         | ![Downloads](https://img.shields.io/npm/dm/@langchain/google-vertexai)     | ![NPM](https://img.shields.io/npm/v/@langchain/google-vertexai)     |
| [Google VertexAI (Web Environments)](/oss/javascript/integrations/chat/google_vertex_ai) | [`@langchain/google-vertexai-web`](https://www.npmjs.com/package/@langchain/google-vertexai-web) | ![Downloads](https://img.shields.io/npm/dm/@langchain/google-vertexai-web) | ![NPM](https://img.shields.io/npm/v/@langchain/google-vertexai-web) |
| [Groq](/oss/javascript/integrations/chat/groq)                                           | [`@langchain/groq`](https://www.npmjs.com/package/@langchain/groq)                               | ![Downloads](https://img.shields.io/npm/dm/@langchain/groq)                | ![NPM](https://img.shields.io/npm/v/@langchain/groq)                |
| [MistralAI](/oss/javascript/integrations/chat/mistral)                                   | [`@langchain/mistralai`](https://www.npmjs.com/package/@langchain/mistralai)                     | ![Downloads](https://img.shields.io/npm/dm/@langchain/mistralai)           | ![NPM](https://img.shields.io/npm/v/@langchain/mistralai)           |
| [MongoDB](/oss/javascript/integrations/vectorstores/mongodb_atlas)                       | [`@langchain/mongodb`](https://www.npmjs.com/package/@langchain/mongodb)                         | ![Downloads](https://img.shields.io/npm/dm/@langchain/mongodb)             | ![NPM](https://img.shields.io/npm/v/@langchain/mongodb)             |
| [Nomic](/oss/javascript/integrations/text_embedding/nomic)                               | [`@langchain/nomic`](https://www.npmjs.com/package/@langchain/nomic)                             | ![Downloads](https://img.shields.io/npm/dm/@langchain/nomic)               | ![NPM](https://img.shields.io/npm/v/@langchain/nomic)               |
| [OpenAI](/oss/javascript/integrations/providers/openai)                                  | [`@langchain/openai`](https://www.npmjs.com/package/@langchain/openai)                           | ![Downloads](https://img.shields.io/npm/dm/@langchain/openai)              | ![NPM](https://img.shields.io/npm/v/@langchain/openai)              |
| [Pinecone](/oss/javascript/integrations/vectorstores/pinecone)                           | [`@langchain/pinecone`](https://www.npmjs.com/package/@langchain/pinecone)                       | ![Downloads](https://img.shields.io/npm/dm/@langchain/pinecone)            | ![NPM](https://img.shields.io/npm/v/@langchain/pinecone)            |
| [Qdrant](/oss/javascript/integrations/vectorstores/qdrant)                               | [`@langchain/qdrant`](https://www.npmjs.com/package/@langchain/qdrant)                           | ![Downloads](https://img.shields.io/npm/dm/@langchain/qdrant)              | ![NPM](https://img.shields.io/npm/v/@langchain/qdrant)              |
| [Tavily](/oss/javascript/integrations/retrievers/tavily)                                 | [`@langchain/tavily`](https://www.npmjs.com/package/@langchain/tavily)                           | ![Downloads](https://img.shields.io/npm/dm/@langchain/tavily)              | ![NPM](https://img.shields.io/npm/v/@langchain/tavily)              |
| [Weaviate](/oss/javascript/integrations/vectorstores/weaviate)                           | [`@langchain/weaviate`](https://www.npmjs.com/package/@langchain/weaviate)                       | ![Downloads](https://img.shields.io/npm/dm/@langchain/weaviate)            | ![NPM](https://img.shields.io/npm/v/@langchain/weaviate)            |
| [xAI](/oss/javascript/integrations/chat/xai)                                             | [`@langchain/xai`](https://www.npmjs.com/package/@langchain/xai)                                 | ![Downloads](https://img.shields.io/npm/dm/@langchain/xai)                 | ![NPM](https://img.shields.io/npm/v/@langchain/xai)                 |
| [Yandex](/oss/javascript/integrations/chat/yandex)                                       | [`@langchain/yandex`](https://www.npmjs.com/package/@langchain/yandex)                           | ![Downloads](https://img.shields.io/npm/dm/@langchain/yandex)              | ![NPM](https://img.shields.io/npm/v/@langchain/yandex)              |

[See all providers](/oss/javascript/integrations/providers/all_providers) or search for a provider using the search field.

<Info>
  If you'd like to contribute an integration, see [Contributing integrations](/oss/javascript/contributing#add-a-new-integration).
</Info>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/javascript/integrations/providers/overview.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Interact with your self-hosted instance of LangSmith

**URL:** llms-txt#interact-with-your-self-hosted-instance-of-langsmith

**Contents:**
  - Configuring the application you want to use with LangSmith
  - Self-Signed Certificates

Source: https://docs.langchain.com/langsmith/self-host-usage

This guide will walk you through the process of using your self-hosted instance of LangSmith.

<Info>
  This guide assumes you have already deployed a self-hosted LangSmith instance. If you have not, please refer to the [kubernetes deployment guide](/langsmith/kubernetes) or the [docker deployment guide](/langsmith/docker).
</Info>

### Configuring the application you want to use with LangSmith

LangSmith has a single API for interacting with both the hub and the LangSmith backend.

1. Once you have deployed your instance, you can access the LangSmith UI at `http(s)://<host>`.
2. The LangSmith API will be available at `http(s)://<host>/api/v1`
3. The LangSmith Control Plane will be available at `http(s)://<host>/api-host`

To use the API of your instance, you will need to set the following environment variables in your application:

You can also configure these variables directly in the LangSmith SDK client:

After setting the above, you should be able to run your code and see the results in your self-hosted instance. We recommend running through the [*quickstart guide*](https://docs.smith.langchain.com/#quick-start) to get a feel for how to use LangSmith.

### Self-Signed Certificates

If you are using self-signed certificates for your self-hosted LangSmith instance, this can be problematic as Python comes with its own set of trusted certificates, which may not include your self-signed certificate. To resolve this, you may need to use something like `truststore` to load system certificates into your Python environment.

You can do this like so:

1. pip install truststore (or similar depending on the package manager you are using)

Then use the following code to load the system certificates:

```python  theme={null}
import truststore
truststore.inject_into_ssl()

**Examples:**

Example 1 (unknown):
```unknown
You can also configure these variables directly in the LangSmith SDK client:
```

Example 2 (unknown):
```unknown
After setting the above, you should be able to run your code and see the results in your self-hosted instance. We recommend running through the [*quickstart guide*](https://docs.smith.langchain.com/#quick-start) to get a feel for how to use LangSmith.

### Self-Signed Certificates

If you are using self-signed certificates for your self-hosted LangSmith instance, this can be problematic as Python comes with its own set of trusted certificates, which may not include your self-signed certificate. To resolve this, you may need to use something like `truststore` to load system certificates into your Python environment.

You can do this like so:

1. pip install truststore (or similar depending on the package manager you are using)

Then use the following code to load the system certificates:
```

---

## Interrupts

**URL:** llms-txt#interrupts

**Contents:**
- Pause using `interrupt`
- Resuming interrupts
- Common patterns
  - Approve or reject
  - Review and edit state
  - Interrupts in tools
  - Validating human input
- Rules of interrupts
  - Do not wrap `interrupt` calls in try/catch
  - Do not reorder `interrupt` calls within a node

Source: https://docs.langchain.com/oss/javascript/langgraph/interrupts

Interrupts allow you to pause graph execution at specific points and wait for external input before continuing. This enables human-in-the-loop patterns where you need external input to proceed. When an interrupt is triggered, LangGraph saves the graph state using its [persistence](/oss/javascript/langgraph/persistence) layer and waits indefinitely until you resume execution.

Interrupts work by calling the `interrupt()` function at any point in your graph nodes. The function accepts any JSON-serializable value which is surfaced to the caller. When you're ready to continue, you resume execution by re-invoking the graph using `Command`, which then becomes the return value of the `interrupt()` call from inside the node.

Unlike static breakpoints (which pause before or after specific nodes), interrupts are **dynamic**—they can be placed anywhere in your code and can be conditional based on your application logic.

* **Checkpointing keeps your place:** the checkpointer writes the exact graph state so you can resume later, even when in an error state.
* **`thread_id` is your pointer:** use `{ configurable: { thread_id: ... } }` as options to the `invoke` method to tell the checkpointer which state to load.
* **Interrupt payloads surface as `__interrupt__`:** the values you pass to `interrupt()` return to the caller in the `__interrupt__` field so you know what the graph is waiting on.

The `thread_id` you choose is effectively your persistent cursor. Reusing it resumes the same checkpoint; using a new value starts a brand-new thread with an empty state.

## Pause using `interrupt`

The [`interrupt`](https://langchain-ai.github.io/langgraphjs/reference/functions/langgraph.interrupt-2.html) function pauses graph execution and returns a value to the caller. When you call [`interrupt`](https://langchain-ai.github.io/langgraphjs/reference/functions/langgraph.interrupt-2.html) within a node, LangGraph saves the current graph state and waits for you to resume execution with input.

To use [`interrupt`](https://langchain-ai.github.io/langgraphjs/reference/functions/langgraph.interrupt-2.html), you need:

1. A **checkpointer** to persist the graph state (use a durable checkpointer in production)
2. A **thread ID** in your config so the runtime knows which state to resume from
3. To call `interrupt()` where you want to pause (payload must be JSON-serializable)

When you call [`interrupt`](https://langchain-ai.github.io/langgraphjs/reference/functions/langgraph.interrupt-2.html), here's what happens:

1. **Graph execution gets suspended** at the exact point where [`interrupt`](https://langchain-ai.github.io/langgraphjs/reference/functions/langgraph.interrupt-2.html) is called
2. **State is saved** using the checkpointer so execution can be resumed later, In production, this should be a persistent checkpointer (e.g. backed by a database)
3. **Value is returned** to the caller under `__interrupt__`; it can be any JSON-serializable value (string, object, array, etc.)
4. **Graph waits indefinitely** until you resume execution with a response
5. **Response is passed back** into the node when you resume, becoming the return value of the `interrupt()` call

## Resuming interrupts

After an interrupt pauses execution, you resume the graph by invoking it again with a `Command` that contains the resume value. The resume value is passed back to the `interrupt` call, allowing the node to continue execution with the external input.

**Key points about resuming:**

* You must use the **same thread ID** when resuming that was used when the interrupt occurred
* The value passed to `Command(resume=...)` becomes the return value of the [`interrupt`](https://langchain-ai.github.io/langgraphjs/reference/functions/langgraph.interrupt-2.html) call
* The node restarts from the beginning of the node where the [`interrupt`](https://langchain-ai.github.io/langgraphjs/reference/functions/langgraph.interrupt-2.html) was called when resumed, so any code before the [`interrupt`](https://langchain-ai.github.io/langgraphjs/reference/functions/langgraph.interrupt-2.html) runs again
* You can pass any JSON-serializable value as the resume value

The key thing that interrupts unlock is the ability to pause execution and wait for external input. This is useful for a variety of use cases, including:

* <Icon icon="check-circle" /> [Approval workflows](#approve-or-reject): Pause before executing critical actions (API calls, database changes, financial transactions)
* <Icon icon="pencil" /> [Review and edit](#review-and-edit-state): Let humans review and modify LLM outputs or tool calls before continuing
* <Icon icon="wrench" /> [Interrupting tool calls](#interrupts-in-tools): Pause before executing tool calls to review and edit the tool call before execution
* <Icon icon="shield-check" /> [Validating human input](#validating-human-input): Pause before proceeding to the next step to validate human input

### Approve or reject

One of the most common uses of interrupts is to pause before a critical action and ask for approval. For example, you might want to ask a human to approve an API call, a database change, or any other important decision.

When you resume the graph, pass `true` to approve or `false` to reject:

<Accordion title="Full example">
  
</Accordion>

### Review and edit state

Sometimes you want to let a human review and edit part of the graph state before continuing. This is useful for correcting LLMs, adding missing information, or making adjustments.

When resuming, provide the edited content:

<Accordion title="Full example">
  
</Accordion>

### Interrupts in tools

You can also place interrupts directly inside tool functions. This makes the tool itself pause for approval whenever it's called, and allows for human review and editing of the tool call before it is executed.

First, define a tool that uses [`interrupt`](https://langchain-ai.github.io/langgraphjs/reference/functions/langgraph.interrupt-2.html):

This approach is useful when you want the approval logic to live with the tool itself, making it reusable across different parts of your graph. The LLM can call the tool naturally, and the interrupt will pause execution whenever the tool is invoked, allowing you to approve, edit, or cancel the action.

<Accordion title="Full example">
  
</Accordion>

### Validating human input

Sometimes you need to validate input from humans and ask again if it's invalid. You can do this using multiple [`interrupt`](https://langchain-ai.github.io/langgraphjs/reference/functions/langgraph.interrupt-2.html) calls in a loop.

Each time you resume the graph with invalid input, it will ask again with a clearer message. Once valid input is provided, the node completes and the graph continues.

<Accordion title="Full example">
  
</Accordion>

## Rules of interrupts

When you call [`interrupt`](https://langchain-ai.github.io/langgraphjs/reference/functions/langgraph.interrupt-2.html) within a node, LangGraph suspends execution by raising an exception that signals the runtime to pause. This exception propagates up through the call stack and is caught by the runtime, which notifies the graph to save the current state and wait for external input.

When execution resumes (after you provide the requested input), the runtime restarts the entire node from the beginning—it does not resume from the exact line where [`interrupt`](https://langchain-ai.github.io/langgraphjs/reference/functions/langgraph.interrupt-2.html) was called. This means any code that ran before the [`interrupt`](https://langchain-ai.github.io/langgraphjs/reference/functions/langgraph.interrupt-2.html) will execute again. Because of this, there's a few important rules to follow when working with interrupts to ensure they behave as expected.

### Do not wrap `interrupt` calls in try/catch

The way that [`interrupt`](https://langchain-ai.github.io/langgraphjs/reference/functions/langgraph.interrupt-2.html) pauses execution at the point of the call is by throwing a special exception. If you wrap the [`interrupt`](https://langchain-ai.github.io/langgraphjs/reference/functions/langgraph.interrupt-2.html) call in a try/catch block, you will catch this exception and the interrupt will not be passed back to the graph.

* ✅ Separate [`interrupt`](https://langchain-ai.github.io/langgraphjs/reference/functions/langgraph.interrupt-2.html) calls from error-prone code
* ✅ Conditionally catch errors if needed

* 🔴 Do not wrap [`interrupt`](https://langchain-ai.github.io/langgraphjs/reference/functions/langgraph.interrupt-2.html) calls in bare try/catch blocks

### Do not reorder `interrupt` calls within a node

It's common to use multiple interrupts in a single node, however this can lead to unexpected behavior if not handled carefully.

When a node contains multiple interrupt calls, LangGraph keeps a list of resume values specific to the task executing the node. Whenever execution resumes, it starts at the beginning of the node. For each interrupt encountered, LangGraph checks if a matching value exists in the task's resume list. Matching is **strictly index-based**, so the order of interrupt calls within the node is important.

* ✅ Keep [`interrupt`](https://langchain-ai.github.io/langgraphjs/reference/functions/langgraph.interrupt-2.html) calls consistent across node executions

* 🔴 Do not conditionally skip [`interrupt`](https://langchain-ai.github.io/langgraphjs/reference/functions/langgraph.interrupt-2.html) calls within a node
* 🔴 Do not loop [`interrupt`](https://langchain-ai.github.io/langgraphjs/reference/functions/langgraph.interrupt-2.html) calls using logic that isn't deterministic across executions

### Do not return complex values in `interrupt` calls

Depending on which checkpointer is used, complex values may not be serializable (e.g. you can't serialize a function). To make your graphs adaptable to any deployment, it's best practice to only use values that can be reasonably serialized.

* ✅ Pass simple, JSON-serializable types to [`interrupt`](https://langchain-ai.github.io/langgraphjs/reference/functions/langgraph.interrupt-2.html)
* ✅ Pass dictionaries/objects with simple values

* 🔴 Do not pass functions, class instances, or other complex objects to [`interrupt`](https://langchain-ai.github.io/langgraphjs/reference/functions/langgraph.interrupt-2.html)

### Side effects called before `interrupt` must be idempotent

Because interrupts work by re-running the nodes they were called from, side effects called before [`interrupt`](https://langchain-ai.github.io/langgraphjs/reference/functions/langgraph.interrupt-2.html) should (ideally) be idempotent. For context, idempotency means that the same operation can be applied multiple times without changing the result beyond the initial execution.

As an example, you might have an API call to update a record inside of a node. If [`interrupt`](https://langchain-ai.github.io/langgraphjs/reference/functions/langgraph.interrupt-2.html) is called after that call is made, it will be re-run multiple times when the node is resumed, potentially overwriting the initial update or creating duplicate records.

* ✅ Use idempotent operations before [`interrupt`](https://langchain-ai.github.io/langgraphjs/reference/functions/langgraph.interrupt-2.html)
* ✅ Place side effects after [`interrupt`](https://langchain-ai.github.io/langgraphjs/reference/functions/langgraph.interrupt-2.html) calls
* ✅ Separate side effects into separate nodes when possible

* 🔴 Do not perform non-idempotent operations before [`interrupt`](https://langchain-ai.github.io/langgraphjs/reference/functions/langgraph.interrupt-2.html)
* 🔴 Do not create new records without checking if they exist

## Using with subgraphs called as functions

When invoking a subgraph within a node, the parent graph will resume execution from the **beginning of the node** where the subgraph was invoked and the [`interrupt`](https://langchain-ai.github.io/langgraphjs/reference/functions/langgraph.interrupt-2.html) was triggered. Similarly, the **subgraph** will also resume from the beginning of the node where [`interrupt`](https://langchain-ai.github.io/langgraphjs/reference/functions/langgraph.interrupt-2.html) was called.

## Debugging with interrupts

To debug and test a graph, you can use static interrupts as breakpoints to step through the graph execution one node at a time. Static interrupts are triggered at defined points either before or after a node executes. You can set these by specifying `interruptBefore` and `interruptAfter` when compiling the graph.

<Note>
  Static interrupts are **not** recommended for human-in-the-loop workflows. Use the [`interrupt`](https://langchain-ai.github.io/langgraphjs/reference/functions/langgraph.interrupt-2.html) method instead.
</Note>

<Tabs>
  <Tab title="At compile time">

1. The breakpoints are set during `compile` time.
    2. `interruptBefore` specifies the nodes where execution should pause before the node is executed.
    3. `interruptAfter` specifies the nodes where execution should pause after the node is executed.
    4. A checkpointer is required to enable breakpoints.
    5. The graph is run until the first breakpoint is hit.
    6. The graph is resumed by passing in `null` for the input. This will run the graph until the next breakpoint is hit.
  </Tab>

<Tab title="At run time">

1. `graph.invoke` is called with the `interruptBefore` and `interruptAfter` parameters. This is a run-time configuration and can be changed for every invocation.
    2. `interruptBefore` specifies the nodes where execution should pause before the node is executed.
    3. `interruptAfter` specifies the nodes where execution should pause after the node is executed.
    4. The graph is run until the first breakpoint is hit.
    5. The graph is resumed by passing in `null` for the input. This will run the graph until the next breakpoint is hit.
  </Tab>
</Tabs>

### Using LangGraph Studio

You can use [LangGraph Studio](/langsmith/studio) to set static interrupts in your graph in the UI before running the graph. You can also use the UI to inspect the graph state at any point in the execution.

<img src="https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/static-interrupt.png?fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=5aa4e7cea2ab147cef5b4e210dd6c4a1" alt="image" data-og-width="1252" width="1252" data-og-height="1040" height="1040" data-path="oss/images/static-interrupt.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/static-interrupt.png?w=280&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=52d02b507d0a6a879f7fb88d9c6767d0 280w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/static-interrupt.png?w=560&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=e363cd4980edff9bab422f4f1c0ee3c8 560w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/static-interrupt.png?w=840&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=49d26a3641953c23ef3fbc51e828c305 840w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/static-interrupt.png?w=1100&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=2dba15683b3baa1a61bc3bcada35ae1e 1100w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/static-interrupt.png?w=1650&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=9f9a2c0f2631c0e69cd248f6319933fe 1650w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/static-interrupt.png?w=2500&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=5a46b765b436ab5d0dc2f41c01ffad80 2500w" />

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/interrupts.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
When you call [`interrupt`](https://langchain-ai.github.io/langgraphjs/reference/functions/langgraph.interrupt-2.html), here's what happens:

1. **Graph execution gets suspended** at the exact point where [`interrupt`](https://langchain-ai.github.io/langgraphjs/reference/functions/langgraph.interrupt-2.html) is called
2. **State is saved** using the checkpointer so execution can be resumed later, In production, this should be a persistent checkpointer (e.g. backed by a database)
3. **Value is returned** to the caller under `__interrupt__`; it can be any JSON-serializable value (string, object, array, etc.)
4. **Graph waits indefinitely** until you resume execution with a response
5. **Response is passed back** into the node when you resume, becoming the return value of the `interrupt()` call

## Resuming interrupts

After an interrupt pauses execution, you resume the graph by invoking it again with a `Command` that contains the resume value. The resume value is passed back to the `interrupt` call, allowing the node to continue execution with the external input.
```

Example 2 (unknown):
```unknown
**Key points about resuming:**

* You must use the **same thread ID** when resuming that was used when the interrupt occurred
* The value passed to `Command(resume=...)` becomes the return value of the [`interrupt`](https://langchain-ai.github.io/langgraphjs/reference/functions/langgraph.interrupt-2.html) call
* The node restarts from the beginning of the node where the [`interrupt`](https://langchain-ai.github.io/langgraphjs/reference/functions/langgraph.interrupt-2.html) was called when resumed, so any code before the [`interrupt`](https://langchain-ai.github.io/langgraphjs/reference/functions/langgraph.interrupt-2.html) runs again
* You can pass any JSON-serializable value as the resume value

## Common patterns

The key thing that interrupts unlock is the ability to pause execution and wait for external input. This is useful for a variety of use cases, including:

* <Icon icon="check-circle" /> [Approval workflows](#approve-or-reject): Pause before executing critical actions (API calls, database changes, financial transactions)
* <Icon icon="pencil" /> [Review and edit](#review-and-edit-state): Let humans review and modify LLM outputs or tool calls before continuing
* <Icon icon="wrench" /> [Interrupting tool calls](#interrupts-in-tools): Pause before executing tool calls to review and edit the tool call before execution
* <Icon icon="shield-check" /> [Validating human input](#validating-human-input): Pause before proceeding to the next step to validate human input

### Approve or reject

One of the most common uses of interrupts is to pause before a critical action and ask for approval. For example, you might want to ask a human to approve an API call, a database change, or any other important decision.
```

Example 3 (unknown):
```unknown
When you resume the graph, pass `true` to approve or `false` to reject:
```

Example 4 (unknown):
```unknown
<Accordion title="Full example">
```

---

## Interrupt concurrent

**URL:** llms-txt#interrupt-concurrent

**Contents:**
- Setup
- Create runs
- View run results

Source: https://docs.langchain.com/langsmith/interrupt-concurrent

This guide assumes knowledge of what double-texting is, which you can learn about in the [double-texting conceptual guide](/langsmith/double-texting).

The guide covers the `interrupt` option for double texting, which interrupts the prior run of the graph and starts a new one with the double-text. This option does not delete the first run, but rather keeps it in the database but sets its status to `interrupted`. Below is a quick example of using the `interrupt` option.

First, we will define a quick helper function for printing out JS and CURL model outputs (you can skip this if using Python):

<Tabs>
  <Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

Now, let's import our required packages and instantiate our client, assistant, and thread.

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

Now we can start our two runs and join the second one until it has completed:

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

We can see that the thread has partial data from the first run + data from the second run

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

Verify that the original, interrupted run was interrupted

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>
</Tabs>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/interrupt-concurrent.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
</Tab>

  <Tab title="CURL">
```

Example 2 (unknown):
```unknown
</Tab>
</Tabs>

Now, let's import our required packages and instantiate our client, assistant, and thread.

<Tabs>
  <Tab title="Python">
```

Example 3 (unknown):
```unknown
</Tab>

  <Tab title="Javascript">
```

Example 4 (unknown):
```unknown
</Tab>

  <Tab title="CURL">
```

---

## Invoke

**URL:** llms-txt#invoke

**Contents:**
- Evaluator-optimizer
- Agents

state = orchestrator_worker.invoke({"topic": "Create a report on LLM scaling laws"})

from IPython.display import Markdown
Markdown(state["final_report"])
python Graph API theme={null}
  # Graph state
  class State(TypedDict):
      joke: str
      topic: str
      feedback: str
      funny_or_not: str

# Schema for structured output to use in evaluation
  class Feedback(BaseModel):
      grade: Literal["funny", "not funny"] = Field(
          description="Decide if the joke is funny or not.",
      )
      feedback: str = Field(
          description="If the joke is not funny, provide feedback on how to improve it.",
      )

# Augment the LLM with schema for structured output
  evaluator = llm.with_structured_output(Feedback)

# Nodes
  def llm_call_generator(state: State):
      """LLM generates a joke"""

if state.get("feedback"):
          msg = llm.invoke(
              f"Write a joke about {state['topic']} but take into account the feedback: {state['feedback']}"
          )
      else:
          msg = llm.invoke(f"Write a joke about {state['topic']}")
      return {"joke": msg.content}

def llm_call_evaluator(state: State):
      """LLM evaluates the joke"""

grade = evaluator.invoke(f"Grade the joke {state['joke']}")
      return {"funny_or_not": grade.grade, "feedback": grade.feedback}

# Conditional edge function to route back to joke generator or end based upon feedback from the evaluator
  def route_joke(state: State):
      """Route back to joke generator or end based upon feedback from the evaluator"""

if state["funny_or_not"] == "funny":
          return "Accepted"
      elif state["funny_or_not"] == "not funny":
          return "Rejected + Feedback"

# Build workflow
  optimizer_builder = StateGraph(State)

# Add the nodes
  optimizer_builder.add_node("llm_call_generator", llm_call_generator)
  optimizer_builder.add_node("llm_call_evaluator", llm_call_evaluator)

# Add edges to connect nodes
  optimizer_builder.add_edge(START, "llm_call_generator")
  optimizer_builder.add_edge("llm_call_generator", "llm_call_evaluator")
  optimizer_builder.add_conditional_edges(
      "llm_call_evaluator",
      route_joke,
      {  # Name returned by route_joke : Name of next node to visit
          "Accepted": END,
          "Rejected + Feedback": "llm_call_generator",
      },
  )

# Compile the workflow
  optimizer_workflow = optimizer_builder.compile()

# Show the workflow
  display(Image(optimizer_workflow.get_graph().draw_mermaid_png()))

# Invoke
  state = optimizer_workflow.invoke({"topic": "Cats"})
  print(state["joke"])
  python Functional API theme={null}
  # Schema for structured output to use in evaluation
  class Feedback(BaseModel):
      grade: Literal["funny", "not funny"] = Field(
          description="Decide if the joke is funny or not.",
      )
      feedback: str = Field(
          description="If the joke is not funny, provide feedback on how to improve it.",
      )

# Augment the LLM with schema for structured output
  evaluator = llm.with_structured_output(Feedback)

# Nodes
  @task
  def llm_call_generator(topic: str, feedback: Feedback):
      """LLM generates a joke"""
      if feedback:
          msg = llm.invoke(
              f"Write a joke about {topic} but take into account the feedback: {feedback}"
          )
      else:
          msg = llm.invoke(f"Write a joke about {topic}")
      return msg.content

@task
  def llm_call_evaluator(joke: str):
      """LLM evaluates the joke"""
      feedback = evaluator.invoke(f"Grade the joke {joke}")
      return feedback

@entrypoint()
  def optimizer_workflow(topic: str):
      feedback = None
      while True:
          joke = llm_call_generator(topic, feedback).result()
          feedback = llm_call_evaluator(joke).result()
          if feedback.grade == "funny":
              break

# Invoke
  for step in optimizer_workflow.stream("Cats", stream_mode="updates"):
      print(step)
      print("\n")
  python Using tools theme={null}
from langchain.tools import tool

**Examples:**

Example 1 (unknown):
```unknown
## Evaluator-optimizer

In evaluator-optimizer workflows, one LLM call creates a response and the other evaluates that response. If the evaluator or a [human-in-the-loop](/oss/python/langgraph/interrupts) determines the response needs refinement, feedback is provided and the response is recreated. This loop continues until an acceptable response is generated.

Evaluator-optimizer workflows are commonly used when there's particular success criteria for a task, but iteration is required to meet that criteria. For example, there's not always a perfect match when translating text between two languages. It might take a few iterations to generate a translation with the same meaning across the two languages.

<img src="https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/evaluator_optimizer.png?fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=9bd0474f42b6040b14ed6968a9ab4e3c" alt="evaluator_optimizer.png" data-og-width="1004" width="1004" data-og-height="340" height="340" data-path="oss/images/evaluator_optimizer.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/evaluator_optimizer.png?w=280&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=ab36856e5f9a518b22e71278aa8b1711 280w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/evaluator_optimizer.png?w=560&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=3ec597c92270278c2bac203d36b611c2 560w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/evaluator_optimizer.png?w=840&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=3ad3bfb734a0e509d9b87fdb4e808bfd 840w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/evaluator_optimizer.png?w=1100&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=e82bd25a463d3cdf76036649c03358a9 1100w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/evaluator_optimizer.png?w=1650&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=d31717ae3e76243dd975a53f46e8c1f6 1650w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/evaluator_optimizer.png?w=2500&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=a9bb4fb1583f6ad06c0b13602cd14811 2500w" />

<CodeGroup>
```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
</CodeGroup>

## Agents

Agents are typically implemented as an LLM performing actions using [tools](/oss/python/langchain/tools). They operate in continuous feedback loops, and are used in situations where problems and solutions are unpredictable. Agents have more autonomy than workflows, and can make decisions about the tools they use and how to solve problems. You can still define the available toolset and guidelines for how agents behave.

<img src="https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/agent.png?fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=bd8da41dbf8b5e6fc9ea6bb10cb63e38" alt="agent.png" data-og-width="1732" width="1732" data-og-height="712" height="712" data-path="oss/images/agent.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/agent.png?w=280&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=f7a590604edc49cfa273b5856f3a3ee3 280w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/agent.png?w=560&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=dff9b17d345fe0fea25616b3b0dc6ebf 560w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/agent.png?w=840&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=bd932835b919f5e58be77221b6d0f194 840w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/agent.png?w=1100&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=d53318b0c9c898a6146991691cbac058 1100w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/agent.png?w=1650&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=ea66fb96bc07c595d321b8b71e651ddb 1650w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/agent.png?w=2500&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=b02599a3c9ba2a5c830b9a346f9d26c9 2500w" />

<Note>
  To get started with agents, see the [quickstart](/oss/python/langchain/quickstart) or read more about [how they work](/oss/python/langchain/agents) in LangChain.
</Note>
```

---

## Invoke the agent

**URL:** llms-txt#invoke-the-agent

result = agent.invoke({
    "messages": [{"role": "user", "content": "Delete the file temp.txt"}]
}, config=config)

---

## Invoke the augmented LLM

**URL:** llms-txt#invoke-the-augmented-llm

output = structured_llm.invoke("How does Calcium CT score relate to high cholesterol?")

---

## Invoke the graph

**URL:** llms-txt#invoke-the-graph

config = {"configurable": {"thread_id": "2", "user_id": "1"}}

---

## Invoke the graph with an input and print the result

**URL:** llms-txt#invoke-the-graph-with-an-input-and-print-the-result

**Contents:**
  - Pass private state between nodes

print(graph.invoke({"question": "hi"}))

{'answer': 'bye'}
python  theme={null}
from langgraph.graph import StateGraph, START, END
from typing_extensions import TypedDict

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
Notice that the output of invoke only includes the output schema.

### Pass private state between nodes

In some cases, you may want nodes to exchange information that is crucial for intermediate logic but doesn't need to be part of the main schema of the graph. This private data is not relevant to the overall input/output of the graph and should only be shared between certain nodes.

Below, we'll create an example sequential graph consisting of three nodes (node\_1, node\_2 and node\_3), where private data is passed between the first two steps (node\_1 and node\_2), while the third step (node\_3) only has access to the public overall state.
```

---

## Invoke the graph with the initial state

**URL:** llms-txt#invoke-the-graph-with-the-initial-state

**Contents:**
  - Use Pydantic models for graph state

response = graph.invoke(
    {
        "a": "set at start",
    }
)

print()
print(f"Output of graph invocation: {response}")

Entered node `node_1`:
    Input: {'a': 'set at start'}.
    Returned: {'private_data': 'set by node_1'}
Entered node `node_2`:
    Input: {'private_data': 'set by node_1'}.
    Returned: {'a': 'set by node_2'}
Entered node `node_3`:
    Input: {'a': 'set by node_2'}.
    Returned: {'a': 'set by node_3'}

Output of graph invocation: {'a': 'set by node_3'}
python  theme={null}
from langgraph.graph import StateGraph, START, END
from typing_extensions import TypedDict
from pydantic import BaseModel

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
### Use Pydantic models for graph state

A [StateGraph](https://langchain-ai.github.io/langgraph/reference/graphs.md#langgraph.graph.StateGraph) accepts a [`state_schema`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.AgentMiddleware.state_schema) argument on initialization that specifies the "shape" of the state that the nodes in the graph can access and update.

In our examples, we typically use a python-native `TypedDict` or [`dataclass`](https://docs.python.org/3/library/dataclasses.html) for `state_schema`, but [`state_schema`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.AgentMiddleware.state_schema) can be any [type](https://docs.python.org/3/library/stdtypes.html#type-objects).

Here, we'll see how a [Pydantic BaseModel](https://docs.pydantic.dev/latest/api/base_model/) can be used for [`state_schema`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.AgentMiddleware.state_schema) to add run-time validation on **inputs**.

<Note>
  **Known Limitations**

  * Currently, the output of the graph will **NOT** be an instance of a pydantic model.
  * Run-time validation only occurs on inputs to the first node in the graph, not on subsequent nodes or outputs.
  * The validation error trace from pydantic does not show which node the error arises in.
  * Pydantic's recursive validation can be slow. For performance-sensitive applications, you may want to consider using a `dataclass` instead.
</Note>
```

---

## Invoke the LLM with input that triggers the tool call

**URL:** llms-txt#invoke-the-llm-with-input-that-triggers-the-tool-call

msg = llm_with_tools.invoke("What is 2 times 3?")

---

## Invoke with custom state

**URL:** llms-txt#invoke-with-custom-state

**Contents:**
  - Execution order
  - Agent jumps
  - Best practices
- Examples
  - Dynamically selecting tools
- Additional resources

result = agent.invoke({
    "messages": [HumanMessage("Hello")],
    "model_call_count": 0,
    "user_id": "user-123",
})
python  theme={null}
agent = create_agent(
    model="gpt-4o",
    middleware=[middleware1, middleware2, middleware3],
    tools=[...],
)
python  theme={null}
class EarlyExitMiddleware(AgentMiddleware):
    def before_model(self, state: AgentState, runtime) -> dict[str, Any] | None:
        # Check some condition
        if should_exit(state):
            return {
                "messages": [AIMessage("Exiting early due to condition.")],
                "jump_to": "end"
            }
        return None
python  theme={null}
from langchain.agents.middleware import AgentMiddleware, hook_config
from typing import Any

class ConditionalMiddleware(AgentMiddleware):
    @hook_config(can_jump_to=["end", "tools"])
    def after_model(self, state: AgentState, runtime) -> dict[str, Any] | None:
        if some_condition(state):
            return {"jump_to": "end"}
        return None
python  theme={null}
from langchain.agents import create_agent
from langchain.agents.middleware import AgentMiddleware, ModelRequest
from typing import Callable

class ToolSelectorMiddleware(AgentMiddleware):
    def wrap_model_call(
        self,
        request: ModelRequest,
        handler: Callable[[ModelRequest], ModelResponse],
    ) -> ModelResponse:
        """Middleware to select relevant tools based on state/context."""
        # Select a small, relevant subset of tools based on state/context
        relevant_tools = select_relevant_tools(request.state, request.runtime)
        request.tools = relevant_tools
        return handler(request)

agent = create_agent(
    model="gpt-4o",
    tools=all_tools,  # All available tools need to be registered upfront
    # Middleware can be used to select a smaller subset that's relevant for the given run.
    middleware=[ToolSelectorMiddleware()],
)
python  theme={null}
  from dataclasses import dataclass
  from typing import Literal, Callable

from langchain.agents import create_agent
  from langchain.agents.middleware import AgentMiddleware, ModelRequest, ModelResponse
  from langchain_core.tools import tool

@tool
  def github_create_issue(repo: str, title: str) -> dict:
      """Create an issue in a GitHub repository."""
      return {"url": f"https://github.com/{repo}/issues/1", "title": title}

@tool
  def gitlab_create_issue(project: str, title: str) -> dict:
      """Create an issue in a GitLab project."""
      return {"url": f"https://gitlab.com/{project}/-/issues/1", "title": title}

all_tools = [github_create_issue, gitlab_create_issue]

@dataclass
  class Context:
      provider: Literal["github", "gitlab"]

class ToolSelectorMiddleware(AgentMiddleware):
      def wrap_model_call(
          self,
          request: ModelRequest,
          handler: Callable[[ModelRequest], ModelResponse],
      ) -> ModelResponse:
          """Select tools based on the VCS provider."""
          provider = request.runtime.context.provider

if provider == "gitlab":
              selected_tools = [t for t in request.tools if t.name == "gitlab_create_issue"]
          else:
              selected_tools = [t for t in request.tools if t.name == "github_create_issue"]

request.tools = selected_tools
          return handler(request)

agent = create_agent(
      model="gpt-4o",
      tools=all_tools,
      middleware=[ToolSelectorMiddleware()],
      context_schema=Context,
  )

# Invoke with GitHub context
  agent.invoke(
      {
          "messages": [{"role": "user", "content": "Open an issue titled 'Bug: where are the cats' in the repository `its-a-cats-game`"}]
      },
      context=Context(provider="github"),
  )
  ```

* Register all tools upfront
  * Middleware selects the relevant subset per request
  * Use `context_schema` for configuration requirements
</Expandable>

## Additional resources

* [Middleware API reference](https://reference.langchain.com/python/langchain/middleware/) - Complete guide to custom middleware
* [Human-in-the-loop](/oss/python/langchain/human-in-the-loop) - Add human review for sensitive operations
* [Testing agents](/oss/python/langchain/test) - Strategies for testing safety mechanisms

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/middleware.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
### Execution order

When using multiple middleware, understanding execution order is important:
```

Example 2 (unknown):
```unknown
<Accordion title="Execution flow (click to expand)">
  **Before hooks run in order:**

  1. `middleware1.before_agent()`
  2. `middleware2.before_agent()`
  3. `middleware3.before_agent()`

  **Agent loop starts**

  5. `middleware1.before_model()`
  6. `middleware2.before_model()`
  7. `middleware3.before_model()`

  **Wrap hooks nest like function calls:**

  8. `middleware1.wrap_model_call()` → `middleware2.wrap_model_call()` → `middleware3.wrap_model_call()` → model

  **After hooks run in reverse order:**

  9. `middleware3.after_model()`
  10. `middleware2.after_model()`
  11. `middleware1.after_model()`

  **Agent loop ends**

  13. `middleware3.after_agent()`
  14. `middleware2.after_agent()`
  15. `middleware1.after_agent()`
</Accordion>

**Key rules:**

* `before_*` hooks: First to last
* `after_*` hooks: Last to first (reverse)
* `wrap_*` hooks: Nested (first middleware wraps all others)

### Agent jumps

To exit early from middleware, return a dictionary with `jump_to`:
```

Example 3 (unknown):
```unknown
Available jump targets:

* `"end"`: Jump to the end of the agent execution
* `"tools"`: Jump to the tools node
* `"model"`: Jump to the model node (or the first `before_model` hook)

**Important:** When jumping from `before_model` or `after_model`, jumping to `"model"` will cause all `before_model` middleware to run again.

To enable jumping, decorate your hook with `@hook_config(can_jump_to=[...])`:
```

Example 4 (unknown):
```unknown
### Best practices

1. Keep middleware focused - each should do one thing well
2. Handle errors gracefully - don't let middleware errors crash the agent
3. **Use appropriate hook types**:
   * Node-style for sequential logic (logging, validation)
   * Wrap-style for control flow (retry, fallback, caching)
4. Clearly document any custom state properties
5. Unit test middleware independently before integrating
6. Consider execution order - place critical middleware first in the list
7. Use built-in middleware when possible, don't reinvent the wheel :)

## Examples

### Dynamically selecting tools

Select relevant tools at runtime to improve performance and accuracy.

<Tip>
  **Benefits:**

  * **Shorter prompts** - Reduce complexity by exposing only relevant tools
  * **Better accuracy** - Models choose correctly from fewer options
  * **Permission control** - Dynamically filter tools based on user access
</Tip>
```

---

## In pyproject.toml

**URL:** llms-txt#in-pyproject.toml

**Contents:**
- Usage
- Custom Embeddings

[project]
dependencies = [
    "langchain>=0.3.8"
]

langchain>=0.3.8
python  theme={null}
def search_memory(state: State, *, store: BaseStore):
    # Search the store using semantic similarity
    # The namespace tuple helps organize different types of memories
    # e.g., ("user_facts", "preferences") or ("conversation", "summaries")
    results = store.search(
        namespace=("memory", "facts"),  # Organize memories by type
        query="your search query",
        limit=3  # number of results to return
    )
    return results
json  theme={null}
{
    ...
    "store": {
        "index": {
            "embed": "path/to/embedding_function.py:embed",
            "dims": 1536,
            "fields": ["$"]
        }
    }
}
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
Or if using [requirements.txt](/langsmith/setup-app-requirements-txt):
```

Example 2 (unknown):
```unknown
## Usage

Once configured, you can use semantic search in your [nodes](/oss/python/langgraph/graph-api#nodes). The store requires a namespace tuple to organize memories:
```

Example 3 (unknown):
```unknown
## Custom Embeddings

If you want to use custom embeddings, you can pass a path to a custom embedding function:
```

Example 4 (unknown):
```unknown
The deployment will look for the function in the specified path. The function must be async and accept a list of strings:
```

---

## In your auth handler:

**URL:** llms-txt#in-your-auth-handler:

**Contents:**
- Supported resources
- Next steps

@auth.authenticate
async def authenticate(headers: dict) -> Auth.types.MinimalUserDict:
    ...
    return {
        "identity": "user-123",
        "is_authenticated": True,
        "permissions": ["threads:write", "threads:read"]  # Define permissions in auth
    }

def _default(ctx: Auth.types.AuthContext, value: dict):
    metadata = value.setdefault("metadata", {})
    metadata["owner"] = ctx.user.identity
    return {"owner": ctx.user.identity}

@auth.on.threads.create
async def create_thread(ctx: Auth.types.AuthContext, value: dict):
    if "threads:write" not in ctx.permissions:
        raise Auth.exceptions.HTTPException(
            status_code=403,
            detail="Unauthorized"
        )
    return _default(ctx, value)

@auth.on.threads.read
async def rbac_create(ctx: Auth.types.AuthContext, value: dict):
    if "threads:read" not in ctx.permissions and "threads:write" not in ctx.permissions:
        raise Auth.exceptions.HTTPException(
            status_code=403,
            detail="Unauthorized"
        )
    return _default(ctx, value)
python  theme={null}
  @auth.on.threads.create
  async def on_thread_create(
  ctx: Auth.types.AuthContext,
  value: Auth.types.on.threads.create.value  # Specific type for thread creation
  ):
  ...

@auth.on.threads
  async def on_threads(
  ctx: Auth.types.AuthContext,
  value: Auth.types.on.threads.value  # Union type of all thread actions
  ):
  ...

@auth.on
  async def on_all(
  ctx: Auth.types.AuthContext,
  value: dict  # Union type of all possible actions
  ):
  ...
  ```

More specific handlers provide better type hints since they handle fewer action types.
</Tip>

<a id="supported-actions" />

#### Supported actions and types

Here are all the supported action handlers:

| Resource       | Handler                       | Description                | Value Type                                                                                                                       |
| -------------- | ----------------------------- | -------------------------- | -------------------------------------------------------------------------------------------------------------------------------- |
| **Threads**    | `@auth.on.threads.create`     | Thread creation            | [`ThreadsCreate`](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.types.ThreadsCreate)       |
|                | `@auth.on.threads.read`       | Thread retrieval           | [`ThreadsRead`](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.types.ThreadsRead)           |
|                | `@auth.on.threads.update`     | Thread updates             | [`ThreadsUpdate`](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.types.ThreadsUpdate)       |
|                | `@auth.on.threads.delete`     | Thread deletion            | [`ThreadsDelete`](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.types.ThreadsDelete)       |
|                | `@auth.on.threads.search`     | Listing threads            | [`ThreadsSearch`](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.types.ThreadsSearch)       |
|                | `@auth.on.threads.create_run` | Creating or updating a run | [`RunsCreate`](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.types.RunsCreate)             |
| **Assistants** | `@auth.on.assistants.create`  | Assistant creation         | [`AssistantsCreate`](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.types.AssistantsCreate) |
|                | `@auth.on.assistants.read`    | Assistant retrieval        | [`AssistantsRead`](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.types.AssistantsRead)     |
|                | `@auth.on.assistants.update`  | Assistant updates          | [`AssistantsUpdate`](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.types.AssistantsUpdate) |
|                | `@auth.on.assistants.delete`  | Assistant deletion         | [`AssistantsDelete`](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.types.AssistantsDelete) |
|                | `@auth.on.assistants.search`  | Listing assistants         | [`AssistantsSearch`](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.types.AssistantsSearch) |
| **Crons**      | `@auth.on.crons.create`       | Cron job creation          | [`CronsCreate`](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.types.CronsCreate)           |
|                | `@auth.on.crons.read`         | Cron job retrieval         | [`CronsRead`](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.types.CronsRead)               |
|                | `@auth.on.crons.update`       | Cron job updates           | [`CronsUpdate`](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.types.CronsUpdate)           |
|                | `@auth.on.crons.delete`       | Cron job deletion          | [`CronsDelete`](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.types.CronsDelete)           |
|                | `@auth.on.crons.search`       | Listing cron jobs          | [`CronsSearch`](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.types.CronsSearch)           |

<Note>
  "About Runs"

Runs are scoped to their parent thread for access control. This means permissions are typically inherited from the thread, reflecting the conversational nature of the data model. All run operations (reading, listing) except creation are controlled by the thread's handlers.
  There is a specific `create_run` handler for creating new runs because it had more arguments that you can view in the handler.
</Note>

For implementation details:

* Check out the introductory tutorial on [setting up authentication](/langsmith/set-up-custom-auth)
* See the how-to guide on implementing a [custom auth handlers](/langsmith/custom-auth)

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/auth.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
## Supported resources

LangGraph provides three levels of authorization handlers, from most general to most specific:

1. **Global Handler** (`@auth.on`): Matches all resources and actions
2. **Resource Handler** (e.g., `@auth.on.threads`, `@auth.on.assistants`, `@auth.on.crons`): Matches all actions for a specific resource
3. **Action Handler** (e.g., `@auth.on.threads.create`, `@auth.on.threads.read`): Matches a specific action on a specific resource

The most specific matching handler will be used. For example, `@auth.on.threads.create` takes precedence over `@auth.on.threads` for thread creation.
If a more specific handler is registered, the more general handler will not be called for that resource and action.

<Tip>
  "Type Safety"
  Each handler has type hints available for its `value` parameter at `Auth.types.on.<resource>.<action>.value`. For example:
```

---

## is a distance metric that varies inversely with similarity.

**URL:** llms-txt#is-a-distance-metric-that-varies-inversely-with-similarity.

**Contents:**
- 4. Retrievers
- Next steps

results = vector_store.similarity_search_with_score("What was Nike's revenue in 2023?")
doc, score = results[0]
print(f"Score: {score}\n")
print(doc)
output  theme={null}
Score: 0.23699893057346344

page_content='Table of Contents
FISCAL 2023 NIKE BRAND REVENUE HIGHLIGHTS
The following tables present NIKE Brand revenues disaggregated by reportable operating segment, distribution channel and major product line:
FISCAL 2023 COMPARED TO FISCAL 2022
•NIKE, Inc. Revenues were $51.2 billion in fiscal 2023, which increased 10% and 16% compared to fiscal 2022 on a reported and currency-neutral basis, respectively.
The increase was due to higher revenues in North America, Europe, Middle East & Africa ("EMEA"), APLA and Greater China, which contributed approximately 7, 6,
2 and 1 percentage points to NIKE, Inc. Revenues, respectively.
•NIKE Brand revenues, which represented over 90% of NIKE, Inc. Revenues, increased 10% and 16% on a reported and currency-neutral basis, respectively. This
increase was primarily due to higher revenues in Men's, the Jordan Brand, Women's and Kids' which grew 17%, 35%,11% and 10%, respectively, on a wholesale
equivalent basis.' metadata={'page': 35, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 0}
python  theme={null}
embedding = embeddings.embed_query("How were Nike's margins impacted in 2023?")

results = vector_store.similarity_search_by_vector(embedding)
print(results[0])
output  theme={null}
page_content='Table of Contents
GROSS MARGIN
FISCAL 2023 COMPARED TO FISCAL 2022
For fiscal 2023, our consolidated gross profit increased 4% to $22,292 million compared to $21,479 million for fiscal 2022. Gross margin decreased 250 basis points to
43.5% for fiscal 2023 compared to 46.0% for fiscal 2022 due to the following:
*Wholesale equivalent
The decrease in gross margin for fiscal 2023 was primarily due to:
•Higher NIKE Brand product costs, on a wholesale equivalent basis, primarily due to higher input costs and elevated inbound freight and logistics costs as well as
product mix;
•Lower margin in our NIKE Direct business, driven by higher promotional activity to liquidate inventory in the current period compared to lower promotional activity in
the prior period resulting from lower available inventory supply;
•Unfavorable changes in net foreign currency exchange rates, including hedges; and
•Lower off-price margin, on a wholesale equivalent basis.
This was partially offset by:' metadata={'page': 36, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 0}
python  theme={null}
from typing import List

from langchain_core.documents import Document
from langchain_core.runnables import chain

@chain
def retriever(query: str) -> List[Document]:
    return vector_store.similarity_search(query, k=1)

retriever.batch(
    [
        "How many distribution centers does Nike have in the US?",
        "When was Nike incorporated?",
    ],
)
output  theme={null}
[[Document(metadata={'page': 4, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 3125}, page_content='direct to consumer operations sell products through the following number of retail stores in the United States:\nU.S. RETAIL STORES NUMBER\nNIKE Brand factory stores 213 \nNIKE Brand in-line stores (including employee-only stores) 74 \nConverse stores (including factory stores) 82 \nTOTAL 369 \nIn the United States, NIKE has eight significant distribution centers. Refer to Item 2. Properties for further information.\n2023 FORM 10-K 2')],
 [Document(metadata={'page': 3, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 0}, page_content='Table of Contents\nPART I\nITEM 1. BUSINESS\nGENERAL\nNIKE, Inc. was incorporated in 1967 under the laws of the State of Oregon. As used in this Annual Report on Form 10-K (this "Annual Report"), the terms "we," "us," "our,"\n"NIKE" and the "Company" refer to NIKE, Inc. and its predecessors, subsidiaries and affiliates, collectively, unless the context indicates otherwise.\nOur principal business activity is the design, development and worldwide marketing and selling of athletic footwear, apparel, equipment, accessories and services. NIKE is\nthe largest seller of athletic footwear and apparel in the world. We sell our products through NIKE Direct operations, which are comprised of both NIKE-owned retail stores\nand sales through our digital platforms (also referred to as "NIKE Brand Digital"), to retail accounts and to a mix of independent distributors, licensees and sales')]]
python  theme={null}
retriever = vector_store.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 1},
)

retriever.batch(
    [
        "How many distribution centers does Nike have in the US?",
        "When was Nike incorporated?",
    ],
)
output  theme={null}
[[Document(metadata={'page': 4, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 3125}, page_content='direct to consumer operations sell products through the following number of retail stores in the United States:\nU.S. RETAIL STORES NUMBER\nNIKE Brand factory stores 213 \nNIKE Brand in-line stores (including employee-only stores) 74 \nConverse stores (including factory stores) 82 \nTOTAL 369 \nIn the United States, NIKE has eight significant distribution centers. Refer to Item 2. Properties for further information.\n2023 FORM 10-K 2')],
 [Document(metadata={'page': 3, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 0}, page_content='Table of Contents\nPART I\nITEM 1. BUSINESS\nGENERAL\nNIKE, Inc. was incorporated in 1967 under the laws of the State of Oregon. As used in this Annual Report on Form 10-K (this "Annual Report"), the terms "we," "us," "our,"\n"NIKE" and the "Company" refer to NIKE, Inc. and its predecessors, subsidiaries and affiliates, collectively, unless the context indicates otherwise.\nOur principal business activity is the design, development and worldwide marketing and selling of athletic footwear, apparel, equipment, accessories and services. NIKE is\nthe largest seller of athletic footwear and apparel in the world. We sell our products through NIKE Direct operations, which are comprised of both NIKE-owned retail stores\nand sales through our digital platforms (also referred to as "NIKE Brand Digital"), to retail accounts and to a mix of independent distributors, licensees and sales')]]
```

`VectorStoreRetriever` supports search types of `"similarity"` (default), `"mmr"` (maximum marginal relevance, described above), and `"similarity_score_threshold"`. We can use the latter to threshold documents output by the retriever by similarity score.

Retrievers can easily be incorporated into more complex applications, such as [retrieval-augmented generation (RAG)](/oss/python/langchain/retrieval) applications that combine a given question with retrieved context into a prompt for a LLM. To learn more about building such an application, check out the [RAG tutorial](/oss/python/langchain/rag) tutorial.

You've now seen how to build a semantic search engine over a PDF document.

For more on document loaders:

* [Overview](/oss/python/langchain/retrieval#document_loaders)
* [Available integrations](/oss/python/integrations/document_loaders/)

For more on embeddings:

* [Overview](/oss/python/langchain/retrieval#embedding_models/)
* [Available integrations](/oss/python/integrations/text_embedding/)

For more on vector stores:

* [Overview](/oss/python/langchain/retrieval#vectorstores/)
* [Available integrations](/oss/python/integrations/vectorstores/)

For more on RAG, see:

* [Build a Retrieval Augmented Generation (RAG) App](/oss/python/langchain/rag/)

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/knowledge-base.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
Return documents based on similarity to an embedded query:
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
Learn more:

* [API Reference](https://reference.langchain.com/python/langchain_core/vectorstores/?h=#langchain_core.vectorstores.base.VectorStore)
* [Integration-specific docs](/oss/python/integrations/vectorstores)

## 4. Retrievers

LangChain [`VectorStore`](https://reference.langchain.com/python/langchain_core/vectorstores/?h=#langchain_core.vectorstores.base.VectorStore) objects do not subclass @\[Runnable]. LangChain @\[Retrievers] are Runnables, so they implement a standard set of methods (e.g., synchronous and asynchronous `invoke` and `batch` operations). Although we can construct retrievers from vector stores, retrievers can interface with non-vector store sources of data, as well (such as external APIs).

We can create a simple version of this ourselves, without subclassing `Retriever`. If we choose what method we wish to use to retrieve documents, we can create a runnable easily. Below we will build one around the `similarity_search` method:
```

---

## It's not something you will have in your actual code.

**URL:** llms-txt#it's-not-something-you-will-have-in-your-actual-code.

@task()
def get_info():
    """
    Simulates a task that fails once before succeeding.
    Raises an exception on the first attempt, then returns "OK" on subsequent tries.
    """
    global attempts
    attempts += 1

if attempts < 2:
        raise ValueError("Failure")  # Simulate a failure on the first attempt
    return "OK"

---

## it will take precedence for any "create" actions on the "threads" resources

**URL:** llms-txt#it-will-take-precedence-for-any-"create"-actions-on-the-"threads"-resources

@auth.on.threads.create
async def on_thread_create(
    ctx: Auth.types.AuthContext,
    value: Auth.types.threads.create.value
):
    # Reject if the user does not have write access
    if "write" not in ctx.permissions:
        raise Auth.exceptions.HTTPException(
            status_code=403,
            detail="User lacks the required permissions."
        )
    # Setting metadata on the thread being created
    # will ensure that the resource contains an "owner" field
    # Then any time a user tries to access this thread or runs within the thread,
    # we can filter by owner
    metadata = value.setdefault("metadata", {})
    metadata["owner"] = ctx.user.identity
    return {"owner": ctx.user.identity}

---

## it will take precedence for any "read" actions on the "threads" resource

**URL:** llms-txt#it-will-take-precedence-for-any-"read"-actions-on-the-"threads"-resource

@auth.on.threads.read
async def on_thread_read(
    ctx: Auth.types.AuthContext,
    value: Auth.types.threads.read.value
):
    # Since we are reading (and not creating) a thread,
    # we don't need to set metadata. We just need to
    # return a filter to ensure users can only see their own threads
    return {"owner": ctx.user.identity}

---

## Judge LLM

**URL:** llms-txt#judge-llm

grader_llm = init_chat_model("gpt-4o-mini", temperature=0).with_structured_output(Grade, method="json_schema", strict=True)

---

## Keep our previous handlers...

**URL:** llms-txt#keep-our-previous-handlers...

from langgraph_sdk import Auth

@auth.on.threads.create
async def on_thread_create(
    ctx: Auth.types.AuthContext,
    value: Auth.types.on.threads.create.value,
):
    """Add owner when creating threads.

This handler runs when creating new threads and does two things:
    1. Sets metadata on the thread being created to track ownership
    2. Returns a filter that ensures only the creator can access it
    """
    # Example value:
    #  {'thread_id': UUID('99b045bc-b90b-41a8-b882-dabc541cf740'), 'metadata': {}, 'if_exists': 'raise'}

# Add owner metadata to the thread being created
    # This metadata is stored with the thread and persists
    metadata = value.setdefault("metadata", {})
    metadata["owner"] = ctx.user.identity

# Return filter to restrict access to just the creator
    return {"owner": ctx.user.identity}

@auth.on.threads.read
async def on_thread_read(
    ctx: Auth.types.AuthContext,
    value: Auth.types.on.threads.read.value,
):
    """Only let users read their own threads.

This handler runs on read operations. We don't need to set
    metadata since the thread already exists - we just need to
    return a filter to ensure users can only see their own threads.
    """
    return {"owner": ctx.user.identity}

@auth.on.assistants
async def on_assistants(
    ctx: Auth.types.AuthContext,
    value: Auth.types.on.assistants.value,
):
    # For illustration purposes, we will deny all requests
    # that touch the assistants resource
    # Example value:
    # {
    #     'assistant_id': UUID('63ba56c3-b074-4212-96e2-cc333bbc4eb4'),
    #     'graph_id': 'agent',
    #     'config': {},
    #     'metadata': {},
    #     'name': 'Untitled'
    # }
    raise Auth.exceptions.HTTPException(
        status_code=403,
        detail="User lacks the required permissions.",
    )

---

## Keep our resource authorization from the previous tutorial

**URL:** llms-txt#keep-our-resource-authorization-from-the-previous-tutorial

**Contents:**
- 4. Test authentication flow

@auth.on
async def add_owner(ctx, value):
    """Make resources private to their creator using resource metadata."""
    filters = {"owner": ctx.user.identity}
    metadata = value.setdefault("metadata", {})
    metadata.update(filters)
    return filters
python  theme={null}
import os
import httpx
from getpass import getpass
from langgraph_sdk import get_client

**Examples:**

Example 1 (unknown):
```unknown
The most important change is that we're now validating tokens with a real authentication server. Our authentication handler has the private key for our Supabase project, which we can use to validate the user's token and extract their information.

## 4. Test authentication flow

Let's test out the new authentication flow. You can run the following code in a file or notebook. You will need to provide:

* A valid email address
* A Supabase project URL (from [above](#setup-auth-provider))
* A Supabase anon **public key** (also from [above](#setup-auth-provider))
```

---

## Keep our test users from the previous tutorial

**URL:** llms-txt#keep-our-test-users-from-the-previous-tutorial

**Contents:**
- 2. Test private conversations

VALID_TOKENS = {
    "user1-token": {"id": "user1", "name": "Alice"},
    "user2-token": {"id": "user2", "name": "Bob"},
}

@auth.authenticate
async def get_current_user(authorization: str | None) -> Auth.types.MinimalUserDict:
    """Our authentication handler from the previous tutorial."""
    assert authorization
    scheme, token = authorization.split()
    assert scheme.lower() == "bearer"

if token not in VALID_TOKENS:
        raise Auth.exceptions.HTTPException(status_code=401, detail="Invalid token")

user_data = VALID_TOKENS[token]
    return {
        "identity": user_data["id"],
    }

@auth.on
async def add_owner(
    ctx: Auth.types.AuthContext,  # Contains info about the current user
    value: dict,  # The resource being created/accessed
):
    """Make resources private to their creator."""
    # Examples:
    # ctx: AuthContext(
    #     permissions=[],
    #     user=ProxyUser(
    #         identity='user1',
    #         is_authenticated=True,
    #         display_name='user1'
    #     ),
    #     resource='threads',
    #     action='create_run'
    # )
    # value:
    # {
    #     'thread_id': UUID('1e1b2733-303f-4dcd-9620-02d370287d72'),
    #     'assistant_id': UUID('fe096781-5601-53d2-b2f6-0d3403f7e9ca'),
    #     'run_id': UUID('1efbe268-1627-66d4-aa8d-b956b0f02a41'),
    #     'status': 'pending',
    #     'metadata': {},
    #     'prevent_insert_if_inflight': True,
    #     'multitask_strategy': 'reject',
    #     'if_not_exists': 'reject',
    #     'after_seconds': 0,
    #     'kwargs': {
    #         'input': {'messages': [{'role': 'user', 'content': 'Hello!'}]},
    #         'command': None,
    #         'config': {
    #             'configurable': {
    #                 'langgraph_auth_user': ... Your user object...
    #                 'langgraph_auth_user_id': 'user1'
    #             }
    #         },
    #         'stream_mode': ['values'],
    #         'interrupt_before': None,
    #         'interrupt_after': None,
    #         'webhook': None,
    #         'feedback_keys': None,
    #         'temporary': False,
    #         'subgraphs': False
    #     }
    # }

# Does 2 things:
    # 1. Add the user's ID to the resource's metadata. Each LangGraph resource has a `metadata` dict that persists with the resource.
    # this metadata is useful for filtering in read and update operations
    # 2. Return a filter that lets users only see their own resources
    filters = {"owner": ctx.user.identity}
    metadata = value.setdefault("metadata", {})
    metadata.update(filters)

# Only let users see their own resources
    return filters
python  theme={null}
from langgraph_sdk import get_client

**Examples:**

Example 1 (unknown):
```unknown
The handler receives two parameters:

1. `ctx` ([AuthContext](/langsmith/langgraph-python-sdk#langgraph_sdk.auth.types.AuthContext)): contains info about the current `user`, the user's `permissions`, the `resource` ("threads", "crons", "assistants"), and the `action` being taken ("create", "read", "update", "delete", "search", "create\_run")
2. `value` (`dict`): data that is being created or accessed. The contents of this dict depend on the resource and action being accessed. See [adding scoped authorization handlers](#scoped-authorization) below for information on how to get more tightly scoped access control.

Notice that the simple handler does two things:

1. Adds the user's ID to the resource's metadata.
2. Returns a metadata filter so users only see resources they own.

## 2. Test private conversations

Test your authorization. If you have set things up correctly, you will see all ✅ messages. Be sure to have your development server running (run `langgraph dev`):
```

---

## Key-value stores

**URL:** llms-txt#key-value-stores

**Contents:**
- Overview
- Interface
- Built-in stores for local development
- Custom stores
- All integrations

Source: https://docs.langchain.com/oss/javascript/integrations/stores/index

LangChain provides a key-value store interface for storing and retrieving data by key. The key-value store interface in LangChain is primarily used for caching [embeddings](/oss/javascript/integrations/text_embedding).

All [`BaseStores`](https://api.js.langchain.com/classes/langchain_core.stores.BaseStore.html) are **generic** and support the following interface, where `K` represents the key type and `V` represents the value type:

* `mget(keys: K[]): Promise<(V | undefined)[]>`: get the values for multiple keys, returning `undefined` if a key does not exist
* `mset(keyValuePairs: [K, V][]): Promise<void>`: set the values for multiple keys
* `mdelete(keys: K[]): Promise<void>`: delete multiple keys
* `yieldKeys(prefix?: string): AsyncGenerator<K | string>`: asynchronously yield all keys in the store, optionally filtering by a prefix

The generic nature of the interface allows you to use different types for keys and values. For example, `BaseStore<string, BaseMessage>` would store messages with string keys, while `BaseStore<string, number[]>` would store arrays of numbers.

<Note>
  Base stores are designed to work with **multiple** key-value pairs at once for efficiency. This saves on network round-trips and may allow for more efficient batch operations in the underlying store.
</Note>

## Built-in stores for local development

<Columns cols={2}>
  <Card title="InMemoryStore" icon="link" href="/oss/javascript/integrations/stores/in_memory" arrow="true" cta="View guide" />

<Card title="LocalFileStore" icon="link" href="/oss/javascript/integrations/stores/file_system" arrow="true" cta="View guide" />
</Columns>

You can also implement your own custom store by extending the [`BaseStore`](https://langchain-ai.github.io/langgraphjs/reference/classes/checkpoint.BaseStore.html) class. See the [store interface documentation](https://api.js.langchain.com/classes/langchain_core.stores.BaseStore.html) for more details.

<Columns cols={3}>
  <Card title="Cassandra KV" icon="link" href="/oss/javascript/integrations/stores/cassandra_storage" arrow="true" cta="View guide" />

<Card title="IORedis" icon="link" href="/oss/javascript/integrations/stores/ioredis_storage" arrow="true" cta="View guide" />

<Card title="Upstash Redis" icon="link" href="/oss/javascript/integrations/stores/upstash_redis_storage" arrow="true" cta="View guide" />

<Card title="Vercel KV" icon="link" href="/oss/javascript/integrations/stores/vercel_kv_storage" arrow="true" cta="View guide" />
</Columns>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/javascript/integrations/stores/index.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## LangChain Academy

**URL:** llms-txt#langchain-academy

Source: https://docs.langchain.com/oss/javascript/langchain/academy

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/academy.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## LangChain overview

**URL:** llms-txt#langchain-overview

**Contents:**
- <Icon icon="download" size={20} /> Install
- <Icon icon="wand-magic-sparkles" /> Create an agent
- <Icon icon="star" size={20} /> Core benefits

Source: https://docs.langchain.com/oss/javascript/langchain/overview

<Callout icon="bullhorn" color="#DFC5FE" iconType="regular">
  **LangChain v1.0 is now available!**

For a complete list of changes and instructions on how to upgrade your code, see the [release notes](/oss/javascript/releases/langchain-v1) and [migration guide](/oss/javascript/migrate/langchain-v1).

If you encounter any issues or have feedback, please [open an issue](https://github.com/langchain-ai/docs/issues/new?template=01-langchain.yml) so we can improve. To view v0.x documentation, [go to the archived content](https://github.com/langchain-ai/langchainjs/tree/v0.3/docs/core_docs/docs).
</Callout>

LangChain is the easiest way to start building agents and applications powered by LLMs. With under 10 lines of code, you can connect to OpenAI, Anthropic, Google, and [more](/oss/javascript/integrations/providers/overview). LangChain provides a pre-built agent architecture and model integrations to help you get started quickly and seamlessly incorporate LLMs into your agents and applications.

We recommend you use LangChain if you want to quickly build agents and autonomous applications. Use [LangGraph](/oss/javascript/langgraph/overview), our low-level agent orchestration framework and runtime, when you have more advanced needs that require a combination of deterministic and agentic workflows, heavy customization, and carefully controlled latency.

LangChain [agents](/oss/javascript/langchain/agents) are built on top of LangGraph in order to provide durable execution, streaming, human-in-the-loop, persistence, and more. You do not need to know LangGraph for basic LangChain agent usage.

## <Icon icon="download" size={20} /> Install

## <Icon icon="wand-magic-sparkles" /> Create an agent

## <Icon icon="star" size={20} /> Core benefits

<Columns cols={2}>
  <Card title="Standard model interface" icon="arrows-rotate" href="/oss/javascript/langchain/models" arrow cta="Learn more">
    Different providers have unique APIs for interacting with models, including the format of responses. LangChain standardizes how you interact with models so that you can seamlessly swap providers and avoid lock-in.
  </Card>

<Card title="Easy to use, highly flexible agent" icon="wand-magic-sparkles" href="/oss/javascript/langchain/agents" arrow cta="Learn more">
    LangChain's agent abstraction is designed to be easy to get started with, letting you build a simple agent in under 10 lines of code. But it also provides enough flexibility to allow you to do all the context engineering your heart desires.
  </Card>

<Card title="Built on top of LangGraph" icon="circle-nodes" href="/oss/javascript/langgraph/overview" arrow cta="Learn more">
    LangChain's agents are built on top of LangGraph. This allows us to take advantage of LangGraph's durable execution, human-in-the-loop support, persistence, and more.
  </Card>

<Card title="Debug with LangSmith" icon="eye" href="/langsmith/home" arrow cta="Learn more">
    Gain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.
  </Card>
</Columns>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/overview.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>

## <Icon icon="wand-magic-sparkles" /> Create an agent
```

---

## LangChain SDK

**URL:** llms-txt#langchain-sdk

Source: https://docs.langchain.com/oss/javascript/reference/langchain-javascript

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/reference/langchain-javascript.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## LangChain v1 migration guide

**URL:** llms-txt#langchain-v1-migration-guide

**Contents:**
- `createAgent`
  - Import path
  - Prompts
  - Pre-model hook
  - Post-model hook
  - Custom state
  - Model
  - Tools
  - Structured output
  - Streaming node name rename

Source: https://docs.langchain.com/oss/javascript/migrate/langchain-v1

This migration guide outlines the major changes in LangChain v1. To learn more about the new features of v1, see the [introductory post](/oss/javascript/releases/langchain-v1).

In v1, the react agent prebuilt is now in the langchain package. The table below outlines what functionality has changed:

| Section                                            | What changed                                                                             |
| -------------------------------------------------- | ---------------------------------------------------------------------------------------- |
| [Import path](#import-path)                        | Package moved from `@langchain/langgraph/prebuilts` to `langchain`                       |
| [Prompts](#prompts)                                | Parameter renamed to `systemPrompt`, dynamic prompts use middleware                      |
| [Pre-model hook](#pre-model-hook)                  | Replaced by middleware with `beforeModel` method                                         |
| [Post-model hook](#post-model-hook)                | Replaced by middleware with `afterModel` method                                          |
| [Custom state](#custom-state)                      | Defined in middleware, zod objects only                                                  |
| [Model](#model)                                    | Dynamic selection via middleware, pre-bound models not supported                         |
| [Tools](#tools)                                    | Tool error handling moved to middleware with `wrapToolCall`                              |
| [Structured output](#structured-output)            | prompted output removed, use `toolStrategy`/`providerStrategy`                           |
| [Streaming node name](#streaming-node-name-rename) | Node name changed from `"agent"` to `"model"`                                            |
| [Runtime context](#runtime-context)                | `context` property instead of `config.configurable`                                      |
| [Namespace](#simplified-namespace)                 | Streamlined to focus on agent building blocks, legacy code moved to `@langchain/classic` |

The import path for the react agent prebuilt has changed from `@langchain/langgraph/prebuilts` to `langchain`. The name of the function has changed from `createReactAgent` to `createAgent`:

#### Static prompt rename

The `prompt` parameter has been renamed to `systemPrompt`:

If using `SystemMessage` objects in the system prompt, the string content is now used directly:

Dynamic prompts are a core context engineering pattern— they adapt what you tell the model based on the current conversation state. To do this, use `dynamicSystemPromptMiddleware`:

Pre-model hooks are now implemented as middleware with the `beforeModel` method. This pattern is more extensible--you can define multiple middlewares to run before the model is called and reuse them across agents.

Common use cases include:

* Summarizing conversation history
* Trimming messages
* Input guardrails, like PII redaction

v1 includes built-in summarization middleware:

Post-model hooks are now implemented as middleware with the `afterModel` method. This lets you compose multiple handlers after the model responds.

Common use cases include:

* Human-in-the-loop approval
* Output guardrails

v1 includes a built-in human-in-the-loop middleware:

Custom state is now defined in middleware using the `stateSchema` property. Use Zod to declare additional state fields that are carried through the agent run.

Dynamic model selection now happens via middleware. Use `wrapModelCall` to swap models (and tools) based on state or runtime context. In `createReactAgent`, this was done via a function passed to the `model` parameter.

This functionality has been ported to the middleware interface in v1.

#### Dynamic model selection

#### Pre-bound models

To better support structured output, `createAgent` should receive a plain model (string or instance) and a separate `tools` list. Avoid passing models pre-bound with tools when using structured output.

The `tools` argument to `createAgent` accepts:

* Functions created with `tool`
* LangChain tool instances
* Objects that represent built-in provider tools

Use middleware `wrapToolCall` to centralize error handling and logging for tools.

### Structured output

Structured output used to be generated in a separate node from the main agent. This is no longer the case. Structured output is generated in the main loop (no extra LLM call), reducing cost and latency.

#### Tool and provider strategies

In v1, there are two strategies:

* `toolStrategy` uses artificial tool calling to generate structured output
* `providerStrategy` uses provider-native structured output generation

#### Prompted output removed

Prompted output via custom instructions in `responseFormat` is removed in favor of the above strategies.

### Streaming node name rename

When streaming events from agents, the node name was changed from `"agent"` to `"model"` to better reflect the node's purpose.

When invoking an agent, pass static, read-only configuration via the `context` config argument. This replaces patterns that used `config.configurable`.

<Note>
  The old `config.configurable` pattern still works for backward compatibility, but using the new `context` parameter is recommended for new applications or applications migrating to v1.
</Note>

In v1, messages gain provider-agnostic standard content blocks. Access them via `message.contentBlocks` for a consistent, typed view across providers. The existing `message.content` field remains unchanged for strings or provider-native structures.

* New `contentBlocks` property on messages for normalized content.
* New TypeScript types under `ContentBlock` for strong typing.
* Optional serialization of standard blocks into `content` via `LC_OUTPUT_VERSION=v1` or `outputVersion: "v1"`.

### Read standardized content

### Create multimodal messages

### Example block types

See the content blocks [reference](/oss/javascript/langchain/messages#content-block-reference) for more details.

### Serialize standard content

Standard content blocks are **not serialized** into the `content` attribute by default. If you need to access standard content blocks in the `content` attribute (e.g., when sending messages to a client), you can opt-in to serializing them into `content`.

<Note>
  Learn more: [Messages](/oss/javascript/langchain/messages#message-content) and [Standard content blocks](/oss/javascript/langchain/messages#standard-content-blocks). See [Multimodal](/oss/javascript/langchain/messages#multimodal) for input examples.
</Note>

## Simplified package

The `langchain` package namespace is streamlined to focus on agent building blocks. Legacy functionality has moved to `@langchain/classic`. The new package exposes only the most useful and relevant functionality.

The v1 package includes:

| Module      | What's available                              | Notes                              |
| ----------- | --------------------------------------------- | ---------------------------------- |
| Agents      | `createAgent`, `AgentState`                   | Core agent creation functionality  |
| Messages    | Message types, content blocks, `trimMessages` | Re-exported from `@langchain/core` |
| Tools       | `tool`, tool classes                          | Re-exported from `@langchain/core` |
| Chat models | `initChatModel`, `BaseChatModel`              | Unified model initialization       |

### `@langchain/classic`

If you use legacy chains, the indexing API, or functionality previously re-exported from `@langchain/community`, install `@langchain/classic` and update imports:

### Dropped Node 18 support

All LangChain packages now require **Node.js 20 or higher**. Node.js 18 reached [end of life](https://nodejs.org/en/about/releases/) in March 2025.

### New build outputs

Builds for all langchain packages now use a bundler based approach instead of using raw typescript outputs. If you were importing files from the `dist/` directory (which is not recommended), you will need to update your imports to use the new module system.

### Legacy code moved to `@langchain/classic`

Legacy functionality outside the focus of standard interfaces and agents has been moved to the [`@langchain/classic`](https://www.npmjs.com/package/@langchain/classic) package. See the [Simplified package](#simplified-package) section for details on what's available in the core `langchain` package and what moved to `@langchain/classic`.

### Removal of deprecated APIs

Methods, functions, and other objects that were already deprecated and slated for removal in 1.0 have been deleted.

<Accordion title="View removed deprecated APIs">
  The following deprecated APIs have been removed in v1:

#### Core functionality

* `TraceGroup` - Use LangSmith tracing instead
  * `BaseDocumentLoader.loadAndSplit` - Use `.load()` followed by a text splitter
  * `RemoteRunnable` - No longer supported

* `BasePromptTemplate.serialize` and `.deserialize` - Use JSON serialization directly
  * `ChatPromptTemplate.fromPromptMessages` - Use `ChatPromptTemplate.fromMessages`

* `BaseRetrieverInterface.getRelevantDocuments` - Use `.invoke()` instead

* `Runnable.bind` - Use `.bindTools()` or other specific binding methods
  * `Runnable.map` - Use `.batch()` instead
  * `RunnableBatchOptions.maxConcurrency` - Use `maxConcurrency` in the config object

* `BaseChatModel.predictMessages` - Use `.invoke()` instead
  * `BaseChatModel.predict` - Use `.invoke()` instead
  * `BaseChatModel.serialize` - Use JSON serialization directly
  * `BaseChatModel.callPrompt` - Use `.invoke()` instead
  * `BaseChatModel.call` - Use `.invoke()` instead

* `BaseLLMParams.concurrency` - Use `maxConcurrency` in the config object
  * `BaseLLM.call` - Use `.invoke()` instead
  * `BaseLLM.predict` - Use `.invoke()` instead
  * `BaseLLM.predictMessages` - Use `.invoke()` instead
  * `BaseLLM.serialize` - Use JSON serialization directly

* `createChatMessageChunkEncoderStream` - Use `.stream()` method directly

* `BaseTracer.runMap` - Use LangSmith tracing APIs
  * `getTracingCallbackHandler` - Use LangSmith tracing
  * `getTracingV2CallbackHandler` - Use LangSmith tracing
  * `LangChainTracerV1` - Use LangSmith tracing

#### Memory and storage

* `BaseListChatMessageHistory.addAIChatMessage` - Use `.addMessage()` with `AIMessage`
  * `BaseStoreInterface` - Use specific store implementations

* `getRuntimeEnvironmentSync` - Use async `getRuntimeEnvironment()`
</Accordion>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/javascript/migrate/langchain-v1.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>

## `createAgent`

In v1, the react agent prebuilt is now in the langchain package. The table below outlines what functionality has changed:

| Section                                            | What changed                                                                             |
| -------------------------------------------------- | ---------------------------------------------------------------------------------------- |
| [Import path](#import-path)                        | Package moved from `@langchain/langgraph/prebuilts` to `langchain`                       |
| [Prompts](#prompts)                                | Parameter renamed to `systemPrompt`, dynamic prompts use middleware                      |
| [Pre-model hook](#pre-model-hook)                  | Replaced by middleware with `beforeModel` method                                         |
| [Post-model hook](#post-model-hook)                | Replaced by middleware with `afterModel` method                                          |
| [Custom state](#custom-state)                      | Defined in middleware, zod objects only                                                  |
| [Model](#model)                                    | Dynamic selection via middleware, pre-bound models not supported                         |
| [Tools](#tools)                                    | Tool error handling moved to middleware with `wrapToolCall`                              |
| [Structured output](#structured-output)            | prompted output removed, use `toolStrategy`/`providerStrategy`                           |
| [Streaming node name](#streaming-node-name-rename) | Node name changed from `"agent"` to `"model"`                                            |
| [Runtime context](#runtime-context)                | `context` property instead of `config.configurable`                                      |
| [Namespace](#simplified-namespace)                 | Streamlined to focus on agent building blocks, legacy code moved to `@langchain/classic` |

### Import path

The import path for the react agent prebuilt has changed from `@langchain/langgraph/prebuilts` to `langchain`. The name of the function has changed from `createReactAgent` to `createAgent`:
```

---

## LangGraph CLI

**URL:** llms-txt#langgraph-cli

**Contents:**
- Installation
  - Quick commands
- Configuration file
  - Examples
- Commands
  - `dev`
  - `build`
  - `up`
  - `dockerfile`

Source: https://docs.langchain.com/langsmith/cli

**LangGraph CLI** is a command-line tool for building and running the [Agent Server](/langsmith/agent-server) locally. The resulting server exposes all API endpoints for runs, threads, assistants, etc., and includes supporting services such as a managed database for checkpointing and storage.

1. Ensure Docker is installed (e.g., `docker --version`).

3. Verify the install

| Command                               | What it does                                                                                                                         |
| ------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------ |
| [`langgraph dev`](#dev)               | Starts a lightweight local dev server (no Docker required), ideal for rapid testing.                                                 |
| [`langgraph build`](#build)           | Builds a Docker image of your LangGraph API server for deployment.                                                                   |
| [`langgraph dockerfile`](#dockerfile) | Emits a Dockerfile derived from your config for custom builds.                                                                       |
| [`langgraph up`](#up)                 | Starts the LangGraph API server locally in Docker. Requires Docker running; LangSmith API key for local dev; license for production. |

For JS, use `npx @langchain/langgraph-cli <command>` (or `langgraphjs` if installed globally).

## Configuration file

To build and run a valid application, the LangGraph CLI requires a JSON configuration file that follows this [schema](https://raw.githubusercontent.com/langchain-ai/langgraph/refs/heads/main/libs/cli/schemas/schema.json). It contains the following properties:

<Note>The LangGraph CLI defaults to using the configuration file named <strong>langgraph.json</strong> in the current directory.</Note>

<Tabs>
  <Tab title="Python">
    | Key                                                              | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
    | ---------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
    | <span style={{ whiteSpace: "nowrap" }}>`dependencies`</span>     | **Required**. Array of dependencies for LangSmith API server. Dependencies can be one of the following: <ul><li>A single period (`"."`), which will look for local Python packages.</li><li>The directory path where `pyproject.toml`, `setup.py` or `requirements.txt` is located.<br />For example, if `requirements.txt` is located in the root of the project directory, specify `"./"`. If it's located in a subdirectory called `local_package`, specify `"./local_package"`. Do not specify the string `"requirements.txt"` itself.</li><li>A Python package name.</li></ul>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
    | <span style={{ whiteSpace: "nowrap" }}>`graphs`</span>           | **Required**. Mapping from graph ID to path where the compiled graph or a function that makes a graph is defined. Example: <ul><li>`./your_package/your_file.py:variable`, where `variable` is an instance of `langgraph.graph.state.CompiledStateGraph`</li><li>`./your_package/your_file.py:make_graph`, where `make_graph` is a function that takes a config dictionary (`langchain_core.runnables.RunnableConfig`) and returns an instance of `langgraph.graph.state.StateGraph` or `langgraph.graph.state.CompiledStateGraph`. See [how to rebuild a graph at runtime](/langsmith/graph-rebuild) for more details.</li></ul>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
    | <span style={{ whiteSpace: "nowrap" }}>`auth`</span>             | *(Added in v0.0.11)* Auth configuration containing the path to your authentication handler. Example: `./your_package/auth.py:auth`, where `auth` is an instance of `langgraph_sdk.Auth`. See [authentication guide](/langsmith/auth) for details.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
    | <span style={{ whiteSpace: "nowrap" }}>`base_image`</span>       | Optional. Base image to use for the LangGraph API server. Defaults to `langchain/langgraph-api` or `langchain/langgraphjs-api`. Use this to pin your builds to a particular version of the langgraph API, such as `"langchain/langgraph-server:0.2"`. See [https://hub.docker.com/r/langchain/langgraph-server/tags](https://hub.docker.com/r/langchain/langgraph-server/tags) for more details. (added in `langgraph-cli==0.2.8`)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
    | <span style={{ whiteSpace: "nowrap" }}>`image_distro`</span>     | Optional. Linux distribution for the base image. Must be one of `"debian"`, `"wolfi"`, `"bookworm"`, or `"bullseye"`. If omitted, defaults to `"debian"`. Available in `langgraph-cli>=0.2.11`.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
    | <span style={{ whiteSpace: "nowrap" }}>`env`</span>              | Path to `.env` file or a mapping from environment variable to its value.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
    | <span style={{ whiteSpace: "nowrap" }}>`store`</span>            | Configuration for adding semantic search and/or time-to-live (TTL) to the BaseStore. Contains the following fields: <ul><li>`index` (optional): Configuration for semantic search indexing with fields `embed`, `dims`, and optional `fields`.</li><li>`ttl` (optional): Configuration for item expiration. An object with optional fields: `refresh_on_read` (boolean, defaults to `true`), `default_ttl` (float, lifespan in **minutes**; applied to newly created items only; existing items are unchanged; defaults to no expiration), and `sweep_interval_minutes` (integer, how often to check for expired items, defaults to no sweeping).</li></ul>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
    | <span style={{ whiteSpace: "nowrap" }}>`ui`</span>               | Optional. Named definitions of UI components emitted by the agent, each pointing to a JS/TS file. (added in `langgraph-cli==0.1.84`)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
    | <span style={{ whiteSpace: "nowrap" }}>`python_version`</span>   | `3.11`, `3.12`, or `3.13`. Defaults to `3.11`.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
    | <span style={{ whiteSpace: "nowrap" }}>`node_version`</span>     | Specify `node_version: 20` to use LangGraph.js.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
    | <span style={{ whiteSpace: "nowrap" }}>`pip_config_file`</span>  | Path to `pip` config file.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
    | <span style={{ whiteSpace: "nowrap" }}>`pip_installer`</span>    | *(Added in v0.3)* Optional. Python package installer selector. It can be set to `"auto"`, `"pip"`, or `"uv"`. From version 0.3 onward the default strategy is to run `uv pip`, which typically delivers faster builds while remaining a drop-in replacement. In the uncommon situation where `uv` cannot handle your dependency graph or the structure of your `pyproject.toml`, specify `"pip"` here to revert to the earlier behaviour.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
    | <span style={{ whiteSpace: "nowrap" }}>`keep_pkg_tools`</span>   | *(Added in v0.3.4)* Optional. Control whether to retain Python packaging tools (`pip`, `setuptools`, `wheel`) in the final image. Accepted values: <ul><li><code>true</code> : Keep all three tools (skip uninstall).</li><li><code>false</code> / omitted : Uninstall all three tools (default behaviour).</li><li><code>list\[str]</code> : Names of tools <strong>to retain</strong>. Each value must be one of "pip", "setuptools", "wheel".</li></ul>. By default, all three tools are uninstalled.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
    | <span style={{ whiteSpace: "nowrap" }}>`dockerfile_lines`</span> | Array of additional lines to add to Dockerfile following the import from parent image.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |
    | <span style={{ whiteSpace: "nowrap" }}>`checkpointer`</span>     | Configuration for the checkpointer. Supports: <ul><li>`ttl` (optional): Object with `strategy`, `sweep_interval_minutes`, `default_ttl` controlling checkpoint expiry.</li><li>`serde` (optional, 0.5+): Object with `allowed_json_modules` and `pickle_fallback` to tune deserialization behavior.</li></ul>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
    | <span style={{ whiteSpace: "nowrap" }}>`http`</span>             | HTTP server configuration with the following fields: <ul><li>`app`: Path to custom Starlette/FastAPI app (e.g., `"./src/agent/webapp.py:app"`). See [custom routes guide](/langsmith/custom-routes).</li><li>`cors`: CORS configuration with fields such as `allow_origins`, `allow_methods`, `allow_headers`, `allow_credentials`, `allow_origin_regex`, `expose_headers`, and `max_age`.</li><li>`configurable_headers`: Define which request headers to expose as configurable values via `includes` / `excludes` patterns.</li><li>`logging_headers`: Mirror of `configurable_headers` for excluding sensitive headers from logs.</li><li>`middleware_order`: Choose how custom middleware and auth interact. `auth_first` runs authentication hooks before custom middleware, while `middleware_first` (default) runs your middleware first.</li><li>`enable_custom_route_auth`: Apply auth checks to routes added through `app`.</li><li>`disable_assistants`, `disable_mcp`, `disable_meta`, `disable_runs`, `disable_store`, `disable_threads`, `disable_ui`, `disable_webhooks`: Disable built-in routes or hooks.</li><li>`mount_prefix`: Prefix for mounted routes (e.g., "/my-deployment/api").</li></ul> |
    | <span style={{ whiteSpace: "nowrap" }}>`api_version`</span>      | *(Added in v0.3.7)* Which semantic version of the LangGraph API server to use (e.g., `"0.3"`). Defaults to latest. Check the server [changelog](/langsmith/agent-server-changelog) for details on each release.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
  </Tab>

<Tab title="JS">
    | Key                                                              | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |
    | ---------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
    | <span style={{ whiteSpace: "nowrap" }}>`graphs`</span>           | **Required**. Mapping from graph ID to path where the compiled graph or a function that makes a graph is defined. Example: <ul><li>`./src/graph.ts:variable`, where `variable` is an instance of [`CompiledStateGraph`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.CompiledStateGraph)</li><li>`./src/graph.ts:makeGraph`, where `makeGraph` is a function that takes a config dictionary (`LangGraphRunnableConfig`) and returns an instance of [`StateGraph`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.StateGraph) or [`CompiledStateGraph`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.CompiledStateGraph). See [how to rebuild a graph at runtime](/langsmith/graph-rebuild) for more details.</li></ul> |
    | <span style={{ whiteSpace: "nowrap" }}>`env`</span>              | Path to `.env` file or a mapping from environment variable to its value.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
    | <span style={{ whiteSpace: "nowrap" }}>`store`</span>            | Configuration for adding semantic search and/or time-to-live (TTL) to the BaseStore. Contains the following fields: <ul><li>`index` (optional): Configuration for semantic search indexing with fields `embed`, `dims`, and optional `fields`.</li><li>`ttl` (optional): Configuration for item expiration. An object with optional fields: `refresh_on_read` (boolean, defaults to `true`), `default_ttl` (float, lifespan in **minutes**; applied to newly created items only; existing items are unchanged; defaults to no expiration), and `sweep_interval_minutes` (integer, how often to check for expired items, defaults to no sweeping).</li></ul>                                                                                                                                                                |
    | <span style={{ whiteSpace: "nowrap" }}>`node_version`</span>     | Specify `node_version: 20` to use LangGraph.js.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
    | <span style={{ whiteSpace: "nowrap" }}>`dockerfile_lines`</span> | Array of additional lines to add to Dockerfile following the import from parent image.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
    | <span style={{ whiteSpace: "nowrap" }}>`checkpointer`</span>     | Configuration for the checkpointer. Supports: <ul><li>`ttl` (optional): Object with `strategy`, `sweep_interval_minutes`, `default_ttl` controlling checkpoint expiry.</li><li>`serde` (optional, 0.5+): Object with `allowed_json_modules` and `pickle_fallback` to tune deserialization behavior.</li></ul>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
    | <span style={{ whiteSpace: "nowrap" }}>`http`</span>             | HTTP server configuration mirroring the Python options: <ul><li>`cors` with `allow_origins`, `allow_methods`, `allow_headers`, `allow_credentials`, `allow_origin_regex`, `expose_headers`, `max_age`.</li><li>`configurable_headers` and `logging_headers` pattern lists.</li><li>`middleware_order` (`auth_first` or `middleware_first`).</li><li>`enable_custom_route_auth` plus the same boolean route toggles as above.</li></ul>                                                                                                                                                                                                                                                                                                                                                                                     |
    | <span style={{ whiteSpace: "nowrap" }}>`api_version`</span>      | *(Added in v0.3.7)* Which semantic version of the LangGraph API server to use (e.g., `"0.3"`). Defaults to latest. Check the server [changelog](/langsmith/agent-server-changelog) for details on each release.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
  </Tab>
</Tabs>

<Tabs>
  <Tab title="Python">
    #### Basic configuration

#### Using Wolfi base images

You can specify the Linux distribution for your base image using the `image_distro` field. Valid options are `debian`, `wolfi`, `bookworm`, or `bullseye`. Wolfi is the recommended option as it provides smaller and more secure images. This is available in `langgraph-cli>=0.2.11`.

#### Adding semantic search to the store

All deployments come with a DB-backed BaseStore. Adding an "index" configuration to your `langgraph.json` will enable [semantic search](/langsmith/semantic-search) within the BaseStore of your deployment.

The `index.fields` configuration determines which parts of your documents to embed:

* If omitted or set to `["$"]`, the entire document will be embedded
    * To embed specific fields, use JSON path notation: `["metadata.title", "content.text"]`
    * Documents missing specified fields will still be stored but won't have embeddings for those fields
    * You can still override which fields to embed on a specific item at `put` time using the `index` parameter

<Note>
      **Common model dimensions**

* `openai:text-embedding-3-large`: 3072
      * `openai:text-embedding-3-small`: 1536
      * `openai:text-embedding-ada-002`: 1536
      * `cohere:embed-english-v3.0`: 1024
      * `cohere:embed-english-light-v3.0`: 384
      * `cohere:embed-multilingual-v3.0`: 1024
      * `cohere:embed-multilingual-light-v3.0`: 384
    </Note>

#### Semantic search with a custom embedding function

If you want to use semantic search with a custom embedding function, you can pass a path to a custom embedding function:

The `embed` field in store configuration can reference a custom function that takes a list of strings and returns a list of embeddings. Example implementation:

#### Adding custom authentication

See the [authentication conceptual guide](/langsmith/auth) for details, and the [setting up custom authentication](/langsmith/set-up-custom-auth) guide for a practical walk through of the process.

#### Configuring store item Time-to-Live

You can configure default data expiration for items/memories in the BaseStore using the `store.ttl` key. This determines how long items are retained after they are last accessed (with reads potentially refreshing the timer based on `refresh_on_read`). Note that these defaults can be overwritten on a per-call basis by modifying the corresponding arguments in `get`, `search`, etc.

The `ttl` configuration is an object containing optional fields:

* `refresh_on_read`: If `true` (the default), accessing an item via `get` or `search` resets its expiration timer. Set to `false` to only refresh TTL on writes (`put`).
    * `default_ttl`: The default lifespan of an item in **minutes**. Applies only to newly created items; existing items are not modified. If not set, items do not expire by default.
    * `sweep_interval_minutes`: How frequently (in minutes) the system should run a background process to delete expired items. If not set, sweeping does not occur automatically.

Here is an example enabling a 7-day TTL (10080 minutes), refreshing on reads, and sweeping every hour:

#### Configuring checkpoint Time-to-Live

You can configure the time-to-live (TTL) for checkpoints using the `checkpointer` key. This determines how long checkpoint data is retained before being automatically handled according to the specified strategy (e.g., deletion). Two optional sub-objects are supported:

* `ttl`: Includes `strategy`, `sweep_interval_minutes`, and `default_ttl`, which collectively set how checkpoints expire.
    * `serde` *(Agent server 0.5+)* : Lets you control deserialization behavior for checkpoint payloads.

Here's an example setting a default TTL of 30 days (43200 minutes):

In this example, checkpoints older than 30 days will be deleted, and the check runs every 10 minutes.

#### Configuring checkpointer serde

The `checkpointer.serde` object shapes deserialization:

* `allowed_json_modules` defines an allow list for custom Python objects you want the server to be able to deserialize from payloads saved in "json" mode. This is a list of `[path, to, module, file, symbol]` sequences. If omitted, only LangChain-safe defaults are allowed. You can unsafely set to `true` to allow any module to be deserialized.
    * `pickle_fallback`: Whether to fall back to pickle deserialization when JSON decoding fails.

#### Customizing HTTP middleware and headers

The `http` block lets you fine-tune request handling:

* `middleware_order`: Choose `"auth_first"` to run authentication before your middleware, or `"middleware_first"` (default) to invert that order.
    * `enable_custom_route_auth`: Extend authentication to routes you mount through `http.app`.
    * `configurable_headers` / `logging_headers`: Each accepts an object with optional `includes` and `excludes` arrays; wildcards are supported and exclusions run before inclusions.
    * `cors`: In addition to `allow_origins`, `allow_methods`, and `allow_headers`, you can set `allow_credentials`, `allow_origin_regex`, `expose_headers`, and `max_age` for detailed browser control.

<a id="api-version" />

#### Pinning API version

You can pin the API version of the Agent Server by using the `api_version` key. This is useful if you want to ensure that your server uses a specific version of the API.
    By default, builds in Cloud deployments use the latest stable version of the server. This can be pinned by setting the `api_version` key to a specific version.

<Tab title="JS">
    #### Basic configuration

<a id="api-version" />

#### Pinning API version

You can pin the API version of the Agent Server by using the `api_version` key. This is useful if you want to ensure that your server uses a specific version of the API.
    By default, builds in Cloud deployments use the latest stable version of the server. This can be pinned by setting the `api_version` key to a specific version.

<Tabs>
  <Tab title="Python">
    The base command for the LangGraph CLI is `langgraph`.

<Tab title="JS">
    The base command for the LangGraph.js CLI is `langgraphjs`.

We recommend using `npx` to always use the latest version of the CLI.
  </Tab>
</Tabs>

<Tabs>
  <Tab title="Python">
    Run LangGraph API server in development mode with hot reloading and debugging capabilities. This lightweight server requires no Docker installation and is suitable for development and testing. State is persisted to a local directory.

<Note>Currently, the CLI only supports Python >= 3.11.</Note>

This command requires the "inmem" extra to be installed:

| Option                        | Default          | Description                                                                                                                                                                  |
    | ----------------------------- | ---------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
    | `-c, --config FILE`           | `langgraph.json` | Path to configuration file declaring dependencies, graphs and environment variables                                                                                          |
    | `--host TEXT`                 | `127.0.0.1`      | Host to bind the server to                                                                                                                                                   |
    | `--port INTEGER`              | `2024`           | Port to bind the server to                                                                                                                                                   |
    | `--no-reload`                 |                  | Disable auto-reload                                                                                                                                                          |
    | `--n-jobs-per-worker INTEGER` |                  | Number of jobs per worker. Default is 10                                                                                                                                     |
    | `--debug-port INTEGER`        |                  | Port for debugger to listen on                                                                                                                                               |
    | `--wait-for-client`           | `False`          | Wait for a debugger client to connect to the debug port before starting the server                                                                                           |
    | `--no-browser`                |                  | Skip automatically opening the browser when the server starts                                                                                                                |
    | `--studio-url TEXT`           |                  | URL of the Studio instance to connect to. Defaults to [https://smith.langchain.com](https://smith.langchain.com)                                                             |
    | `--allow-blocking`            | `False`          | Do not raise errors for synchronous I/O blocking operations in your code (added in `0.2.6`)                                                                                  |
    | `--tunnel`                    | `False`          | Expose the local server via a public tunnel (Cloudflare) for remote frontend access. This avoids issues with browsers like Safari or networks blocking localhost connections |
    | `--help`                      |                  | Display command documentation                                                                                                                                                |
  </Tab>

<Tab title="JS">
    Run LangGraph API server in development mode with hot reloading capabilities. This lightweight server requires no Docker installation and is suitable for development and testing. State is persisted to a local directory.

| Option                        | Default          | Description                                                                                                                                                      |
    | ----------------------------- | ---------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------- |
    | `-c, --config FILE`           | `langgraph.json` | Path to configuration file declaring dependencies, graphs and environment variables                                                                              |
    | `--host TEXT`                 | `127.0.0.1`      | Host to bind the server to                                                                                                                                       |
    | `--port INTEGER`              | `2024`           | Port to bind the server to                                                                                                                                       |
    | `--no-reload`                 |                  | Disable auto-reload                                                                                                                                              |
    | `--n-jobs-per-worker INTEGER` |                  | Number of jobs per worker. Default is 10                                                                                                                         |
    | `--debug-port INTEGER`        |                  | Port for debugger to listen on                                                                                                                                   |
    | `--wait-for-client`           | `False`          | Wait for a debugger client to connect to the debug port before starting the server                                                                               |
    | `--no-browser`                |                  | Skip automatically opening the browser when the server starts                                                                                                    |
    | `--studio-url TEXT`           |                  | URL of the Studio instance to connect to. Defaults to [https://smith.langchain.com](https://smith.langchain.com)                                                 |
    | `--allow-blocking`            | `False`          | Do not raise errors for synchronous I/O blocking operations in your code                                                                                         |
    | `--tunnel`                    | `False`          | Expose the local server via a public tunnel (Cloudflare) for remote frontend access. This avoids issues with browsers or networks blocking localhost connections |
    | `--help`                      |                  | Display command documentation                                                                                                                                    |
  </Tab>
</Tabs>

<Tabs>
  <Tab title="Python">
    Build LangSmith API server Docker image.

| Option                                | Default          | Description                                                                                                                                             |
    | ------------------------------------- | ---------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------- |
    | `--platform TEXT`                     |                  | Target platform(s) to build the Docker image for. Example: `langgraph build --platform linux/amd64,linux/arm64`                                         |
    | `-t, --tag TEXT`                      |                  | **Required**. Tag for the Docker image. Example: `langgraph build -t my-image`                                                                          |
    | `--pull / --no-pull`                  | `--pull`         | Build with latest remote Docker image. Use `--no-pull` for running the LangSmith API server with locally built images.                                  |
    | `-c, --config FILE`                   | `langgraph.json` | Path to configuration file declaring dependencies, graphs and environment variables.                                                                    |
    | `--build-command TEXT`<sup>\*</sup>   |                  | Build command to run. Runs from the directory where your `langgraph.json` file lives. Example: `langgraph build --build-command "yarn run turbo build"` |
    | `--install-command TEXT`<sup>\*</sup> |                  | Install command to run. Runs from the directory where you call `langgraph build` from. Example: `langgraph build --install-command "yarn install"`      |
    | `--help`                              |                  | Display command documentation.                                                                                                                          |

<sup>\*</sup>Only supported for JS deployments, will have no impact on Python deployments.
  </Tab>

<Tab title="JS">
    Build LangSmith API server Docker image.

| Option              | Default          | Description                                                                                                     |
    | ------------------- | ---------------- | --------------------------------------------------------------------------------------------------------------- |
    | `--platform TEXT`   |                  | Target platform(s) to build the Docker image for. Example: `langgraph build --platform linux/amd64,linux/arm64` |
    | `-t, --tag TEXT`    |                  | **Required**. Tag for the Docker image. Example: `langgraph build -t my-image`                                  |
    | `--no-pull`         |                  | Use locally built images. Defaults to `false` to build with latest remote Docker image.                         |
    | `-c, --config FILE` | `langgraph.json` | Path to configuration file declaring dependencies, graphs and environment variables.                            |
    | `--help`            |                  | Display command documentation.                                                                                  |
  </Tab>
</Tabs>

<Tabs>
  <Tab title="Python">
    Start LangGraph API server. For local testing, requires a LangSmith API key with access to LangSmith. Requires a license key for production use.

| Option                       | Default                   | Description                                                                                                             |
    | ---------------------------- | ------------------------- | ----------------------------------------------------------------------------------------------------------------------- |
    | `--wait`                     |                           | Wait for services to start before returning. Implies --detach                                                           |
    | `--base-image TEXT`          | `langchain/langgraph-api` | Base image to use for the LangGraph API server. Pin to specific versions using version tags.                            |
    | `--image TEXT`               |                           | Docker image to use for the langgraph-api service. If specified, skips building and uses this image directly.           |
    | `--postgres-uri TEXT`        | Local database            | Postgres URI to use for the database.                                                                                   |
    | `--watch`                    |                           | Restart on file changes                                                                                                 |
    | `--debugger-base-url TEXT`   | `http://127.0.0.1:[PORT]` | URL used by the debugger to access LangGraph API.                                                                       |
    | `--debugger-port INTEGER`    |                           | Pull the debugger image locally and serve the UI on specified port                                                      |
    | `--verbose`                  |                           | Show more output from the server logs.                                                                                  |
    | `-c, --config FILE`          | `langgraph.json`          | Path to configuration file declaring dependencies, graphs and environment variables.                                    |
    | `-d, --docker-compose FILE`  |                           | Path to docker-compose.yml file with additional services to launch.                                                     |
    | `-p, --port INTEGER`         | `8123`                    | Port to expose. Example: `langgraph up --port 8000`                                                                     |
    | `--pull / --no-pull`         | `pull`                    | Pull latest images. Use `--no-pull` for running the server with locally-built images. Example: `langgraph up --no-pull` |
    | `--recreate / --no-recreate` | `no-recreate`             | Recreate containers even if their configuration and image haven't changed                                               |
    | `--help`                     |                           | Display command documentation.                                                                                          |
  </Tab>

<Tab title="JS">
    Start LangGraph API server. For local testing, requires a LangSmith API key with access to LangSmith. Requires a license key for production use.

| Option                                                                    | Default                                                                 | Description                                                                                                   |
    | ------------------------------------------------------------------------- | ----------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------- |
    | <span style={{ whiteSpace: "nowrap" }}>`--wait`</span>                    |                                                                         | Wait for services to start before returning. Implies --detach                                                 |
    | <span style={{ whiteSpace: "nowrap" }}>`--base-image TEXT`</span>         | <span style={{ whiteSpace: "nowrap" }}>`langchain/langgraph-api`</span> | Base image to use for the LangGraph API server. Pin to specific versions using version tags.                  |
    | <span style={{ whiteSpace: "nowrap" }}>`--image TEXT`</span>              |                                                                         | Docker image to use for the langgraph-api service. If specified, skips building and uses this image directly. |
    | <span style={{ whiteSpace: "nowrap" }}>`--postgres-uri TEXT`</span>       | Local database                                                          | Postgres URI to use for the database.                                                                         |
    | <span style={{ whiteSpace: "nowrap" }}>`--watch`</span>                   |                                                                         | Restart on file changes                                                                                       |
    | <span style={{ whiteSpace: "nowrap" }}>`-c, --config FILE`</span>         | `langgraph.json`                                                        | Path to configuration file declaring dependencies, graphs and environment variables.                          |
    | <span style={{ whiteSpace: "nowrap" }}>`-d, --docker-compose FILE`</span> |                                                                         | Path to docker-compose.yml file with additional services to launch.                                           |
    | <span style={{ whiteSpace: "nowrap" }}>`-p, --port INTEGER`</span>        | `8123`                                                                  | Port to expose. Example: `langgraph up --port 8000`                                                           |
    | <span style={{ whiteSpace: "nowrap" }}>`--no-pull`</span>                 |                                                                         | Use locally built images. Defaults to `false` to build with latest remote Docker image.                       |
    | <span style={{ whiteSpace: "nowrap" }}>`--recreate`</span>                |                                                                         | Recreate containers even if their configuration and image haven't changed                                     |
    | <span style={{ whiteSpace: "nowrap" }}>`--help`</span>                    |                                                                         | Display command documentation.                                                                                |
  </Tab>
</Tabs>

<Tabs>
  <Tab title="Python">
    Generate a Dockerfile for building a LangSmith API server Docker image.

| Option              | Default          | Description                                                                                                     |
    | ------------------- | ---------------- | --------------------------------------------------------------------------------------------------------------- |
    | `-c, --config FILE` | `langgraph.json` | Path to the [configuration file](#configuration-file) declaring dependencies, graphs and environment variables. |
    | `--help`            |                  | Show this message and exit.                                                                                     |

This generates a Dockerfile that looks similar to:

<Note>The `langgraph dockerfile` command translates all the configuration in your `langgraph.json` file into Dockerfile commands. When using this command, you will have to re-run it whenever you update your `langgraph.json` file. Otherwise, your changes will not be reflected when you build or run the dockerfile.</Note>
  </Tab>

<Tab title="JS">
    Generate a Dockerfile for building a LangSmith API server Docker image.

| Option              | Default          | Description                                                                                                     |
    | ------------------- | ---------------- | --------------------------------------------------------------------------------------------------------------- |
    | `-c, --config FILE` | `langgraph.json` | Path to the [configuration file](#configuration-file) declaring dependencies, graphs and environment variables. |
    | `--help`            |                  | Show this message and exit.                                                                                     |

This generates a Dockerfile that looks similar to:

<Note>The `npx @langchain/langgraph-cli dockerfile` command translates all the configuration in your `langgraph.json` file into Dockerfile commands. When using this command, you will have to re-run it whenever you update your `langgraph.json` file. Otherwise, your changes will not be reflected when you build or run the dockerfile.</Note>
  </Tab>
</Tabs>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/cli.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

3. Verify the install

   <CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>

### Quick commands

| Command                               | What it does                                                                                                                         |
| ------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------ |
| [`langgraph dev`](#dev)               | Starts a lightweight local dev server (no Docker required), ideal for rapid testing.                                                 |
| [`langgraph build`](#build)           | Builds a Docker image of your LangGraph API server for deployment.                                                                   |
| [`langgraph dockerfile`](#dockerfile) | Emits a Dockerfile derived from your config for custom builds.                                                                       |
| [`langgraph up`](#up)                 | Starts the LangGraph API server locally in Docker. Requires Docker running; LangSmith API key for local dev; license for production. |

For JS, use `npx @langchain/langgraph-cli <command>` (or `langgraphjs` if installed globally).

## Configuration file

To build and run a valid application, the LangGraph CLI requires a JSON configuration file that follows this [schema](https://raw.githubusercontent.com/langchain-ai/langgraph/refs/heads/main/libs/cli/schemas/schema.json). It contains the following properties:

<Note>The LangGraph CLI defaults to using the configuration file named <strong>langgraph.json</strong> in the current directory.</Note>

<Tabs>
  <Tab title="Python">
    | Key                                                              | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
    | ---------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
    | <span style={{ whiteSpace: "nowrap" }}>`dependencies`</span>     | **Required**. Array of dependencies for LangSmith API server. Dependencies can be one of the following: <ul><li>A single period (`"."`), which will look for local Python packages.</li><li>The directory path where `pyproject.toml`, `setup.py` or `requirements.txt` is located.<br />For example, if `requirements.txt` is located in the root of the project directory, specify `"./"`. If it's located in a subdirectory called `local_package`, specify `"./local_package"`. Do not specify the string `"requirements.txt"` itself.</li><li>A Python package name.</li></ul>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
    | <span style={{ whiteSpace: "nowrap" }}>`graphs`</span>           | **Required**. Mapping from graph ID to path where the compiled graph or a function that makes a graph is defined. Example: <ul><li>`./your_package/your_file.py:variable`, where `variable` is an instance of `langgraph.graph.state.CompiledStateGraph`</li><li>`./your_package/your_file.py:make_graph`, where `make_graph` is a function that takes a config dictionary (`langchain_core.runnables.RunnableConfig`) and returns an instance of `langgraph.graph.state.StateGraph` or `langgraph.graph.state.CompiledStateGraph`. See [how to rebuild a graph at runtime](/langsmith/graph-rebuild) for more details.</li></ul>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
    | <span style={{ whiteSpace: "nowrap" }}>`auth`</span>             | *(Added in v0.0.11)* Auth configuration containing the path to your authentication handler. Example: `./your_package/auth.py:auth`, where `auth` is an instance of `langgraph_sdk.Auth`. See [authentication guide](/langsmith/auth) for details.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
    | <span style={{ whiteSpace: "nowrap" }}>`base_image`</span>       | Optional. Base image to use for the LangGraph API server. Defaults to `langchain/langgraph-api` or `langchain/langgraphjs-api`. Use this to pin your builds to a particular version of the langgraph API, such as `"langchain/langgraph-server:0.2"`. See [https://hub.docker.com/r/langchain/langgraph-server/tags](https://hub.docker.com/r/langchain/langgraph-server/tags) for more details. (added in `langgraph-cli==0.2.8`)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
    | <span style={{ whiteSpace: "nowrap" }}>`image_distro`</span>     | Optional. Linux distribution for the base image. Must be one of `"debian"`, `"wolfi"`, `"bookworm"`, or `"bullseye"`. If omitted, defaults to `"debian"`. Available in `langgraph-cli>=0.2.11`.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
    | <span style={{ whiteSpace: "nowrap" }}>`env`</span>              | Path to `.env` file or a mapping from environment variable to its value.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
    | <span style={{ whiteSpace: "nowrap" }}>`store`</span>            | Configuration for adding semantic search and/or time-to-live (TTL) to the BaseStore. Contains the following fields: <ul><li>`index` (optional): Configuration for semantic search indexing with fields `embed`, `dims`, and optional `fields`.</li><li>`ttl` (optional): Configuration for item expiration. An object with optional fields: `refresh_on_read` (boolean, defaults to `true`), `default_ttl` (float, lifespan in **minutes**; applied to newly created items only; existing items are unchanged; defaults to no expiration), and `sweep_interval_minutes` (integer, how often to check for expired items, defaults to no sweeping).</li></ul>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
    | <span style={{ whiteSpace: "nowrap" }}>`ui`</span>               | Optional. Named definitions of UI components emitted by the agent, each pointing to a JS/TS file. (added in `langgraph-cli==0.1.84`)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
    | <span style={{ whiteSpace: "nowrap" }}>`python_version`</span>   | `3.11`, `3.12`, or `3.13`. Defaults to `3.11`.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
    | <span style={{ whiteSpace: "nowrap" }}>`node_version`</span>     | Specify `node_version: 20` to use LangGraph.js.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
    | <span style={{ whiteSpace: "nowrap" }}>`pip_config_file`</span>  | Path to `pip` config file.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
    | <span style={{ whiteSpace: "nowrap" }}>`pip_installer`</span>    | *(Added in v0.3)* Optional. Python package installer selector. It can be set to `"auto"`, `"pip"`, or `"uv"`. From version 0.3 onward the default strategy is to run `uv pip`, which typically delivers faster builds while remaining a drop-in replacement. In the uncommon situation where `uv` cannot handle your dependency graph or the structure of your `pyproject.toml`, specify `"pip"` here to revert to the earlier behaviour.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
    | <span style={{ whiteSpace: "nowrap" }}>`keep_pkg_tools`</span>   | *(Added in v0.3.4)* Optional. Control whether to retain Python packaging tools (`pip`, `setuptools`, `wheel`) in the final image. Accepted values: <ul><li><code>true</code> : Keep all three tools (skip uninstall).</li><li><code>false</code> / omitted : Uninstall all three tools (default behaviour).</li><li><code>list\[str]</code> : Names of tools <strong>to retain</strong>. Each value must be one of "pip", "setuptools", "wheel".</li></ul>. By default, all three tools are uninstalled.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
    | <span style={{ whiteSpace: "nowrap" }}>`dockerfile_lines`</span> | Array of additional lines to add to Dockerfile following the import from parent image.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |
    | <span style={{ whiteSpace: "nowrap" }}>`checkpointer`</span>     | Configuration for the checkpointer. Supports: <ul><li>`ttl` (optional): Object with `strategy`, `sweep_interval_minutes`, `default_ttl` controlling checkpoint expiry.</li><li>`serde` (optional, 0.5+): Object with `allowed_json_modules` and `pickle_fallback` to tune deserialization behavior.</li></ul>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
    | <span style={{ whiteSpace: "nowrap" }}>`http`</span>             | HTTP server configuration with the following fields: <ul><li>`app`: Path to custom Starlette/FastAPI app (e.g., `"./src/agent/webapp.py:app"`). See [custom routes guide](/langsmith/custom-routes).</li><li>`cors`: CORS configuration with fields such as `allow_origins`, `allow_methods`, `allow_headers`, `allow_credentials`, `allow_origin_regex`, `expose_headers`, and `max_age`.</li><li>`configurable_headers`: Define which request headers to expose as configurable values via `includes` / `excludes` patterns.</li><li>`logging_headers`: Mirror of `configurable_headers` for excluding sensitive headers from logs.</li><li>`middleware_order`: Choose how custom middleware and auth interact. `auth_first` runs authentication hooks before custom middleware, while `middleware_first` (default) runs your middleware first.</li><li>`enable_custom_route_auth`: Apply auth checks to routes added through `app`.</li><li>`disable_assistants`, `disable_mcp`, `disable_meta`, `disable_runs`, `disable_store`, `disable_threads`, `disable_ui`, `disable_webhooks`: Disable built-in routes or hooks.</li><li>`mount_prefix`: Prefix for mounted routes (e.g., "/my-deployment/api").</li></ul> |
    | <span style={{ whiteSpace: "nowrap" }}>`api_version`</span>      | *(Added in v0.3.7)* Which semantic version of the LangGraph API server to use (e.g., `"0.3"`). Defaults to latest. Check the server [changelog](/langsmith/agent-server-changelog) for details on each release.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
  </Tab>

  <Tab title="JS">
    | Key                                                              | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |
    | ---------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
    | <span style={{ whiteSpace: "nowrap" }}>`graphs`</span>           | **Required**. Mapping from graph ID to path where the compiled graph or a function that makes a graph is defined. Example: <ul><li>`./src/graph.ts:variable`, where `variable` is an instance of [`CompiledStateGraph`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.CompiledStateGraph)</li><li>`./src/graph.ts:makeGraph`, where `makeGraph` is a function that takes a config dictionary (`LangGraphRunnableConfig`) and returns an instance of [`StateGraph`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.StateGraph) or [`CompiledStateGraph`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.CompiledStateGraph). See [how to rebuild a graph at runtime](/langsmith/graph-rebuild) for more details.</li></ul> |
    | <span style={{ whiteSpace: "nowrap" }}>`env`</span>              | Path to `.env` file or a mapping from environment variable to its value.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
    | <span style={{ whiteSpace: "nowrap" }}>`store`</span>            | Configuration for adding semantic search and/or time-to-live (TTL) to the BaseStore. Contains the following fields: <ul><li>`index` (optional): Configuration for semantic search indexing with fields `embed`, `dims`, and optional `fields`.</li><li>`ttl` (optional): Configuration for item expiration. An object with optional fields: `refresh_on_read` (boolean, defaults to `true`), `default_ttl` (float, lifespan in **minutes**; applied to newly created items only; existing items are unchanged; defaults to no expiration), and `sweep_interval_minutes` (integer, how often to check for expired items, defaults to no sweeping).</li></ul>                                                                                                                                                                |
    | <span style={{ whiteSpace: "nowrap" }}>`node_version`</span>     | Specify `node_version: 20` to use LangGraph.js.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
    | <span style={{ whiteSpace: "nowrap" }}>`dockerfile_lines`</span> | Array of additional lines to add to Dockerfile following the import from parent image.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
    | <span style={{ whiteSpace: "nowrap" }}>`checkpointer`</span>     | Configuration for the checkpointer. Supports: <ul><li>`ttl` (optional): Object with `strategy`, `sweep_interval_minutes`, `default_ttl` controlling checkpoint expiry.</li><li>`serde` (optional, 0.5+): Object with `allowed_json_modules` and `pickle_fallback` to tune deserialization behavior.</li></ul>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
    | <span style={{ whiteSpace: "nowrap" }}>`http`</span>             | HTTP server configuration mirroring the Python options: <ul><li>`cors` with `allow_origins`, `allow_methods`, `allow_headers`, `allow_credentials`, `allow_origin_regex`, `expose_headers`, `max_age`.</li><li>`configurable_headers` and `logging_headers` pattern lists.</li><li>`middleware_order` (`auth_first` or `middleware_first`).</li><li>`enable_custom_route_auth` plus the same boolean route toggles as above.</li></ul>                                                                                                                                                                                                                                                                                                                                                                                     |
    | <span style={{ whiteSpace: "nowrap" }}>`api_version`</span>      | *(Added in v0.3.7)* Which semantic version of the LangGraph API server to use (e.g., `"0.3"`). Defaults to latest. Check the server [changelog](/langsmith/agent-server-changelog) for details on each release.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
  </Tab>
</Tabs>

### Examples

<Tabs>
  <Tab title="Python">
    #### Basic configuration
```

---

## LangGraph JS/TS SDK

**URL:** llms-txt#langgraph-js/ts-sdk

Source: https://docs.langchain.com/langsmith/langgraph-js-ts-sdk

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/langgraph-js-ts-sdk.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## LangGraph overview

**URL:** llms-txt#langgraph-overview

**Contents:**
- <Icon icon="download" size={20} /> Install
- Core benefits
- LangGraph ecosystem
- Acknowledgements

Source: https://docs.langchain.com/oss/javascript/langgraph/overview

<Callout icon="bullhorn" color="#DFC5FE" iconType="regular">
  **LangGraph v1.0 is now available!**

For a complete list of changes and instructions on how to upgrade your code, see the [release notes](/oss/javascript/releases/langgraph-v1) and [migration guide](/oss/javascript/migrate/langgraph-v1).

If you encounter any issues or have feedback, please [open an issue](https://github.com/langchain-ai/docs/issues/new?template=02-langgraph.yml\&labels=langgraph,js/ts) so we can improve. To view v0.x documentation, [go to the archived content](https://github.com/langchain-ai/langgraphjs/tree/main/docs/docs).
</Callout>

Trusted by companies shaping the future of agents-- including Klarna, Replit, Elastic, and more-- LangGraph is a low-level orchestration framework and runtime for building, managing, and deploying long-running, stateful agents.

LangGraph is very low-level, and focused entirely on agent **orchestration**. Before using LangGraph, we recommend you familiarize yourself with some of the components used to build agents, starting with [models](/oss/javascript/langchain/models) and [tools](/oss/javascript/langchain/tools).

We will commonly use [LangChain](/oss/javascript/langchain/overview) components throughout the documentation to integrate models and tools, but you don't need to use LangChain to use LangGraph. If you are just getting started with agents or want a higher-level abstraction, we recommend you use LangChain's [agents](/oss/javascript/langchain/agents) that provide pre-built architectures for common LLM and tool-calling loops.

LangGraph is focused on the underlying capabilities important for agent orchestration: durable execution, streaming, human-in-the-loop, and more.

## <Icon icon="download" size={20} /> Install

Then, create a simple hello world example:

LangGraph provides low-level supporting infrastructure for *any* long-running, stateful workflow or agent. LangGraph does not abstract prompts or architecture, and provides the following central benefits:

* [Durable execution](/oss/javascript/langgraph/durable-execution): Build agents that persist through failures and can run for extended periods, resuming from where they left off.
* [Human-in-the-loop](/oss/javascript/langgraph/interrupts): Incorporate human oversight by inspecting and modifying agent state at any point.
* [Comprehensive memory](/oss/javascript/concepts/memory): Create stateful agents with both short-term working memory for ongoing reasoning and long-term memory across sessions.
* [Debugging with LangSmith](/langsmith/home): Gain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.
* [Production-ready deployment](/langsmith/deployments): Deploy sophisticated agent systems confidently with scalable infrastructure designed to handle the unique challenges of stateful, long-running workflows.

## LangGraph ecosystem

While LangGraph can be used standalone, it also integrates seamlessly with any LangChain product, giving developers a full suite of tools for building agents. To improve your LLM application development, pair LangGraph with:

* [LangSmith](http://www.langchain.com/langsmith) — Helpful for agent evals and observability. Debug poor-performing LLM app runs, evaluate agent trajectories, gain visibility in production, and improve performance over time.
* [LangSmith](/langsmith/home) — Deploy and scale agents effortlessly with a purpose-built deployment platform for long running, stateful workflows. Discover, reuse, configure, and share agents across teams — and iterate quickly with visual prototyping in [Studio](/langsmith/studio).
* [LangChain](/oss/javascript/langchain/overview) - Provides integrations and composable components to streamline LLM application development. Contains agent abstractions built on top of LangGraph.

LangGraph is inspired by [Pregel](https://research.google/pubs/pub37252/) and [Apache Beam](https://beam.apache.org/). The public interface draws inspiration from [NetworkX](https://networkx.org/documentation/latest/). LangGraph is built by LangChain Inc, the creators of LangChain, but can be used without LangChain.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/overview.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>

Then, create a simple hello world example:
```

---

## LangGraph Python SDK

**URL:** llms-txt#langgraph-python-sdk

Source: https://docs.langchain.com/langsmith/langgraph-python-sdk

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/langgraph-python-sdk.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## LangGraph runtime

**URL:** llms-txt#langgraph-runtime

**Contents:**
- Overview
- Actors
- Channels
- Examples
- High-level API

Source: https://docs.langchain.com/oss/javascript/langgraph/pregel

[`Pregel`](https://langchain-ai.github.io/langgraphjs/reference/classes/langgraph.Pregel.html) implements LangGraph's runtime, managing the execution of LangGraph applications.

Compiling a [StateGraph](https://langchain-ai.github.io/langgraphjs/reference/classes/langgraph.StateGraph.html) or creating an [entrypoint](https://langchain-ai.github.io/langgraphjs/reference/functions/langgraph.entrypoint.html) produces a [`Pregel`](https://langchain-ai.github.io/langgraphjs/reference/classes/langgraph.Pregel.html) instance that can be invoked with input.

This guide explains the runtime at a high level and provides instructions for directly implementing applications with Pregel.

> **Note:** The [`Pregel`](https://langchain-ai.github.io/langgraphjs/reference/classes/langgraph.Pregel.html) runtime is named after [Google's Pregel algorithm](https://research.google/pubs/pub37252/), which describes an efficient method for large-scale parallel computation using graphs.

In LangGraph, Pregel combines [**actors**](https://en.wikipedia.org/wiki/Actor_model) and **channels** into a single application. **Actors** read data from channels and write data to channels. Pregel organizes the execution of the application into multiple steps, following the **Pregel Algorithm**/**Bulk Synchronous Parallel** model.

Each step consists of three phases:

* **Plan**: Determine which **actors** to execute in this step. For example, in the first step, select the **actors** that subscribe to the special **input** channels; in subsequent steps, select the **actors** that subscribe to channels updated in the previous step.
* **Execution**: Execute all selected **actors** in parallel, until all complete, or one fails, or a timeout is reached. During this phase, channel updates are invisible to actors until the next step.
* **Update**: Update the channels with the values written by the **actors** in this step.

Repeat until no **actors** are selected for execution, or a maximum number of steps is reached.

An **actor** is a `PregelNode`. It subscribes to channels, reads data from them, and writes data to them. It can be thought of as an **actor** in the Pregel algorithm. `PregelNodes` implement LangChain's Runnable interface.

Channels are used to communicate between actors (PregelNodes). Each channel has a value type, an update type, and an update function – which takes a sequence of updates and modifies the stored value. Channels can be used to send data from one chain to another, or to send data from a chain to itself in a future step. LangGraph provides a number of built-in channels:

* @\[`LastValue`]: The default channel, stores the last value sent to the channel, useful for input and output values, or for sending data from one step to the next.
* @\[`Topic`]: A configurable PubSub Topic, useful for sending multiple values between **actors**, or for accumulating output. Can be configured to deduplicate values or to accumulate values over the course of multiple steps.
* [`BinaryOperatorAggregate`](https://langchain-ai.github.io/langgraphjs/reference/classes/langgraph.BinaryOperatorAggregate.html): stores a persistent value, updated by applying a binary operator to the current value and each update sent to the channel, useful for computing aggregates over multiple steps; e.g.,`total = BinaryOperatorAggregate(int, operator.add)`

While most users will interact with Pregel through the [StateGraph](https://langchain-ai.github.io/langgraphjs/reference/classes/langgraph.StateGraph.html) API or the [entrypoint](https://langchain-ai.github.io/langgraphjs/reference/functions/langgraph.entrypoint.html) decorator, it is possible to interact with Pregel directly.

Below are a few different examples to give you a sense of the Pregel API.

<Tabs>
  <Tab title="Single node">

<Tab title="Multiple nodes">

<Tab title="BinaryOperatorAggregate">
    This example demonstrates how to use the [`BinaryOperatorAggregate`](https://langchain-ai.github.io/langgraphjs/reference/classes/langgraph.BinaryOperatorAggregate.html) channel to implement a reducer.

<Tab title="Cycle">
    This example demonstrates how to introduce a cycle in the graph, by having
    a chain write to a channel it subscribes to. Execution will continue
    until a `null` value is written to the channel.

LangGraph provides two high-level APIs for creating a Pregel application: the [StateGraph (Graph API)](/oss/javascript/langgraph/graph-api) and the [Functional API](/oss/javascript/langgraph/functional-api).

<Tabs>
  <Tab title="StateGraph (Graph API)">
    The [StateGraph (Graph API)](https://langchain-ai.github.io/langgraphjs/reference/classes/langgraph.StateGraph.html) is a higher-level abstraction that simplifies the creation of Pregel applications. It allows you to define a graph of nodes and edges. When you compile the graph, the StateGraph API automatically creates the Pregel application for you.

The compiled Pregel instance will be associated with a list of nodes and channels. You can inspect the nodes and channels by printing them.

You will see something like this:

You should see something like this

<Tab title="Functional API">
    In the [Functional API](/oss/javascript/langgraph/functional-api), you can use an [`entrypoint`](https://langchain-ai.github.io/langgraphjs/reference/functions/langgraph.entrypoint.html) to create a Pregel application. The `entrypoint` decorator allows you to define a function that takes input and returns output.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/pregel.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</Tab>

  <Tab title="Multiple nodes">
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</Tab>

  <Tab title="Topic">
```

---

## LangGraph SDK

**URL:** llms-txt#langgraph-sdk

Source: https://docs.langchain.com/oss/javascript/reference/langgraph-javascript

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/reference/langgraph-javascript.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## LangGraph v1 migration guide

**URL:** llms-txt#langgraph-v1-migration-guide

**Contents:**
- Summary of changes
- Deprecation: `createReactAgent` → `createAgent`
- Typed interrupts
- Event stream encoding
- Breaking changes
  - Dropped Node 18 support
  - New build outputs

Source: https://docs.langchain.com/oss/javascript/migrate/langgraph-v1

This guide outlines changes in LangGraph v1 and how to migrate from previous versions. For a high-level overview of what's new, see the [release notes](/oss/javascript/releases/langgraph-v1).

## Summary of changes

| Area                             | What changed                                               |
| -------------------------------- | ---------------------------------------------------------- |
| React prebuilt                   | `createReactAgent` deprecated; use LangChain `createAgent` |
| Interrupts                       | Typed interrupts supported via `interrupts` config         |
| `toLangGraphEventStream` removed | Use `graph.stream` with the desired `encoding` format      |
| `useStream`                      | Supports custom transports                                 |

## Deprecation: `createReactAgent` → `createAgent`

LangGraph v1 deprecates the `createReactAgent` prebuilt. Use LangChain's `createAgent`, which runs on LangGraph and adds a flexible middleware system.

See the LangChain v1 docs for details:

* [Release notes](/oss/javascript/releases/langchain-v1#createagent)
* [Migration guide](/oss/javascript/migrate/langchain-v1#createagent)

You can now define interrupt types at graph construction to strictly type the values passed to and received from interrupts.

See [Interrupts](/oss/javascript/langgraph/interrupts) to learn more.

## Event stream encoding

The low-level `toLangGraphEventStream` helper is removed. Streaming responses are handled by the SDK; when using low-level clients, select the wire format via an `encoding` option passed to `graph.stream`.

### Dropped Node 18 support

All LangGraph packages now require **Node.js 20 or higher**. Node.js 18 reached [end of life](https://nodejs.org/en/about/releases/) in March 2025.

### New build outputs

Builds for all langgraph packages now use a bundler based approach instead of using raw typescript outputs. If you were importing files from the `dist/` directory (which is not recommended), you will need to update your imports to use the new module system.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/javascript/migrate/langgraph-v1.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>

## Summary of changes

| Area                             | What changed                                               |
| -------------------------------- | ---------------------------------------------------------- |
| React prebuilt                   | `createReactAgent` deprecated; use LangChain `createAgent` |
| Interrupts                       | Typed interrupts supported via `interrupts` config         |
| `toLangGraphEventStream` removed | Use `graph.stream` with the desired `encoding` format      |
| `useStream`                      | Supports custom transports                                 |

***

## Deprecation: `createReactAgent` → `createAgent`

LangGraph v1 deprecates the `createReactAgent` prebuilt. Use LangChain's `createAgent`, which runs on LangGraph and adds a flexible middleware system.

See the LangChain v1 docs for details:

* [Release notes](/oss/javascript/releases/langchain-v1#createagent)
* [Migration guide](/oss/javascript/migrate/langchain-v1#createagent)

<CodeGroup>
```

---

## LangSmith Agent Builder Slack App

**URL:** llms-txt#langsmith-agent-builder-slack-app

**Contents:**
- How to install
- Permissions
- Privacy policy
- AI components and disclaimers
  - What you should know
  - Technical details
- Pricing

Source: https://docs.langchain.com/langsmith/agent-builder-slack-app

Connect the LangSmith Agent Builder to your Slack workspace to power AI agents.

The LangSmith Agent Builder Slack app integrates your agents with Slack for secure, context-aware communication inside your Slack workspace.

After installation, your agents will be able to:

* Send direct messages.
* Post to channels.
* Read thread messages.
* Reply in threads.
* Read conversation history.

To install the LangSmith Agent Builder for Slack:

1. Navigate to Agent Builder in your [LangSmith workspace](https://smith.langchain.com).
2. Create or edit an agent.
3. Add Slack as a trigger or enable Slack tools.
4. When prompted, authorize the Slack connection.
5. Follow the OAuth flow to grant permissions to your Slack workspace.

The app will be installed automatically when you complete the authorization.

The LangSmith Agent Builder requires the following permissions to your Slack workspace:

* **Send messages** - Send direct messages and post to channels
* **Read messages** - Read channel history and thread messages
* **View channels** - Access basic channel information
* **View users** - Look up user information for messaging

These permissions enable agents to communicate effectively within your Slack workspace.

The LangSmith Agent Builder Slack app collects, manages, and stores third-party data in accordance with our privacy policy. For full details on how your data is handled, please see [our privacy policy](https://www.langchain.com/privacy-policy).

## AI components and disclaimers

The LangSmith Agent Builder uses large language models (LLMs) to power AI agents that interact with users in Slack. While these models are powerful, they have the potential to generate inaccurate responses, summaries, or other outputs.

### What you should know

* **AI-generated content**: All responses from agents are generated by AI and may contain errors or inaccuracies. Always verify important information.
* **Data usage**: Slack data is not used to train LLMs. Your workspace data remains private and is only used to provide agent functionality.
* **Transparency**: The Agent Builder is transparent about the actions it will take once added to your workspace, as outlined in the permissions section above.

### Technical details

The Agent Builder uses the following approach to AI:

* **Model**: Uses LLMs provided through the LangSmith platform
* **Data retention**: User data is retained according to LangSmith's data retention policies
* **Data tenancy**: Data is handled according to your LangSmith organization settings
* **Data residency**: Data residency follows your LangSmith configuration

For more information about AI safety and best practices, see the [Agent Builder documentation](/langsmith/agent-builder).

The LangSmith Agent Builder Slack app itself does not have any direct pricing. However, agent runs and traces are billed through the [LangSmith platform](https://smith.langchain.com) according to your organization's plan.

For current pricing information, see the [LangSmith pricing page](https://www.langchain.com/pricing).

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/agent-builder-slack-app.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## LangSmith API reference

**URL:** llms-txt#langsmith-api-reference

Source: https://docs.langchain.com/langsmith/smith-api-ref

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/smith-api-ref.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## LangSmith control plane

**URL:** llms-txt#langsmith-control-plane

**Contents:**
- Control plane UI
- Control plane API
  - Integrations
  - Deployments
  - Revisions
  - Listeners
- Control plane features
  - Deployment types
  - Database provisioning
  - Asynchronous deployment

Source: https://docs.langchain.com/langsmith/control-plane

The *control plane* is the part of LangSmith that manages deployments. It includes the control plane UI, where users create and update [Agent Servers](/langsmith/agent-server), and the control plane APIs, which support the UI and provide programmatic access.

When you make an update through the control plane, the update is stored in control plane state. The [data plane](/langsmith/data-plane) “listener” polls for these updates by calling the control plane APIs.

From the control plane UI, you can:

* View a list of outstanding deployments.
* View details of an individual deployment.
* Create a new deployment.
* Update a deployment.
* Update environment variables for a deployment.
* View build and server logs of a deployment.
* View deployment metrics such as CPU and memory usage.
* Delete a deployment.

The Control plane UI is embedded in [LangSmith](https://docs.smith.langchain.com).

This section describes the data model of the control plane API. The API is used to create, update, and delete deployments. See the [control plane API reference](/langsmith/api-ref-control-plane) for more details.

An integration is an abstraction for a `git` repository provider (e.g. GitHub). It contains all of the required metadata needed to connect with and deploy from a `git` repository.

A deployment is an instance of an Agent Server. A single deployment can have many revisions.

A revision is an iteration of a deployment. When a new deployment is created, an initial revision is automatically created. To deploy code changes or update secrets for a deployment, a new revision must be created.

A listener is an instance of a ["listener" application](/langsmith/data-plane#”listener”-application). A listener contains metadata about the application (e.g. version) and metadata about the compute infrastructure where it can deploy to (e.g. Kubernetes namespaces).

The listener data model only applies for [Hybrid](/langsmith/hybrid) and [Self-Hosted](/langsmith/self-hosted) deployments.

## Control plane features

This section describes various features of the control plane.

For simplicity, the control plane offers two deployment types with different resource allocations: `Development` and `Production`.

| **Deployment Type** | **CPU/Memory**  | **Scaling**       | **Database**                                                                     |
| ------------------- | --------------- | ----------------- | -------------------------------------------------------------------------------- |
| Development         | 1 CPU, 1 GB RAM | Up to 1 replica   | 10 GB disk, no backups                                                           |
| Production          | 2 CPU, 2 GB RAM | Up to 10 replicas | Autoscaling disk, automatic backups, highly available (multi-zone configuration) |

CPU and memory resources are per replica.

<Warning>
  **Immutable Deployment Type**
  Once a deployment is created, the deployment type cannot be changed.
</Warning>

<Info>
  **Self-Hosted Deployment**
  Resources for [Hybrid](/langsmith/hybrid) and [Self-Hosted](/langsmith/self-hosted) deployments can be fully customized. Deployment types are only applicable for [Cloud](/langsmith/cloud) deployments.
</Info>

`Production` type deployments are suitable for "production" workloads. For example, select `Production` for customer-facing applications in the critical path.

Resources for `Production` type deployments can be manually increased on a case-by-case basis depending on use case and capacity constraints. Contact [support@langchain.dev](mailto:support@langchain.dev) to request an increase in resources.

`Development` type deployments are suitable development and testing. For example, select `Development` for internal testing environments. `Development` type deployments are not suitable for "production" workloads.

<Danger>
  **Preemptible Compute Infrastructure**
  `Development` type deployments (API server, queue server, and database) are provisioned on preemptible compute infrastructure. This means the compute infrastructure **may be terminated at any time without notice**. This may result in intermittent...

* Redis connection timeouts/errors
  * Postgres connection timeouts/errors
  * Failed or retrying background runs

This behavior is expected. Preemptible compute infrastructure **significantly reduces the cost to provision a `Development` type deployment**. By design, Agent Server is fault-tolerant. The implementation will automatically attempt to recover from Redis/Postgres connection errors and retry failed background runs.

`Production` type deployments are provisioned on durable compute infrastructure, not preemptible compute infrastructure.
</Danger>

Database disk size for `Development` type deployments can be manually increased on a case-by-case basis depending on use case and capacity constraints. For most use cases, [TTLs](/langsmith/configure-ttl) should be configured to manage disk usage. Contact [support@langchain.dev](mailto:support@langchain.dev) to request an increase in resources.

### Database provisioning

The control plane and [data plane](/langsmith/data-plane) "listener" application coordinate to automatically create a Postgres database for each deployment. The database serves as the [persistence layer](/oss/python/langgraph/persistence#memory-store) for the deployment.

When implementing a LangGraph application, a [checkpointer](/oss/python/langgraph/persistence#checkpointer-libraries) does not need to be configured by the developer. Instead, a checkpointer is automatically configured for the graph. Any checkpointer configured for a graph will be replaced by the one that is automatically configured.

There is no direct access to the database. All access to the database occurs through the [Agent Server](/langsmith/agent-server).

The database is never deleted until the deployment itself is deleted.

<Info>
  A custom Postgres instance can be configured for [Hybrid](/langsmith/hybrid) and [Self-Hosted](/langsmith/self-hosted) deployments.
</Info>

### Asynchronous deployment

Infrastructure for deployments and revisions are provisioned and deployed asynchronously. They are not deployed immediately after submission. Currently, deployment can take up to several minutes.

* When a new deployment is created, a new database is created for the deployment. Database creation is a one-time step. This step contributes to a longer deployment time for the initial revision of the deployment.
* When a subsequent revision is created for a deployment, there is no database creation step. The deployment time for a subsequent revision is significantly faster compared to the deployment time of the initial revision.
* The deployment process for each revision contains a build step, which can take up to a few minutes.

The control plane and [data plane](/langsmith/data-plane) "listener" application coordinate to achieve asynchronous deployments.

After a deployment is ready, the control plane monitors the deployment and records various metrics, such as:

* CPU and memory usage of the deployment.
* Number of container restarts.
* Number of replicas (this will increase with [autoscaling](/langsmith/data-plane#autoscaling)).
* [PostgreSQL](/langsmith/data-plane#postgres) CPU, memory usage, and disk usage.
* [Agent Server queue](/langsmith/agent-server#persistence-and-task-queue) pending/active run count.
* [Agent Server API](/langsmith/agent-server) success response count, error response count, and latency.

These metrics are displayed as charts in the Control Plane UI.

### LangSmith integration

A [LangSmith](/langsmith/home) tracing project is automatically created for each deployment. The tracing project has the same name as the deployment. When creating a deployment, the `LANGCHAIN_TRACING` and `LANGSMITH_API_KEY`/`LANGCHAIN_API_KEY` environment variables do not need to be specified; they are set automatically by the control plane.

When a deployment is deleted, the traces and the tracing project are not deleted.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/control-plane.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## LangSmith data plane

**URL:** llms-txt#langsmith-data-plane

**Contents:**
- Server infrastructure
- "Listener" application
- PostgreSQL
- Redis
  - Communication
  - Ephemeral metadata
- Data plane features
  - Data region
  - Autoscaling
  - Static IP addresses

Source: https://docs.langchain.com/langsmith/data-plane

The *data plane* consists of your [Agent Servers](/langsmith/agent-server) (deployments), their supporting infrastructure, and the "listener" application that continuously polls for updates from the [LangSmith control plane](/langsmith/control-plane).

## Server infrastructure

In addition to the [Agent Server](/langsmith/agent-server) itself, the following infrastructure components for each server are also included in the broad definition of "data plane":

* **PostgreSQL**: persistence layer for user, run, and memory data.
* **Redis**: communication and ephemeral metadata for workers.
* **Secrets store**: secure management of environment secrets.
* **Autoscalers**: scale server containers based on load.

## "Listener" application

The data plane "listener" application periodically calls [control plane APIs](/langsmith/control-plane#control-plane-api) to:

* Determine if new deployments should be created.
* Determine if existing deployments should be updated (i.e. new revisions).
* Determine if existing deployments should be deleted.

In other words, the data plane "listener" reads the latest state of the control plane (desired state) and takes action to reconcile outstanding deployments (current state) to match the latest state.

PostgreSQL is the persistence layer for all user, run, and long-term memory data in a Agent Server. This stores both checkpoints (see more info [here](/oss/python/langgraph/persistence)), server resources (threads, runs, assistants and crons), as well as items saved in the long-term memory store (see more info [here](/oss/python/langgraph/persistence#memory-store)).

Redis is used in each Agent Server as a way for server and queue workers to communicate, and to store ephemeral metadata. No user or run data is stored in Redis.

All runs in an Agent Server are executed by a pool of background workers that are part of each deployment. In order to enable some features for those runs (such as cancellation and output streaming) we need a channel for two-way communication between the server and the worker handling a particular run. We use Redis to organize that communication.

1. A Redis list is used as a mechanism to wake up a worker as soon as a new run is created. Only a sentinel value is stored in this list, no actual run information. The run information is then retrieved from PostgreSQL by the worker.
2. A combination of a Redis string and Redis PubSub channel is used for the server to communicate a run cancellation request to the appropriate worker.
3. A Redis PubSub channel is used by the worker to broadcast streaming output from an agent while the run is being handled. Any open `/stream` request in the server will subscribe to that channel and forward any events to the response as they arrive. No events are stored in Redis at any time.

### Ephemeral metadata

Runs in an Agent Server may be retried for specific failures (currently only for transient PostgreSQL errors encountered during the run). In order to limit the number of retries (currently limited to 3 attempts per run) we record the attempt number in a Redis string when it is picked up. This contains no run-specific info other than its ID, and expires after a short delay.

## Data plane features

This section describes various features of the data plane.

<Info>
  **Only for Cloud**
  Data regions are only applicable for [Cloud](/langsmith/cloud) deployments.
</Info>

Deployments can be created in 2 data regions: US and EU

The data region for a deployment is implied by the data region of the LangSmith organization where the deployment is created. Deployments and the underlying database for the deployments cannot be migrated between data regions.

[`Production` type](/langsmith/control-plane#deployment-types) deployments automatically scale up to 10 containers. Scaling is based on 3 metrics:

1. CPU utilization
2. Memory utilization
3. Number of pending (in progress) [runs](/langsmith/assistants#execution)

For CPU utilization, the autoscaler targets 75% utilization. This means the autoscaler will scale the number of containers up or down to ensure that CPU utilization is at or near 75%. For memory utilization, the autoscaler targets 75% utilization as well.

For number of pending runs, the autoscaler targets 10 pending runs. For example, if the current number of containers is 1, but the number of pending runs is 20, the autoscaler will scale up the deployment to 2 containers (20 pending runs / 2 containers = 10 pending runs per container).

Each metric is computed independently and the autoscaler will determine the scaling action based on the metric that results in the largest number of containers.

Scale down actions are delayed for 30 minutes before any action is taken. In other words, if the autoscaler decides to scale down a deployment, it will first wait for 30 minutes before scaling down. After 30 minutes, the metrics are recomputed and the deployment will scale down if the recomputed metrics result in a lower number of containers than the current number. Otherwise, the deployment remains scaled up. This "cool down" period ensures that deployments do not scale up and down too frequently.

### Static IP addresses

<Info>
  **Only for Cloud**
  Static IP addresses are only available for [Cloud](/langsmith/cloud) deployments.
</Info>

All traffic from deployments created after January 6th 2025 will come through a NAT gateway. This NAT gateway will have several static IP addresses depending on the data region. Refer to the table below for the list of static IP addresses:

| US             | EU             |
| -------------- | -------------- |
| 35.197.29.146  | 34.13.192.67   |
| 34.145.102.123 | 34.147.105.64  |
| 34.169.45.153  | 34.90.22.166   |
| 34.82.222.17   | 34.147.36.213  |
| 35.227.171.135 | 34.32.137.113  |
| 34.169.88.30   | 34.91.238.184  |
| 34.19.93.202   | 35.204.101.241 |
| 34.19.34.50    | 35.204.48.32   |
| 34.59.244.194  |                |
| 34.9.99.224    |                |
| 34.68.27.146   |                |
| 34.41.178.137  |                |
| 34.123.151.210 |                |
| 34.135.61.140  |                |
| 34.121.166.52  |                |
| 34.31.121.70   |                |

### Custom PostgreSQL

<Info>
  Custom PostgreSQL instances are only available for [hybrid](/langsmith/hybrid) and [self-hosted](/langsmith/self-hosted) deployments.
</Info>

A custom PostgreSQL instance can be used instead of the [one automatically created by the control plane](/langsmith/control-plane#database-provisioning). Specify the [`POSTGRES_URI_CUSTOM`](/langsmith/env-var#postgres-uri-custom) environment variable to use a custom PostgreSQL instance.

Multiple deployments can share the same PostgreSQL instance. For example, for `Deployment A`, `POSTGRES_URI_CUSTOM` can be set to `postgres://<user>:<password>@/<database_name_1>?host=<hostname_1>` and for `Deployment B`, `POSTGRES_URI_CUSTOM` can be set to `postgres://<user>:<password>@/<database_name_2>?host=<hostname_1>`. `<database_name_1>` and `database_name_2` are different databases within the same instance, but `<hostname_1>` is shared. **The same database cannot be used for separate deployments**.

<Info>
  Custom Redis instances are only available for [Hybrid](/langsmith/hybrid) and [Self-Hosted](/langsmith/self-hosted) deployments.
</Info>

A custom Redis instance can be used instead of the one automatically created by the control plane. Specify the [REDIS\_URI\_CUSTOM](/langsmith/env-var#redis-uri-custom) environment variable to use a custom Redis instance.

Multiple deployments can share the same Redis instance. For example, for `Deployment A`, `REDIS_URI_CUSTOM` can be set to `redis://<hostname_1>:<port>/1` and for `Deployment B`, `REDIS_URI_CUSTOM` can be set to `redis://<hostname_1>:<port>/2`. `1` and `2` are different database numbers within the same instance, but `<hostname_1>` is shared. **The same database number cannot be used for separate deployments**.

### LangSmith tracing

Agent Server is automatically configured to send traces to LangSmith. See the table below for details with respect to each deployment option.

| Cloud                                  | Hybrid                                                    | Self-Hosted                                                                                |
| -------------------------------------- | --------------------------------------------------------- | ------------------------------------------------------------------------------------------ |
| Required<br />Trace to LangSmith SaaS. | Optional<br />Disable tracing or trace to LangSmith SaaS. | Optional<br />Disable tracing, trace to LangSmith SaaS, or trace to Self-Hosted LangSmith. |

Agent Server is automatically configured to report telemetry metadata for billing purposes. See the table below for details with respect to each deployment option.

| Cloud                             | Hybrid                            | Self-Hosted                                                                                                              |
| --------------------------------- | --------------------------------- | ------------------------------------------------------------------------------------------------------------------------ |
| Telemetry sent to LangSmith SaaS. | Telemetry sent to LangSmith SaaS. | Self-reported usage (audit) for air-gapped license key.<br />Telemetry sent to LangSmith SaaS for LangSmith License Key. |

Agent Server is automatically configured to perform license key validation. See the table below for details with respect to each deployment option.

| Cloud                                               | Hybrid                                              | Self-Hosted                                                                      |
| --------------------------------------------------- | --------------------------------------------------- | -------------------------------------------------------------------------------- |
| LangSmith API Key validated against LangSmith SaaS. | LangSmith API Key validated against LangSmith SaaS. | Air-gapped license key or Platform License Key validated against LangSmith SaaS. |

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/data-plane.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## LangSmith Deployment components

**URL:** llms-txt#langsmith-deployment-components

Source: https://docs.langchain.com/langsmith/components

When running self-hosted [LangSmith Deployment](/langsmith/deploy-self-hosted-full-platform), your installation includes several key components. Together these tools and services provide a complete solution for building, deploying, and managing graphs (including agentic applications) in your own infrastructure:

* [Agent Server](/langsmith/agent-server): Defines an opinionated API and runtime for deploying graphs and agents. Handles execution, state management, and persistence so you can focus on building logic rather than server infrastructure.
* [LangGraph CLI](/langsmith/cli): A command-line interface to build, package, and interact with graphs locally and prepare them for deployment.
* [Studio](/langsmith/studio): A specialized IDE for visualization, interaction, and debugging. Connects to a local Agent Server for developing and testing your graph.
* [Python/JS SDK](/langsmith/sdk): The Python/JS SDK provides a programmatic way to interact with deployed graphs and agents from your applications.
* [RemoteGraph](/langsmith/use-remote-graph): Allows you to interact with a deployed graph as though it were running locally.
* [Control Plane](/langsmith/control-plane): The UI and APIs for creating, updating, and managing Agent Server deployments.
* [Data plane](/langsmith/data-plane): The runtime layer that executes your graphs, including Agent Servers, their backing services (PostgreSQL, Redis, etc.), and the listener that reconciles state from the control plane.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/components.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## LangSmith Deployment

**URL:** llms-txt#langsmith-deployment

**Contents:**
- What you'll learn
  - Related

Source: https://docs.langchain.com/langsmith/deployments

<Callout icon="rocket" color="#4F46E5" iconType="regular">
  **Start here if you're building or operating agent applications.** This section is about deploying **your application**. If you need to set up LangSmith infrastructure, the [Platform setup section](/langsmith/platform-setup) covers infrastructure options (cloud, hybrid, self-hosted) and setup guides for hybrid and self-hosted deployments.
</Callout>

This section covers how to package, build, and deploy your *agents* and applications as [Agent Servers](/langsmith/agent-server).

A typical deployment workflow consists of the following steps:

<Steps>
  <Step title={<a href="/langsmith/local-server">Test locally</a>}>
    Run your application on a local server.
  </Step>

<Step title={<a href="/langsmith/application-structure">Configure app for deployment</a>}>
    Set up dependencies, project structure, and environment configuration.
  </Step>

<Step title={<a href="/langsmith/platform-setup">Choose hosting</a>}>
    (Required for deployment) Select Cloud, Hybrid, or Self-hosted.
  </Step>

<Step title="Deploy your app">
    * [**Cloud**](/langsmith/deploy-to-cloud): Push code from a git repository
    * [**Hybrid or Self-hosted with control plane**](/langsmith/deploy-with-control-plane): Build and push Docker images, deploy via UI
    * [**Standalone servers**](/langsmith/deploy-standalone-server): Deploy directly without control plane
  </Step>

<Step title={<a href="/langsmith/observability">Monitor & manage</a>}>
    Track traces, alerts, and dashboards.
  </Step>
</Steps>

* Configure your [app for deployment](/langsmith/application-structure) (dependencies, [project setup](/langsmith/setup-app-requirements-txt), and [monorepo support](/langsmith/monorepo-support)).
* Build, deploy, and update [Agent Servers](/langsmith/agent-server).
* Secure your deployments with [authentication and access control](/langsmith/auth).
* Customize your server runtime ([lifespan hooks](/langsmith/custom-lifespan), [middleware](/langsmith/custom-middleware), and [routes](/langsmith/custom-routes)).
* Debug, observe, and troubleshoot deployed agents using the [Studio UI](/langsmith/studio).

<Columns cols={1}>
  <Card title="Get started with deployment" icon="robot" href="/langsmith/application-structure" cta="Configure your app">
    Package, build, and deploy your agents and graphs to Agent Server.
  </Card>
</Columns>

* [Agent Server](/langsmith/agent-server)
* [Application structure](/langsmith/application-structure)
* [Local server testing](/langsmith/local-server)

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/deployments.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## LangSmith docs

**URL:** llms-txt#langsmith-docs

**Contents:**
- Get started
- Workflow

Source: https://docs.langchain.com/langsmith/home

**LangSmith provides tools for developing, debugging, and deploying LLM applications.**
It helps you trace requests, evaluate outputs, test prompts, and manage deployments in one place.
LangSmith is framework agnostic, so you can use it with or without LangChain's open-source libraries
[`langchain`](/oss/python/langchain/overview) and [`langgraph`](/oss/python/langgraph/overview).
Prototype locally, then move to production with integrated monitoring and evaluation to build more reliable AI systems.

<Callout icon="bullhorn" color="#DFC5FE" iconType="regular">
  LangGraph Platform is now [LangSmith Deployment](/langsmith/deployments). For more information, check out the [Changelog](https://changelog.langchain.com/announcements/product-naming-changes-langsmith-deployment-and-langsmith-studio).
</Callout>

<Steps>
  <Step title="Create an account" icon="user-plus">
    Sign up at [smith.langchain.com](https://smith.langchain.com) (no credit card required).
    You can log in with **Google**, **GitHub**, or **email**.
  </Step>

<Step title="Create an API key" icon="key">
    Go to your [Settings page](https://smith.langchain.com/settings) → **API Keys** → **Create API Key**.
    Copy the key and save it securely.
  </Step>
</Steps>

Once your account and API key are ready, choose a quickstart to begin building with LangSmith:

<Columns cols={3}>
  <Card title="Observability" icon="magnifying-glass" href="/langsmith/observability-quickstart" arrow="true" cta="Start tracing">
    Gain visibility into every step your application takes to debug faster and improve reliability.
  </Card>

<Card title="Evaluation" icon="chart-line" href="/langsmith/evaluation-quickstart" arrow="true" cta="Evaluate your app">
    Measure and track quality over time to ensure your AI applications are consistent and trustworthy.
  </Card>

<Card title="Deployment" icon="cloud-arrow-up" iconType="solid" href="/langsmith/deployments" arrow="true" cta="Deploy your agents">
    Deploy your agents as Agent Servers, ready to scale in production.
  </Card>

<Card title="Platform setup" icon="server" iconType="solid" href="/langsmith/platform-setup" arrow="true" cta="Choose how to set up LangSmith">
    Use LangSmith in managed cloud, in a self-hosted environment, or hybrid to match your infrastructure and compliance needs.
  </Card>

<Card title="Prompt Testing" icon="flask" href="/langsmith/prompt-engineering-quickstart" arrow="true" cta="Test your prompts">
    Iterate on prompts with built-in versioning and collaboration to ship improvements faster.
  </Card>

<Card title="Studio" icon="window" href="/langsmith/quick-start-studio" arrow="true" cta="Develop with Studio">
    Use a visual interface to design, test, and refine applications end-to-end.
  </Card>
</Columns>

<Callout icon="lock" color="#DFC5FE" iconType="regular">
  LangSmith meets the highest standards of data security and privacy with HIPAA, SOC 2 Type 2, and GDPR compliance. For more information, see the [Trust Center](https://trust.langchain.com/).
</Callout>

LangSmith combines observability, evaluation, deployment, and platform setup in one integrated workflow—from local development to production.

<img className="block dark:hidden" src="https://mintcdn.com/langchain-5e9cc07a/gnE-C9CZc2IgnmJo/langsmith/images/overview-light.svg?fit=max&auto=format&n=gnE-C9CZc2IgnmJo&q=85&s=84312d9c72c44e44ac513eaa78abefc6" alt="Diagram showing how LangSmith integrates observability, evaluation, deployment, and platform setup in a single workflow from development to production." data-og-width="1138" width="1138" data-og-height="549" height="549" data-path="langsmith/images/overview-light.svg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/gnE-C9CZc2IgnmJo/langsmith/images/overview-light.svg?w=280&fit=max&auto=format&n=gnE-C9CZc2IgnmJo&q=85&s=32fa495af106d1eb30c9d512b59c3926 280w, https://mintcdn.com/langchain-5e9cc07a/gnE-C9CZc2IgnmJo/langsmith/images/overview-light.svg?w=560&fit=max&auto=format&n=gnE-C9CZc2IgnmJo&q=85&s=f30ac54ecdc55dd22b5762800f33a5bb 560w, https://mintcdn.com/langchain-5e9cc07a/gnE-C9CZc2IgnmJo/langsmith/images/overview-light.svg?w=840&fit=max&auto=format&n=gnE-C9CZc2IgnmJo&q=85&s=aee1643d7b4ab4cf74ffb54b555e3788 840w, https://mintcdn.com/langchain-5e9cc07a/gnE-C9CZc2IgnmJo/langsmith/images/overview-light.svg?w=1100&fit=max&auto=format&n=gnE-C9CZc2IgnmJo&q=85&s=65946469f91ff9cfbd26d154d31c6caf 1100w, https://mintcdn.com/langchain-5e9cc07a/gnE-C9CZc2IgnmJo/langsmith/images/overview-light.svg?w=1650&fit=max&auto=format&n=gnE-C9CZc2IgnmJo&q=85&s=a310e56592be23e233075ebffa1b124d 1650w, https://mintcdn.com/langchain-5e9cc07a/gnE-C9CZc2IgnmJo/langsmith/images/overview-light.svg?w=2500&fit=max&auto=format&n=gnE-C9CZc2IgnmJo&q=85&s=f9e2edcac083797d0498806cb512d5de 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/langchain-5e9cc07a/Dai6LUVulRwLjxET/langsmith/images/overview-dark.svg?fit=max&auto=format&n=Dai6LUVulRwLjxET&q=85&s=f930d2f475ab68e1cdfe7105b5f1abe6" alt="Diagram showing how LangSmith integrates observability, evaluation, deployment, and platform setup in a single workflow from development to production." data-og-width="1157" width="1157" data-og-height="549" height="549" data-path="langsmith/images/overview-dark.svg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Dai6LUVulRwLjxET/langsmith/images/overview-dark.svg?w=280&fit=max&auto=format&n=Dai6LUVulRwLjxET&q=85&s=cab90851ba7c49d855f89c86f6bfdd0b 280w, https://mintcdn.com/langchain-5e9cc07a/Dai6LUVulRwLjxET/langsmith/images/overview-dark.svg?w=560&fit=max&auto=format&n=Dai6LUVulRwLjxET&q=85&s=d04a2b423106d1f64fa0a3a05948a19a 560w, https://mintcdn.com/langchain-5e9cc07a/Dai6LUVulRwLjxET/langsmith/images/overview-dark.svg?w=840&fit=max&auto=format&n=Dai6LUVulRwLjxET&q=85&s=84c821c5464a20cf4a3d397e5baf7357 840w, https://mintcdn.com/langchain-5e9cc07a/Dai6LUVulRwLjxET/langsmith/images/overview-dark.svg?w=1100&fit=max&auto=format&n=Dai6LUVulRwLjxET&q=85&s=202f8ed1e7c38c5a6184b9e1b2ea377f 1100w, https://mintcdn.com/langchain-5e9cc07a/Dai6LUVulRwLjxET/langsmith/images/overview-dark.svg?w=1650&fit=max&auto=format&n=Dai6LUVulRwLjxET&q=85&s=f8d03af520cd2f0363f9b41f86428396 1650w, https://mintcdn.com/langchain-5e9cc07a/Dai6LUVulRwLjxET/langsmith/images/overview-dark.svg?w=2500&fit=max&auto=format&n=Dai6LUVulRwLjxET&q=85&s=576885f21fd2e6bc4ecb4e4176515a3d 2500w" />

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/home.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## LangSmith Evaluations

**URL:** llms-txt#langsmith-evaluations

Source: https://docs.langchain.com/langsmith/evaluation

The following sections help you create datasets, run evaluations, and analyze results:

<Columns cols={3}>
  <Card title="Evaluation concepts" icon="circle-info" href="/langsmith/evaluation-concepts" arrow="true">
    Review core terminology and concepts to understand how evaluations work in LangSmith.
  </Card>

<Card title="Manage datasets" icon="database" href="/langsmith/manage-datasets" arrow="true">
    Create and manage datasets for evaluation through the UI or SDK.
  </Card>

<Card title="Run evaluations" icon="microscope" href="/langsmith/evaluate-llm-application" arrow="true">
    Evaluate your applications with different evaluators and techniques to measure quality.
  </Card>

<Card title="Analyze results" icon="chart-bar" href="/langsmith/analyze-an-experiment" arrow="true">
    View and analyze evaluation results, compare experiments, filter data, and export findings.
  </Card>

<Card title="Collect feedback" icon="comments" href="/langsmith/annotation-queues" arrow="true">
    Gather human feedback through annotation queues and inline annotation on outputs.
  </Card>

<Card title="Follow tutorials" icon="book" href="/langsmith/evaluate-chatbot-tutorial" arrow="true">
    Learn by following step-by-step tutorials, from simple chatbots to complex agent evaluations.
  </Card>
</Columns>

<Note>
  To set up a LangSmith instance, visit the [Platform setup section](/langsmith/platform-setup) to choose between cloud, hybrid, or self-hosted. All options include observability, evaluation, prompt engineering, and deployment.
</Note>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/evaluation.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## LangSmith ingests JSON format events

**URL:** llms-txt#langsmith-ingests-json-format-events

export OTEL_EXPORTER_OTLP_LOGS_PROTOCOL=http/json

---

## LangSmith JS/TS SDK

**URL:** llms-txt#langsmith-js/ts-sdk

Source: https://docs.langchain.com/langsmith/smith-js-ts-sdk

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/smith-js-ts-sdk.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## LangSmith-managed ClickHouse

**URL:** llms-txt#langsmith-managed-clickhouse

**Contents:**
- Architecture Overview
- Requirements
- Data storage
  - Stored feedback data fields
  - Stored run data fields

Source: https://docs.langchain.com/langsmith/langsmith-managed-clickhouse

<Check>
  Please read the [LangSmith architectural overview](/langsmith/self-hosted) and [guide on connecting to external ClickHouse](/langsmith/self-host-external-clickhouse) before proceeding with this guide.
</Check>

LangSmith uses ClickHouse as the primary storage engine for **traces** and **feedback**. For easier management and scaling, it is recommended to connect a self-hosted LangSmith instance to an external ClickHouse instance. LangSmith-managed ClickHouse is an option that allows you to use a fully managed ClickHouse instance that is monitored and maintained by the LangSmith team.

## Architecture Overview

The architecture of using LangSmith-managed ClickHouse with your self-hosted LangSmith instance is similar to using a fully self-hosted ClickHouse instance, with a few key differences:

* You will need to set up a private network connection between your LangSmith instance and the LangSmith-managed ClickHouse instance. This is to ensure that your data is secure and that you can connect to the ClickHouse instance from your self-hosted LangSmith instance.
* With this option, sensitive information (inputs and outputs) of your traces will be stored in cloud object storage (S3 or GCS) within your cloud instead of ClickHouse to ensure that sensitive information doesn't leave your VPC. For more details on where particular data fields are stored, refer to [Data storage](#data-storage).
* The LangSmith team will monitor your ClickHouse instance and ensure that it is running smoothly. This allows us to track metrics like run-ingestion delay and query performance.

The overall architecture looks like this:

<img className="block dark:hidden" src="https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/managed-clickhouse-light.png?fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=26fae5c3f413c15302ea0c00bebf8e93" alt="LangSmith managed ClickHouse architecture." data-og-width="2196" width="2196" data-og-height="1755" height="1755" data-path="langsmith/images/managed-clickhouse-light.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/managed-clickhouse-light.png?w=280&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=b9cb43d51325b0d9858123066c0f8812 280w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/managed-clickhouse-light.png?w=560&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=f21865c150f4047d9cfd0eee690099af 560w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/managed-clickhouse-light.png?w=840&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=3df50b05e66bc3092558e23e321d11fe 840w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/managed-clickhouse-light.png?w=1100&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=821f427c87e452c5d93aec971de3e5c3 1100w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/managed-clickhouse-light.png?w=1650&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=ffd26e217759a683feeacdacde65c047 1650w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/managed-clickhouse-light.png?w=2500&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=75b39fb829a3ed7491f3550ad5323965 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/managed-clickhouse-dark.png?fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=a3062f45f9c01f05e6917bca3f34735e" alt="LangSmith managed ClickHouse architecture." data-og-width="2196" width="2196" data-og-height="1755" height="1755" data-path="langsmith/images/managed-clickhouse-dark.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/managed-clickhouse-dark.png?w=280&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=c2a591803a03df470900d351a3ba59dc 280w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/managed-clickhouse-dark.png?w=560&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=80f1fd346d92de102121180ac7995e0f 560w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/managed-clickhouse-dark.png?w=840&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=6bb18f947823c721cba47bc275999cc2 840w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/managed-clickhouse-dark.png?w=1100&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=0d7313f3682d35e743a82f44465b9af6 1100w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/managed-clickhouse-dark.png?w=1650&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=2f96d154dff0885379ce9d0b6a8e40e9 1650w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/managed-clickhouse-dark.png?w=2500&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=b750629ab6f6ce608695538ff3ad46f5 2500w" />

* **You must use a supported blob storage option.** Read the [blob storage guide](/langsmith/self-host-blob-storage) for more information.
* To use private endpoints, ensure that your VPC is in a ClickHouse Cloud supported [region](https://clickhouse.com/docs/en/cloud/reference/supported-regions). Otherwise, you will need to use a public endpoint we will secure with firewall rules. Your VPC will need to have a NAT gateway to allow us to allowlist your traffic.
* You must have a VPC that can connect to the LangSmith-managed ClickHouse service. You will need to work with our team to set up the necessary networking.
* You must have a LangSmith self-hosted instance running. You can use our managed ClickHouse service with both [Kubernetes](/langsmith/kubernetes) and [Docker](/langsmith/docker) installations.

ClickHouse stores **runs** and **feedback** data, specifically:

* All feedback data fields.
* Some run data fields.

For a list of fields, refer to [Stored run data fields](#stored-run-data-fields) and [Stored feedback data fields](#stored-feedback-data-fields).

LangChain defines sensitive application data as `inputs`, `outputs`, `errors`, `manifests`, `extras`, and `events` of a run, since these fields may contain LLM prompts and completions. With LangSmith-managed ClickHouse, these sensitive fields are stored in cloud object storage (S3 or GCS) within your cloud, while the rest of the run data is stored in ClickHouse, ensuring sensitive information never leaves your VPC.

### Stored feedback data fields

<Note>
  Because all feedback data is stored in ClickHouse, do not send sensitive information in feedback (scores and annotations/comments) or in any other run fields that are mentioned in [Stored run data fields](#stored-run-data-fields).
</Note>

Using a LangSmith-managed ClickHouse setup, **all feedback data fields are stored in ClickHouse**:

| Field Name                | Type     | Description                                                                                            |
| ------------------------- | -------- | ------------------------------------------------------------------------------------------------------ |
| id                        | UUID     | Unique identifier for the record itself                                                                |
| created\_at               | datetime | Timestamp when the record was created                                                                  |
| modified\_at              | datetime | Timestamp when the record was last modified                                                            |
| session\_id               | UUID     | Unique identifier for the experiment or tracing project the run was a part of                          |
| run\_id                   | UUID     | Unique identifier for a specific run within a session                                                  |
| key                       | string   | A key describing the criteria of the feedback, eg "correctness"                                        |
| score                     | number   | Numerical score associated with the feedback key                                                       |
| value                     | string   | Reserved for storing a value associated with the score. Useful for categorical feedback.               |
| comment                   | string   | Any comment or annotation associated with the record. This can be a justification for the score given. |
| correction                | object   | Reserved for storing correction details, if any                                                        |
| feedback\_source          | object   | Object containing information about the feedback source                                                |
| feedback\_source.type     | string   | The type of source where the feedback originated, eg "api", "app", "evaluator"                         |
| feedback\_source.metadata | object   | Reserved for additional metadata, currently                                                            |
| feedback\_source.user\_id | UUID     | Unique identifier for the user providing feedback                                                      |

This [reference doc](/langsmith/feedback-data-format) explains the stored feedback format, which is the LangSmith's way of representing evaluation scores and annotations on runs.

### Stored run data fields

Run data fields are split between the managed ClickHouse database and your cloud object storage (e.g., S3 or GCS).

<Note>
  For run fields stored in object storage, only a reference or pointer is kept in ClickHouse. For example, `inputs` and `outputs` content are offloaded to S3/GCS, with the ClickHouse record storing corresponding S3 URLs in the `inputs_s3_urls` and `outputs_s3_urls` fields.
</Note>

The table details each run field and where it is stored:

| Field                          | Storage Location   |
| ------------------------------ | ------------------ |
| `id`                           | ClickHouse         |
| `name`                         | ClickHouse         |
| `inputs`                       | **Object Storage** |
| `run_type`                     | ClickHouse         |
| `start_time`                   | ClickHouse         |
| `end_time`                     | ClickHouse         |
| `extra`                        | **Object Storage** |
| `error`                        | **Object Storage** |
| `outputs`                      | **Object Storage** |
| `events`                       | **Object Storage** |
| `tags`                         | ClickHouse         |
| `trace_id`                     | ClickHouse         |
| `dotted_order`                 | ClickHouse         |
| `status`                       | ClickHouse         |
| `child_run_ids`                | ClickHouse         |
| `direct_child_run_ids`         | ClickHouse         |
| `parent_run_ids`               | ClickHouse         |
| `feedback_stats`               | ClickHouse         |
| `reference_example_id`         | ClickHouse         |
| `total_tokens`                 | ClickHouse         |
| `prompt_tokens`                | ClickHouse         |
| `completion_tokens`            | ClickHouse         |
| `total_cost`                   | ClickHouse         |
| `prompt_cost`                  | ClickHouse         |
| `completion_cost`              | ClickHouse         |
| `first_token_time`             | ClickHouse         |
| `session_id`                   | ClickHouse         |
| `in_dataset`                   | ClickHouse         |
| `parent_run_id`                | ClickHouse         |
| `execution_order` (deprecated) | ClickHouse         |
| `serialized`                   | ClickHouse         |
| `manifest_id` (deprecated)     | ClickHouse         |
| `manifest_s3_id`               | ClickHouse         |
| `inputs_s3_urls`               | ClickHouse         |
| `outputs_s3_urls`              | ClickHouse         |
| `price_model_id`               | ClickHouse         |
| `app_path`                     | ClickHouse         |
| `last_queued_at`               | ClickHouse         |
| `share_token`                  | ClickHouse         |

This [reference doc](/langsmith/run-data-format) explains the format of stored runs (spans), which are the building blocks of traces.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/langsmith-managed-clickhouse.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## LangSmith Observability

**URL:** llms-txt#langsmith-observability

Source: https://docs.langchain.com/langsmith/observability

The following sections help you set up and use tracing, monitoring, and observability features:

<Columns cols={3}>
  <Card title="Set up tracing" icon="gear" href="/langsmith/observability-quickstart" arrow="true">
    Configure tracing with basic options, framework integrations, or advanced settings for full control.
  </Card>

<Card title="View traces" icon="route" href="/langsmith/filter-traces-in-application" arrow="true">
    Access and manage traces via UI or API with filtering, exporting, sharing, and comparison tools.
  </Card>

<Card title="Monitor performance" icon="chart-area" href="/langsmith/dashboards" arrow="true">
    Create dashboards and set alerts to track performance and get notified when issues arise.
  </Card>

<Card title="Configure automations" icon="robot" href="/langsmith/rules" arrow="true">
    Use rules, webhooks, and online evaluations to streamline observability workflows.
  </Card>

<Card title="Collect feedback" icon="users" href="/langsmith/attach-user-feedback" arrow="true">
    Gather and manage annotations on outputs using queues and inline annotation.
  </Card>

<Card title="Trace a RAG app" icon="book-open" href="/langsmith/observability-llm-tutorial" arrow="true">
    Follow a step-by-step tutorial to trace a Retrieval-Augmented Generation application from start to finish.
  </Card>
</Columns>

For terminology definitions and core concepts, refer to [Observability concepts](/langsmith/observability-concepts).

<Note>
  To set up a LangSmith instance, visit the [Platform setup section](/langsmith/platform-setup) to choose between cloud, hybrid, or self-hosted. All options include observability, evaluation, prompt engineering, and deployment.
</Note>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/observability.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## LangSmith on Cloud

**URL:** llms-txt#langsmith-on-cloud

**Contents:**
- Prerequisites
- Create new deployment
- Create new revision
- View build and server logs
- View deployment metrics
- Interrupt revision
- Delete deployment
- Deployment settings
- Add or remove GitHub repositories
- Allowlisting IP addresses

Source: https://docs.langchain.com/langsmith/deploy-to-cloud

This is the comprehensive setup and management guide for deploying applications to LangSmith Cloud.

<Callout icon="zap" color="#4F46E5" iconType="regular">
  **Looking for a quick setup?** Try the [quickstart guide](/langsmith/deployment-quickstart) first.
</Callout>

Before setting up, review the [Cloud overview page](/langsmith/cloud) to understand the Cloud hosting model.

1. LangSmith applications are deployed from GitHub repositories. Configure and upload a LangSmith application to a GitHub repository in order to deploy it to LangSmith.
2. [Verify that the LangGraph API runs locally](/langsmith/local-server). If the API does not run successfully (i.e. `langgraph dev`), deploying to LangSmith will fail as well.

## Create new deployment

Starting from the <a href="https://smith.langchain.com/" target="_blank">LangSmith UI</a>:

1. In the left-hand navigation panel, select **Deployments**, which contains a list of existing deployments.
2. In the top-right corner, select **+ New Deployment** to create a new deployment.
3. In the `Create New Deployment` panel, fill out the required fields.
4. `Deployment details`
5. Select `Import from GitHub` and follow the GitHub OAuth workflow to install and authorize LangChain's `hosted-langserve` GitHub app to access the selected repositories. After installation is complete, return to the `Create New Deployment` panel and select the GitHub repository to deploy from the dropdown menu. **Note**: The GitHub user installing LangChain's `hosted-langserve` GitHub app must be an [owner](https://docs.github.com/en/organizations/managing-peoples-access-to-your-organization-with-roles/roles-in-an-organization#organization-owners) of the organization or account.
6. Specify a name for the deployment.
7. Specify the desired `Git Branch`. A deployment is linked to a branch. When a new revision is created, code for the linked branch will be deployed. The branch can be updated later in the [Deployment Settings](#deployment-settings).
8. Specify the full path to the [LangGraph API config file](/langsmith/cli#configuration-file) including the file name. For example, if the file `langgraph.json` is in the root of the repository, simply specify `langgraph.json`.
9. Check/uncheck checkbox to `Automatically update deployment on push to branch`. If checked, the deployment will automatically be updated when changes are pushed to the specified `Git Branch`. This setting can be enabled/disabled later in the [Deployment Settings](#deployment-settings).
10. Select the desired `Deployment Type`.
11. `Development` deployments are meant for non-production use cases and are provisioned with minimal resources.
12. `Production` deployments can serve up to 500 requests/second and are provisioned with highly available storage with automatic backups.
13. Determine if the deployment should be `Shareable through Studio`.
14. If unchecked, the deployment will only be accessible with a valid LangSmith API key for the workspace.
15. If checked, the deployment will be accessible through Studio to any LangSmith user. A direct URL to Studio for the deployment will be provided to share with other LangSmith users.
16. Specify `Environment Variables` and secrets. See the [Environment Variables reference](/langsmith/env-var) to configure additional variables for the deployment.
17. Sensitive values such as API keys (e.g. `OPENAI_API_KEY`) should be specified as secrets.
18. Additional non-secret environment variables can be specified as well.
19. A new LangSmith `Tracing Project` is automatically created with the same name as the deployment.
20. In the top-right corner, select `Submit`. After a few seconds, the `Deployment` view appears and the new deployment will be queued for provisioning.

## Create new revision

When [creating a new deployment](#create-new-deployment), a new revision is created by default. Subsequent revisions can be created to deploy new code changes.

Starting from the <a href="https://smith.langchain.com/" target="_blank">LangSmith UI</a>...

1. In the left-hand navigation panel, select **Deployments**, which contains a list of existing deployments.
2. Select an existing deployment to create a new revision for.
3. In the `Deployment` view, in the top-right corner, select `+ New Revision`.
4. In the `New Revision` modal, fill out the required fields.
5. Specify the full path to the [LangGraph API config file](/langsmith/cli#configuration-file) including the file name. For example, if the file `langgraph.json` is in the root of the repository, simply specify `langgraph.json`.
6. Determine if the deployment should be `Shareable through Studio`.
7. If unchecked, the deployment will only be accessible with a valid LangSmith API key for the workspace.
8. If checked, the deployment will be accessible through Studio to any LangSmith user. A direct URL to Studio for the deployment will be provided to share with other LangSmith users.
9. Specify `Environment Variables` and secrets. Existing secrets and environment variables are prepopulated. See the [Environment Variables reference](/langsmith/env-var) to configure additional variables for the revision.
10. Add new secrets or environment variables.
11. Remove existing secrets or environment variables.
12. Update the value of existing secrets or environment variables.
13. Select `Submit`. After a few seconds, the `New Revision` modal will close and the new revision will be queued for deployment.

## View build and server logs

Build and server logs are available for each revision.

Starting from the **Deployments** view:

1. Select the desired revision from the `Revisions` table. A panel slides open from the right-hand side and the `Build` tab is selected by default, which displays build logs for the revision.
2. In the panel, select the `Server` tab to view server logs for the revision. Server logs are only available after a revision has been deployed.
3. Within the `Server` tab, adjust the date/time range picker as needed. By default, the date/time range picker is set to the `Last 7 days`.

## View deployment metrics

Starting from the <a href="https://smith.langchain.com/" target="_blank">LangSmith UI</a>...

1. In the left-hand navigation panel, select **Deployments**, which contains a list of existing deployments.
2. Select an existing deployment to monitor.
3. Select the `Monitoring` tab to view the deployment metrics. See a list of [all available metrics](/langsmith/control-plane#monitoring).
4. Within the `Monitoring` tab, use the date/time range picker as needed. By default, the date/time range picker is set to the `Last 15 minutes`.

## Interrupt revision

Interrupting a revision will stop deployment of the revision.

<Warning>
  **Undefined Behavior**
  Interrupted revisions have undefined behavior. This is only useful if you need to deploy a new revision and you already have a revision "stuck" in progress. In the future, this feature may be removed.
</Warning>

Starting from the **Deployments** view:

1. Select the menu icon (three dots) on the right-hand side of the row for the desired revision from the `Revisions` table.
2. Select `Interrupt` from the menu.
3. A modal will appear. Review the confirmation message. Select `Interrupt revision`.

Starting from the <a href="https://smith.langchain.com/" target="_blank">LangSmith UI</a>...

1. In the left-hand navigation panel, select **Deployments**, which contains a list of existing deployments.
2. Select the menu icon (three dots) on the right-hand side of the row for the desired deployment and select `Delete`.
3. A `Confirmation` modal will appear. Select `Delete`.

## Deployment settings

Starting from the **Deployments** view:

1. In the top-right corner, select the gear icon (`Deployment Settings`).
2. Update the `Git Branch` to the desired branch.
3. Check/uncheck checkbox to `Automatically update deployment on push to branch`.
   1. Branch creation/deletion and tag creation/deletion events will not trigger an update. Only pushes to an existing branch will trigger an update.
   2. Pushes in quick succession to a branch will queue subsequent updates. Once a build completes, the most recent commit will begin building and the other queued builds will be skipped.

## Add or remove GitHub repositories

After installing and authorizing LangChain's `hosted-langserve` GitHub app, repository access for the app can be modified to add new repositories or remove existing repositories. If a new repository is created, it may need to be added explicitly.

1. From the GitHub profile, navigate to `Settings` > `Applications` > `hosted-langserve` > click `Configure`.
2. Under `Repository access`, select `All repositories` or `Only select repositories`. If `Only select repositories` is selected, new repositories must be explicitly added.
3. Click `Save`.
4. When creating a new deployment, the list of GitHub repositories in the dropdown menu will be updated to reflect the repository access changes.

## Allowlisting IP addresses

All traffic from LangSmith deployments created after January 6th 2025 will come through a NAT gateway.
This NAT gateway will have several static ip addresses depending on the region you are deploying in. Refer to the table below for the list of IP addresses to allowlist:

| US             | EU             |
| -------------- | -------------- |
| 35.197.29.146  | 34.90.213.236  |
| 34.145.102.123 | 34.13.244.114  |
| 34.169.45.153  | 34.32.180.189  |
| 34.82.222.17   | 34.34.69.108   |
| 35.227.171.135 | 34.32.145.240  |
| 34.169.88.30   | 34.90.157.44   |
| 34.19.93.202   | 34.141.242.180 |
| 34.19.34.50    | 34.32.141.108  |
| 34.59.244.194  |                |
| 34.9.99.224    |                |
| 34.68.27.146   |                |
| 34.41.178.137  |                |
| 34.123.151.210 |                |
| 34.135.61.140  |                |
| 34.121.166.52  |                |
| 34.31.121.70   |                |

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/deploy-to-cloud.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## LangSmith Python SDK

**URL:** llms-txt#langsmith-python-sdk

Source: https://docs.langchain.com/langsmith/smith-python-sdk

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/smith-python-sdk.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## LangSmith Studio

**URL:** llms-txt#langsmith-studio

**Contents:**
- Features
  - Graph mode
  - Chat mode
- Learn more
- Video guide

Source: https://docs.langchain.com/langsmith/studio

<Info>
  **Prerequisites**

* [LangSmith](/langsmith/home)
  * [Agent Server](/langsmith/agent-server)
  * [LangGraph CLI](/langsmith/cli)
</Info>

Studio is a specialized agent IDE that enables visualization, interaction, and debugging of agentic systems that implement the Agent Server API protocol. Studio also integrates with [tracing](/langsmith/observability-concepts), [evaluation](/langsmith/evaluation), and [prompt engineering](/langsmith/prompt-engineering).

Key features of Studio:

* Visualize your graph architecture
* [Run and interact with your agent](/langsmith/use-studio#run-application)
* [Manage assistants](/langsmith/use-studio#manage-assistants)
* [Manage threads](/langsmith/use-studio#manage-threads)
* [Iterate on prompts](/langsmith/observability-studio)
* [Run experiments over a dataset](/langsmith/observability-studio#run-experiments-over-a-dataset)
* Manage [long term memory](/oss/python/concepts/memory)
* Debug agent state via [time travel](/oss/python/langgraph/use-time-travel)

Studio works for graphs that are deployed on [LangSmith](/langsmith/deployment-quickstart) or for graphs that are running locally via the [Agent Server](/langsmith/local-server).

Studio supports two modes:

Graph mode exposes the full feature-set and is useful when you would like as many details about the execution of your agent, including the nodes traversed, intermediate states, and LangSmith integrations (such as adding to datasets and playground).

Chat mode is a simpler UI for iterating on and testing chat-specific agents. It is useful for business users and those who want to test overall agent behavior. Chat mode is only supported for graph's whose state includes or extends [`MessagesState`](/oss/python/langgraph/use-graph-api#messagesstate).

* See this guide on how to [get started](/langsmith/quick-start-studio) with Studio.

<iframe className="w-full aspect-video rounded-xl" src="https://www.youtube.com/embed/Mi1gSlHwZLM?si=oWCeHQ640zPHoLwn" title="YouTube video player" frameBorder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowFullScreen />

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/studio.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Learn

**URL:** llms-txt#learn

**Contents:**
- Use Cases
  - LangChain
  - LangGraph
- Conceptual Overviews
- Additional Resources

Source: https://docs.langchain.com/oss/javascript/learn

Tutorials, conceptual guides, and resources to help you get started.

In the **Learn** section of the documentation, you'll find a collection of tutorials, conceptual overviews, and additional resources to help you build powerful applications with LangChain and LangGraph.

Below are tutorials for common use cases, organized by framework.

[LangChain](/oss/javascript/langchain/overview) [agent](/oss/javascript/langchain/agents) implementations make it easy to get started for most use cases.

<Card title="Semantic Search" icon="magnifying-glass" href="/oss/javascript/langchain/knowledge-base" horizontal>
  Build a semantic search engine over a PDF with LangChain components.
</Card>

<Card title="RAG Agent" icon="user-magnifying-glass" href="/oss/javascript/langchain/rag" horizontal>
  Create a Retrieval Augmented Generation (RAG) agent.
</Card>

<Card title="SQL Agent" icon="database" href="/oss/javascript/langchain/sql-agent" horizontal>
  Build a SQL agent to interact with databases with human-in-the-loop review.
</Card>

LangChain's [agent](/oss/javascript/langchain/agents) implementations use [LangGraph](/oss/javascript/langgraph/overview) primitives.
If deeper customization is required, agents can be implemented directly in LangGraph.

<Card title="Custom RAG Agent" icon="user-magnifying-glass" href="/oss/javascript/langgraph/agentic-rag" horizontal>
  Build a RAG agent using LangGraph primitives for fine-grained control.
</Card>

## Conceptual Overviews

These guides explain the core concepts and APIs underlying LangChain and LangGraph.

<Card title="Memory" icon="brain" href="/oss/javascript/concepts/memory" horizontal>
  Understand persistence of interactions within and across threads.
</Card>

<Card title="Context engineering" icon="book-open" href="/oss/javascript/concepts/context" horizontal>
  Learn methods for providing AI applications the right information and tools to accomplish a task.
</Card>

<Card title="Graph API" icon="chart-network" href="/oss/javascript/langgraph/graph-api" horizontal>
  Explore LangGraph’s declarative graph-building API.
</Card>

<Card title="Functional API" icon="code" href="/oss/javascript/langgraph/functional-api" horizontal>
  Build agents as a single function.
</Card>

## Additional Resources

<Card title="LangChain Academy" icon="graduation-cap" href="https://academy.langchain.com/" horizontal>
  Courses and exercises to level up your LangChain skills.
</Card>

<Card title="Case Studies" icon="screen-users" href="/oss/javascript/langgraph/case-studies" horizontal>
  See how teams are using LangChain and LangGraph in production.
</Card>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/learn.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Let's configure the RetryPolicy to retry on ValueError.

**URL:** llms-txt#let's-configure-the-retrypolicy-to-retry-on-valueerror.

---

## Let's say hi again

**URL:** llms-txt#let's-say-hi-again

**Contents:**
- Checkpointer libraries
  - Checkpointer interface
  - Serializer

for update in graph.stream(
    {"messages": [{"role": "user", "content": "hi, tell me about my memories"}]}, config, stream_mode="updates"
):
    print(update)
json  theme={null}
{
    ...
    "store": {
        "index": {
            "embed": "openai:text-embeddings-3-small",
            "dims": 1536,
            "fields": ["$"]
        }
    }
}
python  theme={null}
from langgraph.checkpoint.memory import InMemorySaver
from langgraph.checkpoint.serde.jsonplus import JsonPlusSerializer

**Examples:**

Example 1 (unknown):
```unknown
When we use the LangSmith, either locally (e.g., in [Studio](/langsmith/studio)) or [hosted with LangSmith](/langsmith/platform-setup), the base store is available to use by default and does not need to be specified during graph compilation. To enable semantic search, however, you **do** need to configure the indexing settings in your `langgraph.json` file. For example:
```

Example 2 (unknown):
```unknown
See the [deployment guide](/langsmith/semantic-search) for more details and configuration options.

## Checkpointer libraries

Under the hood, checkpointing is powered by checkpointer objects that conform to [`BaseCheckpointSaver`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.base.BaseCheckpointSaver) interface. LangGraph provides several checkpointer implementations, all implemented via standalone, installable libraries:

* `langgraph-checkpoint`: The base interface for checkpointer savers ([`BaseCheckpointSaver`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.base.BaseCheckpointSaver)) and serialization/deserialization interface ([`SerializerProtocol`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.serde.base.SerializerProtocol)). Includes in-memory checkpointer implementation ([`InMemorySaver`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.memory.InMemorySaver)) for experimentation. LangGraph comes with `langgraph-checkpoint` included.
* `langgraph-checkpoint-sqlite`: An implementation of LangGraph checkpointer that uses SQLite database ([`SqliteSaver`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.sqlite.SqliteSaver) / [`AsyncSqliteSaver`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.sqlite.aio.AsyncSqliteSaver)). Ideal for experimentation and local workflows. Needs to be installed separately.
* `langgraph-checkpoint-postgres`: An advanced checkpointer that uses Postgres database ([`PostgresSaver`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.postgres.PostgresSaver) / [`AsyncPostgresSaver`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.postgres.aio.AsyncPostgresSaver)), used in LangSmith. Ideal for using in production. Needs to be installed separately.

### Checkpointer interface

Each checkpointer conforms to [`BaseCheckpointSaver`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.base.BaseCheckpointSaver) interface and implements the following methods:

* `.put` - Store a checkpoint with its configuration and metadata.
* `.put_writes` - Store intermediate writes linked to a checkpoint (i.e. [pending writes](#pending-writes)).
* `.get_tuple` - Fetch a checkpoint tuple using for a given configuration (`thread_id` and `checkpoint_id`). This is used to populate `StateSnapshot` in `graph.get_state()`.
* `.list` - List checkpoints that match a given configuration and filter criteria. This is used to populate state history in `graph.get_state_history()`

If the checkpointer is used with asynchronous graph execution (i.e. executing the graph via `.ainvoke`, `.astream`, `.abatch`), asynchronous versions of the above methods will be used (`.aput`, `.aput_writes`, `.aget_tuple`, `.alist`).

<Note>
  For running your graph asynchronously, you can use [`InMemorySaver`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.memory.InMemorySaver), or async versions of Sqlite/Postgres checkpointers -- [`AsyncSqliteSaver`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.sqlite.aio.AsyncSqliteSaver) / [`AsyncPostgresSaver`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.postgres.aio.AsyncPostgresSaver) checkpointers.
</Note>

### Serializer

When checkpointers save the graph state, they need to serialize the channel values in the state. This is done using serializer objects.

`langgraph_checkpoint` defines [protocol](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.serde.base.SerializerProtocol) for implementing serializers provides a default implementation ([`JsonPlusSerializer`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.serde.jsonplus.JsonPlusSerializer)) that handles a wide variety of types, including LangChain and LangGraph primitives, datetimes, enums and more.

#### Serialization with `pickle`

The default serializer, [`JsonPlusSerializer`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.serde.jsonplus.JsonPlusSerializer), uses ormsgpack and JSON under the hood, which is not suitable for all types of objects.

If you want to fallback to pickle for objects not currently supported by our msgpack encoder (such as Pandas dataframes),
you can use the `pickle_fallback` argument of the [`JsonPlusSerializer`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.serde.jsonplus.JsonPlusSerializer):
```

---

## List buckets

**URL:** llms-txt#list-buckets

aws s3 --endpoint-url=<endpoint_url> ls /

---

## List of standard content blocks

**URL:** llms-txt#list-of-standard-content-blocks

**Contents:**
  - Standard content blocks
  - Multimodal
  - Content block reference
- Use with chat models

human_message = HumanMessage(content_blocks=[
    {"type": "text", "text": "Hello, how are you?"},
    {"type": "image", "url": "https://example.com/image.jpg"},
])
python  theme={null}
    from langchain.messages import AIMessage

message = AIMessage(
        content=[
            {"type": "thinking", "thinking": "...", "signature": "WaUjzkyp..."},
            {"type": "text", "text": "..."},
        ],
        response_metadata={"model_provider": "anthropic"}
    )
    message.content_blocks
    
    [{'type': 'reasoning',
      'reasoning': '...',
      'extras': {'signature': 'WaUjzkyp...'}},
     {'type': 'text', 'text': '...'}]
    python  theme={null}
    from langchain.messages import AIMessage

message = AIMessage(
        content=[
            {
                "type": "reasoning",
                "id": "rs_abc123",
                "summary": [
                    {"type": "summary_text", "text": "summary 1"},
                    {"type": "summary_text", "text": "summary 2"},
                ],
            },
            {"type": "text", "text": "...", "id": "msg_abc123"},
        ],
        response_metadata={"model_provider": "openai"}
    )
    message.content_blocks
    
    [{'type': 'reasoning', 'id': 'rs_abc123', 'reasoning': 'summary 1'},
     {'type': 'reasoning', 'id': 'rs_abc123', 'reasoning': 'summary 2'},
     {'type': 'text', 'text': '...', 'id': 'msg_abc123'}]
    python  theme={null}
  from langchain.chat_models import init_chat_model

model = init_chat_model("gpt-5-nano", output_version="v1")
  python Image input theme={null}
  # From URL
  message = {
      "role": "user",
      "content": [
          {"type": "text", "text": "Describe the content of this image."},
          {"type": "image", "url": "https://example.com/path/to/image.jpg"},
      ]
  }

# From base64 data
  message = {
      "role": "user",
      "content": [
          {"type": "text", "text": "Describe the content of this image."},
          {
              "type": "image",
              "base64": "AAAAIGZ0eXBtcDQyAAAAAGlzb21tcDQyAAACAGlzb2...",
              "mime_type": "image/jpeg",
          },
      ]
  }

# From provider-managed File ID
  message = {
      "role": "user",
      "content": [
          {"type": "text", "text": "Describe the content of this image."},
          {"type": "image", "file_id": "file-abc123"},
      ]
  }
  python PDF document input theme={null}
  # From URL
  message = {
      "role": "user",
      "content": [
          {"type": "text", "text": "Describe the content of this document."},
          {"type": "file", "url": "https://example.com/path/to/document.pdf"},
      ]
  }

# From base64 data
  message = {
      "role": "user",
      "content": [
          {"type": "text", "text": "Describe the content of this document."},
          {
              "type": "file",
              "base64": "AAAAIGZ0eXBtcDQyAAAAAGlzb21tcDQyAAACAGlzb2...",
              "mime_type": "application/pdf",
          },
      ]
  }

# From provider-managed File ID
  message = {
      "role": "user",
      "content": [
          {"type": "text", "text": "Describe the content of this document."},
          {"type": "file", "file_id": "file-abc123"},
      ]
  }
  python Audio input theme={null}
  # From base64 data
  message = {
      "role": "user",
      "content": [
          {"type": "text", "text": "Describe the content of this audio."},
          {
              "type": "audio",
              "base64": "AAAAIGZ0eXBtcDQyAAAAAGlzb21tcDQyAAACAGlzb2...",
              "mime_type": "audio/wav",
          },
      ]
  }

# From provider-managed File ID
  message = {
      "role": "user",
      "content": [
          {"type": "text", "text": "Describe the content of this audio."},
          {"type": "audio", "file_id": "file-abc123"},
      ]
  }
  python Video input theme={null}
  # From base64 data
  message = {
      "role": "user",
      "content": [
          {"type": "text", "text": "Describe the content of this video."},
          {
              "type": "video",
              "base64": "AAAAIGZ0eXBtcDQyAAAAAGlzb21tcDQyAAACAGlzb2...",
              "mime_type": "video/mp4",
          },
      ]
  }

# From provider-managed File ID
  message = {
      "role": "user",
      "content": [
          {"type": "text", "text": "Describe the content of this video."},
          {"type": "video", "file_id": "file-abc123"},
      ]
  }
  python  theme={null}
        {
            "type": "text",
            "text": "Hello world",
            "annotations": []
        }
        python  theme={null}
        {
            "type": "reasoning",
            "reasoning": "The user is asking about...",
            "extras": {"signature": "abc123"},
        }
        python  theme={null}
        {
            "type": "tool_call",
            "name": "search",
            "args": {"query": "weather"},
            "id": "call_123"
        }
        ```
      </Accordion>

<Accordion title="ToolCallChunk" icon="puzzle-piece">
        **Purpose:** Streaming tool call fragments

<ParamField body="type" type="string" required>
          Always `"tool_call_chunk"`
        </ParamField>

<ParamField body="name" type="string">
          Name of the tool being called
        </ParamField>

<ParamField body="args" type="string">
          Partial tool arguments (may be incomplete JSON)
        </ParamField>

<ParamField body="id" type="string">
          Tool call identifier
        </ParamField>

<ParamField body="index" type="number | string">
          Position of this chunk in the stream
        </ParamField>
      </Accordion>

<Accordion title="InvalidToolCall" icon="triangle-exclamation">
        **Purpose:** Malformed calls, intended to catch JSON parsing errors.

<ParamField body="type" type="string" required>
          Always `"invalid_tool_call"`
        </ParamField>

<ParamField body="name" type="string">
          Name of the tool that failed to be called
        </ParamField>

<ParamField body="args" type="object">
          Arguments to pass to the tool
        </ParamField>

<ParamField body="error" type="string">
          Description of what went wrong
        </ParamField>
      </Accordion>
    </AccordionGroup>
  </Accordion>

<Accordion title="Server-Side Tool Execution" icon="server">
    <AccordionGroup>
      <Accordion title="ServerToolCall" icon="wrench">
        **Purpose:** Tool call that is executed server-side.

<ParamField body="type" type="string" required>
          Always `"server_tool_call"`
        </ParamField>

<ParamField body="id" type="string" required>
          An identifier associated with the tool call.
        </ParamField>

<ParamField body="name" type="string" required>
          The name of the tool to be called.
        </ParamField>

<ParamField body="args" type="string" required>
          Partial tool arguments (may be incomplete JSON)
        </ParamField>
      </Accordion>

<Accordion title="ServerToolCallChunk" icon="puzzle-piece">
        **Purpose:** Streaming server-side tool call fragments

<ParamField body="type" type="string" required>
          Always `"server_tool_call_chunk"`
        </ParamField>

<ParamField body="id" type="string">
          An identifier associated with the tool call.
        </ParamField>

<ParamField body="name" type="string">
          Name of the tool being called
        </ParamField>

<ParamField body="args" type="string">
          Partial tool arguments (may be incomplete JSON)
        </ParamField>

<ParamField body="index" type="number | string">
          Position of this chunk in the stream
        </ParamField>
      </Accordion>

<Accordion title="ServerToolResult" icon="box-open">
        **Purpose:** Search results

<ParamField body="type" type="string" required>
          Always `"server_tool_result"`
        </ParamField>

<ParamField body="tool_call_id" type="string" required>
          Identifier of the corresponding server tool call.
        </ParamField>

<ParamField body="id" type="string">
          Identifier associated with the server tool result.
        </ParamField>

<ParamField body="status" type="string" required>
          Execution status of the server-side tool. `"success"` or `"error"`.
        </ParamField>

<ParamField body="output">
          Output of the executed tool.
        </ParamField>
      </Accordion>
    </AccordionGroup>
  </Accordion>

<Accordion title="Provider-Specific Blocks" icon="plug">
    <Accordion title="NonStandardContentBlock" icon="asterisk">
      **Purpose:** Provider-specific escape hatch

<ParamField body="type" type="string" required>
        Always `"non_standard"`
      </ParamField>

<ParamField body="value" type="object" required>
        Provider-specific data structure
      </ParamField>

**Usage:** For experimental or provider-unique features
    </Accordion>

Additional provider-specific content types may be found within the [reference documentation](/oss/python/integrations/providers/overview) of each model provider.
  </Accordion>
</AccordionGroup>

<Tip>
  View the canonical type definitions in the [API reference](https://reference.langchain.com/python/langchain/messages).
</Tip>

<Info>
  Content blocks were introduced as a new property on messages in LangChain v1 to standardize content formats across providers while maintaining backward compatibility with existing code. Content blocks are not a replacement for the [`content`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.messages.BaseMessage.content) property, but rather a new property that can be used to access the content of a message in a standardized format.
</Info>

## Use with chat models

[Chat models](/oss/python/langchain/models) accept a sequence of message objects as input and return an [`AIMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessage) as output. Interactions are often stateless, so that a simple conversational loop involves invoking a model with a growing list of messages.

Refer to the below guides to learn more:

* Built-in features for [persisting and managing conversation histories](/oss/python/langchain/short-term-memory)
* Strategies for managing context windows, including [trimming and summarizing messages](/oss/python/langchain/short-term-memory#common-patterns)

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/messages.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
<Tip>
  Specifying `content_blocks` when initializing a message will still populate message
  `content`, but provides a type-safe interface for doing so.
</Tip>

### Standard content blocks

LangChain provides a standard representation for message content that works across providers.

Message objects implement a `content_blocks` property that will lazily parse the `content` attribute into a standard, type-safe representation. For example, messages generated from [ChatAnthropic](/oss/python/integrations/chat/anthropic) or [ChatOpenAI](/oss/python/integrations/chat/openai) will include `thinking` or `reasoning` blocks in the format of the respective provider, but can be lazily parsed into a consistent [`ReasoningContentBlock`](#content-block-reference) representation:

<Tabs>
  <Tab title="Anthropic">
```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
</Tab>

  <Tab title="OpenAI">
```

Example 4 (unknown):
```unknown

```

---

## List threads - each user only sees their own

**URL:** llms-txt#list-threads---each-user-only-sees-their-own

**Contents:**
- 3. Add scoped authorization handlers

alice_threads = await alice.threads.search()
bob_threads = await bob.threads.search()
print(f"✅ Alice sees {len(alice_threads)} thread")
print(f"✅ Bob sees {len(bob_threads)} thread")
bash  theme={null}
✅ Alice created assistant: fc50fb08-78da-45a9-93cc-1d3928a3fc37
✅ Alice created thread: 533179b7-05bc-4d48-b47a-a83cbdb5781d
✅ Bob correctly denied access: Client error '404 Not Found' for url 'http://localhost:2024/threads/533179b7-05bc-4d48-b47a-a83cbdb5781d'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404
✅ Bob created his own thread: 437c36ed-dd45-4a1e-b484-28ba6eca8819
✅ Alice sees 1 thread
✅ Bob sees 1 thread
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
Output:
```

Example 2 (unknown):
```unknown
This means:

1. Each user can create and chat in their own threads
2. Users can't see each other's threads
3. Listing threads only shows your own

<a id="scoped-authorization" />

## 3. Add scoped authorization handlers

The broad `@auth.on` handler matches on all [authorization events](/langsmith/auth#supported-resources). This is concise, but it means the contents of the `value` dict are not well-scoped, and the same user-level access control is applied to every resource. If you want to be more fine-grained, you can also control specific actions on resources.

Update `src/security/auth.py` to add handlers for specific resource types:
```

---

## LLMs

**URL:** llms-txt#llms

**Contents:**
- All LLMs

Source: https://docs.langchain.com/oss/javascript/integrations/llms/index

<Warning>
  **You are currently on a page documenting the use of text completion models. Many of the latest and most popular models are [chat completion models](/oss/javascript/langchain/models).**

Unless you are specifically using more advanced prompting techniques, you are probably looking for [this page instead](/oss/javascript/integrations/chat/).
</Warning>

[LLMs](/oss/javascript/langchain/models) are language models that takes a string as input and return a string as output.

<Columns cols={3}>
  <Card title="AI21" icon="link" href="/oss/javascript/integrations/llms/ai21" arrow="true" cta="View guide" />

<Card title="AlephAlpha" icon="link" href="/oss/javascript/integrations/llms/aleph_alpha" arrow="true" cta="View guide" />

<Card title="Arcjet Redact" icon="link" href="/oss/javascript/integrations/llms/arcjet" arrow="true" cta="View guide" />

<Card title="AWS SageMakerEndpoint" icon="link" href="/oss/javascript/integrations/llms/aws_sagemaker" arrow="true" cta="View guide" />

<Card title="Azure OpenAI" icon="link" href="/oss/javascript/integrations/llms/azure" arrow="true" cta="View guide" />

<Card title="Bedrock" icon="link" href="/oss/javascript/integrations/llms/bedrock" arrow="true" cta="View guide" />

<Card title="ChromeAI" icon="link" href="/oss/javascript/integrations/llms/chrome_ai" arrow="true" cta="View guide" />

<Card title="Cloudflare Workers AI" icon="link" href="/oss/javascript/integrations/llms/cloudflare_workersai" arrow="true" cta="View guide" />

<Card title="Cohere" icon="link" href="/oss/javascript/integrations/llms/cohere" arrow="true" cta="View guide" />

<Card title="Deep Infra" icon="link" href="/oss/javascript/integrations/llms/deep_infra" arrow="true" cta="View guide" />

<Card title="Fireworks" icon="link" href="/oss/javascript/integrations/llms/fireworks" arrow="true" cta="View guide" />

<Card title="Friendli" icon="link" href="/oss/javascript/integrations/llms/friendli" arrow="true" cta="View guide" />

<Card title="Google Vertex AI" icon="link" href="/oss/javascript/integrations/llms/google_vertex_ai" arrow="true" cta="View guide" />

<Card title="Gradient AI" icon="link" href="/oss/javascript/integrations/llms/gradient_ai" arrow="true" cta="View guide" />

<Card title="HuggingFaceInference" icon="link" href="/oss/javascript/integrations/llms/huggingface_inference" arrow="true" cta="View guide" />

<Card title="IBM watsonx.ai" icon="link" href="/oss/javascript/integrations/llms/ibm" arrow="true" cta="View guide" />

<Card title="JigsawStack Prompt Engine" icon="link" href="/oss/javascript/integrations/llms/jigsawstack" arrow="true" cta="View guide" />

<Card title="Layerup Security" icon="link" href="/oss/javascript/integrations/llms/layerup_security" arrow="true" cta="View guide" />

<Card title="Llama CPP" icon="link" href="/oss/javascript/integrations/llms/llama_cpp" arrow="true" cta="View guide" />

<Card title="MistralAI" icon="link" href="/oss/javascript/integrations/llms/mistral" arrow="true" cta="View guide" />

<Card title="Ollama" icon="link" href="/oss/javascript/integrations/llms/ollama" arrow="true" cta="View guide" />

<Card title="OpenAI" icon="link" href="/oss/javascript/integrations/llms/openai" arrow="true" cta="View guide" />

<Card title="RaycastAI" icon="link" href="/oss/javascript/integrations/llms/raycast" arrow="true" cta="View guide" />

<Card title="Replicate" icon="link" href="/oss/javascript/integrations/llms/replicate" arrow="true" cta="View guide" />

<Card title="Together AI" icon="link" href="/oss/javascript/integrations/llms/together" arrow="true" cta="View guide" />

<Card title="WRITER" icon="link" href="/oss/javascript/integrations/llms/writer" arrow="true" cta="View guide" />

<Card title="YandexGPT" icon="link" href="/oss/javascript/integrations/llms/yandex" arrow="true" cta="View guide" />
</Columns>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/javascript/integrations/llms/index.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## LLM-as-judge instructions

**URL:** llms-txt#llm-as-judge-instructions

grader_instructions = """You are a teacher grading a quiz.

You will be given a QUESTION, the GROUND TRUTH (correct) RESPONSE, and the STUDENT RESPONSE.

Here is the grade criteria to follow:
(1) Grade the student responses based ONLY on their factual accuracy relative to the ground truth answer.
(2) Ensure that the student response does not contain any conflicting statements.
(3) It is OK if the student response contains more information than the ground truth response, as long as it is factually accurate relative to the  ground truth response.

Correctness:
True means that the student's response meets all of the criteria.
False means that the student's response does not meet all of the criteria.

Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct."""

---

## LLM-as-judge output schema

**URL:** llms-txt#llm-as-judge-output-schema

class Grade(TypedDict):
    """Compare the expected and actual answers and grade the actual answer."""
    reasoning: Annotated[str, ..., "Explain your reasoning for whether the actual response is correct or not."]
    is_correct: Annotated[bool, ..., "True if the student response is mostly or exactly correct, otherwise False."]

---

## Load all documents

**URL:** llms-txt#load-all-documents

documents = loader.load()

---

## Load environment variables

**URL:** llms-txt#load-environment-variables

dotenv.load_dotenv(".env.local")

---

## Load files and create attachments

**URL:** llms-txt#load-files-and-create-attachments

image_data = load_file("my_image.png")
audio_data = load_file("my_mp3.mp3")
video_data = load_file("my_video.mp4")
pdf_data = load_file("my_document.pdf")

image_attachment = Attachment(mime_type="image/png", data=image_data)
audio_attachment = Attachment(mime_type="audio/mpeg", data=audio_data)
video_attachment = Attachment(mime_type="video/mp4", data=video_data)
pdf_attachment = ("application/pdf", pdf_data) # Can just define as tuple of (mime_type, data)
csv_attachment = Attachment(mime_type="text/csv", data=Path(os.getcwd()) / "my_csv.csv")

---

## Logs: [OTel Example](/langsmith/langsmith-collector#logs)

**URL:** llms-txt#logs:-[otel-example](/langsmith/langsmith-collector#logs)

All services that are part of the LangSmith self-hosted deployment write logs to their node's filesystem and to stdout. In order to access these logs, you need to set up your collector to read from either the filesystem or stdout. Most popular collectors support reading logs from filesystems.

* **OpenTelemetry**: [File Log Receiver](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/filelogreceiver)
* **FluentBit**: [Tail Input](https://docs.fluentbit.io/manual/pipeline/inputs/tail)
* **Datadog**: [Kubernetes Log Collection](https://docs.datadoghq.com/containers/kubernetes/log/?tab=datadogoperator)

---

## Log in as user 1

**URL:** llms-txt#log-in-as-user-1

user1_token = await login(email1, password)
user1_client = get_client(
    url="http://localhost:2024", headers={"Authorization": f"Bearer {user1_token}"}
)

---

## Log LLM calls

**URL:** llms-txt#log-llm-calls

**Contents:**
- Messages Format
  - Examples
- Converting custom I/O formats into LangSmith compatible formats
- Identifying a custom model in traces
- Provide token and cost information
  - Setting run metadata
  - Setting run outputs
- Time-to-first-token

Source: https://docs.langchain.com/langsmith/log-llm-trace

This guide will cover how to log LLM calls to LangSmith when you are using a custom model or a custom input/output format. To make the most of LangSmith's LLM trace processing, you should log your LLM traces in one of the specified formats.

LangSmith offers the following benefits for LLM traces:

* Rich, structured rendering of message lists
* Token and cost tracking per LLM call, per trace and across traces over time

If you don't log your LLM traces in the suggested formats, you will still be able to log the data to LangSmith, but it may not be processed or rendered in expected ways.

If you are using [LangChain OSS](https://python.langchain.com/docs/tutorials/llm_chain/) to call language models or LangSmith wrappers ([OpenAI](/langsmith/trace-openai), [Anthropic](/langsmith/trace-anthropic)), these approaches will automatically log traces in the correct format.

<Note>
  The examples on this page use the `traceable` decorator/wrapper to log the model run (which is the recommended approach for Python and JS/TS). However, the same idea applies if you are using the [RunTree](/langsmith/annotate-code#use-the-runtree-api) or [API](https://api.smith.langchain.com/redoc) directly.
</Note>

When tracing a custom model or a custom input/output format, it must either follow the LangChain format, OpenAI completions format or Anthropic messages format. For more details,  refer to the [OpenAI Chat Completions](https://platform.openai.com/docs/api-reference/chat/create) or [Anthropic Messages](https://docs.claude.com/en/api/messages) documentation. The LangChain format is:

<Expandable title="LangChain format">
  <ParamField path="messages" type="array" required>
    A list of messages containing the content of the conversation.

<ParamField path="role" type="string" required>
      Identifies the message type. One of: <code>system</code> | <code>reasoning</code> | <code>user</code> | <code>assistant</code> | <code>tool</code>
    </ParamField>

<ParamField path="content" type="array" required>
      Content of the message. List of typed dictionaries.

<Expandable title="Content options">
        <ParamField path="type" type="string" required>
          One of: <code>text</code> | <code>image</code> | <code>file</code> | <code>audio</code> | <code>video</code> | <code>tool\_call</code> | <code>server\_tool\_call</code> | <code>server\_tool\_result</code>.
        </ParamField>

<Expandable title="text">
          <ParamField path="type" type="literal('text')" required />

<ParamField path="text" type="string" required>
            Text content.
          </ParamField>

<ParamField path="annotations" type="object[]">
            List of annotations for the text
          </ParamField>

<ParamField path="extras" type="object">
            Additional provider-specific data.
          </ParamField>
        </Expandable>

<Expandable title="reasoning">
          <ParamField path="type" type="literal('reasoning')" required />

<ParamField path="text" type="string" required>
            Text content.
          </ParamField>

<ParamField path="extras" type="object">
            Additional provider-specific data.
          </ParamField>
        </Expandable>

<Expandable title="image">
          <ParamField path="type" type="literal('image')" required />

<ParamField path="url" type="string">
            URL pointing to the image location.
          </ParamField>

<ParamField path="base64" type="string" required>
            Base64-encoded image data.
          </ParamField>

<ParamField path="id" type="string">
            Reference ID to an externally stored image (e.g., in a provider’s file system or in a bucket).
          </ParamField>

<ParamField path="mime_type" type="string">
            Image [MIME type](https://www.iana.org/assignments/media-types/media-types.xhtml#image) (e.g., `image/jpeg`, `image/png`).
          </ParamField>
        </Expandable>

<Expandable title="file (e.g., PDFs)">
          <ParamField path="type" type="literal('file')" required />

<ParamField path="url" type="string">
            URL pointing to the file.
          </ParamField>

<ParamField path="base64" type="string" required>
            Base64-encoded file data.
          </ParamField>

<ParamField path="id" type="string">
            Reference ID to an externally stored file (e.g., in a provider’s file system or in a bucket).
          </ParamField>

<ParamField path="mime_type" type="string">
            File [MIME type](https://www.iana.org/assignments/media-types/media-types.xhtml#image) (e.g., `application/pdf`).
          </ParamField>
        </Expandable>

<Expandable title="audio">
          <ParamField path="type" type="literal('audio')" required />

<ParamField path="url" type="string">
            URL pointing to the audio file.
          </ParamField>

<ParamField path="base64" type="string" required>
            Base64-encoded audio data.
          </ParamField>

<ParamField path="id" type="string">
            Reference ID to an externally stored audio file (e.g., in a provider’s file system or in a bucket).
          </ParamField>

<ParamField path="mime_type" type="string">
            Audio [MIME type](https://www.iana.org/assignments/media-types/media-types.xhtml#image) (e.g., `audio/mpeg`, `audio/wav`).
          </ParamField>
        </Expandable>

<Expandable title="video">
          <ParamField path="type" type="literal('video')" required />

<ParamField path="url" type="string">
            URL pointing to the video file.
          </ParamField>

<ParamField path="base64" type="string" required>
            Base64-encoded video data.
          </ParamField>

<ParamField path="id" type="string">
            Reference ID to an externally stored video file (e.g., in a provider’s file system or in a bucket).
          </ParamField>

<ParamField path="mime_type" type="string">
            Video [MIME type](https://www.iana.org/assignments/media-types/media-types.xhtml#image) (e.g., `video/mp4`, `video/webm`).
          </ParamField>
        </Expandable>

<Expandable title="tool_call">
          <ParamField path="type" type="literal('tool_call')" required />

<ParamField path="name" type="string" />

<ParamField path="args" type="object" required>
            Arguments to pass to the tool.
          </ParamField>

<ParamField path="id" type="string">
            Unique identifier for this tool call.
          </ParamField>
        </Expandable>

<Expandable title="server_tool_call">
          <ParamField path="type" type="literal('server_tool_call')" required />

<ParamField path="id" type="string" required>
            Unique identifier for this tool call.
          </ParamField>

<ParamField path="name" type="string" required>
            The name of the tool to be called.
          </ParamField>

<ParamField path="args" type="object" required>
            Arguments to pass to the tool.
          </ParamField>
        </Expandable>

<Expandable title="server_tool_result">
          <ParamField path="type" type="literal('server_tool_result')" required />

<ParamField path="tool_call_id" type="string" required>
            Identifier of the corresponding server tool call.
          </ParamField>

<ParamField path="id" type="string">
            Unique identifier for this tool call.
          </ParamField>

<ParamField path="status" type="string" required>
            Execution status of the server-side tool. One of: <code>success</code> | <code>error</code>.
          </ParamField>

<ParamField path="output">
            Output of the executed tool.
          </ParamField>
        </Expandable>
      </Expandable>
    </ParamField>

<ParamField path="tool_call_id" type="string">
      Must match the <code>id</code> of a prior <code>assistant</code> message’s <code>tool\_calls\[i]</code> entry. Only valid when <code>role</code> is <code>tool</code>.
    </ParamField>

<ParamField path="usage_metadata" type="object">
      Use this field to send token counts and/or costs with your model's output. See [this guide](/langsmith/log-llm-trace#provide-token-and-cost-information) for more details.
    </ParamField>
  </ParamField>
</Expandable>

## Converting custom I/O formats into LangSmith compatible formats

If you're using a custom input or output format, you can convert it to a LangSmith compatible format using `process_inputs`/`processInputs` and `process_outputs`/`processOutputs` functions on the [`@traceable` decorator](https://docs.smith.langchain.com/reference/python/run_helpers/langsmith.run_helpers.traceable) (Python) or [`traceable` function](https://docs.smith.langchain.com/reference/js/functions/traceable.traceable) (TS).

`process_inputs`/`processInputs` and `process_outputs`/`processOutputs` accept functions that allow you to transform the inputs and outputs of a specific trace before they are logged to LangSmith. They have access to the trace's inputs and outputs, and can return a new dictionary with the processed data.

Here's a boilerplate example of how to use `process_inputs` and `process_outputs` to convert a custom I/O format into a LangSmith compatible format:

<Expandable title="the code">
  <CodeGroup>
    
  </CodeGroup>
</Expandable>

## Identifying a custom model in traces

When using a custom model, it is recommended to also provide the following `metadata` fields to identify the model when viewing traces and when filtering.

* `ls_provider`: The provider of the model, eg "openai", "anthropic", etc.
* `ls_model_name`: The name of the model, eg "gpt-4o-mini", "claude-3-opus-20240229", etc.

This code will log the following trace:

<div style={{ textAlign: 'center' }}>
  <img className="block dark:hidden" src="https://mintcdn.com/langchain-5e9cc07a/9cRCWDFnPjFk6hYc/langsmith/images/chat-model-light.png?fit=max&auto=format&n=9cRCWDFnPjFk6hYc&q=85&s=f152f49a6313d98e29d3a7b42b76c11f" alt="LangSmith UI showing an LLM call trace called ChatOpenAI with a system and human input followed by an AI Output." data-og-width="1169" width="1169" data-og-height="548" height="548" data-path="langsmith/images/chat-model-light.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/9cRCWDFnPjFk6hYc/langsmith/images/chat-model-light.png?w=280&fit=max&auto=format&n=9cRCWDFnPjFk6hYc&q=85&s=083affd641c8eb41b0fcce26c8485076 280w, https://mintcdn.com/langchain-5e9cc07a/9cRCWDFnPjFk6hYc/langsmith/images/chat-model-light.png?w=560&fit=max&auto=format&n=9cRCWDFnPjFk6hYc&q=85&s=8ff2ced008bb1be8db587c40cc4a6cd8 560w, https://mintcdn.com/langchain-5e9cc07a/9cRCWDFnPjFk6hYc/langsmith/images/chat-model-light.png?w=840&fit=max&auto=format&n=9cRCWDFnPjFk6hYc&q=85&s=8891eae04f01247ec86c6e6b3de7a9cb 840w, https://mintcdn.com/langchain-5e9cc07a/9cRCWDFnPjFk6hYc/langsmith/images/chat-model-light.png?w=1100&fit=max&auto=format&n=9cRCWDFnPjFk6hYc&q=85&s=3287ed0315422c879ff151bf2561e199 1100w, https://mintcdn.com/langchain-5e9cc07a/9cRCWDFnPjFk6hYc/langsmith/images/chat-model-light.png?w=1650&fit=max&auto=format&n=9cRCWDFnPjFk6hYc&q=85&s=f9995381307324951553d7cfe8d00cdd 1650w, https://mintcdn.com/langchain-5e9cc07a/9cRCWDFnPjFk6hYc/langsmith/images/chat-model-light.png?w=2500&fit=max&auto=format&n=9cRCWDFnPjFk6hYc&q=85&s=4fd10d74ce1253b1b3da84e49a439e33 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/langchain-5e9cc07a/9cRCWDFnPjFk6hYc/langsmith/images/chat-model-dark.png?fit=max&auto=format&n=9cRCWDFnPjFk6hYc&q=85&s=1da2f0a1adc972aa6de6df94cbfc1407" alt="LangSmith UI showing an LLM call trace called ChatOpenAI with a system and human input followed by an AI Output." data-og-width="1168" width="1168" data-og-height="563" height="563" data-path="langsmith/images/chat-model-dark.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/9cRCWDFnPjFk6hYc/langsmith/images/chat-model-dark.png?w=280&fit=max&auto=format&n=9cRCWDFnPjFk6hYc&q=85&s=6662af6b4871f8250ab39659fd594df8 280w, https://mintcdn.com/langchain-5e9cc07a/9cRCWDFnPjFk6hYc/langsmith/images/chat-model-dark.png?w=560&fit=max&auto=format&n=9cRCWDFnPjFk6hYc&q=85&s=c3986c36300cef013831eb0ba951b0fc 560w, https://mintcdn.com/langchain-5e9cc07a/9cRCWDFnPjFk6hYc/langsmith/images/chat-model-dark.png?w=840&fit=max&auto=format&n=9cRCWDFnPjFk6hYc&q=85&s=05d5f21e2509e36e8176fb8ace2c1e79 840w, https://mintcdn.com/langchain-5e9cc07a/9cRCWDFnPjFk6hYc/langsmith/images/chat-model-dark.png?w=1100&fit=max&auto=format&n=9cRCWDFnPjFk6hYc&q=85&s=464d2babbc58e4c42b3e720520af680c 1100w, https://mintcdn.com/langchain-5e9cc07a/9cRCWDFnPjFk6hYc/langsmith/images/chat-model-dark.png?w=1650&fit=max&auto=format&n=9cRCWDFnPjFk6hYc&q=85&s=db6c45b71fa4ff605edc8c7f88404ec2 1650w, https://mintcdn.com/langchain-5e9cc07a/9cRCWDFnPjFk6hYc/langsmith/images/chat-model-dark.png?w=2500&fit=max&auto=format&n=9cRCWDFnPjFk6hYc&q=85&s=cffed55abf3973cb5b908725433e001e 2500w" />
</div>

If you implement a custom streaming chat\_model, you can "reduce" the outputs into the same format as the non-streaming version. This is currently only supported in Python.

<Check>
  If `ls_model_name` is not present in `extra.metadata`, other fields might be used from the `extra.metadata` for estimating token counts. The following fields are used in the order of precedence:

1. `metadata.ls_model_name`
  2. `inputs.model`
  3. `inputs.model_name`
</Check>

To learn more about how to use the `metadata` fields, refer to the [Add metadata and tags](/langsmith/add-metadata-tags) guide.

## Provide token and cost information

LangSmith calculates costs automatically by using the [model pricing table](https://smith.langchain.com/settings/workspaces/models) when token counts are provided. To learn how LangSmith calculates token-based costs, see [this guide](/langsmith/calculate-token-based-costs).

Many models include token counts as part of the response. You can provide token counts to LangSmith in one of two ways:

1. Extract usage within your traced function and set a `usage_metadata` field on the run's metadata.
2. Return a `usage_metadata` field in your traced function outputs.

In both cases, the usage metadata you send should contain a subset of the following LangSmith-recognized fields:

<Warning>
  You cannot set any fields other than the ones listed below. You do not need to include all fields.
</Warning>

Note that the usage data can also include cost information, in case you do not want to rely on LangSmith's token-based cost formula. This is useful for models with pricing that is not linear by token type.

### Setting run metadata

You can [modify the current run's metadata](/langsmith/add-metadata-tags) with usage information within your traced function. The advantage of this approach is that you do not need to change your traced function's runtime outputs. Here's an example:

<Note>
  Requires `langsmith>=0.3.43` (Python) and `langsmith>=0.3.30` (JS/TS).
</Note>

### Setting run outputs

You can add a `usage_metadata` key to the function's response to set manual token counts and costs.

## Time-to-first-token

If you are using `traceable` or one of our SDK wrappers, LangSmith will automatically populate time-to-first-token for streaming LLM runs.
However, if you are using the `RunTree` API directly, you will need to add a `new_token` event to the run tree in order to properly populate time-to-first-token.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/log-llm-trace.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>

## Converting custom I/O formats into LangSmith compatible formats

If you're using a custom input or output format, you can convert it to a LangSmith compatible format using `process_inputs`/`processInputs` and `process_outputs`/`processOutputs` functions on the [`@traceable` decorator](https://docs.smith.langchain.com/reference/python/run_helpers/langsmith.run_helpers.traceable) (Python) or [`traceable` function](https://docs.smith.langchain.com/reference/js/functions/traceable.traceable) (TS).

`process_inputs`/`processInputs` and `process_outputs`/`processOutputs` accept functions that allow you to transform the inputs and outputs of a specific trace before they are logged to LangSmith. They have access to the trace's inputs and outputs, and can return a new dictionary with the processed data.

Here's a boilerplate example of how to use `process_inputs` and `process_outputs` to convert a custom I/O format into a LangSmith compatible format:

<Expandable title="the code">
  <CodeGroup>
```

---

## Log multimodal traces

**URL:** llms-txt#log-multimodal-traces

Source: https://docs.langchain.com/langsmith/log-multimodal-traces

LangSmith supports logging and rendering images as part of traces. This is currently supported for multimodal LLM runs.

In order to log images, use `wrap_openai`/ `wrapOpenAI` in Python or TypeScript respectively and pass an image URL or base64 encoded image as part of the input.

The image will be rendered as part of the trace in the LangSmith UI.

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multimodal.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=ff41711a0992c77f86cbc9f523e2ae93" alt="" data-og-width="1600" width="1600" data-og-height="1216" height="1216" data-path="langsmith/images/multimodal.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multimodal.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=d1119193b53a405869cbda1d17c88544 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multimodal.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=147037549a28d87622e4c7d4d15ccc2b 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multimodal.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=145003b20883ea39001adaf0eaf45529 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multimodal.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=01cc9ca22bca3312a5d4c1b356e5b8fc 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multimodal.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=047b36000ced498139fa793c1815440a 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multimodal.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=f7868994ae8fd23e9239b4b8d77ee4a0 2500w" />

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/log-multimodal-traces.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

---

## Log retriever traces

**URL:** llms-txt#log-retriever-traces

Source: https://docs.langchain.com/langsmith/log-retriever-trace

<Note>
  Nothing will break if you don't log retriever traces in the correct format and data will still be logged. However, the data will not be rendered in a way that is specific to retriever steps.
</Note>

Many LLM applications require looking up documents from vector databases, knowledge graphs, or other types of indexes. Retriever traces are a way to log the documents that are retrieved by the retriever. LangSmith provides special rendering for retrieval steps in traces to make it easier to understand and diagnose retrieval issues. In order for retrieval steps to be rendered correctly, a few small steps need to be taken.

1. Annotate the retriever step with `run_type="retriever"`.

2. Return a list of Python dictionaries or TypeScript objects from the retriever step. Each dictionary should contain the following keys:

* `page_content`: The text of the document.
   * `type`: This should always be "Document".
   * `metadata`: A python dictionary or TypeScript object containing metadata about the document. This metadata will be displayed in the trace.

The following code snippets show how to log a retrieval steps in Python and TypeScript.

The following image shows how a retriever step is rendered in a trace. The contents along with the metadata are displayed with each document.

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/retriever-trace.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=786c74f63e4c94d35535aa46ac9f38f4" alt="" data-og-width="1614" width="1614" data-og-height="736" height="736" data-path="langsmith/images/retriever-trace.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/retriever-trace.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=c2e2720a208cc2402e869e214609ec21 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/retriever-trace.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=8949f5484519221d159191e9437a862f 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/retriever-trace.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=7f376f3ca4575ffc08eb7fc6bdc0db4c 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/retriever-trace.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=06ac1c1dfa24dab91376f2d641fe9aa2 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/retriever-trace.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=5046d388f4583d270740adfbd1e58539 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/retriever-trace.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=aa3aae9c839dd00ca450cfeb7b285371 2500w" />

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/log-retriever-trace.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

---

## Log traces to a specific project

**URL:** llms-txt#log-traces-to-a-specific-project

**Contents:**
- Set the destination project statically
- Set the destination project dynamically

Source: https://docs.langchain.com/langsmith/log-traces-to-project

You can change the destination project of your traces both statically through environment variables and dynamically at runtime.

## Set the destination project statically

As mentioned in the [Tracing Concepts](/langsmith/observability-concepts#projects) section, LangSmith uses the concept of a `Project` to group traces. If left unspecified, the project is set to `default`. You can set the `LANGSMITH_PROJECT` environment variable to configure a custom project name for an entire application run. This should be done before executing your application.

<Warning>
  The `LANGSMITH_PROJECT` flag is only supported in JS SDK versions >= 0.2.16, use `LANGCHAIN_PROJECT` instead if you are using an older version.
</Warning>

If the project specified does not exist, it will be created automatically when the first trace is ingested.

## Set the destination project dynamically

You can also set the project name at program runtime in various ways, depending on how you are [annotating your code for tracing](/langsmith/annotate-code). This is useful when you want to log traces to different projects within the same application.

<Note>
  Setting the project name dynamically using one of the below methods overrides the project name set by the `LANGSMITH_PROJECT` environment variable.
</Note>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/log-traces-to-project.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
export LANGSMITH_PROJECT=my-custom-project
```

Example 2 (unknown):
```unknown

```

---

## Log user feedback using the SDK

**URL:** llms-txt#log-user-feedback-using-the-sdk

**Contents:**
- Use [create\_feedback()](https://docs.smith.langchain.com/reference/python/client/langsmith.client.Client#langsmith.client.Client.create_feedback) / [createFeedback()](https://docs.smith.langchain.com/reference/js/classes/client.Client#createfeedback)

Source: https://docs.langchain.com/langsmith/attach-user-feedback

<Tip>
  **Key concepts**

* [Conceptual guide on tracing and feedback](/langsmith/observability-concepts)
  * [Reference guide on feedback data format](/langsmith/feedback-data-format)
</Tip>

LangSmith makes it easy to attach feedback to traces.
This feedback can come from users, annotators, automated evaluators, etc., and is crucial for monitoring and evaluating applications.

## Use [create\_feedback()](https://docs.smith.langchain.com/reference/python/client/langsmith.client.Client#langsmith.client.Client.create_feedback) / [createFeedback()](https://docs.smith.langchain.com/reference/js/classes/client.Client#createfeedback)

Here we'll walk through how to log feedback using the SDK.

<Info>
  **Child runs**
  You can attach user feedback to ANY child run of a trace, not just the trace (root run) itself.
  This is useful for critiquing specific steps of the LLM application, such as the retrieval step or generation step of a RAG pipeline.
</Info>

<Tip>
  **Non-blocking creation (Python only)**
  The Python client will automatically background feedback creation if you pass `trace_id=` to [create\_feedback()](https://docs.smith.langchain.com/reference/python/client/langsmith.client.Client#langsmith.client.Client.create_feedback).
  This is essential for low-latency environments, where you want to make sure your application isn't blocked on feedback creation.
</Tip>

You can even log feedback for in-progress runs using `create_feedback() / createFeedback()`. See [this guide](/langsmith/access-current-span) for how to get the run ID of an in-progress run.

To learn more about how to filter traces based on various attributes, including user feedback, see [this guide](/langsmith/filter-traces-in-application).

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/attach-user-feedback.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

---

## longlived: "7776000"  # 90 days (default is 400 days)

**URL:** llms-txt#longlived:-"7776000"--#-90-days-(default-is-400-days)

---

## Long-term memory

**URL:** llms-txt#long-term-memory

**Contents:**
- Overview
- Memory storage
- Read long-term memory in tools
- Write long-term memory from tools

Source: https://docs.langchain.com/oss/javascript/langchain/long-term-memory

LangChain agents use [LangGraph persistence](/oss/javascript/langgraph/persistence#memory-store) to enable long-term memory. This is a more advanced topic and requires knowledge of LangGraph to use.

LangGraph stores long-term memories as JSON documents in a [store](/oss/javascript/langgraph/persistence#memory-store).

Each memory is organized under a custom `namespace` (similar to a folder) and a distinct `key` (like a file name). Namespaces often include user or org IDs or other labels that makes it easier to organize information.

This structure enables hierarchical organization of memories. Cross-namespace searching is then supported through content filters.

For more information about the memory store, see the [Persistence](/oss/javascript/langgraph/persistence#memory-store) guide.

## Read long-term memory in tools

<a id="write-long-term" />

## Write long-term memory from tools

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/long-term-memory.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
For more information about the memory store, see the [Persistence](/oss/javascript/langgraph/persistence#memory-store) guide.

## Read long-term memory in tools
```

Example 2 (unknown):
```unknown
<a id="write-long-term" />

## Write long-term memory from tools
```

---

## Make conversations private

**URL:** llms-txt#make-conversations-private

**Contents:**
- Prerequisites
- 1. Add resource authorization

Source: https://docs.langchain.com/langsmith/resource-auth

In this tutorial, you will extend [the chatbot created in the last tutorial](/langsmith/set-up-custom-auth) to give each user their own private conversations. You'll add [resource-level access control](/langsmith/auth#single-owner-resources) so users can only see their own threads.

<img src="https://mintcdn.com/langchain-5e9cc07a/IMK8wJkjSpMCGODD/langsmith/images/authorization.png?fit=max&auto=format&n=IMK8wJkjSpMCGODD&q=85&s=8daa07dd8efb13d7f9d7aa35117b2138" alt="Authorization flow: after authentication, an authorization handler tags each resource with owner=user id and returns a filter so users only see their own threads." data-og-width="2617" width="2617" data-og-height="1673" height="1673" data-path="langsmith/images/authorization.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/IMK8wJkjSpMCGODD/langsmith/images/authorization.png?w=280&fit=max&auto=format&n=IMK8wJkjSpMCGODD&q=85&s=a617e2e62772c307a7b69a78e627ac40 280w, https://mintcdn.com/langchain-5e9cc07a/IMK8wJkjSpMCGODD/langsmith/images/authorization.png?w=560&fit=max&auto=format&n=IMK8wJkjSpMCGODD&q=85&s=c978257553e23b1cb19348959ed72ffc 560w, https://mintcdn.com/langchain-5e9cc07a/IMK8wJkjSpMCGODD/langsmith/images/authorization.png?w=840&fit=max&auto=format&n=IMK8wJkjSpMCGODD&q=85&s=62370abd0f9ca0093d252fd9c1f7cda8 840w, https://mintcdn.com/langchain-5e9cc07a/IMK8wJkjSpMCGODD/langsmith/images/authorization.png?w=1100&fit=max&auto=format&n=IMK8wJkjSpMCGODD&q=85&s=6388bdfb9d7a61105c32683b8b750db1 1100w, https://mintcdn.com/langchain-5e9cc07a/IMK8wJkjSpMCGODD/langsmith/images/authorization.png?w=1650&fit=max&auto=format&n=IMK8wJkjSpMCGODD&q=85&s=95ff1d87241c5b94fd38483350159abb 1650w, https://mintcdn.com/langchain-5e9cc07a/IMK8wJkjSpMCGODD/langsmith/images/authorization.png?w=2500&fit=max&auto=format&n=IMK8wJkjSpMCGODD&q=85&s=bd94714727ddb6d79a49109d4ca7014f 2500w" />

Before you start this tutorial, ensure you have the [bot from the first tutorial](/langsmith/set-up-custom-auth) running without errors.

## 1. Add resource authorization

Recall that in the last tutorial, the [`Auth`](/langsmith/langgraph-python-sdk#langgraph_sdk.auth.Auth) object lets you register an [authentication function](/langsmith/auth#authentication), which LangSmith uses to validate the bearer tokens in incoming requests. Now you'll use it to register an **authorization** handler.

Authorization handlers are functions that run **after** authentication succeeds. These handlers can add [metadata](/langsmith/auth#filter-operations) to resources (like who owns them) and filter what each user can see.

Update your `src/security/auth.py` and add one authorization handler to run on every request:

```python {highlight={29-39}} title="src/security/auth.py" theme={null}
from langgraph_sdk import Auth

---

## Manage assistants

**URL:** llms-txt#manage-assistants

**Contents:**
- Create an assistant
  - LangGraph SDK
  - LangSmith UI
- Use an assistant
  - LangGraph SDK
  - LangSmith UI
- Create a new version for your assistant
  - LangGraph SDK
  - LangSmith UI
- Use a previous assistant version

Source: https://docs.langchain.com/langsmith/configuration-cloud

In this guide we will show how to create, configure, and manage an [assistant](/langsmith/assistants).

First, as a brief refresher on the concept of context, consider the following simple `call_model` node and context schema.
Observe that this node tries to read and use the `model_name` as defined by the `context` object's `model_name` field.

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>
</Tabs>

For more information on configurations, [see here](/langsmith/configuration-cloud#configuration).

## Create an assistant

To create an assistant, use the [LangGraph SDK](/langsmith/sdk) `create` method. See the [Python](/langsmith/langgraph-python-sdk#langgraph_sdk.client.AssistantsClient.create) and [JS](/langsmith/langgraph-js-ts-sdk#create) SDK reference docs for more information.

This example uses the same context schema as above, and creates an assistant with `model_name` set to `openai`.

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

You can also create assistants from the LangSmith UI.

Inside your deployment, select the "Assistants" tab. This will load a table of all of the assistants in your deployment, across all graphs.

To create a new assistant, select the "+ New assistant" button. This will open a form where you can specify the graph this assistant is for, as well as provide a name, description, and the desired configuration for the assistant based on the configuration schema for that graph.

To confirm, click "Create assistant". This will take you to [Studio](/langsmith/studio) where you can test the assistant. If you go back to the "Assistants" tab in the deployment, you will see the newly created assistant in the table.

We have now created an assistant called "Open AI Assistant" that has `model_name` defined as `openai`. We can now use this assistant with this configuration:

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

Inside your deployment, select the "Assistants" tab. For the assistant you would like to use, click the **Studio** button. This will open Studio with the selected assistant. When you submit an input (either in Graph or Chat mode), the selected assistant and its configuration will be used.

## Create a new version for your assistant

To edit the assistant, use the `update` method. This will create a new version of the assistant with the provided edits. See the [Python](/langsmith/langgraph-python-sdk#langgraph_sdk.client.AssistantsClient.update) and [JS](/langsmith/langgraph-js-ts-sdk#update) SDK reference docs for more information.

<Note>
  **Note**
  You must pass in the ENTIRE context (and metadata if you are using it). The update endpoint creates new versions completely from scratch and does not rely on previous versions.
</Note>

For example, to update your assistant's system prompt:

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

This will create a new version of the assistant with the updated parameters and set this as the active version of your assistant. If you now run your graph and pass in this assistant id, it will use this latest version.

You can also edit assistants from the LangSmith UI.

Inside your deployment, select the "Assistants" tab. This will load a table of all of the assistants in your deployment, across all graphs.

To edit an existing assistant, select the "Edit" button for the specified assistant. This will open a form where you can edit the assistant's name, description, and configuration.

Additionally, if using Studio, you can edit the assistants and create new versions via the "Manage Assistants" button.

## Use a previous assistant version

You can also change the active version of your assistant. To do so, use the `setLatest` method.

In the example above, to rollback to the first version of the assistant:

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

If you now run your graph and pass in this assistant id, it will use the first version of the assistant.

If using Studio, to set the active version of your assistant, click the "Manage Assistants" button and locate the assistant you would like to use. Select the assistant and the version, and then click the "Active" toggle. This will update the assistant to make the selected version active.

<Warning>
  **Deleting Assistants**
  Deleting as assistant will delete ALL of its versions. There is currently no way to delete a single version, but by pointing your assistant to the correct version you can skip any versions that you don't wish to use.
</Warning>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/configuration-cloud.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
</Tab>

  <Tab title="Javascript">
```

Example 2 (unknown):
```unknown
</Tab>
</Tabs>

For more information on configurations, [see here](/langsmith/configuration-cloud#configuration).

## Create an assistant

### LangGraph SDK

To create an assistant, use the [LangGraph SDK](/langsmith/sdk) `create` method. See the [Python](/langsmith/langgraph-python-sdk#langgraph_sdk.client.AssistantsClient.create) and [JS](/langsmith/langgraph-js-ts-sdk#create) SDK reference docs for more information.

This example uses the same context schema as above, and creates an assistant with `model_name` set to `openai`.

<Tabs>
  <Tab title="Python">
```

Example 3 (unknown):
```unknown
</Tab>

  <Tab title="Javascript">
```

Example 4 (unknown):
```unknown
</Tab>

  <Tab title="CURL">
```

---

## Manage billing in your account

**URL:** llms-txt#manage-billing-in-your-account

**Contents:**
- Set up billing for your account
  - Developer Plan: set up billing on your personal organization
  - Plus Plan: set up billing on a shared organization
  - Set up billing for accounts created before pricing introduction
- Update your information
  - Invoice email
  - Business information and tax ID
- Optimize your tracing spend
  - Understand your current usage
  - Optimization 1: manage data retention

Source: https://docs.langchain.com/langsmith/billing

This page describes how to manage billing for your LangSmith organization:

* [Set up billing for your account](#set-up-billing-for-your-account): Complete the billing setup process for Developer and Plus plans, including special instructions for legacy accounts.
* [Update your information](#update-your-information): Modify invoice email addresses, business information, and tax IDs for your organization.
* [Optimize your tracing spend](#optimize-your-tracing-spend): Learn how to reduce costs through data retention management and usage limits.

## Set up billing for your account

<Note>
  Before using this guide, note the following:

* If you are interested in the [Enterprise](https://www.langchain.com/pricing) plan, please [contact sales](https://www.langchain.com/contact-sales). This guide is only for our self-serve billing plans.
  * If you created your LangSmith organization before pricing was introduced on April 2nd, 2024, please [skip to the final section](#set-up-billing-for-accounts-created-before-pricing-was-introduced-on-april-2-2024).
</Note>

To set up billing for your LangSmith organization, navigate to the [Usage and Billing](https://smith.langchain.com/settings/payments) page under **Settings**. Depending on your organization's settings, there are different setup guides:

* [Developer plan](#developer-plan%3A-set-up-billing-on-your-personal-organization)
* [Plus plan](#plus-plan%3A-set-up-billing-on-a-shared-organization)
* [Setup for accounts created before April 2, 2024 pricing introduction](#set-up-billing-for-accounts-created-before-pricing-introduction)

### Developer Plan: set up billing on your personal organization

Personal organizations are limited to 5000 traces per month until a credit card is added. You can add a credit card on the **Plans and Billing** page as follows:

1. Click **Set up Billing**.
2. Add your credit card information. After this step, you will no longer be rate limited to 5000 traces, and you will be charged for any excess traces at rates specified on the [pricing](https://www.langchain.com/pricing-langsmith) page.

### Plus Plan: set up billing on a shared organization

If you have not yet created an organization, you need to follow [this guide](/langsmith/set-up-a-workspace#set-up-an-organization) before setting up billing. The following steps assume you are already in a new organization.

<Note>
  You can't use a new organization until you enter credit card information. After you complete the following steps, you will gain complete access to LangSmith.
</Note>

1. Click **Subscribe** on the **Plus** page.
   <Note>
     If you are a startup building with AI, instead click **Apply Now** on the Startup Plan. You may be eligible for discounted prices and a free, monthly trace allotment.
   </Note>
2. Review your existing members. Before subscribing, LangSmith lets you remove any added users that you do **not** want to be included in the bill.
3. Enter your credit card information. Then, enter business information, invoice email, and tax ID. If this organization belongs to a business, check the **This is a business** checkbox and enter the information accordingly.

For more information, refer to the [Update your information section](#update-your-information).

Once this step is complete, your organization will have access to the rest of LangSmith.

### Set up billing for accounts created before pricing introduction

If you joined LangSmith before pricing was introduced on April 2, 2024, you have the option to upgrade your existing account to set up billing. If you did not set up billing by July 8, 2024, then your account is now rate limited to a maximum of 5,000 traces per month.

1. Navigate to the [Settings](https://smith.langchain.com/settings) page.
2. Click **Set up Billing**.

<img src="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/setup-billing-legacy.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=08c998935eacc89e1627c7b2f779ae85" alt="" data-og-width="3238" width="3238" data-og-height="1940" height="1940" data-path="langsmith/images/setup-billing-legacy.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/setup-billing-legacy.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=56bf31c4ae825446f76b858a6193fc6d 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/setup-billing-legacy.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=9e72f35d8282ea3ce30d0b6f3e56f175 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/setup-billing-legacy.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=0d44f6c0a8e356bfd81786be01a00053 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/setup-billing-legacy.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=8781b4d23063bbdcb8aad7da484358dc 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/setup-billing-legacy.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=f57c672319acaee29f0530d3c1b029a9 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/setup-billing-legacy.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=f451b47e0b730c4a59c69b54adf42781 2500w" />

3. Enter your credit card information. If you are on a Personal organization, this will add you to the Developer plan. If you are on a shared organization, this will add you to the Plus plan. For more information, refer to the guides for the [Developer](#developer-plan%3A-set-up-billing-on-your-personal-organization) or [Plus](#plus-plan%3A-set-up-billing-on-a-shared-organization) plans respectively, starting at step 2.
4. Claim free credits as a thank you for being an early LangSmith user.

## Update your information

To update business information for your LangSmith organization, head to the [Usage and Billing](https://smith.langchain.com/settings/payments) page under **Settings** and click on the [Plans and Billing](https://smith.langchain.com/settings/payments?tab=2) tab.

<Note>
  Business information, tax ID, and invoice email can only be updated for the Plus and Startup plans. Free and Developer plans cannot update this information.
</Note>

<img src="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/update-invoice-email.png?fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=450d0bd275f48aeace29d930a60280ab" alt="" data-og-width="1460" width="1460" data-og-height="1268" height="1268" data-path="langsmith/images/update-invoice-email.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/update-invoice-email.png?w=280&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=0178456f9cfde7888d240102888c3653 280w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/update-invoice-email.png?w=560&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=6781adf6ee1fc51996e9dd5040d98efc 560w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/update-invoice-email.png?w=840&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=df5a872bda147196cfd3234981e379c8 840w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/update-invoice-email.png?w=1100&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=beeef4657cd4455474f8482b25b8bdd5 1100w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/update-invoice-email.png?w=1650&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=8581a701fb9bdcde17e9f31ef40a2375 1650w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/update-invoice-email.png?w=2500&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=3538905553105a39ad253a58777623d8 2500w" />

To update the email address for invoices, follow these steps:

1. Navigate to the **Plans and Billing** tab.
2. Locate the section beneath the payment method, where the current invoice email is displayed.
3. Enter the new email address for invoices in the provided field.
4. The new email address will be automatically saved.

You will receive all future invoices to the updated email address.

### Business information and tax ID

<Note>
  In certain jurisdictions, LangSmith is required to collect sales tax. If you are a business, providing your tax ID may qualify you for a sales tax exemption.
</Note>

<img src="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/update-business-info.png?fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=651639e60dd1c62d8d745bd63f84bc64" alt="" data-og-width="2030" width="2030" data-og-height="1268" height="1268" data-path="langsmith/images/update-business-info.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/update-business-info.png?w=280&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=c0a75f993ca566f9aee49d3abdc05fff 280w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/update-business-info.png?w=560&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=de0e8d12b53caca16cb615b23b517e22 560w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/update-business-info.png?w=840&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=c8fc4605490916ed8362fd1a23eef711 840w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/update-business-info.png?w=1100&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=e4651854e9979a335e6976ce174f9d81 1100w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/update-business-info.png?w=1650&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=4789d1e8bcddd8e4b9a864568301b9b5 1650w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/update-business-info.png?w=2500&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=dfcc7869323c51030d69b80cefed7a41 2500w" />

To update your organization's business information, follow these steps:

1. Navigate to the **Plans and Billing** tab.
2. Below the invoice email section, you will find a checkbox labeled **Business**.
3. Check the **Business** checkbox if your organization belongs to a business.
4. A business information section will appear, allowing you to enter or update the following details:
   * Business Name
   * Address
   * Tax ID for applicable jurisdictions
5. A Tax ID field will appear for applicable jurisdictions after you select a country.
6. After entering the necessary information, click the **Save** button to save your changes.

This ensures that your business information is up-to-date and accurate for billing and tax purposes.

## Optimize your tracing spend

<Check>
  You may find it helpful to read the following pages, before continuing with this section on optimizing your tracing spend:

* [Data Retention Conceptual Docs](/langsmith/administration-overview#data-retention)
  * [Usage Limiting Conceptual Docs](/langsmith/administration-overview#usage-limits)
</Check>

<Note>
  Some of the features mentioned in this guide are not currently available on Enterprise plan due to its custom nature of billing. If you are on the Enterprise plan and have questions about cost optimization, reach out to your sales rep or [support@langchain.dev](mailto:support@langchain.dev).
</Note>

You will learn how to optimize existing spend and prevent future overspend in LangSmith, which includes:

1. Reducing existing costs with data retention policies.
2. Preventing future overspend with usage limits.

This tutorial will use an existing LangSmith organization with high usage. You can transfer the concepts from this example to your own organization. The example organization has three [workspaces](/langsmith/administration-overview#workspaces), one for each deployment stage (`Dev`, `Staging`, and `Prod`):

<img src="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/workspaces.png?fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=56195e0ab898707658e8456028ce1b2d" alt="" data-og-width="3248" width="3248" data-og-height="1940" height="1940" data-path="langsmith/images/workspaces.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/workspaces.png?w=280&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=f6d4464e1f6939b371eb1d5c07c93d23 280w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/workspaces.png?w=560&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=2a959511f37aeeefcc10fb789288c11c 560w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/workspaces.png?w=840&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=f0a3270d5e8cd2604acf52476f51fc50 840w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/workspaces.png?w=1100&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=dd4d5e690cd066fb6b0e293fa6ee5a4a 1100w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/workspaces.png?w=1650&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=bdf8558780bba78264fee212685bc0ae 1650w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/workspaces.png?w=2500&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=659d88e90f8eaae9d0b6da822b4781d0 2500w" />

### Understand your current usage

The first step of any optimization process is to understand current usage. LangSmith provides two ways to do this: [Usage graph](#usage-graph) and [Invoices](#invoices).

The usage graph lets you examine how much of each usage-based pricing metric you have consumed. It does not directly show spend (which you will review later in the draft invoice).

You can navigate to the usage graph under **Settings** -> **Usage and Billing** -> **Usage Graph**.

<img src="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/usage-graph.png?fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=e584af6597a02d72785204dadf72da54" alt="" data-og-width="2316" width="2316" data-og-height="1864" height="1864" data-path="langsmith/images/usage-graph.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/usage-graph.png?w=280&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=6dcfe25fe752262bc22d64dce2f8e4ac 280w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/usage-graph.png?w=560&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=67bdc7704b8388f23c84a760216dca6c 560w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/usage-graph.png?w=840&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=c4ae70ea92f0e29a2c984f34942a0c4f 840w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/usage-graph.png?w=1100&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=4baf1b3f573716301e7bd4792af7e533 1100w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/usage-graph.png?w=1650&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=69ac54e38c7a44e907c7e569493421f0 1650w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/usage-graph.png?w=2500&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=1727041cd6b318ca57f7ca0421beb6ff 2500w" />

This graph shows that there are two usage metrics that LangSmith charges for:

* LangSmith Traces (Base Charge): tracks all traces that you send to LangSmith.
* LangSmith Traces (Extended Data Retention Upgrades): tracks all traces that also have our Extended 400 Day Data Retention.

For more details, refer to the [data retention conceptual docs](/langsmith/administration-overview#data-retention). Notice that these graphs look identical, which you will review later in the tutorial.

LangSmith Traces usage is measured per workspace, because workspaces often represent development environments (as in the example), or teams within an organization. As a LangSmith administrator, you may want to understand spend granularly per each of these units. In this case where you just want to cut spend, you can focus on the environment responsible for the majority of costs first for the greatest savings.

You understand what usage looks like in terms of traces, but you now need to translate that into spend. To do so, navigate to the **Invoices** tab. The first invoice that will appear on screen is a draft of your current month's invoice, which shows your running spend thus far this month.

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/invoice-investigation-v2.gif?s=303cde242c17b0cf1e5e7bd14d6a1d8c" alt="" data-og-width="1148" width="1148" data-og-height="720" height="720" data-path="langsmith/images/invoice-investigation-v2.gif" data-optimize="true" data-opv="3" />

<Note>
  LangSmith's Usage Graph and Invoice use the term `tenant_id` to refer to a workspace ID. They are interchangeable.
</Note>

In the GIF, you'll see that the charges for LangSmith Traces are broken up by "tenant\_id" (i.e., workspace ID), which means you can track tracing spend on each of the workspaces. In the first few days of June, the vast majority of the total spend of roughly \$2,000 is in the production workspace. Further, the majority of spend in that workspace was on extended data retention trace upgrades.

These upgrades occur for two reasons:

1. You use extended data retention tracing, which means by default your traces are retained for 400 days.
2. You use base data retention tracing and use a feature that automatically extends the data retention of a trace. ([Refer to the Auto-Upgrade conceptual docs](/langsmith/administration-overview#data-retention).)

Given that the number of total traces per day is equal to the number of extended retention traces per day, it's most likely the case that this organization is using extended data retention tracing everywhere. As a result, start by optimizing the retention settings.

### Optimization 1: manage data retention

LangSmith charges differently based on a trace's [data retention](/langsmith/administration-overview#data-retention), where short-lived traces are an order of magnitude less expensive than ones that last for a long time. In this optimization, you'll learn how to get optimal settings for data retention without sacrificing historical observability, and see the effect it has on the bill.

#### Change org level retention defaults for new projects

Navigate to the **Usage configuration** tab, and look at the organization level retention settings. Modifying this setting affects all **new projects** that are created going forward in all workspaces in the organizaton.

<Note>
  For backwards compatibility, older organizations may have this defaulted to **Extended**. Organizations created after June 3rd, 2024 have this defaulted to **Base**.
</Note>

<img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p1orgretention-v2.png?fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=135edb99a5db7123452fddfed8eb85f5" alt="" data-og-width="3248" width="3248" data-og-height="1940" height="1940" data-path="langsmith/images/p1orgretention-v2.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p1orgretention-v2.png?w=280&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=0cbeba57f9d57a309a425a30f48b80c8 280w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p1orgretention-v2.png?w=560&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=2bd1b692312d207fe53ab19e2724695b 560w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p1orgretention-v2.png?w=840&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=0f1ce4db73658d634b76406c74b74235 840w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p1orgretention-v2.png?w=1100&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=01fe2a6fc65bf6c20033d3094b42d86f 1100w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p1orgretention-v2.png?w=1650&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=ceb966a10860622f20be10d95fabe616 1650w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p1orgretention-v2.png?w=2500&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=7399b5721195c33f79d0a4419361338d 2500w" />

#### Change project level retention defaults

Data retention settings are adjustable per project on the tracing project page.

Navigate to **Projects** > ***Your project name*** > Select **Retention** and modify the default retention of the project to **Base**. This will only affect retention (and pricing) for **traces going forward**.

<img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p1projectretention.png?fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=394b513df5ef31d0309f5f3c78bd315a" alt="" data-og-width="1358" width="1358" data-og-height="452" height="452" data-path="langsmith/images/p1projectretention.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p1projectretention.png?w=280&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=0ebc83ac05d14858da153e707bd02f6b 280w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p1projectretention.png?w=560&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=0baaf55ed3a4719c2b3c3545778e1ded 560w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p1projectretention.png?w=840&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=30989a6636683bca7e456ea1897c2986 840w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p1projectretention.png?w=1100&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=ee7c87a83e1a15bc4d843bae3ad32811 1100w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p1projectretention.png?w=1650&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=7947cbde975873a53d197019453485e5 1650w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p1projectretention.png?w=2500&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=53e1f6edb92789ee5034d5f7f1153af6 2500w" />

#### Apply extended data retention to a percentage of traces

You may not want all traces to expire after 14 days. You can automatically extend the retention of traces that match some criteria by creating an [automation rule](/langsmith/rules). You might want to apply extended data retention to specific types of traces, such as:

* 10% of all traces: For general analysis or analyzing trends long term.
* Errored traces: To investigate and debug issues thoroughly.
* Traces with specific metadata: For long-term examination of particular features or user flows.

1. Navigate to **Projects** > ***Your project name*** > Select **+ New** > Select **New Automation**.
2. Name your rule and optionally apply filters or a sample rate. For more information on configuring filters, refer to [filtering techniques](/langsmith/filter-traces-in-application#filter-operators).

<Note>
  When an automation rule matches any [run](/langsmith/observability-concepts#runs) within a [trace](/langsmith/observability-concepts#traces), then all runs within the trace are upgraded to be retained for 400 days.
</Note>

For example, this is the expected configuration to keep 10% of all traces for extended data retention:

<img src="https://mintcdn.com/langchain-5e9cc07a/Xbr8HuVd9jPi6qTU/langsmith/images/P2SampleTraces.png?fit=max&auto=format&n=Xbr8HuVd9jPi6qTU&q=85&s=09bbdf5ef7cf3a5a99d6bf0a704e2143" alt="" data-og-width="640" width="640" data-og-height="610" height="610" data-path="langsmith/images/P2SampleTraces.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Xbr8HuVd9jPi6qTU/langsmith/images/P2SampleTraces.png?w=280&fit=max&auto=format&n=Xbr8HuVd9jPi6qTU&q=85&s=b40ff90a0e3ef5f52ecda25965ec1daa 280w, https://mintcdn.com/langchain-5e9cc07a/Xbr8HuVd9jPi6qTU/langsmith/images/P2SampleTraces.png?w=560&fit=max&auto=format&n=Xbr8HuVd9jPi6qTU&q=85&s=4381f3239c4d5c71c8ccd9dcdb4fe859 560w, https://mintcdn.com/langchain-5e9cc07a/Xbr8HuVd9jPi6qTU/langsmith/images/P2SampleTraces.png?w=840&fit=max&auto=format&n=Xbr8HuVd9jPi6qTU&q=85&s=81da02c26dcc067fbbc4a4d7de318dee 840w, https://mintcdn.com/langchain-5e9cc07a/Xbr8HuVd9jPi6qTU/langsmith/images/P2SampleTraces.png?w=1100&fit=max&auto=format&n=Xbr8HuVd9jPi6qTU&q=85&s=5f703039c242ba31d192fdb8f4742362 1100w, https://mintcdn.com/langchain-5e9cc07a/Xbr8HuVd9jPi6qTU/langsmith/images/P2SampleTraces.png?w=1650&fit=max&auto=format&n=Xbr8HuVd9jPi6qTU&q=85&s=e293a6cca1f7da6c8ca62b64e249f6b2 1650w, https://mintcdn.com/langchain-5e9cc07a/Xbr8HuVd9jPi6qTU/langsmith/images/P2SampleTraces.png?w=2500&fit=max&auto=format&n=Xbr8HuVd9jPi6qTU&q=85&s=558b6d18d598241a2f81e8327b13ce3d 2500w" />

If you want to keep a subset of traces for **longer than 400 days** for data collection purposes, you can create another run rule that sends some runs to a dataset of your choosing. A dataset allows you to store the trace inputs and outputs (e.g., as a key-value dataset), and will persist indefinitely, even after the trace gets deleted.

#### See results after 7 days

While the total amount of traces per day stayed the same, the extended data retention traces was cut heavily. In the invoice, the spend reduced to roughly \$900 in the last 7 days, as opposed to \$2,000 in the previous 4. That's a cost reduction of nearly 75% per day.

<img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p1endresultinvoice-v2.png?fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=6ca63d6ab2e222fe98afeefb53c39276" alt="" data-og-width="3248" width="3248" data-og-height="1940" height="1940" data-path="langsmith/images/p1endresultinvoice-v2.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p1endresultinvoice-v2.png?w=280&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=03bc474effd5dd77a62d255b6bafa278 280w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p1endresultinvoice-v2.png?w=560&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=6de7c29f70e1db30eb270380386aa447 560w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p1endresultinvoice-v2.png?w=840&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=ebe1dcde6d3270fcad3d209b9cc67950 840w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p1endresultinvoice-v2.png?w=1100&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=4ec173a612b3e60ef6f29d1aef383662 1100w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p1endresultinvoice-v2.png?w=1650&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=b2b1590c5bc6af8083d91136eb613a8f 1650w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p1endresultinvoice-v2.png?w=2500&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=15c2d1ef150646f045ecb79adcf3d766 2500w" />

### Optimization 2: limit usage

In the previous section, you managed data retention settings to **optimize existing spend**. In this section, you will use usage limits to **prevent future overspend**.

LangSmith has two usage limits: total traces and extended retention traces. These correspond to the two metrics tracked on the [usage graph](#usage-graph). You can use these in tandem to have granular control over spend.

To set limits, navigate back to **Settings** -> **Usage and Billing** -> **Usage configuration**. There is a table at the bottom of the page that lets you set usage limits per workspace. For each workspace, the two limits appear, along with a cost estimate:

<img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p2usagelimitsempty-v2.png?fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=bbd59c73ade4d3f52062bb8261ca3cdb" alt="" data-og-width="3248" width="3248" data-og-height="1940" height="1940" data-path="langsmith/images/p2usagelimitsempty-v2.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p2usagelimitsempty-v2.png?w=280&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=b5b3ff431b66b82373a5ad092c7a7569 280w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p2usagelimitsempty-v2.png?w=560&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=bbbb1b2ec896daa2e313f0d904842c44 560w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p2usagelimitsempty-v2.png?w=840&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=2969c0fdd5dd9ac09452d013e6648d87 840w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p2usagelimitsempty-v2.png?w=1100&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=023637ca6beb9ba97913f052d4429498 1100w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p2usagelimitsempty-v2.png?w=1650&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=5a3b1b709d8bb4bc7ff9db2c5bdca576 1650w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p2usagelimitsempty-v2.png?w=2500&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=76dd6e90d1e69a469aa78c37aa71ce46 2500w" />

Start by setting limits on production usage, since that is where the majority of spend comes from.

#### Set a good total traces limit

Picking the right total traces limit depends on the expected load of traces that you will send to LangSmith. It is important to consider potential growth before setting a limit. For example:

* **Current Load**: The gen AI application is called between 1.2-1.5 times per second, and each API request has a trace associated with it, meaning it logs around 100,000-130,000 traces per day.
* **Expected Growth in Load**: The expectation is that this will double in size in the near future.

From these assumptions, you can calculate an approximate limit:

Click on the edit icon on the right side of the table for the **Prod** row to enter the limit.

<img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p2alllimitonly-v2.png?fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=35073744719e152c1766c5d57309e131" alt="" data-og-width="3248" width="3248" data-og-height="1940" height="1940" data-path="langsmith/images/p2alllimitonly-v2.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p2alllimitonly-v2.png?w=280&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=04a36fe432e50025a8371371e905f3dd 280w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p2alllimitonly-v2.png?w=560&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=03ade0debf4fad232aeba1bd9a3d25ae 560w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p2alllimitonly-v2.png?w=840&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=05c1b245c764ddfcd4886795a8cfc791 840w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p2alllimitonly-v2.png?w=1100&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=0f6ee99dede0d9e9b4728e47b0e9c9fc 1100w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p2alllimitonly-v2.png?w=1650&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=48c63eeaf566fb85ce91340ef109836d 1650w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p2alllimitonly-v2.png?w=2500&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=c489dbb1a608a646147bde274df81146 2500w" />

<Note>
  When set without the extended data retention traces limit, the maximum cost estimator assumes that all traces are using extended data retention.
</Note>

#### Cut maximum spend with an extended data retention limit

From [Optimization 1](#optimization-1-manage-data-retention), you learned that the easiest way to cut cost was through managing data retention. The same is true for limits. If you only want to keep roughly 10% of traces to be around more than 14 days, you can set a limit on the maximum high retention traces you can keep. This would result in `.10 * 7,800,000 = 780,000`.

<img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p2bothlimits-v2.png?fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=7c9e489f6513fa29b1ce6b47789a402a" alt="" data-og-width="3248" width="3248" data-og-height="1940" height="1940" data-path="langsmith/images/p2bothlimits-v2.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p2bothlimits-v2.png?w=280&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=157b5be0d03a9ce5c597d6de43ebd397 280w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p2bothlimits-v2.png?w=560&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=cd60626cc2e355caf40a09ec61712391 560w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p2bothlimits-v2.png?w=840&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=5149c8505dfe51d6e6a4f92877e0d928 840w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p2bothlimits-v2.png?w=1100&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=147e457bb693f14daa6b6e4a0761e305 1100w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p2bothlimits-v2.png?w=1650&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=cc15640d5a46026da950354ad2456b03 1650w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p2bothlimits-v2.png?w=2500&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=4e5d52aabfa18020d558e5706ee7d439 2500w" />

The maximum cost is cut from \~40k per month to \~7.5k per month, because you no longer allow as many expensive data retention upgrades. This ensures that new users on the platform will not accidentally cause cost to balloon.

<Note>
  The extended data retention limit can cause features other than traces to stop working once reached. If you plan to use this feature, read more about its [functionality and side effects](/langsmith/administration-overview#side-effects-of-extended-data-retention-traces-limit).
</Note>

#### Set dev/staging limits and view total spent limit across workspaces

Following a similar logic for the `dev` and `staging` environments, you can set limits at 10% of the production limit on usage for each workspace.

While this works with this usage pattern, setting good dev and staging limits may vary depending on your use case with LangSmith. For example, if you run evals as part of CI/CD in dev or staging, you may want to be more flexible with your usage limits to avoid test failures.

With the limits set, LangSmith shows a maximum spend estimate across all workspaces:

<img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p2totalspendlimits-v2.png?fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=928eb58f3d41623fa2e72e191b3d2dfb" alt="" data-og-width="3248" width="3248" data-og-height="1940" height="1940" data-path="langsmith/images/p2totalspendlimits-v2.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p2totalspendlimits-v2.png?w=280&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=808034f2e5697b33344968cd4ee722db 280w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p2totalspendlimits-v2.png?w=560&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=5da581360d8adca6c93ac8ba6c7980dd 560w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p2totalspendlimits-v2.png?w=840&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=a2fcbdccff84d7f12102a8012557a59a 840w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p2totalspendlimits-v2.png?w=1100&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=8f9eafa50af5140dff3166f629d7180d 1100w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p2totalspendlimits-v2.png?w=1650&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=0ba8e03537bf499021ee642fd4a93517 1650w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p2totalspendlimits-v2.png?w=2500&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=2576e5daea8e6b715a3ae3f46371d2c8 2500w" />

You can use the cost estimate to plan for your invoice total.

If you have questions about further optimizing your spend, please reach out to [support@langchain.dev](mailto:support@langchain.dev).

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/billing.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Manage datasets

**URL:** llms-txt#manage-datasets

**Contents:**
- Version a dataset
  - Create a new version of a dataset
  - Tag a version

Source: https://docs.langchain.com/langsmith/manage-datasets

LangSmith provides tools for managing and working with your [*datasets*](/langsmith/evaluation-concepts#datasets). This page describes dataset operations including:

* [Versioning datasets](#version-a-dataset) to track changes over time.
* [Filtering](#evaluate-on-a-filtered-view-of-a-dataset) and [splitting](#evaluate-on-a-dataset-split) datasets for evaluation.
* [Sharing datasets](#share-a-dataset) publicly.
* [Exporting datasets](#export-a-dataset) in various formats.

You'll also learn how to [export filtered traces](#export-filtered-traces-from-experiment-to-dataset) from [experiments](/langsmith/evaluation-concepts#experiment) back to datasets for further analysis and iteration.

In LangSmith, datasets are versioned. This means that every time you add, update, or delete examples in your dataset, a new version of the dataset is created.

### Create a new version of a dataset

Any time you add, update, or delete examples in your dataset, a new [version](/langsmith/evaluation-concepts#versions) of your dataset is created. This allows you to track changes to your dataset over time and understand how your dataset has evolved.

By default, the version is defined by the timestamp of the change. When you click on a particular version of a dataset (by timestamp) in the **Examples** tab, you will find the state of the dataset at that point in time.

<img src="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/version-dataset.png?fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=da312a60576f449797be71e24229ea31" alt="Version Datasets" data-og-width="2544" width="2544" data-og-height="1241" height="1241" data-path="langsmith/images/version-dataset.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/version-dataset.png?w=280&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=4ab823875dfefd0578cf98b5e7722c59 280w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/version-dataset.png?w=560&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=bbf9ef95b76653348454895c30475c88 560w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/version-dataset.png?w=840&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=4f169ff2abd8401334aea31668389cff 840w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/version-dataset.png?w=1100&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=71a528f7c942c60acf5aeff46a0abc56 1100w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/version-dataset.png?w=1650&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=b36ec02adba0f4df157298892b4eddb0 1650w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/version-dataset.png?w=2500&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=44b47e051fd375b396203b5912f5be13 2500w" />

Note that examples are read-only when viewing a past version of the dataset. You will also see the operations that were between this version of the dataset and the latest version of the dataset.

<Note>
  By default, the latest version of the dataset is shown in the **Examples** tab and experiments from all versions are shown in the **Tests** tab.
</Note>

In the **Tests** tab, you will find the results of tests run on the dataset at different versions.

<img src="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/version-dataset-tests.png?fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=42c5ac800ef7282fa65013f6de02e45a" alt="Version Datasets" data-og-width="2483" width="2483" data-og-height="963" height="963" data-path="langsmith/images/version-dataset-tests.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/version-dataset-tests.png?w=280&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=d4a0e67f595dce4ae3769d3ecf01705c 280w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/version-dataset-tests.png?w=560&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=a7743a10620ad86b18cf84dcba7943c8 560w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/version-dataset-tests.png?w=840&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=146ca947da81dbbdd8f88067d1630874 840w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/version-dataset-tests.png?w=1100&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=066ce46f2b51a55c28f9c43430b7db8b 1100w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/version-dataset-tests.png?w=1650&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=3c970aa22e36c5d8a5cc17ee1398b95a 1650w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/version-dataset-tests.png?w=2500&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=2eab0c66d507ab2f4f35f07b774aec19 2500w" />

You can also tag versions of your dataset to give them a more human-readable name, which can be useful for marking important milestones in your dataset's history.

For example, you might tag a version of your dataset as "prod" and use it to run tests against your LLM pipeline.

You can tag a version of your dataset in the UI by clicking on **+ Tag this version** in the **Examples** tab.

<img src="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tag-this-version.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=c3f7a97c92eb645f7b0888f4e35ffd48" alt="Tagging Datasets" data-og-width="662" width="662" data-og-height="124" height="124" data-path="langsmith/images/tag-this-version.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tag-this-version.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=93c44c3c1243429e10fea238ca078d37 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tag-this-version.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=99d4bd65c4b30f29c75443846c785b0a 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tag-this-version.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=34c4a8a96a8024cdebcad7815825d0a0 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tag-this-version.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=af3c62be520a88604634349c8a5884db 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tag-this-version.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=c9a00d92647ebd171976304ad4def786 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tag-this-version.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=21c96cdd5609bb4bfb5148ac7dd8e098 2500w" />

You can also tag versions of your dataset using the SDK. Here's an example of how to tag a version of a dataset using the [Python SDK](https://docs.smith.langchain.com/reference/python/reference):

```python  theme={null}
from langsmith import Client
from datetime import datetime

client = Client()
initial_time = datetime(2024, 1, 1, 0, 0, 0) # The timestamp of the version you want to tag

---

## Manage prompts

**URL:** llms-txt#manage-prompts

**Contents:**
- Commit tags
  - Create a tag
  - Move a tag
  - Delete a tag
  - Use tags in code

Source: https://docs.langchain.com/langsmith/manage-prompts

LangSmith provides several tools to help you manage your [*prompts*](/langsmith/prompt-engineering-concepts) effectively. This page describes the following features:

* [Commit tags](#commit-tags) for version control and environment management.
* [Webhook triggers](#trigger-a-webhook-on-prompt-commit) for automating workflows when prompts are updated.
* [Public prompt hub](#public-prompt-hub) for discovering and using community-created prompts.

[*Commit tags*](/langsmith/prompt-engineering-concepts#tags) are labels that reference a specific [*commit*](/langsmith/prompt-engineering-concepts#commits) in your prompt's version history. They help you mark significant versions and control which versions run in different environments. By referencing tags rather than commit IDs in your code, you can update which version is being used without modifying the code itself.

Each tag references exactly one commit, though you can reassign a tag to point to a different commit.

<Note>
  **Not to be confused with resource tags**: Commit tags are specific to prompt versioning and reference individual commits in a prompt's history. [Resource tags](/langsmith/set-up-resource-tags) are key-value pairs used to organize workspace resources like projects, datasets, and prompts. While both can use similar naming conventions (like `prod` or `staging`), commit tags control **which version** of a prompt runs, while resource tags help you **organize and filter** resources across your workspace.
</Note>

To create a tag, navigate to the **Commits** tab for a prompt. Click on the tag icon next to the commit you want to tag. Click **New Tag** and enter a name for the tag.

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/commits-tab.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=b0cb7961f70d5c0bab9af960041bc54f" alt="" data-og-width="2868" width="2868" data-og-height="992" height="992" data-path="langsmith/images/commits-tab.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/commits-tab.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=5786acfc0582c73c73efb5535a954ab4 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/commits-tab.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=410aa69f0ef5cc13ff7651df331495b7 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/commits-tab.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=b9845d256b71636a9bb6ee60c774a29e 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/commits-tab.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=e16686b15bf2a95709a5d963ffc339d0 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/commits-tab.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=cb9560e6aa774505cb9ff1857f95ae61 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/commits-tab.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=bef88e11fa0de6173fc4dfd9c9339f39 2500w" /> <img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-new-prompt-tag.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=cc27df9c5392a9b71319969c14924c61" alt="" data-og-width="1410" width="1410" data-og-height="872" height="872" data-path="langsmith/images/create-new-prompt-tag.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-new-prompt-tag.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=ed473769995e6148b60aec9cdd652ab6 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-new-prompt-tag.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=1f817be111dfc54af9717f9cdb664752 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-new-prompt-tag.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=fb0b979bf3362bd8faf796e4d617ac1c 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-new-prompt-tag.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=22c63285bd22dd0c82d53b8d46c6638b 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-new-prompt-tag.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=1b4d05163bd948c4bce8ea08b5ceb70f 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-new-prompt-tag.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=c34b5ce29b2f1914399b6a084885e09a 2500w" />

To point a tag to a different commit, click on the tag icon next to the destination commit, and select the tag you want to move. This will automatically update the tag to point to the new commit.

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/move-prompt-tag.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=3cb3c6218961cbdd8f6f1fb6d06b50e3" alt="" data-og-width="874" width="874" data-og-height="694" height="694" data-path="langsmith/images/move-prompt-tag.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/move-prompt-tag.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=c2094e7486cfd3a7f4d7979e7883d1bc 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/move-prompt-tag.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=163481cc4580e4fe7934a5e02497fb9f 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/move-prompt-tag.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=9d53ef06dc6d69cc4024821fe8cfc32d 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/move-prompt-tag.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=00ce6c08638961bfbbe5713c0066929b 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/move-prompt-tag.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=fa898ee3d7089b443e741d4f10b7845a 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/move-prompt-tag.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=cd97d2004b3b0a2ac0d3c7e4d3e7fff7 2500w" />

To delete a tag, click on the delete icon next to the tag you want to delete. This will delete the tag altogether and it will no longer be associated with any commit.

Tags provide a stable way to reference specific versions of your prompts in code. Instead of using commit hashes directly, you can reference tags that can be updated without changing your code.

Here is an example of pulling a prompt by tag in Python:

```python  theme={null}
prompt = client.pull_prompt("joke-generator:prod")

---

## Manage prompts programmatically

**URL:** llms-txt#manage-prompts-programmatically

**Contents:**
- Install packages
- Configure environment variables
- Push a prompt
- Pull a prompt
- Use a prompt without LangChain
  - OpenAI
  - Anthropic
- List, delete, and like prompts

Source: https://docs.langchain.com/langsmith/manage-prompts-programmatically

You can use the LangSmith Python and TypeScript SDK to manage prompts programmatically.

<Note>
  Previously this functionality lived in the `langchainhub` package which is now deprecated. All functionality going forward will live in the `langsmith` package.
</Note>

In Python, you can directly use the LangSmith SDK (*recommended, full functionality*) or you can use through the LangChain package (limited to pushing and pulling prompts).

In TypeScript, you must use the LangChain npm package for pulling prompts (it also allows pushing). For all other functionality, use the LangSmith package.

## Configure environment variables

If you already have `LANGSMITH_API_KEY` set to your current workspace's api key from LangSmith, you can skip this step.

Otherwise, get an API key for your workspace by navigating to `Settings > API Keys > Create API Key` in LangSmith.

Set your environment variable.

<Note>
  What we refer to as "prompts" used to be called "repos", so any references to "repo" in the code are referring to a prompt.
</Note>

To create a new prompt or update an existing prompt, you can use the `push prompt` method.

You can also push a prompt as a RunnableSequence of a prompt and a model. This is useful for storing the model configuration you want to use with this prompt. The provider must be supported by the LangSmith playground. (see settings here: [Supported Providers](https://langsmith.com/playground))

To pull a prompt, you can use the `pull prompt` method, which returns a the prompt as a langchain `PromptTemplate`.

To pull a **private prompt** you do not need to specify the owner handle (though you can, if you have one set).

To pull a **public prompt** from the LangChain Hub, you need to specify the handle of the prompt's author.

Similar to pushing a prompt, you can also pull a prompt as a RunnableSequence of a prompt and a model. Just specify include\_model when pulling the prompt. If the stored prompt includes a model, it will be returned as a RunnableSequence. Make sure you have the proper environment variables set for the model you are using.

When pulling a prompt, you can also specify a specific commit hash or [commit tag](/langsmith/manage-prompts#commit-tags) to pull a specific version of the prompt.

To pull a public prompt from the LangChain Hub, you need to specify the handle of the prompt's author.

<Note>
  For pulling prompts, if you are using Node.js or an environment that supports dynamic imports, we recommend using the `langchain/hub/node` entrypoint, as it handles deserialization of models associated with your prompt configuration automatically.

If you are in a non-Node environment, "includeModel" is not supported for non-OpenAI models and you should use the base `langchain/hub` entrypoint.
</Note>

## Use a prompt without LangChain

If you want to store your prompts in LangSmith but use them directly with a model provider's API, you can use our conversion methods. These convert your prompt into the payload required for the OpenAI or Anthropic API.

These conversion methods rely on logic from within LangChain integration packages, and you will need to install the appropriate package as a dependency in addition to your official SDK of choice. Here are some examples:

## List, delete, and like prompts

You can also list, delete, and like/unlike prompts using the `list prompts`, `delete prompt`, `like prompt` and `unlike prompt` methods. See the [LangSmith SDK client](https://github.com/langchain-ai/langsmith-sdk) for extensive documentation on these methods.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/manage-prompts-programmatically.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
</CodeGroup>

## Configure environment variables

If you already have `LANGSMITH_API_KEY` set to your current workspace's api key from LangSmith, you can skip this step.

Otherwise, get an API key for your workspace by navigating to `Settings > API Keys > Create API Key` in LangSmith.

Set your environment variable.
```

Example 4 (unknown):
```unknown
<Note>
  What we refer to as "prompts" used to be called "repos", so any references to "repo" in the code are referring to a prompt.
</Note>

## Push a prompt

To create a new prompt or update an existing prompt, you can use the `push prompt` method.

<CodeGroup>
```

---

## Manage your organization using the API

**URL:** llms-txt#manage-your-organization-using-the-api

**Contents:**
- Workspaces
- User management
  - RBAC
  - Membership management
- API keys
- Security settings
- User-only endpoints
- Sample code

Source: https://docs.langchain.com/langsmith/manage-organization-by-api

LangSmith's API supports programmatic access via API key to all of the actions available in the UI, with only a few exceptions that are noted in [User-only endpoints](#user-only-endpoints).

<Check>
  Before diving into this content, it might be helpful to read the following:

* [Conceptual guide on organizations and workspaces](/langsmith/administration-overview)
  * [Organization setup how-to guild](/langsmith/set-up-a-workspace#set-up-an-organization)
</Check>

<Note>
  There are a few limitations that will be lifted soon:

* The LangSmith SDKs do not support these organization management actions yet.
  * Organization-scoped [service keys](/langsmith/administration-overview#service-keys) with Organization Admin permission may be used for these actions.
</Note>

<Warning>
  Use the `X-Tenant-Id` header to specify which workspace to target. If the header is not present, operations will default to the workspace the API key was initially created in if it is not organization-scoped.

**If `X-Tenant-Id` is not specified when accessing workspace-scoped resources with an organization-scoped API key, the request will fail with `403 Forbidden`.**
</Warning>

Some commonly-used endpoints and use cases are listed below. For a complete list of available endpoints, see the [API docs](https://api.smith.langchain.com/redoc). **The `X-Organization-Id` header should be present on all requests, and `X-Tenant-Id` header should be present on requests that are scoped to a particular workspace.**

* [List workspaces](https://api.smith.langchain.com/redoc#tag/workspaces/operation/list_workspaces_api_v1_workspaces_get)
* [Create workspace](https://api.smith.langchain.com/redoc#tag/workspaces/operation/create_workspace_api_v1_workspaces_post)
* [Update workspace name](https://api.smith.langchain.com/redoc#tag/workspaces/operation/patch_workspace_api_v1_workspaces__workspace_id__patch)

* [List roles](https://api.smith.langchain.com/redoc#tag/orgs/operation/list_organization_roles_api_v1_orgs_current_roles_get)
* [List permissions](https://api.smith.langchain.com/redoc#tag/orgs/operation/update_organization_roles_api_v1_orgs_current_roles__role_id__patch)
* [Create role](https://api.smith.langchain.com/redoc#tag/orgs/operation/create_organization_roles_api_v1_orgs_current_roles_post)
* [Update role](https://api.smith.langchain.com/redoc#tag/orgs/operation/update_organization_roles_api_v1_orgs_current_roles__role_id__patch)

### Membership management

`List roles` under [RBAC](#rbac) should be used for retrieving role IDs of these operations. `List [organization|workspace] members` endpoints (below) response `"id"`s should be used as `identity_id` in these operations.

* [List active organization members](https://api.smith.langchain.com/redoc#tag/orgs/operation/get_current_active_org_members_api_v1_orgs_current_members_active_get)
* [List pending organization members](https://api.smith.langchain.com/redoc#tag/orgs/operation/get_current_pending_org_members_api_v1_orgs_current_members_pending_get)
* [Invite a user to the organization and one or more workspaces](https://api.smith.langchain.com/redoc#tag/orgs/operation/add_members_to_current_org_batch_api_v1_orgs_current_members_batch_post). This should be used when the user is not already a member in the organization.
* [Update a user's organization role](https://api.smith.langchain.com/redoc#tag/workspaces/operation/add_member_to_current_workspace_api_v1_workspaces_current_members_post)
* [Remove someone from the organization](https://api.smith.langchain.com/redoc#tag/orgs/operation/remove_member_from_current_org_api_v1_orgs_current_members__identity_id__delete)

* [List workspace members](https://api.smith.langchain.com/redoc#tag/workspaces/operation/get_current_workspace_members_api_v1_workspaces_current_members_get)
* [Add a member to a workspace that is already part of the organization](https://api.smith.langchain.com/redoc#tag/workspaces/operation/add_member_to_current_workspace_api_v1_workspaces_current_members_post)
* [Update a user's workspace role](https://api.smith.langchain.com/redoc#tag/workspaces/operation/add_member_to_current_workspace_api_v1_workspaces_current_members_post)
* [Remove someone from a workspace](https://api.smith.langchain.com/redoc#tag/workspaces/operation/delete_current_workspace_member_api_v1_workspaces_current_members__identity_id__delete)

<Note>
  These params should be omitted: `read_only` (deprecated), `password` and `full_name` ([basic auth](/langsmith/authentication-methods) only)
</Note>

* [Create a service key](https://api.smith.langchain.com/redoc#tag/api-key/operation/generate_api_key_api_v1_api_key_post)
* [Delete a service key](https://api.smith.langchain.com/redoc#tag/api-key/operation/delete_api_key_api_v1_api_key__api_key_id__delete)

<Note>
  Organization Admin permissions are required to make these changes.
</Note>

<Note>
  "Shared resources" in this context refer to [public prompts](/langsmith/create-a-prompt#save-your-prompt), [shared runs](/langsmith/share-trace), and [shared datasets](/langsmith/manage-datasets#share-a-dataset).
</Note>

<Warning>
  Updating these settings affects **all resources in the organization**.
</Warning>

You can update these settings under the **Settings > Shared** tab for a workspace, or via API:

* [Update organization sharing settings](https://api.smith.langchain.com/redoc#tag/orgs/operation/update_current_organization_info_api_v1_orgs_current_info_patch)
  * use `unshare_all` to unshare **ALL** shared resources in the organization - use `disable_public_sharing` to prevent future sharing of resources

These settings are only editable via API:

* [Disable/enable PAT creation](https://api.smith.langchain.com/redoc#tag/orgs/operation/update_current_organization_info_api_v1_orgs_current_info_patch) (for self-hosted, available in Helm chart version 0.11.25+)
  * Use `pat_creation_disabled` to disable PAT creation for the entire organization.
  * See the [admin guide](/langsmith/administration-overview#organization-roles) for information about the Organization Viewer role, which cannot create PATs.

## User-only endpoints

These endpoints are user-scoped and require a logged-in user's JWT, so they should only be executed through the UI.

* `/api-key/current` endpoints: these are related a user's PATs
* `/sso/email-verification/send` (Cloud-only): this endpoint is related to [SAML SSO](/langsmith/user-management)

The sample code below goes through a few common workflows related to organization management. Make sure to make necessary replacements wherever `<replace_me>` is in the code.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/manage-organization-by-api.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Matches the "thread" resource and all actions - create, read, update, delete, search

**URL:** llms-txt#matches-the-"thread"-resource-and-all-actions---create,-read,-update,-delete,-search

---

## maxReplicas: 10

**URL:** llms-txt#maxreplicas:-10

---

## maxReplicas: 160

**URL:** llms-txt#maxreplicas:-160

---

## maxReplicas: 16

**URL:** llms-txt#maxreplicas:-16

---

## maxReplicas: 20

**URL:** llms-txt#maxreplicas:-20

---

## maxReplicas: 40

**URL:** llms-txt#maxreplicas:-40

---

## maxReplicas: 4

**URL:** llms-txt#maxreplicas:-4

---

## maxReplicas: 50

**URL:** llms-txt#maxreplicas:-50

---

## maxReplicas: 5

**URL:** llms-txt#maxreplicas:-5

---

## maxReplicas: 6

**URL:** llms-txt#maxreplicas:-6

---

## MCP endpoint in Agent Server

**URL:** llms-txt#mcp-endpoint-in-agent-server

**Contents:**
- Requirements
- Usage overview
  - Client
- Expose an agent as MCP tool
  - Setting name and description
  - Schema

Source: https://docs.langchain.com/langsmith/server-mcp

The Model Context Protocol (MCP) is an open protocol for describing tools and data sources in a model-agnostic format, enabling LLMs to discover and use them via a structured API.

[Agent Server](/langsmith/agent-server) implements MCP using the [Streamable HTTP transport](https://spec.modelcontextprotocol.io/specification/2025-03-26/basic/transports/#streamable-http). This allows LangGraph **agents** to be exposed as **MCP tools**, making them usable with any MCP-compliant client supporting Streamable HTTP.

The MCP endpoint is available at `/mcp` on [Agent Server](/langsmith/agent-server).

You can set up [custom authentication middleware](/langsmith/custom-auth) to authenticate a user with an MCP server to get access to user-scoped tools within your LangSmith deployment.

An example architecture for this flow:

To use MCP, ensure you have the following dependencies installed:

* `langgraph-api >= 0.2.3`
* `langgraph-sdk >= 0.1.61`

* Upgrade to use langgraph-api>=0.2.3. If you are deploying LangSmith, this will be done for you automatically if you create a new revision.
* MCP tools (agents) will be automatically exposed.
* Connect with any MCP-compliant client that supports Streamable HTTP.

Use an MCP-compliant client to connect to the Agent Server. The following examples show how to connect using different programming languages.

<Tabs>
  <Tab title="JavaScript/TypeScript">

> **Note**
    > Replace `serverUrl` with your Agent Server URL and configure authentication headers as needed.

<Tab title="Python">
    Install the adapter with:

Here is an example of how to connect to a remote MCP endpoint and use an agent as a tool:

## Expose an agent as MCP tool

When deployed, your agent will appear as a tool in the MCP endpoint
with this configuration:

* **Tool name**: The agent's name.
* **Tool description**: The agent's description.
* **Tool input schema**: The agent's input schema.

### Setting name and description

You can set the name and description of your agent in `langgraph.json`:

After deployment, you can update the name and description using the LangGraph SDK.

Define clear, minimal input and output schemas to avoid exposing unnecessary internal complexity to the LLM.

The default [MessagesState](/oss/python/langgraph/graph-api#messagesstate) uses `AnyMessage`, which supports many message types but is too general for direct LLM exposure.

Instead, define **custom agents or workflows** that use explicitly typed input and output structures.

For example, a workflow answering documentation questions might look like this:

```python  theme={null}
from langgraph.graph import StateGraph, START, END
from typing_extensions import TypedDict

**Examples:**

Example 1 (unknown):
```unknown
## Requirements

To use MCP, ensure you have the following dependencies installed:

* `langgraph-api >= 0.2.3`
* `langgraph-sdk >= 0.1.61`

Install them with:

<CodeGroup>
```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
</CodeGroup>

## Usage overview

To enable MCP:

* Upgrade to use langgraph-api>=0.2.3. If you are deploying LangSmith, this will be done for you automatically if you create a new revision.
* MCP tools (agents) will be automatically exposed.
* Connect with any MCP-compliant client that supports Streamable HTTP.

### Client

Use an MCP-compliant client to connect to the Agent Server. The following examples show how to connect using different programming languages.

<Tabs>
  <Tab title="JavaScript/TypeScript">
```

Example 4 (unknown):
```unknown
> **Note**
    > Replace `serverUrl` with your Agent Server URL and configure authentication headers as needed.
```

---

## meaning you can use it as you would any other runnable.

**URL:** llms-txt#meaning-you-can-use-it-as-you-would-any-other-runnable.

---

## Memory

**URL:** llms-txt#memory

**Contents:**
- Add short-term memory
  - Use in production
  - Use in subgraphs
- Add long-term memory
  - Use in production
  - Use semantic search
- Manage short-term memory
  - Trim messages
  - Delete messages
  - Summarize messages

Source: https://docs.langchain.com/oss/javascript/langgraph/add-memory

AI applications need [memory](/oss/javascript/concepts/memory) to share context across multiple interactions. In LangGraph, you can add two types of memory:

* [Add short-term memory](#add-short-term-memory) as a part of your agent's [state](/oss/javascript/langgraph/graph-api#state) to enable multi-turn conversations.
* [Add long-term memory](#add-long-term-memory) to store user-specific or application-level data across sessions.

## Add short-term memory

**Short-term** memory (thread-level [persistence](/oss/javascript/langgraph/persistence)) enables agents to track multi-turn conversations. To add short-term memory:

### Use in production

In production, use a checkpointer backed by a database:

<Accordion title="Example: using Postgres checkpointer">

<Tip>
    You need to call `checkpointer.setup()` the first time you're using Postgres checkpointer
  </Tip>

If your graph contains [subgraphs](/oss/javascript/langgraph/use-subgraphs), you only need to provide the checkpointer when compiling the parent graph. LangGraph will automatically propagate the checkpointer to the child subgraphs.

If you want the subgraph to have its own memory, you can compile it with the appropriate checkpointer option. This is useful in [multi-agent](/oss/javascript/langchain/multi-agent) systems, if you want agents to keep track of their internal message histories.

## Add long-term memory

Use long-term memory to store user-specific or application-specific data across conversations.

### Use in production

In production, use a store backed by a database:

<Accordion title="Example: using Postgres store">

<Tip>
    You need to call `store.setup()` the first time you're using Postgres store
  </Tip>

### Use semantic search

Enable semantic search in your graph's memory store to let graph agents search for items in the store by semantic similarity.

<Accordion title="Long-term memory with semantic search">
  
</Accordion>

## Manage short-term memory

With [short-term memory](#add-short-term-memory) enabled, long conversations can exceed the LLM's context window. Common solutions are:

* [Trim messages](#trim-messages): Remove first or last N messages (before calling LLM)
* [Delete messages](#delete-messages) from LangGraph state permanently
* [Summarize messages](#summarize-messages): Summarize earlier messages in the history and replace them with a summary
* [Manage checkpoints](#manage-checkpoints) to store and retrieve message history
* Custom strategies (e.g., message filtering, etc.)

This allows the agent to keep track of the conversation without exceeding the LLM's context window.

Most LLMs have a maximum supported context window (denominated in tokens). One way to decide when to truncate messages is to count the tokens in the message history and truncate whenever it approaches that limit. If you're using LangChain, you can use the trim messages utility and specify the number of tokens to keep from the list, as well as the `strategy` (e.g., keep the last `maxTokens`) to use for handling the boundary.

To trim message history, use the [`trimMessages`](https://js.langchain.com/docs/how_to/trim_messages/) function:

<Accordion title="Full example: trim messages">

You can delete messages from the graph state to manage the message history. This is useful when you want to remove specific messages or clear the entire message history.

To delete messages from the graph state, you can use the `RemoveMessage`. For `RemoveMessage` to work, you need to use a state key with [`messagesStateReducer`](https://langchain-ai.github.io/langgraphjs/reference/functions/langgraph.messagesStateReducer.html) [reducer](/oss/javascript/langgraph/graph-api#reducers), like `MessagesZodState`.

To remove specific messages:

<Warning>
  When deleting messages, **make sure** that the resulting message history is valid. Check the limitations of the LLM provider you're using. For example:

* some providers expect message history to start with a `user` message
  * most providers require `assistant` messages with tool calls to be followed by corresponding `tool` result messages.
</Warning>

<Accordion title="Full example: delete messages">

### Summarize messages

The problem with trimming or removing messages, as shown above, is that you may lose information from culling of the message queue. Because of this, some applications benefit from a more sophisticated approach of summarizing the message history using a chat model.

<img src="https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/summary.png?fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=c8ed3facdccd4ef5c7e52902c72ba938" alt="" data-og-width="609" width="609" data-og-height="242" height="242" data-path="oss/images/summary.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/summary.png?w=280&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=4208b9b0cc9f459f3dc4e5219918471b 280w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/summary.png?w=560&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=7acb77c081545f57042368f4e9d0c8cb 560w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/summary.png?w=840&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=2fcfdb0c481d2e1d361e76db763a41e5 840w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/summary.png?w=1100&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=4abdac693a562788aa0db8681bef8ea7 1100w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/summary.png?w=1650&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=40acfefa91dcb11b247a6e4a7705f22b 1650w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/summary.png?w=2500&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=8d765aaf7551e8b0fc2720de7d2ac2a8 2500w" />

Prompting and orchestration logic can be used to summarize the message history. For example, in LangGraph you can include a `summary` key in the state alongside the `messages` key:

Then, you can generate a summary of the chat history, using any existing summary as context for the next summary. This `summarizeConversation` node can be called after some number of messages have accumulated in the `messages` state key.

<Accordion title="Full example: summarize messages">
  
</Accordion>

### Manage checkpoints

You can view and delete the information stored by the checkpointer.

<a id="checkpoint" />

#### View thread state

<a id="checkpoints" />

#### View the history of the thread

#### Delete all checkpoints for a thread

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/add-memory.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
### Use in production

In production, use a checkpointer backed by a database:
```

Example 2 (unknown):
```unknown
<Accordion title="Example: using Postgres checkpointer">
```

Example 3 (unknown):
```unknown
<Tip>
    You need to call `checkpointer.setup()` the first time you're using Postgres checkpointer
  </Tip>
```

Example 4 (unknown):
```unknown
</Accordion>

### Use in subgraphs

If your graph contains [subgraphs](/oss/javascript/langgraph/use-subgraphs), you only need to provide the checkpointer when compiling the parent graph. LangGraph will automatically propagate the checkpointer to the child subgraphs.
```

---

## Memory overview

**URL:** llms-txt#memory-overview

**Contents:**
- Short-term memory
  - Manage short-term memory
- Long-term memory
  - Semantic memory
  - Episodic memory
  - Procedural memory
  - Writing memories
  - Memory storage

Source: https://docs.langchain.com/oss/javascript/concepts/memory

[Memory](/oss/javascript/langgraph/add-memory) is a system that remembers information about previous interactions. For AI agents, memory is crucial because it lets them remember previous interactions, learn from feedback, and adapt to user preferences. As agents tackle more complex tasks with numerous user interactions, this capability becomes essential for both efficiency and user satisfaction.

This conceptual guide covers two types of memory, based on their recall scope:

* [Short-term memory](#short-term-memory), or [thread](/oss/javascript/langgraph/persistence#threads)-scoped memory, tracks the ongoing conversation by maintaining message history within a session. LangGraph manages short-term memory as a part of your agent's [state](/oss/javascript/langgraph/graph-api#state). State is persisted to a database using a [checkpointer](/oss/javascript/langgraph/persistence#checkpoints) so the thread can be resumed at any time. Short-term memory updates when the graph is invoked or a step is completed, and the State is read at the start of each step.
* [Long-term memory](#long-term-memory) stores user-specific or application-level data across sessions and is shared *across* conversational threads. It can be recalled *at any time* and *in any thread*. Memories are scoped to any custom namespace, not just within a single thread ID. LangGraph provides [stores](/oss/javascript/langgraph/persistence#memory-store) ([reference doc](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.BaseStore)) to let you save and recall long-term memories.

<img src="https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/short-vs-long.png?fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=62665893848db800383dffda7367438a" alt="" data-og-width="571" width="571" data-og-height="372" height="372" data-path="oss/images/short-vs-long.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/short-vs-long.png?w=280&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=b4f9851d9d5e9537fd9b4beeed7eefd5 280w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/short-vs-long.png?w=560&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=52fb6135668273aa8dfc615536c489b3 560w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/short-vs-long.png?w=840&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=0b6a2c6fe724a7db64dd4ad2677f5721 840w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/short-vs-long.png?w=1100&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=0e7ed889aef106cc3190b8b58a159b9c 1100w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/short-vs-long.png?w=1650&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=1997d5223d23ada411cce7d1170d6795 1650w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/short-vs-long.png?w=2500&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=731fe49d8d020f517e88cc90302618a5 2500w" />

[Short-term memory](/oss/javascript/langgraph/add-memory#add-short-term-memory) lets your application remember previous interactions within a single [thread](/oss/javascript/langgraph/persistence#threads) or conversation. A [thread](/oss/javascript/langgraph/persistence#threads) organizes multiple interactions in a session, similar to the way email groups messages in a single conversation.

LangGraph manages short-term memory as part of the agent's state, persisted via thread-scoped checkpoints. This state can normally include the conversation history along with other stateful data, such as uploaded files, retrieved documents, or generated artifacts. By storing these in the graph's state, the bot can access the full context for a given conversation while maintaining separation between different threads.

### Manage short-term memory

Conversation history is the most common form of short-term memory, and long conversations pose a challenge to today's LLMs. A full history may not fit inside an LLM's context window, resulting in an irrecoverable error. Even if your LLM supports the full context length, most LLMs still perform poorly over long contexts. They get "distracted" by stale or off-topic content, all while suffering from slower response times and higher costs.

Chat models accept context using messages, which include developer provided instructions (a system message) and user inputs (human messages). In chat applications, messages alternate between human inputs and model responses, resulting in a list of messages that grows longer over time. Because context windows are limited and token-rich message lists can be costly, many applications can benefit from using techniques to manually remove or forget stale information.

<img src="https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/filter.png?fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=89c50725dda7add80732bd2096e07ef2" alt="" data-og-width="594" width="594" data-og-height="200" height="200" data-path="oss/images/filter.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/filter.png?w=280&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=c5ffb27755202e7b13498e8c5e1c2765 280w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/filter.png?w=560&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=5abdad922fc7ea2770fa48825eb210ed 560w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/filter.png?w=840&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=d4dd4837a3a08a42b14f267c45f9e73e 840w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/filter.png?w=1100&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=ea09b560d904b68a4d7f370c88b908ef 1100w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/filter.png?w=1650&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=a83c084c37d9a34547f5435d9c6a6cc6 1650w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/filter.png?w=2500&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=164042153f379621c9d1558ea129caac 2500w" />

For more information on common techniques for managing messages, see the [Add and manage memory](/oss/javascript/langgraph/add-memory#manage-short-term-memory) guide.

[Long-term memory](/oss/javascript/langgraph/add-memory#add-long-term-memory) in LangGraph allows systems to retain information across different conversations or sessions. Unlike short-term memory, which is **thread-scoped**, long-term memory is saved within custom "namespaces."

Long-term memory is a complex challenge without a one-size-fits-all solution. However, the following questions provide a framework to help you navigate the different techniques:

* What is the type of memory? Humans use memories to remember facts ([semantic memory](#semantic-memory)), experiences ([episodic memory](#episodic-memory)), and rules ([procedural memory](#procedural-memory)). AI agents can use memory in the same ways. For example, AI agents can use memory to remember specific facts about a user to accomplish a task.
* [When do you want to update memories?](#writing-memories) Memory can be updated as part of an agent's application logic (e.g., "on the hot path"). In this case, the agent typically decides to remember facts before responding to a user. Alternatively, memory can be updated as a background task (logic that runs in the background / asynchronously and generates memories). We explain the tradeoffs between these approaches in the [section below](#writing-memories).

Different applications require various types of memory. Although the analogy isn't perfect, examining [human memory types](https://www.psychologytoday.com/us/basics/memory/types-of-memory?ref=blog.langchain.dev) can be insightful. Some research (e.g., the [CoALA paper](https://arxiv.org/pdf/2309.02427)) have even mapped these human memory types to those used in AI agents.

| Memory Type                      | What is Stored | Human Example              | Agent Example       |
| -------------------------------- | -------------- | -------------------------- | ------------------- |
| [Semantic](#semantic-memory)     | Facts          | Things I learned in school | Facts about a user  |
| [Episodic](#episodic-memory)     | Experiences    | Things I did               | Past agent actions  |
| [Procedural](#procedural-memory) | Instructions   | Instincts or motor skills  | Agent system prompt |

[Semantic memory](https://en.wikipedia.org/wiki/Semantic_memory), both in humans and AI agents, involves the retention of specific facts and concepts. In humans, it can include information learned in school and the understanding of concepts and their relationships. For AI agents, semantic memory is often used to personalize applications by remembering facts or concepts from past interactions.

<Note>
  Semantic memory is different from "semantic search," which is a technique for finding similar content using "meaning" (usually as embeddings). Semantic memory is a term from psychology, referring to storing facts and knowledge, while semantic search is a method for retrieving information based on meaning rather than exact matches.
</Note>

Semantic memories can be managed in different ways:

Memories can be a single, continuously updated "profile" of well-scoped and specific information about a user, organization, or other entity (including the agent itself). A profile is generally just a JSON document with various key-value pairs you've selected to represent your domain.

When remembering a profile, you will want to make sure that you are **updating** the profile each time. As a result, you will want to pass in the previous profile and [ask the model to generate a new profile](https://github.com/langchain-ai/memory-template) (or some [JSON patch](https://github.com/hinthornw/trustcall) to apply to the old profile). This can be become error-prone as the profile gets larger, and may benefit from splitting a profile into multiple documents or **strict** decoding when generating documents to ensure the memory schemas remains valid.

<img src="https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/update-profile.png?fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=8843788f6afd855450986c4cc4cd6abf" alt="" data-og-width="507" width="507" data-og-height="516" height="516" data-path="oss/images/update-profile.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/update-profile.png?w=280&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=0e40fc4d0951eccd4786df184513d73c 280w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/update-profile.png?w=560&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=3724f1b77f1f2fee60fa9fe5e8479fc7 560w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/update-profile.png?w=840&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=ba05c4768f99f62034c863fc7d824a1a 840w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/update-profile.png?w=1100&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=055289accf7ad32f8e884697c1dcdab3 1100w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/update-profile.png?w=1650&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=0ae25038922234d8fd5bc6b5eb5585ac 1650w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/update-profile.png?w=2500&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=bb4c602c662cdc2104825deec43f113f 2500w" />

Alternatively, memories can be a collection of documents that are continuously updated and extended over time. Each individual memory can be more narrowly scoped and easier to generate, which means that you're less likely to **lose** information over time. It's easier for an LLM to generate *new* objects for new information than reconcile new information with an existing profile. As a result, a document collection tends to lead to [higher recall downstream](https://en.wikipedia.org/wiki/Precision_and_recall).

However, this shifts some complexity memory updating. The model must now *delete* or *update* existing items in the list, which can be tricky. In addition, some models may default to over-inserting and others may default to over-updating. See the [Trustcall](https://github.com/hinthornw/trustcall) package for one way to manage this and consider evaluation (e.g., with a tool like [LangSmith](https://docs.smith.langchain.com/tutorials/Developers/evaluation)) to help you tune the behavior.

Working with document collections also shifts complexity to memory **search** over the list. The `Store` currently supports both [semantic search](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.SearchOp.query) and [filtering by content](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.SearchOp.filter).

Finally, using a collection of memories can make it challenging to provide comprehensive context to the model. While individual memories may follow a specific schema, this structure might not capture the full context or relationships between memories. As a result, when using these memories to generate responses, the model may lack important contextual information that would be more readily available in a unified profile approach.

<img src="https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/update-list.png?fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=38851b242981cc87128620091781f7c9" alt="" data-og-width="483" width="483" data-og-height="491" height="491" data-path="oss/images/update-list.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/update-list.png?w=280&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=1086a0d1728b213a85180db2c327b038 280w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/update-list.png?w=560&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=1e1b85bbc04bfef17f131e5b65cababc 560w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/update-list.png?w=840&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=77eb8e9b9d8afd4a2e7a05626b98afa4 840w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/update-list.png?w=1100&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=2745997869c9e572b3b2fe2826aa3b7f 1100w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/update-list.png?w=1650&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=3f69fc65b91598673364628ffe989b89 1650w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/update-list.png?w=2500&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=f8d229bd8bc606b6471b1619aed75be3 2500w" />

Regardless of memory management approach, the central point is that the agent will use the semantic memories to [ground its responses](/oss/javascript/langchain/retrieval), which often leads to more personalized and relevant interactions.

[Episodic memory](https://en.wikipedia.org/wiki/Episodic_memory), in both humans and AI agents, involves recalling past events or actions. The [CoALA paper](https://arxiv.org/pdf/2309.02427) frames this well: facts can be written to semantic memory, whereas *experiences* can be written to episodic memory. For AI agents, episodic memory is often used to help an agent remember how to accomplish a task.

In practice, episodic memories are often implemented through few-shot example prompting, where agents learn from past sequences to perform tasks correctly. Sometimes it's easier to "show" than "tell" and LLMs learn well from examples. Few-shot learning lets you ["program"](https://x.com/karpathy/status/1627366413840322562) your LLM by updating the prompt with input-output examples to illustrate the intended behavior. While various best-practices can be used to generate few-shot examples, often the challenge lies in selecting the most relevant examples based on user input.

Note that the memory [store](/oss/javascript/langgraph/persistence#memory-store) is just one way to store data as few-shot examples. If you want to have more developer involvement, or tie few-shots more closely to your evaluation harness, you can also use a LangSmith Dataset to store your data. Then dynamic few-shot example selectors can be used out-of-the box to achieve this same goal. LangSmith will index the dataset for you and enable retrieval of few shot examples that are most relevant to the user input based upon keyword similarity.

See this how-to [video](https://www.youtube.com/watch?v=37VaU7e7t5o) for example usage of dynamic few-shot example selection in LangSmith. Also, see this [blog post](https://blog.langchain.dev/few-shot-prompting-to-improve-tool-calling-performance/) showcasing few-shot prompting to improve tool calling performance and this [blog post](https://blog.langchain.dev/aligning-llm-as-a-judge-with-human-preferences/) using few-shot example to align an LLMs to human preferences.

### Procedural memory

[Procedural memory](https://en.wikipedia.org/wiki/Procedural_memory), in both humans and AI agents, involves remembering the rules used to perform tasks. In humans, procedural memory is like the internalized knowledge of how to perform tasks, such as riding a bike via basic motor skills and balance. Episodic memory, on the other hand, involves recalling specific experiences, such as the first time you successfully rode a bike without training wheels or a memorable bike ride through a scenic route. For AI agents, procedural memory is a combination of model weights, agent code, and agent's prompt that collectively determine the agent's functionality.

In practice, it is fairly uncommon for agents to modify their model weights or rewrite their code. However, it is more common for agents to modify their own prompts.

One effective approach to refining an agent's instructions is through ["Reflection"](https://blog.langchain.dev/reflection-agents/) or meta-prompting. This involves prompting the agent with its current instructions (e.g., the system prompt) along with recent conversations or explicit user feedback. The agent then refines its own instructions based on this input. This method is particularly useful for tasks where instructions are challenging to specify upfront, as it allows the agent to learn and adapt from its interactions.

For example, we built a [Tweet generator](https://www.youtube.com/watch?v=Vn8A3BxfplE) using external feedback and prompt re-writing to produce high-quality paper summaries for Twitter. In this case, the specific summarization prompt was difficult to specify *a priori*, but it was fairly easy for a user to critique the generated Tweets and provide feedback on how to improve the summarization process.

The below pseudo-code shows how you might implement this with the LangGraph memory [store](/oss/javascript/langgraph/persistence#memory-store), using the store to save a prompt, the `update_instructions` node to get the current prompt (as well as feedback from the conversation with the user captured in `state["messages"]`), update the prompt, and save the new prompt back to the store. Then, the `call_model` get the updated prompt from the store and uses it to generate a response.

<img src="https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/update-instructions.png?fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=13644c954ed79a45b8a1a762b3e39da1" alt="" data-og-width="493" width="493" data-og-height="515" height="515" data-path="oss/images/update-instructions.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/update-instructions.png?w=280&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=90632c71febee5777be6ae2c338f0880 280w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/update-instructions.png?w=560&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=aefcc771a030a2d6a89f815b87e60fd4 560w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/update-instructions.png?w=840&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=9115490b76daffe987e3867bc9176386 840w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/update-instructions.png?w=1100&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=0df26f2e6f669f2fbea59a9a49482fb4 1100w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/update-instructions.png?w=1650&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=132e7b1f377e0b57c03ab31c6d788df4 1650w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/update-instructions.png?w=2500&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=651cd1bb14e445a972a671a196b6a893 2500w" />

There are two primary methods for agents to write memories: ["in the hot path"](#in-the-hot-path) and ["in the background"](#in-the-background).

<img src="https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/hot_path_vs_background.png?fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=edd006d6189dc29a2edcba57c41fd744" alt="" data-og-width="842" width="842" data-og-height="418" height="418" data-path="oss/images/hot_path_vs_background.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/hot_path_vs_background.png?w=280&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=3efd9962012347a64b596d1d36925b33 280w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/hot_path_vs_background.png?w=560&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=add54b5469d7b4a8f22d7da250c19ddf 560w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/hot_path_vs_background.png?w=840&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=1e763d0f2ee8aa4f5b302ad44bc19d2f 840w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/hot_path_vs_background.png?w=1100&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=ce84a68af250e53a4693332d39179136 1100w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/hot_path_vs_background.png?w=1650&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=5f807accb9c63dae57c27c9a1d17f29a 1650w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/hot_path_vs_background.png?w=2500&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=ecef974859d58f691dbb22a4a1cc1572 2500w" />

Creating memories during runtime offers both advantages and challenges. On the positive side, this approach allows for real-time updates, making new memories immediately available for use in subsequent interactions. It also enables transparency, as users can be notified when memories are created and stored.

However, this method also presents challenges. It may increase complexity if the agent requires a new tool to decide what to commit to memory. In addition, the process of reasoning about what to save to memory can impact agent latency. Finally, the agent must multitask between memory creation and its other responsibilities, potentially affecting the quantity and quality of memories created.

As an example, ChatGPT uses a [save\_memories](https://openai.com/index/memory-and-new-controls-for-chatgpt/) tool to upsert memories as content strings, deciding whether and how to use this tool with each user message. See our [memory-agent](https://github.com/langchain-ai/memory-agent) template as an reference implementation.

#### In the background

Creating memories as a separate background task offers several advantages. It eliminates latency in the primary application, separates application logic from memory management, and allows for more focused task completion by the agent. This approach also provides flexibility in timing memory creation to avoid redundant work.

However, this method has its own challenges. Determining the frequency of memory writing becomes crucial, as infrequent updates may leave other threads without new context. Deciding when to trigger memory formation is also important. Common strategies include scheduling after a set time period (with rescheduling if new events occur), using a cron schedule, or allowing manual triggers by users or the application logic.

See our [memory-service](https://github.com/langchain-ai/memory-template) template as an reference implementation.

LangGraph stores long-term memories as JSON documents in a [store](/oss/javascript/langgraph/persistence#memory-store). Each memory is organized under a custom `namespace` (similar to a folder) and a distinct `key` (like a file name). Namespaces often include user or org IDs or other labels that makes it easier to organize information. This structure enables hierarchical organization of memories. Cross-namespace searching is then supported through content filters.

For more information about the memory store, see the [Persistence](/oss/javascript/langgraph/persistence#memory-store) guide.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/concepts/memory.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
<img src="https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/update-instructions.png?fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=13644c954ed79a45b8a1a762b3e39da1" alt="" data-og-width="493" width="493" data-og-height="515" height="515" data-path="oss/images/update-instructions.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/update-instructions.png?w=280&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=90632c71febee5777be6ae2c338f0880 280w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/update-instructions.png?w=560&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=aefcc771a030a2d6a89f815b87e60fd4 560w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/update-instructions.png?w=840&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=9115490b76daffe987e3867bc9176386 840w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/update-instructions.png?w=1100&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=0df26f2e6f669f2fbea59a9a49482fb4 1100w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/update-instructions.png?w=1650&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=132e7b1f377e0b57c03ab31c6d788df4 1650w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/update-instructions.png?w=2500&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=651cd1bb14e445a972a671a196b6a893 2500w" />

### Writing memories

There are two primary methods for agents to write memories: ["in the hot path"](#in-the-hot-path) and ["in the background"](#in-the-background).

<img src="https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/hot_path_vs_background.png?fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=edd006d6189dc29a2edcba57c41fd744" alt="" data-og-width="842" width="842" data-og-height="418" height="418" data-path="oss/images/hot_path_vs_background.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/hot_path_vs_background.png?w=280&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=3efd9962012347a64b596d1d36925b33 280w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/hot_path_vs_background.png?w=560&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=add54b5469d7b4a8f22d7da250c19ddf 560w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/hot_path_vs_background.png?w=840&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=1e763d0f2ee8aa4f5b302ad44bc19d2f 840w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/hot_path_vs_background.png?w=1100&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=ce84a68af250e53a4693332d39179136 1100w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/hot_path_vs_background.png?w=1650&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=5f807accb9c63dae57c27c9a1d17f29a 1650w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/hot_path_vs_background.png?w=2500&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=ecef974859d58f691dbb22a4a1cc1572 2500w" />

#### In the hot path

Creating memories during runtime offers both advantages and challenges. On the positive side, this approach allows for real-time updates, making new memories immediately available for use in subsequent interactions. It also enables transparency, as users can be notified when memories are created and stored.

However, this method also presents challenges. It may increase complexity if the agent requires a new tool to decide what to commit to memory. In addition, the process of reasoning about what to save to memory can impact agent latency. Finally, the agent must multitask between memory creation and its other responsibilities, potentially affecting the quantity and quality of memories created.

As an example, ChatGPT uses a [save\_memories](https://openai.com/index/memory-and-new-controls-for-chatgpt/) tool to upsert memories as content strings, deciding whether and how to use this tool with each user message. See our [memory-agent](https://github.com/langchain-ai/memory-agent) template as an reference implementation.

#### In the background

Creating memories as a separate background task offers several advantages. It eliminates latency in the primary application, separates application logic from memory management, and allows for more focused task completion by the agent. This approach also provides flexibility in timing memory creation to avoid redundant work.

However, this method has its own challenges. Determining the frequency of memory writing becomes crucial, as infrequent updates may leave other threads without new context. Deciding when to trigger memory formation is also important. Common strategies include scheduling after a set time period (with rescheduling if new events occur), using a cron schedule, or allowing manual triggers by users or the application logic.

See our [memory-service](https://github.com/langchain-ai/memory-template) template as an reference implementation.

### Memory storage

LangGraph stores long-term memories as JSON documents in a [store](/oss/javascript/langgraph/persistence#memory-store). Each memory is organized under a custom `namespace` (similar to a folder) and a distinct `key` (like a file name). Namespaces often include user or org IDs or other labels that makes it easier to organize information. This structure enables hierarchical organization of memories. Cross-namespace searching is then supported through content filters.
```

---

## Messages

**URL:** llms-txt#messages

**Contents:**
- Basic usage
  - Text prompts
  - Message prompts
  - Dictionary format
- Message types
  - System Message
  - Human Message
  - AI Message
  - Tool Message
- Message content

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Messages are the fundamental unit of context for models in LangChain. They represent the input and output of models, carrying both the content and metadata needed to represent the state of a conversation when interacting with an LLM.

Messages are objects that contain:

* <Icon icon="user" size={16} /> [**Role**](#message-types) - Identifies the message type (e.g. `system`, `user`)
* <Icon icon="folder-closed" size={16} /> [**Content**](#message-content) - Represents the actual content of the message (like text, images, audio, documents, etc.)
* <Icon icon="tag" size={16} /> [**Metadata**](#message-metadata) - Optional fields such as response information, message IDs, and token usage

LangChain provides a standard message type that works across all model providers, ensuring consistent behavior regardless of the model being called.

The simplest way to use messages is to create message objects and pass them to a model when [invoking](/oss/javascript/langchain/models#invocation).

Text prompts are strings - ideal for straightforward generation tasks where you don't need to retain conversation history.

**Use text prompts when:**

* You have a single, standalone request
* You don't need conversation history
* You want minimal code complexity

Alternatively, you can pass in a list of messages to the model by providing a list of message objects.

**Use message prompts when:**

* Managing multi-turn conversations
* Working with multimodal content (images, audio, files)
* Including system instructions

### Dictionary format

You can also specify messages directly in OpenAI chat completions format.

* <Icon icon="gear" size={16} /> [System message](#system-message) - Tells the model how to behave and provide context for interactions
* <Icon icon="user" size={16} /> [Human message](#human-message) - Represents user input and interactions with the model
* <Icon icon="robot" size={16} /> [AI message](#ai-message) - Responses generated by the model, including text content, tool calls, and metadata
* <Icon icon="wrench" size={16} /> [Tool message](#tool-message) - Represents the outputs of [tool calls](/oss/javascript/langchain/models#tool-calling)

A @\[`SystemMessage`] represent an initial set of instructions that primes the model's behavior. You can use a system message to set the tone, define the model's role, and establish guidelines for responses.

A @\[`HumanMessage`] represents user input and interactions. They can contain text, images, audio, files, and any other amount of multimodal [content](#message-content).

#### Message metadata

<Note>
  The `name` field behavior varies by provider - some use it for user identification, others ignore it. To check, refer to the model provider's [reference](https://reference.langchain.com/python/integrations/).
</Note>

An [`AIMessage`](https://v03.api.js.langchain.com/classes/_langchain_core.messages_ai_message.AIMessage.html) represents the output of a model invocation. They can include multimodal data, tool calls, and provider-specific metadata that you can later access.

[`AIMessage`](https://v03.api.js.langchain.com/classes/_langchain_core.messages_ai_message.AIMessage.html) objects are returned by the model when calling it, which contains all of the associated metadata in the response.

Providers weigh/contextualize types of messages differently, which means it is sometimes helpful to manually create a new [`AIMessage`](https://v03.api.js.langchain.com/classes/_langchain_core.messages_ai_message.AIMessage.html) object and insert it into the message history as if it came from the model.

<Accordion title="Attributes">
  <ParamField path="text" type="string">
    The text content of the message.
  </ParamField>

<ParamField path="content" type="string | ContentBlock[]">
    The raw content of the message.
  </ParamField>

<ParamField path="content_blocks" type="ContentBlock.Standard[]">
    The standardized content blocks of the message. (See [content](#message-content))
  </ParamField>

<ParamField path="tool_calls" type="ToolCall[] | None">
    The tool calls made by the model. Empty if no tools are called.
  </ParamField>

<ParamField path="id" type="string">
    A unique identifier for the message (either automatically generated by LangChain or returned in the provider response)
  </ParamField>

<ParamField path="usage_metadata" type="UsageMetadata | None">
    The usage metadata of the message, which can contain token counts when available. See [`UsageMetadata`](https://reference.langchain.com/javascript/types/_langchain_core.messages.UsageMetadata.html).
  </ParamField>

<ParamField path="response_metadata" type="ResponseMetadata | None">
    The response metadata of the message.
  </ParamField>
</Accordion>

When models make [tool calls](/oss/javascript/langchain/models#tool-calling), they're included in the [`AIMessage`](https://v03.api.js.langchain.com/classes/_langchain_core.messages_ai_message.AIMessage.html):

Other structured data, such as reasoning or citations, can also appear in message [content](/oss/javascript/langchain/messages#message-content).

An [`AIMessage`](https://v03.api.js.langchain.com/classes/_langchain_core.messages_ai_message.AIMessage.html) can hold token counts and other usage metadata in its [`usage_metadata`](https://reference.langchain.com/javascript/types/_langchain_core.messages.UsageMetadata.html) field:

See [`UsageMetadata`](https://reference.langchain.com/javascript/types/_langchain_core.messages.UsageMetadata.html) for details.

#### Streaming and chunks

During streaming, you'll receive [`AIMessageChunk`](https://v03.api.js.langchain.com/classes/_langchain_core.messages_ai_message.AIMessageChunk.html) objects that can be combined into a full message object:

<CodeGroup>
  
</CodeGroup>

* [Streaming tokens from chat models](/oss/javascript/langchain/models#stream)
  * [Streaming tokens and/or steps from agents](/oss/javascript/langchain/streaming)
</Note>

For models that support [tool calling](/oss/javascript/langchain/models#tool-calling), AI messages can contain tool calls. Tool messages are used to pass the results of a single tool execution back to the model.

[Tools](/oss/javascript/langchain/tools) can generate @\[`ToolMessage`] objects directly. Below, we show a simple example. Read more in the [tools guide](/oss/javascript/langchain/tools).

<Accordion title="Attributes">
  <ParamField path="content" type="string" required>
    The stringified output of the tool call.
  </ParamField>

<ParamField path="tool_call_id" type="string" required>
    The ID of the tool call that this message is responding to. (this must match the ID of the tool call in the [`AIMessage`](https://v03.api.js.langchain.com/classes/_langchain_core.messages_ai_message.AIMessage.html))
  </ParamField>

<ParamField path="name" type="string" required>
    The name of the tool that was called.
  </ParamField>

<ParamField path="artifact" type="dict">
    Additional data not sent to the model but can be accessed programmatically.
  </ParamField>
</Accordion>

<Note>
  The `artifact` field stores supplementary data that won't be sent to the model but can be accessed programmatically. This is useful for storing raw results, debugging information, or data for downstream processing without cluttering the model's context.

<Accordion title="Example: Using artifact for retrieval metadata">
    For example, a [retrieval](/oss/javascript/langchain/retrieval) tool could retrieve a passage from a document for reference by a model. Where message `content` contains text that the model will reference, an `artifact` can contain document identifiers or other metadata that an application can use (e.g., to render a page). See example below:

See the [RAG tutorial](/oss/javascript/langchain/rag) for an end-to-end example of building retrieval [agents](/oss/javascript/langchain/agents) with LangChain.
  </Accordion>
</Note>

You can think of a message's content as the payload of data that gets sent to the model. Messages have a `content` attribute that is loosely-typed, supporting strings and lists of untyped objects (e.g., dictionaries). This allows support for provider-native structures directly in LangChain chat models, such as [multimodal](#multimodal) content and other data.

Separately, LangChain provides dedicated content types for text, reasoning, citations, multi-modal data, server-side tool calls, and other message content. See [content blocks](#standard-content-blocks) below.

LangChain chat models accept message content in the `content` attribute, and can contain:

1. A string
2. A list of content blocks in a provider-native format
3. A list of [LangChain's standard content blocks](#standard-content-blocks)

See below for an example using [multimodal](#multimodal) inputs:

### Standard content blocks

LangChain provides a standard representation for message content that works across providers.

Message objects implement a `contentBlocks` property that will lazily parse the `content` attribute into a standard, type-safe representation. For example, messages generated from [ChatAnthropic](/oss/javascript/integrations/chat/anthropic) or [ChatOpenAI](/oss/javascript/integrations/chat/openai) will include `thinking` or `reasoning` blocks in the format of the respective provider, but can be lazily parsed into a consistent [`ReasoningContentBlock`](#content-block-reference) representation:

<Tabs>
  <Tab title="Anthropic">
    
  </Tab>

<Tab title="OpenAI">
    
  </Tab>
</Tabs>

See the [integrations guides](/oss/javascript/integrations/providers/overview) to get started with the
inference provider of your choice.

<Note>
  **Serializing standard content**

If an application outside of LangChain needs access to the standard content block
  representation, you can opt-in to storing content blocks in message content.

To do this, you can set the `LC_OUTPUT_VERSION` environment variable to `v1`. Or,
  initialize any chat model with `outputVersion: "v1"`:

**Multimodality** refers to the ability to work with data that comes in different
forms, such as text, audio, images, and video. LangChain includes standard types
for these data that can be used across providers.

[Chat models](/oss/javascript/langchain/models) can accept multimodal data as input and generate
it as output. Below we show short examples of input messages featuring multimodal data.

<Note>
  Extra keys can be included top-level in the content block or nested in `"extras": {"key": value}`.

[OpenAI](/oss/javascript/integrations/chat/openai#pdfs) and [AWS Bedrock Converse](/oss/javascript/integrations/chat/bedrock),
  for example, require a filename for PDFs. See the [provider page](/oss/javascript/integrations/providers/overview)
  for your chosen model for specifics.
</Note>

<Warning>
  Not all models support all file types. Check the model provider's [reference](https://reference.langchain.com/python/integrations/) for supported formats and size limits.
</Warning>

### Content block reference

Content blocks are represented (either when creating a message or accessing the `contentBlocks` field) as a list of typed objects. Each item in the list must adhere to one of the following block types:

<AccordionGroup>
  <Accordion title="Core" icon="cube">
    <AccordionGroup>
      <Accordion title="ContentBlock.Text" icon="text">
        **Purpose:** Standard text output

<ParamField body="type" type="string" required>
          Always `"text"`
        </ParamField>

<ParamField body="text" type="string" required>
          The text content
        </ParamField>

<ParamField body="annotations" type="Citation[]">
          List of annotations for the text
        </ParamField>

<Accordion title="ContentBlock.Reasoning" icon="brain">
        **Purpose:** Model reasoning steps

<ParamField body="type" type="string" required>
          Always `"reasoning"`
        </ParamField>

<ParamField body="reasoning" type="string" required>
          The reasoning content
        </ParamField>

</Accordion>
    </AccordionGroup>
  </Accordion>

<Accordion title="Multimodal" icon="images">
    <AccordionGroup>
      <Accordion title="ContentBlock.Multimodal.Image" icon="image">
        **Purpose:** Image data

<ParamField body="type" type="string" required>
          Always `"image"`
        </ParamField>

<ParamField body="url" type="string">
          URL pointing to the image location.
        </ParamField>

<ParamField body="data" type="string">
          Base64-encoded image data.
        </ParamField>

<ParamField body="fileId" type="string">
          Reference ID to an externally stored image (e.g., in a provider's file system or in a bucket).
        </ParamField>

<ParamField body="mimeType" type="string">
          Image [MIME type](https://www.iana.org/assignments/media-types/media-types.xhtml#image) (e.g., `image/jpeg`, `image/png`)
        </ParamField>
      </Accordion>

<Accordion title="ContentBlock.Multimodal.Audio" icon="volume-high">
        **Purpose:** Audio data

<ParamField body="type" type="string" required>
          Always `"audio"`
        </ParamField>

<ParamField body="url" type="string">
          URL pointing to the audio location.
        </ParamField>

<ParamField body="data" type="string">
          Base64-encoded audio data.
        </ParamField>

<ParamField body="fileId" type="string">
          Reference ID to an externally stored audio file (e.g., in a provider's file system or in a bucket).
        </ParamField>

<ParamField body="mimeType" type="string">
          Audio [MIME type](https://www.iana.org/assignments/media-types/media-types.xhtml#audio) (e.g., `audio/mpeg`, `audio/wav`)
        </ParamField>
      </Accordion>

<Accordion title="ContentBlock.Multimodal.Video" icon="video">
        **Purpose:** Video data

<ParamField body="type" type="string" required>
          Always `"video"`
        </ParamField>

<ParamField body="url" type="string">
          URL pointing to the video location.
        </ParamField>

<ParamField body="data" type="string">
          Base64-encoded video data.
        </ParamField>

<ParamField body="fileId" type="string">
          Reference ID to an externally stored video file (e.g., in a provider's file system or in a bucket).
        </ParamField>

<ParamField body="mimeType" type="string">
          Video [MIME type](https://www.iana.org/assignments/media-types/media-types.xhtml#video) (e.g., `video/mp4`, `video/webm`)
        </ParamField>
      </Accordion>

<Accordion title="ContentBlock.Multimodal.File" icon="file">
        **Purpose:** Generic files (PDF, etc)

<ParamField body="type" type="string" required>
          Always `"file"`
        </ParamField>

<ParamField body="url" type="string">
          URL pointing to the file location.
        </ParamField>

<ParamField body="data" type="string">
          Base64-encoded file data.
        </ParamField>

<ParamField body="fileId" type="string">
          Reference ID to an externally stored file (e.g., in a provider's file system or in a bucket).
        </ParamField>

<ParamField body="mimeType" type="string">
          File [MIME type](https://www.iana.org/assignments/media-types/media-types.xhtml) (e.g., `application/pdf`)
        </ParamField>
      </Accordion>

<Accordion title="ContentBlock.Multimodal.PlainText" icon="align-left">
        **Purpose:** Document text (`.txt`, `.md`)

<ParamField body="type" type="string" required>
          Always `"text-plain"`
        </ParamField>

<ParamField body="text" type="string" required>
          The text content
        </ParamField>

<ParamField body="title" type="string">
          Title of the text content
        </ParamField>

<ParamField body="mimeType" type="string">
          [MIME type](https://www.iana.org/assignments/media-types/media-types.xhtml) of the text (e.g., `text/plain`, `text/markdown`)
        </ParamField>
      </Accordion>
    </AccordionGroup>
  </Accordion>

<Accordion title="Tool Calling" icon="wrench">
    <AccordionGroup>
      <Accordion title="ContentBlock.Tools.ToolCall" icon="function">
        **Purpose:** Function calls

<ParamField body="type" type="string" required>
          Always `"tool_call"`
        </ParamField>

<ParamField body="name" type="string" required>
          Name of the tool to call
        </ParamField>

<ParamField body="args" type="object" required>
          Arguments to pass to the tool
        </ParamField>

<ParamField body="id" type="string" required>
          Unique identifier for this tool call
        </ParamField>

<Accordion title="ContentBlock.Tools.ToolCallChunk" icon="puzzle-piece">
        **Purpose:** Streaming tool fragments

<ParamField body="type" type="string" required>
          Always `"tool_call_chunk"`
        </ParamField>

<ParamField body="name" type="string">
          Name of the tool being called
        </ParamField>

<ParamField body="args" type="string">
          Partial tool arguments (may be incomplete JSON)
        </ParamField>

<ParamField body="id" type="string">
          Tool call identifier
        </ParamField>

<ParamField body="index" type="number | string" required>
          Position of this chunk in the stream
        </ParamField>
      </Accordion>

<Accordion title="ContentBlock.Tools.InvalidToolCall" icon="triangle-exclamation">
        **Purpose:** Malformed calls

<ParamField body="type" type="string" required>
          Always `"invalid_tool_call"`
        </ParamField>

<ParamField body="name" type="string">
          Name of the tool that failed to be called
        </ParamField>

<ParamField body="args" type="string">
          Raw arguments that failed to parse
        </ParamField>

<ParamField body="error" type="string" required>
          Description of what went wrong
        </ParamField>

**Common errors:** Invalid JSON, missing required fields
      </Accordion>
    </AccordionGroup>
  </Accordion>

<Accordion title="Server-Side Tool Execution" icon="server">
    <AccordionGroup>
      <Accordion title="ContentBlock.Tools.ServerToolCall" icon="wrench">
        **Purpose:** Tool call that is executed server-side.

<ParamField body="type" type="string" required>
          Always `"server_tool_call"`
        </ParamField>

<ParamField body="id" type="string" required>
          An identifier associated with the tool call.
        </ParamField>

<ParamField body="name" type="string" required>
          The name of the tool to be called.
        </ParamField>

<ParamField body="args" type="string" required>
          Partial tool arguments (may be incomplete JSON)
        </ParamField>
      </Accordion>

<Accordion title="ContentBlock.Tools.ServerToolCallChunk" icon="puzzle-piece">
        **Purpose:** Streaming server-side tool call fragments

<ParamField body="type" type="string" required>
          Always `"server_tool_call_chunk"`
        </ParamField>

<ParamField body="id" type="string">
          An identifier associated with the tool call.
        </ParamField>

<ParamField body="name" type="string">
          Name of the tool being called
        </ParamField>

<ParamField body="args" type="string">
          Partial tool arguments (may be incomplete JSON)
        </ParamField>

<ParamField body="index" type="number | string">
          Position of this chunk in the stream
        </ParamField>
      </Accordion>

<Accordion title="ContentBlock.Tools.ServerToolResult" icon="box-open">
        **Purpose:** Search results

<ParamField body="type" type="string" required>
          Always `"server_tool_result"`
        </ParamField>

<ParamField body="tool_call_id" type="string" required>
          Identifier of the corresponding server tool call.
        </ParamField>

<ParamField body="id" type="string">
          Identifier associated with the server tool result.
        </ParamField>

<ParamField body="status" type="string" required>
          Execution status of the server-side tool. `"success"` or `"error"`.
        </ParamField>

<ParamField body="output">
          Output of the executed tool.
        </ParamField>
      </Accordion>
    </AccordionGroup>
  </Accordion>

<Accordion title="Provider-Specific Blocks" icon="plug">
    <Accordion title="ContentBlock.NonStandard" icon="asterisk">
      **Purpose:** Provider-specific escape hatch

<ParamField body="type" type="string" required>
        Always `"non_standard"`
      </ParamField>

<ParamField body="value" type="object" required>
        Provider-specific data structure
      </ParamField>

**Usage:** For experimental or provider-unique features
    </Accordion>

Additional provider-specific content types may be found within the [reference documentation](/oss/javascript/integrations/providers/overview) of each model provider.
  </Accordion>
</AccordionGroup>

Each of these content blocks mentioned above are indvidually addressable as types when importing the @\[`ContentBlock`] type.

<Tip>
  View the canonical type definitions in the @\[API reference]\[langchain.messages].
</Tip>

<Info>
  Content blocks were introduced as a new property on messages in LangChain v1 to standardize content formats across providers while maintaining backward compatibility with existing code. Content blocks are not a replacement for the @\[`content`]\[BaseMessage(content)] property, but rather a new property that can be used to access the content of a message in a standardized format.
</Info>

## Use with chat models

[Chat models](/oss/javascript/langchain/models) accept a sequence of message objects as input and return an [`AIMessage`](https://v03.api.js.langchain.com/classes/_langchain_core.messages_ai_message.AIMessage.html) as output. Interactions are often stateless, so that a simple conversational loop involves invoking a model with a growing list of messages.

Refer to the below guides to learn more:

* Built-in features for [persisting and managing conversation histories](/oss/javascript/langchain/short-term-memory)
* Strategies for managing context windows, including [trimming and summarizing messages](/oss/javascript/langchain/short-term-memory#common-patterns)

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/messages.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
### Text prompts

Text prompts are strings - ideal for straightforward generation tasks where you don't need to retain conversation history.
```

Example 2 (unknown):
```unknown
**Use text prompts when:**

* You have a single, standalone request
* You don't need conversation history
* You want minimal code complexity

### Message prompts

Alternatively, you can pass in a list of messages to the model by providing a list of message objects.
```

Example 3 (unknown):
```unknown
**Use message prompts when:**

* Managing multi-turn conversations
* Working with multimodal content (images, audio, files)
* Including system instructions

### Dictionary format

You can also specify messages directly in OpenAI chat completions format.
```

Example 4 (unknown):
```unknown
## Message types

* <Icon icon="gear" size={16} /> [System message](#system-message) - Tells the model how to behave and provide context for interactions
* <Icon icon="user" size={16} /> [Human message](#human-message) - Represents user input and interactions with the model
* <Icon icon="robot" size={16} /> [AI message](#ai-message) - Responses generated by the model, including text content, tool calls, and metadata
* <Icon icon="wrench" size={16} /> [Tool message](#tool-message) - Represents the outputs of [tool calls](/oss/javascript/langchain/models#tool-calling)

### System Message

A @\[`SystemMessage`] represent an initial set of instructions that primes the model's behavior. You can use a system message to set the tone, define the model's role, and establish guidelines for responses.
```

---

## Messages and content

**URL:** llms-txt#messages-and-content

from langchain.messages import AIMessage, HumanMessage

---

## Metrics: [OTel Example](/langsmith/langsmith-collector#metrics)

**URL:** llms-txt#metrics:-[otel-example](/langsmith/langsmith-collector#metrics)

**Contents:**
- LangSmith Services
- Frontend Nginx
- Postgres + Redis
- Clickhouse

## LangSmith Services

The following LangSmith services expose metrics at an endpoint, in the Prometheus metrics format. The frontend does not currently expose metrics.

* **Backend**: `http://<langsmith_release_name>-backend.<namespace>.svc.cluster.local:1984/metrics`
* **Platform Backend**: `http://<langsmith_release_name>-platform-backend.<namespace>.svc.cluster.local:1986/metrics`
* **Playground**: `http://<langsmith_release_name>-playground.<namespace>.svc.cluster.local:1988/metrics`
* **(LangSmith Control Plane only) Host Backend**: `http://<langsmith_release_name>-host-backend.<namespace>.svc.cluster.local:1985/metrics`

You can use a [Prometheus](https://prometheus.io/docs/prometheus/latest/getting_started/#configure-prometheus-to-monitor-the-sample-targets) or [OpenTelemetry](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/prometheusreceiver) collector to scrape the endpoints, and export metrics to the backend of your choice.

The frontend service exposes its Nginx metrics at the following endpoint: `langsmith-frontend.langsmith.svc.cluster.local:80/nginx_status`. You can either scrape them yourself, or bring up a Prometheus Nginx exporter using the [LangSmith Observability Helm Chart](/langsmith/observability-stack)

<Warning>
  **The following sections apply for in-cluster databases only. If you are using external databases, you will need to configure exposing and fetching metrics.**
</Warning>

If you are using in-cluster Postgres/Redis instances, you can use a Prometheus exporter to expose metrics from your instance. You can deploy your own, or if you would like, you can use the [LangSmith Observability Helm Chart](/langsmith/observability-stack) to deploy an exporter for you.

The in-cluster Clickhouse is configured to expose metrics without the need for an exporter. You can use your collector to scrape metrics at `http://<langsmith_release_name>-clickhouse.<namespace>.svc.cluster.local:9363/metrics`

---

## Microsoft

**URL:** llms-txt#microsoft

**Contents:**
- Chat models
  - Azure OpenAI
  - Azure AI
  - Azure ML Chat Online Endpoint
- LLMs
  - Azure ML
  - Azure OpenAI
- Embedding Models
  - Azure OpenAI
  - Azure AI

Source: https://docs.langchain.com/oss/python/integrations/providers/microsoft

This page covers all LangChain integrations with [Microsoft Azure](https://portal.azure.com) and other [Microsoft](https://www.microsoft.com) products.

Microsoft offers three main options for accessing chat models through Azure:

1. [Azure OpenAI](https://learn.microsoft.com/en-us/azure/ai-services/openai/) - Provides access to OpenAI's powerful models like o3, 4.1, and other models through Microsoft Azure's secure enterprise platform.
2. [Azure AI](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/deploy-models) - Offers access to a variety of models from different providers including Anthropic, DeepSeek, Cohere, Phi and Mistral through a unified API.
3. [Azure ML](https://learn.microsoft.com/en-us/azure/machine-learning/) - Allows deployment and management of your own custom models or fine-tuned open-source models with Azure Machine Learning.

> [Microsoft Azure](https://en.wikipedia.org/wiki/Microsoft_Azure), often referred to as `Azure` is a cloud computing platform run by `Microsoft`, which offers access, management, and development of applications and services through global data centers. It provides a range of capabilities, including software as a service (SaaS), platform as a service (PaaS), and infrastructure as a service (IaaS). `Microsoft Azure` supports many programming languages, tools, and frameworks, including Microsoft-specific and third-party software and systems.

> [Azure OpenAI](https://learn.microsoft.com/en-us/azure/cognitive-services/openai/) is an `Azure` service with powerful language models from OpenAI including the `GPT-3`, `Codex` and Embeddings model series for content generation, summarization, semantic search, and natural language to code translation.

Set the environment variables to get access to the `Azure OpenAI` service.

See a [usage example](/oss/python/integrations/chat/azure_chat_openai)

> [Azure AI Foundry](https://learn.microsoft.com/en-us/azure/developer/python/get-started) provides access to a wide range of models from various providers including Azure OpenAI, DeepSeek R1, Cohere, Phi and Mistral through the `AzureAIChatCompletionsModel` class.

Configure your API key and Endpoint.

See a [usage example](/oss/python/integrations/chat/azure_ai)

### Azure ML Chat Online Endpoint

See the documentation [here](/oss/python/integrations/chat/azureml_chat_endpoint) for accessing chat
models hosted with [Azure Machine Learning](https://azure.microsoft.com/en-us/products/machine-learning/).

See a [usage example](/oss/python/integrations/llms/azure_ml).

See a [usage example](/oss/python/integrations/llms/azure_openai).

Microsoft offers two main options for accessing embedding models through Azure:

See a [usage example](/oss/python/integrations/text_embedding/azure_openai)

Configure your API key and Endpoint.

> [Azure AI Foundry (formerly Azure AI Studio](https://ai.azure.com/) provides the capability to upload data assets
> to cloud storage and register existing data assets from the following sources:
>
> * `Microsoft OneLake`
> * `Azure Blob Storage`
> * `Azure Data Lake gen 2`

First, you need to install several python packages.

See a [usage example](/oss/python/integrations/document_loaders/azure_ai_data).

### Azure AI Document Intelligence

> [Azure AI Document Intelligence](https://aka.ms/doc-intelligence) (formerly known
> as `Azure Form Recognizer`) is machine-learning
> based service that extracts texts (including handwriting), tables, document structures,
> and key-value-pairs
> from digital or scanned PDFs, images, Office and HTML files.
>
> Document Intelligence supports `PDF`, `JPEG/JPG`, `PNG`, `BMP`, `TIFF`, `HEIF`, `DOCX`, `XLSX`, `PPTX` and `HTML`.

First, you need to install a python package.

See a [usage example](/oss/python/integrations/document_loaders/azure_document_intelligence).

### Azure Blob Storage

> [Azure Blob Storage](https://learn.microsoft.com/en-us/azure/storage/blobs/storage-blobs-introduction) is Microsoft's object storage solution for the cloud. Blob Storage is optimized for storing massive amounts of unstructured data. Unstructured data is data that doesn't adhere to a particular data model or definition, such as text or binary data.

`Azure Blob Storage` is designed for:

* Serving images or documents directly to a browser.
* Storing files for distributed access.
* Streaming video and audio.
* Writing to log files.
* Storing data for backup and restore, disaster recovery, and archiving.
* Storing data for analysis by an on-premises or Azure-hosted service.

See [usage examples for the Azure Blob Storage Loader](/oss/python/integrations/document_loaders/azure_blob_storage).

### Microsoft OneDrive

> [Microsoft OneDrive](https://en.wikipedia.org/wiki/OneDrive) (formerly `SkyDrive`) is a file-hosting service operated by Microsoft.

First, you need to install a python package.

See a [usage example](/oss/python/integrations/document_loaders/microsoft_onedrive).

### Microsoft OneDrive File

> [Microsoft OneDrive](https://en.wikipedia.org/wiki/OneDrive) (formerly `SkyDrive`) is a file-hosting service operated by Microsoft.

First, you need to install a python package.

> [Microsoft Word](https://www.microsoft.com/en-us/microsoft-365/word) is a word processor developed by Microsoft.

See a [usage example](/oss/python/integrations/document_loaders/microsoft_word).

> [Microsoft Excel](https://en.wikipedia.org/wiki/Microsoft_Excel) is a spreadsheet editor developed by
> Microsoft for Windows, macOS, Android, iOS and iPadOS.
> It features calculation or computation capabilities, graphing tools, pivot tables, and a macro programming
> language called Visual Basic for Applications (VBA). Excel forms part of the Microsoft 365 suite of software.

The `UnstructuredExcelLoader` is used to load `Microsoft Excel` files. The loader works with both `.xlsx` and `.xls` files.
The page content will be the raw text of the Excel file. If you use the loader in `"elements"` mode, an HTML
representation of the Excel file will be available in the document metadata under the `text_as_html` key.

See a [usage example](/oss/python/integrations/document_loaders/microsoft_excel).

### Microsoft SharePoint

> [Microsoft SharePoint](https://en.wikipedia.org/wiki/SharePoint) is a website-based collaboration system
> that uses workflow applications, “list” databases, and other web parts and security features to
> empower business teams to work together developed by Microsoft.

See a [usage example](/oss/python/integrations/document_loaders/microsoft_sharepoint).

### Microsoft PowerPoint

> [Microsoft PowerPoint](https://en.wikipedia.org/wiki/Microsoft_PowerPoint) is a presentation program by Microsoft.

See a [usage example](/oss/python/integrations/document_loaders/microsoft_powerpoint).

### Microsoft OneNote

First, let's install dependencies:

See a [usage example](/oss/python/integrations/document_loaders/microsoft_onenote).

### Playwright URL Loader

> [Playwright](https://github.com/microsoft/playwright) is an open-source automation tool
> developed by `Microsoft` that allows you to programmatically control and automate
> web browsers. It is designed for end-to-end testing, scraping, and automating
> tasks across various web browsers such as `Chromium`, `Firefox`, and `WebKit`.

First, let's install dependencies:

See a [usage example](/oss/python/integrations/document_loaders/url/#playwright-url-loader).

### Azure Cosmos DB Chat Message History

> [Azure Cosmos DB](https://learn.microsoft.com/azure/cosmos-db/) provides chat message history storage for conversational AI applications, enabling you to persist and retrieve conversation history with low latency and high availability.

Configure your Azure Cosmos DB connection:

AI agents can rely on Azure Cosmos DB as a unified [memory system](https://learn.microsoft.com/en-us/azure/cosmos-db/ai-agents#memory-can-make-or-break-agents) solution, enjoying speed, scale, and simplicity. This service successfully [enabled OpenAI's ChatGPT service](https://www.youtube.com/watch?v=6IIUtEFKJec\&t) to scale dynamically with high reliability and low maintenance. Powered by an atom-record-sequence engine, it is the world's first globally distributed [NoSQL](https://learn.microsoft.com/en-us/azure/cosmos-db/distributed-nosql), [relational](https://learn.microsoft.com/en-us/azure/cosmos-db/distributed-relational), and [vector database](https://learn.microsoft.com/en-us/azure/cosmos-db/vector-database) service that offers a serverless mode.

Below are two available Azure Cosmos DB APIs that can provide vector store functionalities.

#### Azure Cosmos DB for MongoDB (vCore)

> [Azure Cosmos DB for MongoDB vCore](https://learn.microsoft.com/en-us/azure/cosmos-db/mongodb/vcore/) makes it easy to create a database with full native MongoDB support.
> You can apply your MongoDB experience and continue to use your favorite MongoDB drivers, SDKs, and tools by pointing your application to the API for MongoDB vCore account's connection string.
> Use vector search in Azure Cosmos DB for MongoDB vCore to seamlessly integrate your AI-based applications with your data that's stored in Azure Cosmos DB.

##### Installation and Setup

See [detailed configuration instructions](/oss/python/integrations/vectorstores/azure_cosmos_db_mongo_vcore).

We need to install `langchain-azure-ai` and `pymongo` python packages.

##### Deploy Azure Cosmos DB on Microsoft Azure

Azure Cosmos DB for MongoDB vCore provides developers with a fully managed MongoDB-compatible database service for building modern applications with a familiar architecture.

With Cosmos DB for MongoDB vCore, developers can enjoy the benefits of native Azure integrations, low total cost of ownership (TCO), and the familiar vCore architecture when migrating existing applications or building new ones.

[Sign Up](https://azure.microsoft.com/en-us/free/) for free to get started today.

See a [usage example](/oss/python/integrations/vectorstores/azure_cosmos_db_mongo_vcore).

#### Azure Cosmos DB NoSQL

> [Azure Cosmos DB for NoSQL](https://learn.microsoft.com/en-us/azure/cosmos-db/nosql/vector-search) now offers vector indexing and search in preview.
> This feature is designed to handle high-dimensional vectors, enabling efficient and accurate vector search at any scale. You can now store vectors
> directly in the documents alongside your data. This means that each document in your database can contain not only traditional schema-free data,
> but also high-dimensional vectors as other properties of the documents. This colocation of data and vectors allows for efficient indexing and searching,
> as the vectors are stored in the same logical unit as the data they represent. This simplifies data management, AI application architectures, and the
> efficiency of vector-based operations.

##### Installation and Setup

See [detail configuration instructions](/oss/python/integrations/vectorstores/azure_cosmos_db_no_sql).

We need to install `langchain-azure-ai` and `azure-cosmos` python packages.

##### Deploy Azure Cosmos DB on Microsoft Azure

Azure Cosmos DB offers a solution for modern apps and intelligent workloads by being very responsive with dynamic and elastic autoscale. It is available
in every Azure region and can automatically replicate data closer to users. It has SLA guaranteed low-latency and high availability.

[Sign Up](https://learn.microsoft.com/en-us/azure/cosmos-db/nosql/quickstart-python?pivots=devcontainer-codespace) for free to get started today.

See a [usage example](/oss/python/integrations/vectorstores/azure_cosmos_db_no_sql).

### Azure Database for PostgreSQL

> [Azure Database for PostgreSQL - Flexible Server](https://learn.microsoft.com/en-us/azure/postgresql/flexible-server/service-overview) is a relational database service based on the open-source Postgres database engine. It's a fully managed database-as-a-service that can handle mission-critical workloads with predictable performance, security, high availability, and dynamic scalability.

See [set up instructions](https://learn.microsoft.com/en-us/azure/postgresql/flexible-server/quickstart-create-server-portal) for Azure Database for PostgreSQL.

Simply use the [connection string](https://learn.microsoft.com/en-us/azure/postgresql/flexible-server/connect-python?tabs=cmd%2Cpassword#add-authentication-code) from your Azure Portal.

Since Azure Database for PostgreSQL is open-source Postgres, you can use the [LangChain's Postgres support](/oss/python/integrations/vectorstores/pgvector/) to connect to Azure Database for PostgreSQL.

### Azure SQL Database

> [Azure SQL Database](https://learn.microsoft.com/azure/azure-sql/database/sql-database-paas-overview?view=azuresql) is a robust service that combines scalability, security, and high availability, providing all the benefits of a modern database solution.  It also provides a dedicated Vector data type & built-in functions that simplifies the storage and querying of vector embeddings directly within a relational database. This eliminates the need for separate vector databases and related integrations, increasing the security of your solutions while reducing the overall complexity.

By leveraging your current SQL Server databases for vector search, you can enhance data capabilities while minimizing expenses and avoiding the challenges of transitioning to new systems.

##### Installation and Setup

See [detail configuration instructions](/oss/python/integrations/vectorstores/sqlserver).

We need to install the `langchain-sqlserver` python package.

##### Deploy Azure SQL DB on Microsoft Azure

[Sign Up](https://learn.microsoft.com/azure/azure-sql/database/free-offer?view=azuresql) for free to get started today.

See a [usage example](/oss/python/integrations/vectorstores/sqlserver).

[Azure AI Search](https://learn.microsoft.com/azure/search/search-what-is-azure-search) is a cloud search service
that gives developers infrastructure, APIs, and tools for information retrieval of vector, keyword, and hybrid
queries at scale. See [here](/oss/python/integrations/vectorstores/azuresearch) for usage examples.

> [Azure AI Search](https://learn.microsoft.com/en-us/azure/search/search-what-is-azure-search) (formerly known as `Azure Search` or `Azure Cognitive Search` ) is a cloud search service that gives developers infrastructure, APIs, and tools for building a rich search experience over private, heterogeneous content in web, mobile, and enterprise applications.

> Search is foundational to any app that surfaces text to users, where common scenarios include catalog or document search, online retail apps, or data exploration over proprietary content. When you create a search service, you'll work with the following capabilities:
>
> * A search engine for full text search over a search index containing user-owned content
> * Rich indexing, with lexical analysis and optional AI enrichment for content extraction and transformation
> * Rich query syntax for text search, fuzzy search, autocomplete, geo-search and more
> * Programmability through REST APIs and client libraries in Azure SDKs
> * Azure integration at the data layer, machine learning layer, and AI (AI Services)

See [set up instructions](https://learn.microsoft.com/en-us/azure/search/search-create-service-portal).

See a [usage example](/oss/python/integrations/retrievers/azure_ai_search).

### Azure Database for PostgreSQL

> [Azure Database for PostgreSQL - Flexible Server](https://learn.microsoft.com/en-us/azure/postgresql/flexible-server/service-overview) is a relational database service based on the open-source Postgres database engine. It's a fully managed database-as-a-service that can handle mission-critical workloads with predictable performance, security, high availability, and dynamic scalability.

See [set up instructions](https://learn.microsoft.com/en-us/azure/postgresql/flexible-server/quickstart-create-server-portal) for Azure Database for PostgreSQL.

You need to [enable pgvector extension](https://learn.microsoft.com/en-us/azure/postgresql/flexible-server/how-to-use-pgvector) in your database to use Postgres as a vector store. Once you have the extension enabled, you can use the [PGVector in LangChain](/oss/python/integrations/vectorstores/pgvector/) to connect to Azure Database for PostgreSQL.

See a [usage example](/oss/python/integrations/vectorstores/pgvector/). Simply use the [connection string](https://learn.microsoft.com/en-us/azure/postgresql/flexible-server/connect-python?tabs=cmd%2Cpassword#add-authentication-code) from your Azure Portal.

### Azure Container Apps dynamic sessions

We need to get the `POOL_MANAGEMENT_ENDPOINT` environment variable from the Azure Container Apps service.
See the instructions [here](/oss/python/integrations/tools/azure_dynamic_sessions/#setup).

We need to install a python package.

See a [usage example](/oss/python/integrations/tools/azure_dynamic_sessions).

Follow the documentation [here](/oss/python/integrations/tools/bing_search) to get a detail explanations and instructions of this tool.

The environment variable `BING_SUBSCRIPTION_KEY` and `BING_SEARCH_URL` are required from Bing Search resource.

### Azure AI Services

We need to install several python packages.

See a [usage example](/oss/python/integrations/tools/azure_ai_services).

#### Azure AI Services individual tools

The `azure_ai_services` toolkit includes the following tools:

* Image Analysis: [AzureAiServicesImageAnalysisTool](https://python.langchain.com/api_reference/community/tools/langchain_community.tools.azure_ai_services.image_analysis.AzureAiServicesImageAnalysisTool.html)
* Document Intelligence: [AzureAiServicesDocumentIntelligenceTool](https://python.langchain.com/api_reference/community/tools/langchain_community.tools.azure_ai_services.document_intelligence.AzureAiServicesDocumentIntelligenceTool.html)
* Speech to Text: [AzureAiServicesSpeechToTextTool](https://python.langchain.com/api_reference/community/tools/langchain_community.tools.azure_ai_services.speech_to_text.AzureAiServicesSpeechToTextTool.html)
* Text to Speech: [AzureAiServicesTextToSpeechTool](https://python.langchain.com/api_reference/community/tools/langchain_community.tools.azure_ai_services.text_to_speech.AzureAiServicesTextToSpeechTool.html)
* Text Analytics for Health: [AzureAiServicesTextAnalyticsForHealthTool](https://python.langchain.com/api_reference/community/tools/langchain_community.tools.azure_ai_services.text_analytics_for_health.AzureAiServicesTextAnalyticsForHealthTool.html)

### Azure Cognitive Services

We need to install several python packages.

See a [usage example](/oss/python/integrations/tools/azure_cognitive_services).

#### Azure AI Services individual tools

The `azure_ai_services` toolkit includes the tools that queries the `Azure Cognitive Services`:

* `AzureCogsFormRecognizerTool`: Form Recognizer API
* `AzureCogsImageAnalysisTool`: Image Analysis API
* `AzureCogsSpeech2TextTool`: Speech2Text API
* `AzureCogsText2SpeechTool`: Text2Speech API
* `AzureCogsTextAnalyticsHealthTool`: Text Analytics for Health API

### Microsoft Office 365 email and calendar

We need to install `O365` python package.

See a [usage example](/oss/python/integrations/tools/office365).

#### Office 365 individual tools

You can use individual tools from the Office 365 Toolkit:

* `O365CreateDraftMessage`: creating a draft email in Office 365
* `O365SearchEmails`: searching email messages in Office 365
* `O365SearchEvents`: searching calendar events in Office 365
* `O365SendEvent`: sending calendar events in Office 365
* `O365SendMessage`: sending an email in Office 365

### Microsoft Azure PowerBI

We need to install `azure-identity` python package.

See a [usage example](/oss/python/integrations/tools/powerbi).

#### PowerBI individual tools

You can use individual tools from the Azure PowerBI Toolkit:

* `InfoPowerBITool`: getting metadata about a PowerBI Dataset
* `ListPowerBITool`: getting tables names
* `QueryPowerBITool`: querying a PowerBI Dataset

### PlayWright Browser Toolkit

> [Playwright](https://github.com/microsoft/playwright) is an open-source automation tool
> developed by `Microsoft` that allows you to programmatically control and automate
> web browsers. It is designed for end-to-end testing, scraping, and automating
> tasks across various web browsers such as `Chromium`, `Firefox`, and `WebKit`.

We need to install several python packages.

See a [usage example](/oss/python/integrations/tools/playwright).

#### PlayWright Browser individual tools

You can use individual tools from the PlayWright Browser Toolkit.

### Azure Cosmos DB for Apache Gremlin

We need to install a python package.

See a [usage example](/oss/python/integrations/graphs/azure_cosmosdb_gremlin).

> [Microsoft Bing](https://www.bing.com/), commonly referred to as `Bing` or `Bing Search`,
> is a web search engine owned and operated by `Microsoft`.

See a [usage example](/oss/python/integrations/tools/bing_search).

### Microsoft Presidio

> [Presidio](https://microsoft.github.io/presidio/) (Origin from Latin praesidium ‘protection, garrison’)
> helps to ensure sensitive data is properly managed and governed. It provides fast identification and
> anonymization modules for private entities in text and images such as credit card numbers, names,
> locations, social security numbers, bitcoin wallets, US phone numbers, financial data and more.

First, you need to install several python packages and download a `SpaCy` model.

See [usage examples](https://python.langchain.com/v0.1/docs/guides/productionization/safety/presidio_data_anonymization).

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/providers/microsoft.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

Set the environment variables to get access to the `Azure OpenAI` service.
```

Example 3 (unknown):
```unknown
See a [usage example](/oss/python/integrations/chat/azure_chat_openai)
```

Example 4 (unknown):
```unknown
### Azure AI

> [Azure AI Foundry](https://learn.microsoft.com/en-us/azure/developer/python/get-started) provides access to a wide range of models from various providers including Azure OpenAI, DeepSeek R1, Cohere, Phi and Mistral through the `AzureAIChatCompletionsModel` class.

<CodeGroup>
```

---

## Middleware

**URL:** llms-txt#middleware

**Contents:**
- What can middleware do?
- Built-in middleware
  - Summarization
  - Human-in-the-loop
  - Anthropic prompt caching
  - Model call limit
  - Tool call limit
  - Model fallback
  - PII detection
  - To-do list

Source: https://docs.langchain.com/oss/javascript/langchain/middleware

Control and customize agent execution at every step

Middleware provides a way to more tightly control what happens inside the agent.

The core agent loop involves calling a model, letting it choose tools to execute, and then finishing when it calls no more tools:

<div style={{ display: "flex", justifyContent: "center" }}>
  <img src="https://mintcdn.com/langchain-5e9cc07a/Tazq8zGc0yYUYrDl/oss/images/core_agent_loop.png?fit=max&auto=format&n=Tazq8zGc0yYUYrDl&q=85&s=ac72e48317a9ced68fd1be64e89ec063" alt="Core agent loop diagram" className="rounded-lg" data-og-width="300" width="300" data-og-height="268" height="268" data-path="oss/images/core_agent_loop.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Tazq8zGc0yYUYrDl/oss/images/core_agent_loop.png?w=280&fit=max&auto=format&n=Tazq8zGc0yYUYrDl&q=85&s=a4c4b766b6678ef52a6ed556b1a0b032 280w, https://mintcdn.com/langchain-5e9cc07a/Tazq8zGc0yYUYrDl/oss/images/core_agent_loop.png?w=560&fit=max&auto=format&n=Tazq8zGc0yYUYrDl&q=85&s=111869e6e99a52c0eff60a1ef7ddc49c 560w, https://mintcdn.com/langchain-5e9cc07a/Tazq8zGc0yYUYrDl/oss/images/core_agent_loop.png?w=840&fit=max&auto=format&n=Tazq8zGc0yYUYrDl&q=85&s=6c1e21de7b53bd0a29683aca09c6f86e 840w, https://mintcdn.com/langchain-5e9cc07a/Tazq8zGc0yYUYrDl/oss/images/core_agent_loop.png?w=1100&fit=max&auto=format&n=Tazq8zGc0yYUYrDl&q=85&s=88bef556edba9869b759551c610c60f4 1100w, https://mintcdn.com/langchain-5e9cc07a/Tazq8zGc0yYUYrDl/oss/images/core_agent_loop.png?w=1650&fit=max&auto=format&n=Tazq8zGc0yYUYrDl&q=85&s=9b0bdd138e9548eeb5056dc0ed2d4a4b 1650w, https://mintcdn.com/langchain-5e9cc07a/Tazq8zGc0yYUYrDl/oss/images/core_agent_loop.png?w=2500&fit=max&auto=format&n=Tazq8zGc0yYUYrDl&q=85&s=41eb4f053ed5e6b0ba5bad2badf6d755 2500w" />
</div>

Middleware exposes hooks before and after each of those steps:

<div style={{ display: "flex", justifyContent: "center" }}>
  <img src="https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=eb4404b137edec6f6f0c8ccb8323eaf1" alt="Middleware flow diagram" className="rounded-lg" data-og-width="500" width="500" data-og-height="560" height="560" data-path="oss/images/middleware_final.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?w=280&fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=483413aa87cf93323b0f47c0dd5528e8 280w, https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?w=560&fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=41b7dd647447978ff776edafe5f42499 560w, https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?w=840&fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=e9b14e264f68345de08ae76f032c52d4 840w, https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?w=1100&fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=ec45e1932d1279b1beee4a4b016b473f 1100w, https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?w=1650&fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=3bca5ebf8aa56632b8a9826f7f112e57 1650w, https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?w=2500&fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=437f141d1266f08a95f030c2804691d9 2500w" />
</div>

## What can middleware do?

<CardGroup cols={2}>
  <Card title="Monitor" icon="chart-line">
    Track agent behavior with logging, analytics, and debugging
  </Card>

<Card title="Modify" icon="pencil">
    Transform prompts, tool selection, and output formatting
  </Card>

<Card title="Control" icon="sliders">
    Add retries, fallbacks, and early termination logic
  </Card>

<Card title="Enforce" icon="shield">
    Apply rate limits, guardrails, and PII detection
  </Card>
</CardGroup>

Add middleware by passing it to @\[`create_agent`]:

## Built-in middleware

LangChain provides prebuilt middleware for common use cases:

Automatically summarize conversation history when approaching token limits.

<Tip>
  **Perfect for:**

* Long-running conversations that exceed context windows
  * Multi-turn dialogues with extensive history
  * Applications where preserving full conversation context matters
</Tip>

<Accordion title="Configuration options">
  <ParamField body="model" type="string" required>
    Model for generating summaries
  </ParamField>

<ParamField body="maxTokensBeforeSummary" type="number">
    Token threshold for triggering summarization
  </ParamField>

<ParamField body="messagesToKeep" type="number" default="20">
    Recent messages to preserve
  </ParamField>

<ParamField body="tokenCounter" type="function">
    Custom token counting function. Defaults to character-based counting.
  </ParamField>

<ParamField body="summaryPrompt" type="string">
    Custom prompt template. Uses built-in template if not specified.
  </ParamField>

<ParamField body="summaryPrefix" type="string" default="## Previous conversation summary:">
    Prefix for summary messages
  </ParamField>
</Accordion>

### Human-in-the-loop

Pause agent execution for human approval, editing, or rejection of tool calls before they execute.

<Tip>
  **Perfect for:**

* High-stakes operations requiring human approval (database writes, financial transactions)
  * Compliance workflows where human oversight is mandatory
  * Long running conversations where human feedback is used to guide the agent
</Tip>

<Accordion title="Configuration options">
  <ParamField body="interruptOn" type="object" required>
    Mapping of tool names to approval configs
  </ParamField>

**Tool approval config options:**

<ParamField body="allowAccept" type="boolean" default="false">
    Whether approval is allowed
  </ParamField>

<ParamField body="allowEdit" type="boolean" default="false">
    Whether editing is allowed
  </ParamField>

<ParamField body="allowRespond" type="boolean" default="false">
    Whether responding/rejection is allowed
  </ParamField>
</Accordion>

<Note>
  **Important:** Human-in-the-loop middleware requires a [checkpointer](/oss/javascript/langgraph/persistence#checkpoints) to maintain state across interruptions.

See the [human-in-the-loop documentation](/oss/javascript/langchain/human-in-the-loop) for complete examples and integration patterns.
</Note>

### Anthropic prompt caching

Reduce costs by caching repetitive prompt prefixes with Anthropic models.

<Tip>
  **Perfect for:**

* Applications with long, repeated system prompts
  * Agents that reuse the same context across invocations
  * Reducing API costs for high-volume deployments
</Tip>

<Info>
  Learn more about [Anthropic Prompt Caching](https://docs.claude.com/en/docs/build-with-claude/prompt-caching#cache-limitations) strategies and limitations.
</Info>

<Accordion title="Configuration options">
  <ParamField body="ttl" type="string" default="5m">
    Time to live for cached content. Valid values: `"5m"` or `"1h"`
  </ParamField>
</Accordion>

Limit the number of model calls to prevent infinite loops or excessive costs.

<Tip>
  **Perfect for:**

* Preventing runaway agents from making too many API calls
  * Enforcing cost controls on production deployments
  * Testing agent behavior within specific call budgets
</Tip>

<Accordion title="Configuration options">
  <ParamField body="threadLimit" type="number">
    Maximum model calls across all runs in a thread. Defaults to no limit.
  </ParamField>

<ParamField body="runLimit" type="number">
    Maximum model calls per single invocation. Defaults to no limit.
  </ParamField>

<ParamField body="exitBehavior" type="string" default="end">
    Behavior when limit is reached. Options: `"end"` (graceful termination) or `"error"` (throw exception)
  </ParamField>
</Accordion>

Control agent execution by limiting the number of tool calls, either globally across all tools or for specific tools.

<Tip>
  **Perfect for:**

* Preventing excessive calls to expensive external APIs
  * Limiting web searches or database queries
  * Enforcing rate limits on specific tool usage
  * Protecting against runaway agent loops
</Tip>

To limit tool calls globally across all tools or for specific tools, set `toolName`. For each limit, specify one or both of:

* **Thread limit** (`threadLimit`) - Max calls across all runs in a conversation. Persists across invocations. Requires a checkpointer.
* **Run limit** (`runLimit`) - Max calls per single invocation. Resets each turn.

| Behavior                   | Effect                                                     | Best For                                                            |
| -------------------------- | ---------------------------------------------------------- | ------------------------------------------------------------------- |
| **`"continue"`** (default) | Blocks exceeded calls with error messages, agent continues | Most use cases - agent handles limits gracefully                    |
| **`"error"`**              | Raises exception immediately                               | Complex workflows where you want to handle the limit error manually |
| **`"end"`**                | Stops with ToolMessage + AI message                        | Single-tool scenarios (errors if other tools pending)               |

<Accordion title="Configuration options">
  <ParamField body="toolName" type="string">
    Name of specific tool to limit. If not provided, limits apply to **all tools globally**.
  </ParamField>

<ParamField body="threadLimit" type="number">
    Maximum tool calls across all runs in a thread (conversation). Persists across multiple invocations with the same thread ID. Requires a checkpointer to maintain state. `undefined` means no thread limit.
  </ParamField>

<ParamField body="runLimit" type="number">
    Maximum tool calls per single invocation (one user message → response cycle). Resets with each new user message. `undefined` means no run limit.

**Note:** At least one of `threadLimit` or `runLimit` must be specified.
  </ParamField>

<ParamField body="exitBehavior" type="string" default="continue">
    Behavior when limit is reached:

* `"continue"` (default) - Block exceeded tool calls with error messages, let other tools and the model continue. The model decides when to end based on the error messages.
    * `"error"` - Throw a `ToolCallLimitExceededError` exception, stopping execution immediately
    * `"end"` - Stop execution immediately with a ToolMessage and AI message for the exceeded tool call. Only works when limiting a single tool; throws error if other tools have pending calls.
  </ParamField>
</Accordion>

Automatically fallback to alternative models when the primary model fails.

<Tip>
  **Perfect for:**

* Building resilient agents that handle model outages
  * Cost optimization by falling back to cheaper models
  * Provider redundancy across OpenAI, Anthropic, etc.
</Tip>

<Accordion title="Configuration options">
  The middleware accepts a variable number of string arguments representing fallback models in order:

<ParamField body="...models" type="string[]" required>
    One or more fallback model strings to try in order when the primary model fails

</ParamField>
</Accordion>

Detect and handle Personally Identifiable Information in conversations.

<Tip>
  **Perfect for:**

* Healthcare and financial applications with compliance requirements
  * Customer service agents that need to sanitize logs
  * Any application handling sensitive user data
</Tip>

<Accordion title="Configuration options">
  <ParamField body="piiType" type="string" required>
    Type of PII to detect. Can be a built-in type (`email`, `credit_card`, `ip`, `mac_address`, `url`) or a custom type name.
  </ParamField>

<ParamField body="strategy" type="string" default="redact">
    How to handle detected PII. Options:

* `"block"` - Throw error when detected
    * `"redact"` - Replace with `[REDACTED_TYPE]`
    * `"mask"` - Partially mask (e.g., `****-****-****-1234`)
    * `"hash"` - Replace with deterministic hash
  </ParamField>

<ParamField body="detector" type="RegExp">
    Custom detector regex pattern. If not provided, uses built-in detector for the PII type.
  </ParamField>

<ParamField body="applyToInput" type="boolean" default="true">
    Check user messages before model call
  </ParamField>

<ParamField body="applyToOutput" type="boolean" default="false">
    Check AI messages after model call
  </ParamField>

<ParamField body="applyToToolResults" type="boolean" default="false">
    Check tool result messages after execution
  </ParamField>
</Accordion>

Equip agents with task planning and tracking capabilities for complex multi-step tasks.

<Tip>
  **Perfect for:**

* Complex multi-step tasks requiring coordination across multiple tools
  * Long-running operations where progress visibility is important
</Tip>

Just as humans are more effective when they write down and track tasks, agents benefit from structured task management to break down complex problems, adapt plans as new information emerges, and provide transparency into their workflow.

You may have noticed patterns like this in Claude Code, which writes out a to-do list before tackling complex, multi-part tasks.

<Note>
  This middleware automatically provides agents with a `write_todos` tool and system prompts to guide effective task planning.
</Note>

<Accordion title="Configuration options">
  No configuration options available (uses defaults).
</Accordion>

### LLM tool selector

Use an LLM to intelligently select relevant tools before calling the main model.

<Tip>
  **Perfect for:**

* Agents with many tools (10+) where most aren't relevant per query
  * Reducing token usage by filtering irrelevant tools
  * Improving model focus and accuracy
</Tip>

<Accordion title="Configuration options">
  <ParamField body="model" type="string">
    Model for tool selection. Defaults to the agent's main model.
  </ParamField>

<ParamField body="maxTools" type="number">
    Maximum number of tools to select. Defaults to no limit.
  </ParamField>

<ParamField body="alwaysInclude" type="string[]">
    Array of tool names to always include in the selection
  </ParamField>
</Accordion>

Manage conversation context by trimming, summarizing, or clearing tool uses.

<Tip>
  **Perfect for:**

* Long conversations that need periodic context cleanup
  * Removing failed tool attempts from context
  * Custom context management strategies
</Tip>

<Accordion title="Configuration options">
  <ParamField body="edits" type="ContextEdit[]" default="[new ClearToolUsesEdit()]">
    Array of `ContextEdit` strategies to apply
  </ParamField>

**@\[`ClearToolUsesEdit`] options:**

<ParamField body="maxTokens" type="number" default="1000">
    Token count that triggers the edit
  </ParamField>
</Accordion>

Build custom middleware by implementing hooks that run at specific points in the agent execution flow.

## Class-based middleware

<CardGroup cols={2}>
  <Card title="Node-style hooks" icon="diagram-project">
    Run sequentially at specific execution points. Use for logging, validation, and state updates.
  </Card>

<Card title="Wrap-style hooks" icon="arrows-rotate">
    Intercept execution with full control over handler calls. Use for retries, caching, and transformation.
  </Card>
</CardGroup>

#### Node-style hooks

Run at specific points in the execution flow:

* `beforeAgent` - Before agent starts (once per invocation)
* `beforeModel` - Before each model call
* `afterModel` - After each model response
* `afterAgent` - After agent completes (up to once per invocation)

**Example: Logging middleware**

**Example: Conversation length limit**

#### Wrap-style hooks

Intercept execution and control when the handler is called:

* `wrapModelCall` - Around each model call
* `wrapToolCall` - Around each tool call

You decide if the handler is called zero times (short-circuit), once (normal flow), or multiple times (retry logic).

**Example: Model retry middleware**

**Example: Dynamic model selection**

**Example: Tool call monitoring**

### Custom state schema

Middleware can extend the agent's state with custom properties. Define a custom state type and set it as the `state_schema`:

### Context extension

Context properties are configuration values passed through the runnable config. Unlike state, context is read-only and typically used for configuration that doesn't change during execution.

Middleware can define context requirements that must be satisfied through the agent's configuration:

When using multiple middleware, understanding execution order is important:

<Accordion title="Execution flow (click to expand)">
  **Before hooks run in order:**

1. `middleware1.before_agent()`
  2. `middleware2.before_agent()`
  3. `middleware3.before_agent()`

**Agent loop starts**

5. `middleware1.before_model()`
  6. `middleware2.before_model()`
  7. `middleware3.before_model()`

**Wrap hooks nest like function calls:**

8. `middleware1.wrap_model_call()` → `middleware2.wrap_model_call()` → `middleware3.wrap_model_call()` → model

**After hooks run in reverse order:**

9. `middleware3.after_model()`
  10. `middleware2.after_model()`
  11. `middleware1.after_model()`

13. `middleware3.after_agent()`
  14. `middleware2.after_agent()`
  15. `middleware1.after_agent()`
</Accordion>

* `before_*` hooks: First to last
* `after_*` hooks: Last to first (reverse)
* `wrap_*` hooks: Nested (first middleware wraps all others)

To exit early from middleware, return a dictionary with `jump_to`:

Available jump targets:

* `"end"`: Jump to the end of the agent execution
* `"tools"`: Jump to the tools node
* `"model"`: Jump to the model node (or the first `before_model` hook)

**Important:** When jumping from `before_model` or `after_model`, jumping to `"model"` will cause all `before_model` middleware to run again.

To enable jumping, decorate your hook with `@hook_config(can_jump_to=[...])`:

1. Keep middleware focused - each should do one thing well
2. Handle errors gracefully - don't let middleware errors crash the agent
3. **Use appropriate hook types**:
   * Node-style for sequential logic (logging, validation)
   * Wrap-style for control flow (retry, fallback, caching)
4. Clearly document any custom state properties
5. Unit test middleware independently before integrating
6. Consider execution order - place critical middleware first in the list
7. Use built-in middleware when possible, don't reinvent the wheel :)

### Dynamically selecting tools

Select relevant tools at runtime to improve performance and accuracy.

<Tip>
  **Benefits:**

* **Shorter prompts** - Reduce complexity by exposing only relevant tools
  * **Better accuracy** - Models choose correctly from fewer options
  * **Permission control** - Dynamically filter tools based on user access
</Tip>

<Expandable title="Extended example: GitHub vs GitLab tool selection">

* Register all tools upfront
  * Middleware selects the relevant subset per request
  * Use `contextSchema` for configuration requirements
</Expandable>

## Additional resources

* [Middleware API reference](https://reference.langchain.com/python/langchain/middleware/) - Complete guide to custom middleware
* [Human-in-the-loop](/oss/javascript/langchain/human-in-the-loop) - Add human review for sensitive operations
* [Testing agents](/oss/javascript/langchain/test) - Strategies for testing safety mechanisms

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/middleware.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
## Built-in middleware

LangChain provides prebuilt middleware for common use cases:

### Summarization

Automatically summarize conversation history when approaching token limits.

<Tip>
  **Perfect for:**

  * Long-running conversations that exceed context windows
  * Multi-turn dialogues with extensive history
  * Applications where preserving full conversation context matters
</Tip>
```

Example 2 (unknown):
```unknown
<Accordion title="Configuration options">
  <ParamField body="model" type="string" required>
    Model for generating summaries
  </ParamField>

  <ParamField body="maxTokensBeforeSummary" type="number">
    Token threshold for triggering summarization
  </ParamField>

  <ParamField body="messagesToKeep" type="number" default="20">
    Recent messages to preserve
  </ParamField>

  <ParamField body="tokenCounter" type="function">
    Custom token counting function. Defaults to character-based counting.
  </ParamField>

  <ParamField body="summaryPrompt" type="string">
    Custom prompt template. Uses built-in template if not specified.
  </ParamField>

  <ParamField body="summaryPrefix" type="string" default="## Previous conversation summary:">
    Prefix for summary messages
  </ParamField>
</Accordion>

### Human-in-the-loop

Pause agent execution for human approval, editing, or rejection of tool calls before they execute.

<Tip>
  **Perfect for:**

  * High-stakes operations requiring human approval (database writes, financial transactions)
  * Compliance workflows where human oversight is mandatory
  * Long running conversations where human feedback is used to guide the agent
</Tip>
```

Example 3 (unknown):
```unknown
<Accordion title="Configuration options">
  <ParamField body="interruptOn" type="object" required>
    Mapping of tool names to approval configs
  </ParamField>

  **Tool approval config options:**

  <ParamField body="allowAccept" type="boolean" default="false">
    Whether approval is allowed
  </ParamField>

  <ParamField body="allowEdit" type="boolean" default="false">
    Whether editing is allowed
  </ParamField>

  <ParamField body="allowRespond" type="boolean" default="false">
    Whether responding/rejection is allowed
  </ParamField>
</Accordion>

<Note>
  **Important:** Human-in-the-loop middleware requires a [checkpointer](/oss/javascript/langgraph/persistence#checkpoints) to maintain state across interruptions.

  See the [human-in-the-loop documentation](/oss/javascript/langchain/human-in-the-loop) for complete examples and integration patterns.
</Note>

### Anthropic prompt caching

Reduce costs by caching repetitive prompt prefixes with Anthropic models.

<Tip>
  **Perfect for:**

  * Applications with long, repeated system prompts
  * Agents that reuse the same context across invocations
  * Reducing API costs for high-volume deployments
</Tip>

<Info>
  Learn more about [Anthropic Prompt Caching](https://docs.claude.com/en/docs/build-with-claude/prompt-caching#cache-limitations) strategies and limitations.
</Info>
```

Example 4 (unknown):
```unknown
<Accordion title="Configuration options">
  <ParamField body="ttl" type="string" default="5m">
    Time to live for cached content. Valid values: `"5m"` or `"1h"`
  </ParamField>
</Accordion>

### Model call limit

Limit the number of model calls to prevent infinite loops or excessive costs.

<Tip>
  **Perfect for:**

  * Preventing runaway agents from making too many API calls
  * Enforcing cost controls on production deployments
  * Testing agent behavior within specific call budgets
</Tip>
```

---

## minReplicas: 16

**URL:** llms-txt#minreplicas:-16

---

## minReplicas: 20

**URL:** llms-txt#minreplicas:-20

**Contents:**
- Ensure your Redis cache is at least 200 GB

## Ensure your Redis cache is at least 200 GB
redis:
  external:
    enabled: true
    existingSecretName: langsmith-redis-secret # Set the connection url for your external Redis instance (200+ GB)

---

## minReplicas: 2

**URL:** llms-txt#minreplicas:-2

platformBackend:
  deployment:
    replicas: 20 # OR enable autoscaling to this level (example below)

---

## minReplicas: 3

**URL:** llms-txt#minreplicas:-3

**Contents:**
- Ensure your Redis cache is at least 200 GB
  - High reads, low writes <a name="high-reads-low-writes" />

## Ensure your Redis cache is at least 200 GB
redis:
  external:
    enabled: true
    existingSecretName: langsmith-redis-secret # Set the connection url for your external Redis instance (200+ GB)

clickhouse:
  statefulSet:
    persistence:
      # This may depend on your configured TTL (see config section).
      # We recommend 600Gi for every shortlived TTL day if operating at this scale constantly.
      size: 4200Gi # This assumes 7 days TTL and operating a this scale constantly.
    resources:
      requests:
        cpu: "10"
        memory: "32Gi"
      limits:
        cpu: "16"
        memory: "48Gi"

commonEnv:
  - name: "CLICKHOUSE_ASYNC_INSERT_WAIT_PCT_FLOAT"
    value: "0"
yaml  theme={null}
config:
  blobStorage:
    # Please also set the other keys to connect to your blob storage. See configuration section.
    enabled: true

frontend:
  deployment:
    replicas: 2

queue:
  deployment:
    replicas: 6 # OR enable autoscaling to this level (example below)

**Examples:**

Example 1 (unknown):
```unknown
### High reads, low writes <a name="high-reads-low-writes" />

You have a relatively low scale of trace ingestions, but many frontend users querying traces and/or have scripts that hit the `/runs/query` or `/runs/<run-id>` endpoints frequently.

**For this, we strongly recommend setting up a replicated ClickHouse cluster to enable high read scale at low latency.** See our [external ClickHouse doc](/langsmith/self-host-external-clickhouse#ha-replicated-clickhouse-cluster) for more guidance on how to setup a replicated ClickHouse cluster. For this load pattern, we recommend using a 3 node replicated setup, where each replica in the cluster should have resource requests of 8+ cores and 16+ GB memory, and resource limit of 12 cores and 32 GB memory.

For this, we recommend a configuration like this:
```

---

## minReplicas: 40

**URL:** llms-txt#minreplicas:-40

backend:
  deployment:
    replicas: 50 # OR enable autoscaling to this level (example below)

---

## minReplicas: 4

**URL:** llms-txt#minreplicas:-4

backend:
  deployment:
    replicas: 40 # OR enable autoscaling to this level (example below)

---

## minReplicas: 5

**URL:** llms-txt#minreplicas:-5

backend:
  deployment:
    replicas: 16 # OR enable autoscaling to this level (example below)

---

## minReplicas: 8

**URL:** llms-txt#minreplicas:-8

**Contents:**
- Note that we are actively working on improving performance of this service to reduce the number of replicas.

## Note that we are actively working on improving performance of this service to reduce the number of replicas.
queue:
  deployment:
    replicas: 160 # OR enable autoscaling to this level (example below)

---

## Mirror images for your LangSmith installation

**URL:** llms-txt#mirror-images-for-your-langsmith-installation

**Contents:**
- Requirements
- Mirroring the Images

Source: https://docs.langchain.com/langsmith/self-host-mirroring-images

By default, LangSmith will pull images from our public Docker registry. However, if you are running LangSmith in an environment that does not have internet access, or if you would like to use a private Docker registry, you can mirror the images to your own registry and then configure your LangSmith installation to use those images.

* Authenticated access to a Docker registry that your Kubernetes cluster/machine has access to.
* Docker installed on your local machine or a machine that has access to the Docker registry.
* A Kubernetes cluster or a machine where you can run LangSmith.

## Mirroring the Images

For your convenience, we have provided a script that will mirror the images for you. You can find the script in the [LangSmith Helm Chart repository](https://github.com/langchain-ai/helm/blob/main/charts/langsmith/scripts/mirror_langsmith_images.sh)

To use the script, you will need to run the script with the following command specifying your registry and platform:

Where `<your-registry>` is the URL of your Docker registry (e.g. `myregistry.com`) and `<platform>` is the platform you are using (e.g. `linux/amd64`, `linux/arm64`, etc.). If you do not specify a platform, it will default to `linux/amd64`.

For example, if your registry is `myregistry.com`, your platform is `linux/arm64`, and you want to use the latest version of the images, you would run:

Note that this script will assume that you have Docker installed and that you are authenticated to your registry. It will also push the images to the specified registry with the same repository/tag as the original images.

Alternatively, you can pull, mirror, and push the images manually. The images that you will need to mirror are found in the `values.yaml` file of the LangSmith Helm Chart. These can be found here: [LangSmith Helm Chart values.yaml](https://github.com/langchain-ai/helm/blob/main/charts/langsmith/values.yaml#L14)

Here is an example of how to mirror the images using Docker:

```bash  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
Where `<your-registry>` is the URL of your Docker registry (e.g. `myregistry.com`) and `<platform>` is the platform you are using (e.g. `linux/amd64`, `linux/arm64`, etc.). If you do not specify a platform, it will default to `linux/amd64`.

For example, if your registry is `myregistry.com`, your platform is `linux/arm64`, and you want to use the latest version of the images, you would run:
```

Example 2 (unknown):
```unknown
Note that this script will assume that you have Docker installed and that you are authenticated to your registry. It will also push the images to the specified registry with the same repository/tag as the original images.

Alternatively, you can pull, mirror, and push the images manually. The images that you will need to mirror are found in the `values.yaml` file of the LangSmith Helm Chart. These can be found here: [LangSmith Helm Chart values.yaml](https://github.com/langchain-ai/helm/blob/main/charts/langsmith/values.yaml#L14)

Here is an example of how to mirror the images using Docker:
```

---

## Models

**URL:** llms-txt#models

**Contents:**
- Basic usage
  - Initialize a model
  - Key methods
- Parameters
- Invocation
  - Invoke
  - Stream
  - Batch
- Tool calling
- Structured outputs

Source: https://docs.langchain.com/oss/javascript/langchain/models

[LLMs](https://en.wikipedia.org/wiki/Large_language_model) are powerful AI tools that can interpret and generate text like humans. They're versatile enough to write content, translate languages, summarize, and answer questions without needing specialized training for each task.

In addition to text generation, many models support:

* <Icon icon="hammer" size={16} /> [Tool calling](#tool-calling) - calling external tools (like databases queries or API calls) and use results in their responses.
* <Icon icon="shapes" size={16} /> [Structured output](#structured-outputs) - where the model's response is constrained to follow a defined format.
* <Icon icon="image" size={16} /> [Multimodality](#multimodal) - process and return data other than text, such as images, audio, and video.
* <Icon icon="brain" size={16} /> [Reasoning](#reasoning) - models perform multi-step reasoning to arrive at a conclusion.

Models are the reasoning engine of [agents](/oss/javascript/langchain/agents). They drive the agent's decision-making process, determining which tools to call, how to interpret results, and when to provide a final answer.

The quality and capabilities of the model you choose directly impact your agent's reliability and performance. Different models excel at different tasks - some are better at following complex instructions, others at structured reasoning, and some support larger context windows for handling more information.

LangChain's standard model interfaces give you access to many different provider integrations, which makes it easy to experiment with and switch between models to find the best fit for your case.

<Info>
  For provider-specific integration information and capabilities, see the provider's [chat model page](/oss/javascript/integrations/chat).
</Info>

Models can be utilized in two ways:

1. **With agents** - Models can be dynamically specified when creating an [agent](/oss/javascript/langchain/agents#model).
2. **Standalone** - Models can be called directly (outside of the agent loop) for tasks like text generation, classification, or extraction without the need for an agent framework.

The same model interface works in both contexts, which gives you the flexibility to start simple and scale up to more complex agent-based workflows as needed.

### Initialize a model

The easiest way to get started with a standalone model in LangChain is to use `initChatModel` to initialize one from a [provider](/oss/javascript/integrations/providers/overview) of your choice (examples below):

<Tabs>
  <Tab title="OpenAI">
    👉 Read the [OpenAI chat model integration docs](/oss/javascript/integrations/chat/openai/)

</CodeGroup>
  </Tab>

<Tab title="Anthropic">
    👉 Read the [Anthropic chat model integration docs](/oss/javascript/integrations/chat/anthropic/)

</CodeGroup>
  </Tab>

<Tab title="Azure">
    👉 Read the [Azure chat model integration docs](/oss/javascript/integrations/chat/azure/)

</CodeGroup>
  </Tab>

<Tab title="Google Gemini">
    👉 Read the [Google GenAI chat model integration docs](/oss/javascript/integrations/chat/google_generative_ai/)

</CodeGroup>
  </Tab>

<Tab title="Bedrock Converse">
    👉 Read the [AWS Bedrock chat model integration docs](/oss/javascript/integrations/chat/bedrock_converse/)

</CodeGroup>
  </Tab>
</Tabs>

See [`initChatModel`](https://v03.api.js.langchain.com/functions/langchain.chat_models_universal.initChatModel.html) for more detail, including information on how to pass model [parameters](#parameters).

<Card title="Invoke" href="#invoke" icon="paper-plane" arrow="true" horizontal>
  The model takes messages as input and outputs messages after generating a complete response.
</Card>

<Card title="Stream" href="#stream" icon="tower-broadcast" arrow="true" horizontal>
  Invoke the model, but stream the output as it is generated in real-time.
</Card>

<Card title="Batch" href="#batch" icon="grip" arrow="true" horizontal>
  Send multiple requests to a model in a batch for more efficient processing.
</Card>

<Info>
  In addition to chat models, LangChain provides support for other adjacent technologies, such as embedding models and vector stores. See the [integrations page](/oss/javascript/integrations/providers/overview) for details.
</Info>

A chat model takes parameters that can be used to configure its behavior. The full set of supported parameters varies by model and provider, but standard ones include:

<ParamField body="model" type="string" required>
  The name or identifier of the specific model you want to use with a provider.
</ParamField>

<ParamField body="apiKey" type="string">
  The key required for authenticating with the model's provider. This is usually issued when you sign up for access to the model. Often accessed by setting an <Tooltip tip="A variable whose value is set outside the program, typically through functionality built into the operating system or microservice.">environment variable</Tooltip>.
</ParamField>

<ParamField body="temperature" type="number">
  Controls the randomness of the model's output. A higher number makes responses more creative; lower ones make them more deterministic.
</ParamField>

<ParamField body="timeout" type="number">
  The maximum time (in seconds) to wait for a response from the model before canceling the request.
</ParamField>

<ParamField body="maxTokens" type="number">
  Limits the total number of <Tooltip tip="The basic unit that a model reads and generates. Providers may define them differently, but in general, they can represent a whole or part of word.">tokens</Tooltip> in the response, effectively controlling how long the output can be.
</ParamField>

<ParamField body="maxRetries" type="number">
  The maximum number of attempts the system will make to resend a request if it fails due to issues like network timeouts or rate limits.
</ParamField>

Using `initChatModel`, pass these parameters as inline parameters:

<Info>
  Each chat model integration may have additional params used to control provider-specific functionality. For example, @\[`ChatOpenAI`] has `use_responses_api` to dictate whether to use the OpenAI Responses or Completions API.

To find all the parameters supported by a given chat model, head to the [chat model integrations](/oss/javascript/integrations/chat) page.
</Info>

A chat model must be invoked to generate an output. There are three primary invocation methods, each suited to different use cases.

The most straightforward way to call a model is to use [`invoke()`](https://v03.api.js.langchain.com/classes/_langchain_core.language_models_chat_models.BaseChatModel.html#invoke) with a single message or a list of messages.

A list of messages can be provided to a model to represent conversation history. Each message has a role that models use to indicate who sent the message in the conversation. See the [messages](/oss/javascript/langchain/messages) guide for more detail on roles, types, and content.

Most models can stream their output content while it is being generated. By displaying output progressively, streaming significantly improves user experience, particularly for longer responses.

Calling [`stream()`](https://v03.api.js.langchain.com/classes/_langchain_core.language_models_chat_models.BaseChatModel.html#stream) returns an <Tooltip tip="An object that progressively provides access to each item of a collection, in order.">iterator</Tooltip> that yields output chunks as they are produced. You can use a loop to process each chunk in real-time:

As opposed to [`invoke()`](#invoke), which returns a single [`AIMessage`](https://v03.api.js.langchain.com/classes/_langchain_core.messages_ai_message.AIMessage.html) after the model has finished generating its full response, `stream()` returns multiple [`AIMessageChunk`](https://v03.api.js.langchain.com/classes/_langchain_core.messages_ai_message.AIMessageChunk.html) objects, each containing a portion of the output text. Importantly, each chunk in a stream is designed to be gathered into a full message via summation:

The resulting message can be treated the same as a message that was generated with [`invoke()`](#invoke) - for example, it can be aggregated into a message history and passed back to the model as conversational context.

<Warning>
  Streaming only works if all steps in the program know how to process a stream of chunks. For instance, an application that isn't streaming-capable would be one that needs to store the entire output in memory before it can be processed.
</Warning>

<Accordion title="Advanced streaming topics">
  <Accordion title="&#x22;Auto-streaming&#x22; chat models">
    LangChain simplifies streaming from chat models by automatically enabling streaming mode in certain cases, even when you're not explicitly calling the streaming methods. This is particularly useful when you use the non-streaming invoke method but still want to stream the entire application, including intermediate results from the chat model.

In [LangGraph agents](/oss/javascript/langchain/agents), for example, you can call `model.invoke()` within nodes, but LangChain will automatically delegate to streaming if running in a streaming mode.

When you `invoke()` a chat model, LangChain will automatically switch to an internal streaming mode if it detects that you are trying to stream the overall application. The result of the invocation will be the same as far as the code that was using invoke is concerned; however, while the chat model is being streamed, LangChain will take care of invoking @\[`on_llm_new_token`] events in LangChain's callback system.

Callback events allow LangGraph `stream()` and [`streamEvents()`](https://v03.api.js.langchain.com/classes/_langchain_core.language_models_chat_models.BaseChatModel.html#streamEvents) to surface the chat model's output in real-time.
  </Accordion>

<Accordion title="Streaming events">
    LangChain chat models can also stream semantic events using
    \[`streamEvents()`]\[BaseChatModel.streamEvents].

This simplifies filtering based on event types and other metadata, and will aggregate the full message in the background. See below for an example.

See the [`streamEvents()`](https://v03.api.js.langchain.com/classes/_langchain_core.language_models_chat_models.BaseChatModel.html#streamEvents) reference for event types and other details.
  </Accordion>
</Accordion>

Batching a collection of independent requests to a model can significantly improve performance and reduce costs, as the processing can be done in parallel:

<Tip>
  When processing a large number of inputs using `batch()`, you may want to control the maximum number of parallel calls. This can be done by setting the `maxConcurrency` attribute in the [`RunnableConfig`](https://v03.api.js.langchain.com/interfaces/_langchain_core.runnables.RunnableConfig.html) dictionary.

See the [`RunnableConfig`](https://v03.api.js.langchain.com/interfaces/_langchain_core.runnables.RunnableConfig.html) reference for a full list of supported attributes.
</Tip>

For more details on batching, see the [reference](https://v03.api.js.langchain.com/classes/_langchain_core.language_models_chat_models.BaseChatModel.html#batch).

Models can request to call tools that perform tasks such as fetching data from a database, searching the web, or running code. Tools are pairings of:

1. A schema, including the name of the tool, a description, and/or argument definitions (often a JSON schema)
2. A function or <Tooltip tip="A method that can suspend execution and resume at a later time">coroutine</Tooltip> to execute.

<Note>
  You may hear the term "function calling". We use this interchangeably with "tool calling".
</Note>

Here's the basic tool calling flow between a user and a model:

To make tools that you have defined available for use by a model, you must bind them using [`bindTools()`](https://v03.api.js.langchain.com/classes/langchain.chat_models_universal.ConfigurableModel.html#bindTools). In subsequent invocations, the model can choose to call any of the bound tools as needed.

Some model providers offer built-in tools that can be enabled via model or invocation parameters (e.g. [`ChatOpenAI`](/oss/javascript/integrations/chat/openai), [`ChatAnthropic`](/oss/javascript/integrations/chat/anthropic)). Check the respective [provider reference](/oss/javascript/integrations/providers/overview) for details.

<Tip>
  See the [tools guide](/oss/javascript/langchain/tools) for details and other options for creating tools.
</Tip>

When binding user-defined tools, the model's response includes a **request** to execute a tool. When using a model separately from an [agent](/oss/javascript/langchain/agents), it is up to you to perform the requested action and return the result back to the model for use in subsequent reasoning. Note that when using an [agent](/oss/javascript/langchain/agents), the agent loop will handle the tool execution loop for you.

Below, we show some common ways you can use tool calling.

<AccordionGroup>
  <Accordion title="Tool execution loop" icon="arrow-rotate-right">
    When a model returns tool calls, you need to execute the tools and pass the results back to the model. This creates a conversation loop where the model can use tool results to generate its final response. LangChain includes [agent](/oss/javascript/langchain/agents) abstractions that handle this orchestration for you.

Here's a simple example of how to do this:

Each @\[`ToolMessage`] returned by the tool includes a `tool_call_id` that matches the original tool call, helping the model correlate results with requests.
  </Accordion>

<Accordion title="Forcing tool calls" icon="asterisk">
    By default, the model has the freedom to choose which bound tool to use based on the user's input. However, you might want to force choosing a tool, ensuring the model uses either a particular tool or **any** tool from a given list:

</CodeGroup>
  </Accordion>

<Accordion title="Parallel tool calls" icon="layer-group">
    Many models support calling multiple tools in parallel when appropriate. This allows the model to gather information from different sources simultaneously.

The model intelligently determines when parallel execution is appropriate based on the independence of the requested operations.

<Tip>
      Most models supporting tool calling enable parallel tool calls by default. Some (including [OpenAI](/oss/javascript/integrations/chat/openai) and [Anthropic](/oss/javascript/integrations/chat/anthropic)) allow you to disable this feature. To do this, set `parallel_tool_calls=False`:

</Tip>
  </Accordion>

<Accordion title="Streaming tool calls" icon="rss">
    When streaming responses, tool calls are progressively built through @\[`ToolCallChunk`]. This allows you to see tool calls as they're being generated rather than waiting for the complete response.

You can accumulate chunks to build complete tool calls:

</Accordion>
</AccordionGroup>

## Structured outputs

Models can be requested to provide their response in a format matching a given schema. This is useful for ensuring the output can be easily parsed and used in subsequent processing. LangChain supports multiple schema types and methods for enforcing structured outputs.

<Tabs>
  <Tab title="Zod">
    A [zod schema](https://zod.dev/) is the preferred method of defining an output schema. Note that when a zod schema is provided, the model output will also be validated against the schema using zod's parse methods.

<Tab title="JSON Schema">
    For maximum control or interoperability, you can provide a raw JSON Schema.

<Note>
  **Key considerations for structured outputs:**

* **Method parameter**: Some providers support different methods (`'jsonSchema'`, `'functionCalling'`, `'jsonMode'`)
  * **Include raw**: Use @\[`includeRaw: true`]\[BaseChatModel.with\_structured\_output(include\_raw)] to get both the parsed output and the raw [`AIMessage`](https://v03.api.js.langchain.com/classes/_langchain_core.messages_ai_message.AIMessage.html)
  * **Validation**: Zod models provide automatic validation, while JSON Schema requires manual validation
</Note>

<Accordion title="Example: Message output alongside parsed structure">
  It can be useful to return the raw [`AIMessage`](https://v03.api.js.langchain.com/classes/_langchain_core.messages_ai_message.AIMessage.html) object alongside the parsed representation to access response metadata such as [token counts](#token-usage). To do this, set @\[`include_raw=True`]\[BaseChatModel.with\_structured\_output(include\_raw)] when calling @\[`with_structured_output`]\[BaseChatModel.with\_structured\_output]:

<Accordion title="Example: Nested structures">
  Schemas can be nested:

LangChain supports all major model providers, including OpenAI, Anthropic, Google, Azure, AWS Bedrock, and more. Each provider offers a variety of models with different capabilities. For a full list of supported models in LangChain, see the [integrations page](/oss/javascript/integrations/providers/overview).

Certain models can process and return non-textual data such as images, audio, and video. You can pass non-textual data to a model by providing [content blocks](/oss/javascript/langchain/messages#message-content).

<Tip>
  All LangChain chat models with underlying multimodal capabilities support:

1. Data in the cross-provider standard format (see [our messages guide](/oss/javascript/langchain/messages))
  2. OpenAI [chat completions](https://platform.openai.com/docs/api-reference/chat) format
  3. Any format that is native to that specific provider (e.g., Anthropic models accept Anthropic native format)
</Tip>

See the [multimodal section](/oss/javascript/langchain/messages#multimodal) of the messages guide for details.

<Tooltip tip="Not all LLMs are made equally!" cta="See reference" href="https://models.dev/">Some models</Tooltip> can return multimodal data as part of their response. If invoked to do so, the resulting [`AIMessage`](https://v03.api.js.langchain.com/classes/_langchain_core.messages_ai_message.AIMessage.html) will have content blocks with multimodal types.

See the [integrations page](/oss/javascript/integrations/providers/overview) for details on specific providers.

Newer models are capable of performing multi-step reasoning to arrive at a conclusion. This involves breaking down complex problems into smaller, more manageable steps.

**If supported by the underlying model,** you can surface this reasoning process to better understand how the model arrived at its final answer.

Depending on the model, you can sometimes specify the level of effort it should put into reasoning. Similarly, you can request that the model turn off reasoning entirely. This may take the form of categorical "tiers" of reasoning (e.g., `'low'` or `'high'`) or integer token budgets.

For details, see the [integrations page](/oss/javascript/integrations/providers/overview) or [reference](https://reference.langchain.com/python/integrations/) for your respective chat model.

LangChain supports running models locally on your own hardware. This is useful for scenarios where either data privacy is critical, you want to invoke a custom model, or when you want to avoid the costs incurred when using a cloud-based model.

[Ollama](/oss/javascript/integrations/chat/ollama) is one of the easiest ways to run models locally. See the full list of local integrations on the [integrations page](/oss/javascript/integrations/providers/overview).

Many providers offer prompt caching features to reduce latency and cost on repeat processing of the same tokens. These features can be **implicit** or **explicit**:

* **Implicit prompt caching:** providers will automatically pass on cost savings if a request hits a cache. Examples: [OpenAI](/oss/javascript/integrations/chat/openai) and [Gemini](/oss/javascript/integrations/chat/google_generative_ai) (Gemini 2.5 and above).
* **Explicit caching:** providers allow you to manually indicate cache points for greater control or to guarantee cost savings. Examples: @\[`ChatOpenAI`] (via `prompt_cache_key`), Anthropic's [`AnthropicPromptCachingMiddleware`](/oss/javascript/integrations/chat/anthropic#prompt-caching) and [`cache_control`](https://docs.langchain.com/oss/python/integrations/chat/anthropic#prompt-caching) options, [AWS Bedrock](/oss/javascript/integrations/chat/bedrock#prompt-caching), [Gemini](https://python.langchain.com/api_reference/google_genai/chat_models/langchain_google_genai.chat_models.ChatGoogleGenerativeAI.html).

<Warning>
  Prompt caching is often only engaged above a minimum input token threshold. See [provider pages](/oss/javascript/integrations/chat) for details.
</Warning>

Cache usage will be reflected in the [usage metadata](/oss/javascript/langchain/messages#token-usage) of the model response.

### Server-side tool use

Some providers support server-side [tool-calling](#tool-calling) loops: models can interact with web search, code interpreters, and other tools and analyze the results in a single conversational turn.

If a model invokes a tool server-side, the content of the response message will include content representing the invocation and result of the tool. Accessing the [content blocks](/oss/javascript/langchain/messages#standard-content-blocks) of the response will return the server-side tool calls and results in a provider-agnostic format:

This represents a single conversational turn; there are no associated [ToolMessage](/oss/javascript/langchain/messages#tool-message) objects that need to be passed in as in client-side [tool-calling](#tool-calling).

See the [integration page](/oss/javascript/integrations/chat) for your given provider for available tools and usage details.

### Base URL or proxy

For many chat model integrations, you can configure the base URL for API requests, which allows you to use model providers that have OpenAI-compatible APIs or to use a proxy server.

<Accordion title="Base URL" icon="link">
  Many model providers offer OpenAI-compatible APIs (e.g., [Together AI](https://www.together.ai/), [vLLM](https://github.com/vllm-project/vllm)). You can use `initChatModel` with these providers by specifying the appropriate `base_url` parameter:

<Note>
    When using direct chat model class instantiation, the parameter name may vary by provider. Check the respective [reference](/oss/javascript/integrations/providers/overview) for details.
  </Note>
</Accordion>

### Log probabilities

Certain models can be configured to return token-level log probabilities representing the likelihood of a given token by setting the `logprobs` parameter when initializing the model:

A number of model providers return token usage information as part of the invocation response. When available, this information will be included on the [`AIMessage`](https://v03.api.js.langchain.com/classes/_langchain_core.messages_ai_message.AIMessage.html) objects produced by the corresponding model. For more details, see the [messages](/oss/javascript/langchain/messages) guide.

<Note>
  Some provider APIs, notably OpenAI and Azure OpenAI chat completions, require users opt-in to receiving token usage data in streaming contexts. See the [streaming usage metadata](/oss/javascript/integrations/chat/openai#streaming-usage-metadata) section of the integration guide for details.
</Note>

### Invocation config

When invoking a model, you can pass additional configuration through the `config` parameter using a [`RunnableConfig`](https://v03.api.js.langchain.com/interfaces/_langchain_core.runnables.RunnableConfig.html) object. This provides run-time control over execution behavior, callbacks, and metadata tracking.

Common configuration options include:

These configuration values are particularly useful when:

* Debugging with [LangSmith](https://docs.smith.langchain.com/) tracing
* Implementing custom logging or monitoring
* Controlling resource usage in production
* Tracking invocations across complex pipelines

<Accordion title="Key configuration attributes">
  <ParamField body="runName" type="string">
    Identifies this specific invocation in logs and traces. Not inherited by sub-calls.
  </ParamField>

<ParamField body="tags" type="string[]">
    Labels inherited by all sub-calls for filtering and organization in debugging tools.
  </ParamField>

<ParamField body="metadata" type="object">
    Custom key-value pairs for tracking additional context, inherited by all sub-calls.
  </ParamField>

<ParamField body="maxConcurrency" type="number">
    Controls the maximum number of parallel calls when using `batch()`.
  </ParamField>

<ParamField body="callbacks" type="CallbackHandler[]">
    Handlers for monitoring and responding to events during execution.
  </ParamField>

<ParamField body="recursion_limit" type="number">
    Maximum recursion depth for chains to prevent infinite loops in complex pipelines.
  </ParamField>
</Accordion>

<Tip>
  See full [`RunnableConfig`](https://v03.api.js.langchain.com/interfaces/_langchain_core.runnables.RunnableConfig.html) reference for all supported attributes.
</Tip>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/models.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>

    <CodeGroup>
```

---

## model_1 is tagged with "joke"

**URL:** llms-txt#model_1-is-tagged-with-"joke"

model_1 = init_chat_model(model="gpt-4o-mini", tags=['joke'])

---

## model_2 is tagged with "poem"

**URL:** llms-txt#model_2-is-tagged-with-"poem"

model_2 = init_chat_model(model="gpt-4o-mini", tags=['poem'])

graph = ... # define a graph that uses these LLMs

---

## Model caches

**URL:** llms-txt#model-caches

Source: https://docs.langchain.com/oss/javascript/integrations/llm_caching/index

[Caching LLM calls](/oss/javascript/langchain/models#caching) can be useful for testing, cost savings, and speed.

Below are some integrations that allow you to cache results of individual LLM calls using different caches with different strategies.

<Columns cols={3}>
  <Card title="Azure Cosmos DB NoSQL Semantic Cache" icon="link" href="/oss/javascript/integrations/llm_caching/azure_cosmosdb_nosql" arrow="true" cta="View guide" />
</Columns>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/javascript/integrations/llm_caching/index.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Model Context Protocol (MCP)

**URL:** llms-txt#model-context-protocol-(mcp)

**Contents:**
- Install
- Transport types
- Use MCP tools
- Custom MCP servers
- Stateful tool usage
- Additional resources

Source: https://docs.langchain.com/oss/javascript/langchain/mcp

[Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) is an open protocol that standardizes how applications provide tools and context to LLMs. LangChain agents can use tools defined on MCP servers using the [`langchain-mcp-adapters`](https://github.com/langchain-ai/langchainjs/tree/main/libs/langchain-mcp-adapters/adapters) library.

Install the `@langchain/mcp-adapters` library to use MCP tools in LangGraph:

MCP supports different transport mechanisms for client-server communication:

* **stdio** – Client launches server as a subprocess and communicates via standard input/output. Best for local tools and simple setups.
* **Streamable HTTP** – Server runs as an independent process handling HTTP requests. Supports remote connections and multiple clients.
* **Server-Sent Events (SSE)** – a variant of streamable HTTP optimized for real-time streaming communication.

`@langchain/mcp-adapters` enables agents to use tools defined across one or more MCP server.

<Note>
  `MultiServerMCPClient` is **stateless by default**. Each tool invocation creates a fresh MCP `ClientSession`, executes the tool, and then cleans up.
</Note>

## Custom MCP servers

To create your own MCP servers, you can use the `@modelcontextprotocol/sdk` library. This library provides a simple way to define [tools](https://modelcontextprotocol.io/docs/learn/server-concepts#tools-ai-actions) and run them as servers.

Use the following reference implementations to test your agent with MCP tool servers.

## Stateful tool usage

For stateful servers that maintain context between tool calls, use `client.session()` to create a persistent `ClientSession`.

## Additional resources

* [MCP documentation](https://modelcontextprotocol.io/introduction)

* [MCP Transport documentation](https://modelcontextprotocol.io/docs/concepts/transports)

* [`langchain-mcp-adapters`](https://github.com/langchain-ai/langchainjs/tree/main/libs/langchain-mcp-adapters/)

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/mcp.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>

## Transport types

MCP supports different transport mechanisms for client-server communication:

* **stdio** – Client launches server as a subprocess and communicates via standard input/output. Best for local tools and simple setups.
* **Streamable HTTP** – Server runs as an independent process handling HTTP requests. Supports remote connections and multiple clients.
* **Server-Sent Events (SSE)** – a variant of streamable HTTP optimized for real-time streaming communication.

## Use MCP tools

`@langchain/mcp-adapters` enables agents to use tools defined across one or more MCP server.
```

---

## Model for performing extraction.

**URL:** llms-txt#model-for-performing-extraction.

info_llm = init_chat_model("gpt-4o-mini").with_structured_output(
    PurchaseInformation, method="json_schema", include_raw=True
)

---

## Model initialization

**URL:** llms-txt#model-initialization

**Contents:**
  - `langchain-classic`
- Migration guide
- Reporting issues
- Additional resources
- See also

from langchain.chat_models import init_chat_model
from langchain.embeddings import init_embeddings
bash pip theme={null}
  pip install langchain-classic
  bash uv theme={null}
  uv add langchain-classic
  python  theme={null}
from langchain import ...  # [!code --]
from langchain_classic import ...  # [!code ++]

from langchain.chains import ...  # [!code --]
from langchain_classic.chains import ...  # [!code ++]

from langchain.retrievers import ...  # [!code --]
from langchain_classic.retrievers import ...  # [!code ++]

from langchain import hub  # [!code --]
from langchain_classic import hub  # [!code ++]
```

See our [migration guide](/oss/python/migrate/langchain-v1) for help updating your code to LangChain v1.

Please report any issues discovered with 1.0 on [GitHub](https://github.com/langchain-ai/langchain/issues) using the `'v1'` [label](https://github.com/langchain-ai/langchain/issues?q=state%3Aopen%20label%3Av1).

## Additional resources

<CardGroup cols={3}>
  <Card title="LangChain 1.0" icon="rocket" href="https://blog.langchain.com/langchain-langchain-1-0-alpha-releases/">
    Read the announcement
  </Card>

<Card title="Middleware Guide" icon="puzzle-piece" href="https://blog.langchain.com/agent-middleware/">
    Deep dive into middleware
  </Card>

<Card title="Agents Documentation" icon="book" href="/oss/python/langchain/agents" arrow>
    Full agent documentation
  </Card>

<Card title="Message Content" icon="message" href="/oss/python/langchain/messages#message-content" arrow>
    New content blocks API
  </Card>

<Card title="Migration guide" icon="arrow-right-arrow-left" href="/oss/python/migrate/langchain-v1" arrow>
    How to migrate to LangChain v1
  </Card>

<Card title="GitHub" icon="github" href="https://github.com/langchain-ai/langchain">
    Report issues or contribute
  </Card>
</CardGroup>

* [Versioning](/oss/python/versioning) - Understanding version numbers
* [Release policy](/oss/python/release-policy) - Detailed release policies

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/releases/langchain-v1.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
### `langchain-classic`

Legacy functionality has moved to [`langchain-classic`](https://pypi.org/project/langchain-classic) to keep the core packages lean and focused.

**What's in `langchain-classic`:**

* Legacy chains and chain implementations
* Retrievers (e.g. `MultiQueryRetriever` or anything from the previous `langchain.retrievers` module)
* The indexing API
* The hub module (for managing prompts programmatically)
* [`langchain-community`](https://pypi.org/project/langchain-community) exports
* Other deprecated functionality

If you use any of this functionality, install [`langchain-classic`](https://pypi.org/project/langchain-classic):

<CodeGroup>
```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
</CodeGroup>

Then update your imports:
```

---

## Monitor projects with dashboards

**URL:** llms-txt#monitor-projects-with-dashboards

**Contents:**
- Prebuilt dashboards
  - Dashboard sections
  - Group by
- Custom Dashboards
  - Creating a new dashboard
  - Adding charts to your dashboard
  - Chart configuration
  - Save and manage charts
- Linking to a dashboard from a tracing project
- Example: user-journey monitoring

Source: https://docs.langchain.com/langsmith/dashboards

Dashboards give you high-level insights into your trace data, helping you spot trends and monitor the health of your applications. Dashboards are available in the **Monitoring** tab in the left sidebar.

LangSmith offers two dashboard types:

* **Prebuilt dashboards**: Automatically generated for every tracing project.
* **Custom dashboards**: Fully configurable collections of charts tailored to your needs.

## Prebuilt dashboards

Prebuilt dashboards are created automatically for each project and cover essential metrics, such as trace count, error rates, token usage, and more. By default, the prebuilt dashboard for your tracing project can be accessed using the **Dashboard** button on the top right of the tracing project page.

<img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prebuilt.gif?s=cbb7410db7c9036ed6c03af251f13a99" alt="prebuilt" data-og-width="1392" width="1392" data-og-height="1080" height="1080" data-path="langsmith/images/prebuilt.gif" data-optimize="true" data-opv="3" />

<Note>**You cannot modify a prebuilt dashboard. In the future, we plan to allow you to clone a default dashboard in order to have a starting point to customize it.**</Note>

### Dashboard sections

Prebuilt dashboards are broken down into the following sections:

| Section         | What it shows                                                                                                                                                                                                                                                                                                    |
| :-------------- | :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Traces          | Trace count, latency and error rates. A [trace](/langsmith/observability-concepts#traces) is a collection of [runs](/langsmith/observability-concepts#runs) related to a single operation. For example, if a user request triggers an agent, all runs for that agent invocation would be part of the same trace. |
| LLM Calls       | LLM call count and latency. Includes all runs where run type is "llm".                                                                                                                                                                                                                                           |
| Cost & Tokens   | Total and per-trace token counts and costs, broken down by token type. Costs are measured using [LangSmith's cost tracking](/langsmith/log-llm-trace#manually-provide-token-counts).                                                                                                                             |
| Tools           | Run counts, error rates, and latency stats for tool runs broken down by tool name. Includes runs where run type is "tool". Limits to top 5 most frequently occurring tools.                                                                                                                                      |
| Run Types       | Run counts, error rates, and latency stats for runs that are immediate children of the root run. This helps in understanding the high-level execution path of agents. Limits to top 5 most frequently occurring run names. Refer to the image following this table.                                              |
| Feedback Scores | Aggregate stats for the top 5 most frequently occurring types of feedback. Charts show average score for numerical feedback and category counts for categorical feedback.                                                                                                                                        |

For example, for the following trace, the following runs have a depth of 1:

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/run-depth-explained.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=382b7d6064e09c23efc6770fcd983a69" alt="" data-og-width="524" width="524" data-og-height="810" height="810" data-path="langsmith/images/run-depth-explained.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/run-depth-explained.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=03503ce7bfd170a22ff5152a7564e130 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/run-depth-explained.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=a84d5785aaa93bb3e8ab7d2a4143f063 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/run-depth-explained.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=5d6df2d7b6dbe6e358ad2f03df341661 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/run-depth-explained.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=74abfc6a192c9a433dcf62b01a571594 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/run-depth-explained.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=9e3089b4311d828f80594649647f85bd 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/run-depth-explained.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=c6e3a31aded7c2ad7ae96f948613c983 2500w" />

Group by [run tag or metadata](/langsmith/add-metadata-tags) can be used to split data over attributes that are important to your application. The global group by setting appears on the top right hand side of the dashboard. Note that the Tool and Run Type charts already have a group by applied, so the global group by won't take effect; the global group by will apply to all other charts.

<Note>When adding metadata to runs, we recommend having the same metadata on the trace, as well as the specific run (e.g. LLM call). Metadata and tags are not propagated from parent to child runs, or vice versa. So, if you want to see e.g. both your trace charts and your LLM call charts grouped on some metadata key then both your traces (root runs) and your LLM runs need to have that metadata attached.</Note>

Create tailored collections of charts for tracking metrics that matter most for your application.

### Creating a new dashboard

1. Navigate to the **Monitor** tab in the left sidebar.
2. Click on the **+ New Dashboard** button.
3. Give your dashboard a name and a description.
4. Click on **Create**.

### Adding charts to your dashboard

1. Within a dashboard, click on the **+ New Chart** button to open up the chart creation pane.
2. Give your chart a name and a description.
3. Configure the chart.

### Chart configuration

#### Select tracing projects and filter runs

* Select one or more tracing projects to track metrics for.
* Use the **Chart filters** section to refine the matching runs. This filter applies to all data series in the chart. For more information on filtering traces, view our guide on [filtering traces in application](./filter-traces-in-application).

* Choose a metric from the dropdown menu to set the y-axis of your chart. With a project and a metric selected, you'll see a preview of your chart and the matching runs.
* For certain metrics (such as latency, token usage, cost), we support comparing multiple metrics with the same unit. For example, you may want one chart where you can see prompt tokens and completion tokens. Each metric appears as a separate line.

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-metrics.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=3543d454f25f5d11046fbd5bcab7aeff" alt="Multiple metrics" data-og-width="1475" width="1475" data-og-height="741" height="741" data-path="langsmith/images/compare-metrics.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-metrics.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=75f80a706d3e3ddd09117bc9d317ecdc 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-metrics.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=2606ae254c47349e2c5bd14ae4fc49b8 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-metrics.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=4239ab64861d8e3768f90f837dbefe67 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-metrics.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=0e8d5f3e7fcf40fe6f33b6066afa346f 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-metrics.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=d45519451d443538ad76fdcbb0d21f62 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-metrics.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=33024dfb462b8229b16c4fbd9cc650ba 2500w" />

There are two ways to create multiple series in a chart (i.e. create multiple lines in a chart):

1. **Group by**: Group runs by [run tag or metadata](/langsmith/add-metadata-tags), run name, or run type. Group by automatically splits the data into multiple series based on the field selected. Note that group by is limited to the top 5 elements by frequency.

2. **Data series**: Manually define multiple series with individual filters. This is useful for comparing granular data within a single metric.

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiple-data-series.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=e5ed30b317abdb90aea612702d94cf04" alt="Multiple data series" data-og-width="2796" width="2796" data-og-height="1396" height="1396" data-path="langsmith/images/multiple-data-series.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiple-data-series.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=75134cada0ab88b532d6073c3317dc36 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiple-data-series.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=05a0b345dd25435cae0c6e25fee62942 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiple-data-series.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=aede683fd53d369cbfe7c1649d128953 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiple-data-series.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=e96e54ecf149d6a5881bc44d81941220 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiple-data-series.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=fee4a18237d105668c227eafb014d0d5 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiple-data-series.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=6a93006b73bc0451c68acf10189155a4 2500w" />

#### Pick a chart type

* Choose between a line chart and a bar chart for visualizing

### Save and manage charts

* Click `Save` to save your chart to the dashboard.
* Edit or delete a chart by clicking the triple dot button in the top right of the chart.
* Clone a chart by clicking the triple line button in the top right of the chart and selecting **+ Clone**. This will open a new chart creation pane with the same configurations as the original.

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/more-actions-bar.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=b40f5f132a2c0770cfe04a125dcbc7f2" alt="More actions bar" data-og-width="2102" width="2102" data-og-height="758" height="758" data-path="langsmith/images/more-actions-bar.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/more-actions-bar.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=b4033fc996ce15ac3eee9e42430f3089 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/more-actions-bar.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=0176aa9470908b2ee287c6588f8898ae 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/more-actions-bar.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=38af0f07ca3394b40dbdd4f6d83af5ba 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/more-actions-bar.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=b41c74e251946e2f6f31739c71a42096 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/more-actions-bar.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=c5a6f79532146e2596857be1f85f342a 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/more-actions-bar.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=623e4b00a5a04e950e8312cf2a2233b6 2500w" />

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/expanded-chart.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=d0c5597d6adfa7d02888ef4b89ac616d" alt="Expanded chart" data-og-width="2238" width="2238" data-og-height="1662" height="1662" data-path="langsmith/images/expanded-chart.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/expanded-chart.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=73fa53c7cdb5a89eaac5e685035bfc82 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/expanded-chart.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=df3f46036e1692777f6c0d460b78ca0c 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/expanded-chart.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=9c4abdd5d8cf517a9e537d09f6d6e761 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/expanded-chart.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=91a2ff0f83ad36710058479fc5f30698 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/expanded-chart.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=7f16a10cf75c87fb38f3cf6db39e1fe4 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/expanded-chart.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=97d6ff4e7841cdf169c4e069605c7089 2500w" />

## Linking to a dashboard from a tracing project

You can link to any dashboard directly from a tracing project. By default, the prebuilt dashboard for your tracing project is selected. If you have a custom dashboard that you would like to link instead:

1. In your tracing project, click the three dots next to the **Dashboard** button.
2. Choose a dashboard to set as the new default.

<img src="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-project-to-dashboard.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=06b63ab9c4f8d7185c72a77e84862f3a" alt="Tracing project to dashboard" data-og-width="2080" width="2080" data-og-height="770" height="770" data-path="langsmith/images/tracing-project-to-dashboard.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-project-to-dashboard.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=b06a5e1a4fe6b82260032624bdc1ce68 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-project-to-dashboard.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=ea5dd2397337b290c996ef26948836ea 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-project-to-dashboard.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=26832107ff0613d0575432fc4f2424d6 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-project-to-dashboard.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=2ace3a16bd11cd8a8906bbdab77c3f10 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-project-to-dashboard.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=c5b29e7497d837a0d84b97f5c3c522a6 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-project-to-dashboard.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=6055757e9ac1f5fd9381d14bd92cf088 2500w" />

## Example: user-journey monitoring

Use monitoring charts for mapping the decisions made by an agent at a particular node.

Consider an email assistant agent. At a particular node it makes a decision about an email to:

* send an email back
* notify the user
* no response needed

We can create a chart to track and visualize the breakdown of these decisions.

**Creating the chart**

1. **Metric Selection**: Select the metric `Run count`.

2. **Chart Filters**: Add a tree filter to include all of the traces with name `triage_input`. This means we only include traces that hit the `triage_input` node. Also add a chart filter for `Is Root` is `true`, so our count is not inflated by the number of nodes in the trace.
   <img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/chart-filters-for-node-decision.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=f769fb23624b3b1b6c042ff5cfd910e6" alt="Decision at node" data-og-width="2620" width="2620" data-og-height="1698" height="1698" data-path="langsmith/images/chart-filters-for-node-decision.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/chart-filters-for-node-decision.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=e09f22192fa9cb75b32f6812071e1e07 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/chart-filters-for-node-decision.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=e3108ae084660ecc151a68a47fab6912 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/chart-filters-for-node-decision.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=fb75ff0f6c0e9140bc4554cceb93134b 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/chart-filters-for-node-decision.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=704fb96e453fdd92dcce1e34a35a48f2 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/chart-filters-for-node-decision.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=902d095d27fb3f20b8e403f4ddc18e24 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/chart-filters-for-node-decision.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=9afff6512a968017060e6fde0a5fb992 2500w" />

3. **Data Series**: Create a data series for each decision made at the `triage_input` node. The output of the decision is stored in the `triage.response` field of the output object, and the value of the decision is either `no`, `email`, or `notify`. Each of these decisions generates a separate data series in the chart.
   <img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/decision-at-node.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=eb98a2c2c7988b5b6c5c3db9740ed172" alt="Decision at node" data-og-width="2578" width="2578" data-og-height="1692" height="1692" data-path="langsmith/images/decision-at-node.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/decision-at-node.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=665c6cf0c3368c8f180db15cc7bf4cba 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/decision-at-node.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=e68f21187ec95e74e033850200893c66 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/decision-at-node.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=33f729b74f697b7b7554badb6c76cec0 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/decision-at-node.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=1ebed1b633a724ed5c9e2b93d62b1c2e 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/decision-at-node.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=8d3287007e200f37151955edd6cc2632 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/decision-at-node.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=ee0d68172a80dce21c1f1afad533298d 2500w" />

Now we can visualize the decisions made at the `triage_input` node over time.

<iframe className="w-full aspect-video rounded-xl" src="https://www.youtube.com/embed/VxsIvf9NdxI?si=7ksp9qyw-i0lcwxg" title="YouTube video player" frameBorder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowFullScreen />

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/dashboards.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Monorepo support

**URL:** llms-txt#monorepo-support

**Contents:**
- Repository Structure
- LangGraph.json configuration
- Building the application
- Tips and best practices

Source: https://docs.langchain.com/langsmith/monorepo-support

LangSmith supports deploying agents from monorepo setups where your agent code may depend on shared packages located elsewhere in the repository. This guide shows how to structure your monorepo and configure your `langgraph.json` file to work with shared dependencies.

## Repository Structure

For complete working examples, see:

* [Python monorepo example](https://github.com/langchain-ai/python-langraph-monorepo-example)
* [JS monorepo example](https://github.com/langchain-ai/js-langgraph-monorepo-example)

## LangGraph.json configuration

Place the langgraph.json file in your agent’s directory (not in the monorepo root). Ensure the file follows the required structure:

The Python implementation automatically handles packages in parent directories by:

* Detecting relative paths that start with `"."`.
* Adding parent directories to the Docker build context as needed.
* Supporting both real packages (with `pyproject.toml`/`setup.py`) and simple Python modules.

For JavaScript monorepos:

* Shared workspace dependencies are resolved automatically by your package manager.
* Your `package.json` should reference shared packages using workspace syntax.

Example `package.json` in the agent directory:

## Building the application

Run `langgraph build`:

The Python build process:

1. Automatically detects relative dependency paths.
2. Copies shared packages into the Docker build context.
3. Installs all dependencies in the correct order.
4. No special flags or commands required.

The JavaScript build process:

1. Uses the directory you called `langgraph build` from (the monorepo root in this case) as the build context.
2. Automatically detects your package manager (yarn, npm, pnpm, bun)
3. Runs the appropriate install command.
   * If you have one or both of a custom build/install command it will run from the directory you called `langgraph build` from.
   * Otherwise, it will run from the directory where the `langgraph.json` file is located.
4. Optionally runs a custom build command from the directory where the `langgraph.json` file is located (only if you pass the `--build-command` flag).

## Tips and best practices

1. **Keep agent configs in agent directories**: Place `langgraph.json` files in the specific agent directories, not at the monorepo root. This allows you to support multiple agents in the same monorepo, without having to deploy them all in the same LangSmith deployment.

2. **Use relative paths for Python**: For Python monorepos, use relative paths like `"../../shared-package"` in the `dependencies` array.

3. **Leverage workspace features for JS**: For JavaScript/TypeScript, use your package manager's workspace features to manage dependencies between packages.

4. **Test locally first**: Always test your build locally before deploying to ensure all dependencies are correctly resolved.

5. **Environment variables**: Keep environment files (`.env`) in your agent directories for environment-specific configuration.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/monorepo-support.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

## LangGraph.json configuration

Place the langgraph.json file in your agent’s directory (not in the monorepo root). Ensure the file follows the required structure:

<CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>

The Python implementation automatically handles packages in parent directories by:

* Detecting relative paths that start with `"."`.
* Adding parent directories to the Docker build context as needed.
* Supporting both real packages (with `pyproject.toml`/`setup.py`) and simple Python modules.

For JavaScript monorepos:

* Shared workspace dependencies are resolved automatically by your package manager.
* Your `package.json` should reference shared packages using workspace syntax.

Example `package.json` in the agent directory:
```

---

## Multi-agent

**URL:** llms-txt#multi-agent

**Contents:**
- Multi-agent patterns
- Choosing a pattern
- Customizing agent context
- Tool calling
  - Implementation
  - Where to customize
  - Control the input to the subagent
  - Control the output from the subagent
- Handoffs
  - Implementation (Coming soon)

Source: https://docs.langchain.com/oss/javascript/langchain/multi-agent

**Multi-agent systems** break a complex application into multiple specialized agents that work together to solve problems.
Instead of relying on a single agent to handle every step, **multi-agent architectures** allow you to compose smaller, focused agents into a coordinated workflow.

Multi-agent systems are useful when:

* A single agent has too many tools and makes poor decisions about which to use.
* Context or memory grows too large for one agent to track effectively.
* Tasks require **specialization** (e.g., a planner, researcher, math expert).

## Multi-agent patterns

| Pattern                           | How it works                                                                                                                                                     | Control flow                                               | Example use case                                 |
| --------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------- | ------------------------------------------------ |
| [**Tool Calling**](#tool-calling) | A **supervisor** agent calls other agents as *tools*. The “tool” agents don’t talk to the user directly — they just run their task and return results.           | Centralized: all routing passes through the calling agent. | Task orchestration, structured workflows.        |
| [**Handoffs**](#handoffs)         | The current agent decides to **transfer control** to another agent. The active agent changes, and the user may continue interacting directly with the new agent. | Decentralized: agents can change who is active.            | Multi-domain conversations, specialist takeover. |

<Card title="Tutorial: Build a supervisor agent" icon="sitemap" href="/oss/javascript/langchain/supervisor" arrow cta="Learn more">
  Learn how to build a personal assistant using the supervisor pattern, where a central supervisor agent coordinates specialized worker agents.
  This tutorial demonstrates:

* Creating specialized sub-agents for different domains (calendar and email)
  * Wrapping sub-agents as tools for centralized orchestration
  * Adding human-in-the-loop review for sensitive actions
</Card>

## Choosing a pattern

| Question                                              | Tool Calling | Handoffs |
| ----------------------------------------------------- | ------------ | -------- |
| Need centralized control over workflow?               | ✅ Yes        | ❌ No     |
| Want agents to interact directly with the user?       | ❌ No         | ✅ Yes    |
| Complex, human-like conversation between specialists? | ❌ Limited    | ✅ Strong |

<Tip>
  You can mix both patterns — use **handoffs** for agent switching, and have each agent **call subagents as tools** for specialized tasks.
</Tip>

## Customizing agent context

At the heart of multi-agent design is **context engineering** - deciding what information each agent sees. LangChain gives you fine-grained control over:

* Which parts of the conversation or state are passed to each agent.
* Specialized prompts tailored to subagents.
* Inclusion/exclusion of intermediate reasoning.
* Customizing input/output formats per agent.

The quality of your system **heavily depends** on context engineering. The goal is to ensure that each agent has access to the correct data it needs to perform its task, whether it’s acting as a tool or as an active agent.

In **tool calling**, one agent (the “**controller**”) treats other agents as *tools* to be invoked when needed. The controller manages orchestration, while tool agents perform specific tasks and return results.

1. The **controller** receives input and decides which tool (subagent) to call.
2. The **tool agent** runs its task based on the controller’s instructions.
3. The **tool agent** returns results to the controller.
4. The **controller** decides the next step or finishes.

<Tip>
  Agents used as tools are generally **not expected** to continue conversation with the user.
  Their role is to perform a task and return results to the controller agent.
  If you need subagents to be able to converse with the user, use **handoffs** instead.
</Tip>

Below is a minimal example where the main agent is given access to a single subagent via a tool definition:

1. The main agent invokes `call_subagent1` when it decides the task matches the subagent’s description.
2. The subagent runs independently and returns its result.
3. The main agent receives the result and continues orchestration.

### Where to customize

There are several points where you can control how context is passed between the main agent and its subagents:

1. **Subagent name** (`"subagent1_name"`): This is how the main agent refers to the subagent. Since it influences prompting, choose it carefully.
2. **Subagent description** (`"subagent1_description"`): This is what the main agent “knows” about the subagent. It directly shapes how the main agent decides when to call it.
3. **Input to the subagent**: You can customize this input to better shape how the subagent interprets tasks. In the example above, we pass the agent-generated `query` directly.
4. **Output from the subagent**: This is the **response** passed back to the main agent. You can adjust what is returned to control how the main agent interprets results. In the example above, we return the final message text, but you could return additional state or metadata.

### Control the input to the subagent

There are two main levers to control the input that the main agent passes to a subagent:

* **Modify the prompt** – Adjust the main agent's prompt or the tool metadata (i.e., sub-agent's name and description) to better guide when and how it calls the subagent.
* **Context injection** – Add input that isn’t practical to capture in a static prompt (e.g., full message history, prior results, task metadata) by adjusting the tool call to pull from the agent’s state.

### Control the output from the subagent

Two common strategies for shaping what the main agent receives back from a subagent:

* **Modify the prompt** – Refine the subagent’s prompt to specify exactly what should be returned.
  * Useful when outputs are incomplete, too verbose, or missing key details.
  * A common failure mode is that the subagent performs tool calls or reasoning but does **not include the results** in its final message. Remind it that the controller (and user) only see the final output, so all relevant info must be included there.
* **Custom output formatting** – adjust or enrich the subagent's response in code before handing it back to the main agent.
  * Example: pass specific state keys back to the main agent in addition to the final text.
  * This requires wrapping the result in a [`Command`](https://langchain-ai.github.io/langgraphjs/reference/classes/langgraph.Command.html) (or equivalent structure) so you can merge custom state with the subagent’s response.

In **handoffs**, agents can directly pass control to each other. The “active” agent changes, and the user interacts with whichever agent currently has control.

1. The **current agent** decides it needs help from another agent.
2. It passes control (and state) to the **next agent**.
3. The **new agent** interacts directly with the user until it decides to hand off again or finish.

### Implementation (Coming soon)

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/multi-agent.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
<Tip>
  Agents used as tools are generally **not expected** to continue conversation with the user.
  Their role is to perform a task and return results to the controller agent.
  If you need subagents to be able to converse with the user, use **handoffs** instead.
</Tip>

### Implementation

Below is a minimal example where the main agent is given access to a single subagent via a tool definition:
```

Example 2 (unknown):
```unknown
In this pattern:

1. The main agent invokes `call_subagent1` when it decides the task matches the subagent’s description.
2. The subagent runs independently and returns its result.
3. The main agent receives the result and continues orchestration.

### Where to customize

There are several points where you can control how context is passed between the main agent and its subagents:

1. **Subagent name** (`"subagent1_name"`): This is how the main agent refers to the subagent. Since it influences prompting, choose it carefully.
2. **Subagent description** (`"subagent1_description"`): This is what the main agent “knows” about the subagent. It directly shapes how the main agent decides when to call it.
3. **Input to the subagent**: You can customize this input to better shape how the subagent interprets tasks. In the example above, we pass the agent-generated `query` directly.
4. **Output from the subagent**: This is the **response** passed back to the main agent. You can adjust what is returned to control how the main agent interprets results. In the example above, we return the final message text, but you could return additional state or metadata.

### Control the input to the subagent

There are two main levers to control the input that the main agent passes to a subagent:

* **Modify the prompt** – Adjust the main agent's prompt or the tool metadata (i.e., sub-agent's name and description) to better guide when and how it calls the subagent.
* **Context injection** – Add input that isn’t practical to capture in a static prompt (e.g., full message history, prior results, task metadata) by adjusting the tool call to pull from the agent’s state.
```

Example 3 (unknown):
```unknown
### Control the output from the subagent

Two common strategies for shaping what the main agent receives back from a subagent:

* **Modify the prompt** – Refine the subagent’s prompt to specify exactly what should be returned.
  * Useful when outputs are incomplete, too verbose, or missing key details.
  * A common failure mode is that the subagent performs tool calls or reasoning but does **not include the results** in its final message. Remind it that the controller (and user) only see the final output, so all relevant info must be included there.
* **Custom output formatting** – adjust or enrich the subagent's response in code before handing it back to the main agent.
  * Example: pass specific state keys back to the main agent in addition to the final text.
  * This requires wrapping the result in a [`Command`](https://langchain-ai.github.io/langgraphjs/reference/classes/langgraph.Command.html) (or equivalent structure) so you can merge custom state with the subagent’s response.
```

Example 4 (unknown):
```unknown
## Handoffs

In **handoffs**, agents can directly pass control to each other. The “active” agent changes, and the user interacts with whichever agent currently has control.

Flow:

1. The **current agent** decides it needs help from another agent.
2. It passes control (and state) to the **next agent**.
3. The **new agent** interacts directly with the user until it decides to hand off again or finish.
```

---

## Must have 'pandas' installed.

**URL:** llms-txt#must-have-'pandas'-installed.

df = experiment.to_pandas()
df[["inputs.question", "outputs.answer", "reference.answer", "feedback.is_concise"]]
python  theme={null}
{'question': 'What is the largest mammal?'}
{'answer': "What is the largest mammal? is a good question. I don't know the answer."}
{'question': 'What do mammals and birds have in common?'}
{'answer': "What do mammals and birds have in common? is a good question. I don't know the answer."}
```

|   | inputs.question                           | outputs.answer                                                                         | reference.answer           | feedback.is\_concise |
| - | ----------------------------------------- | -------------------------------------------------------------------------------------- | -------------------------- | -------------------- |
| 0 | What is the largest mammal?               | What is the largest mammal? is a good question. I don't know the answer.               | The blue whale             | False                |
| 1 | What do mammals and birds have in common? | What do mammals and birds have in common? is a good question. I don't know the answer. | They are both warm-blooded | False                |

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/local.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

---

## Must set dangerously_allow_filesystem to True if you want to use file paths

**URL:** llms-txt#must-set-dangerously_allow_filesystem-to-true-if-you-want-to-use-file-paths

@traceable(dangerously_allow_filesystem=True)
def trace_with_attachments(
    val: int,
    text: str,
    image: Attachment,
    audio: Attachment,
    video: Attachment,
    pdf: Attachment,
    csv: Attachment,
):
    return f"Processed: {val}, {text}, {len(image.data)}, {len(audio.data)}, {len(video.data)}, {len(pdf.data), {len(csv.data)}}"

---

## my_agent/agent.py

**URL:** llms-txt#my_agent/agent.py

from typing import Literal
from typing_extensions import TypedDict

from langgraph.graph import StateGraph, END, START
from my_agent.utils.nodes import call_model, should_continue, tool_node # import nodes
from my_agent.utils.state import AgentState # import state

---

## my_graph.py.

**URL:** llms-txt#my_graph.py.

**Contents:**
  - Opt-out of configurable headers

@contextlib.asynccontextmanager
async def generate_agent(config):
  organization_id = config["configurable"].get("x-organization-id")
  if organization_id == "org1":
    graph = ...
    yield graph
  else:
    graph = ...
    yield graph

json  theme={null}
{
  "graphs": {"agent": "my_grph.py:generate_agent"}
}
json  theme={null}
{
  "http": {
    "configurable_headers": {
      "exclude": ["*"]
    }
  }
}
```

This will exclude all headers from being added to your run's configuration.

Note that exclusions take precedence over inclusions.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/configurable-headers.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
### Opt-out of configurable headers

If you'd like to opt-out of configurable headers, you can simply set a wildcard pattern in the `exclude` list:
```

---

## Name of the dataset we want to create

**URL:** llms-txt#name-of-the-dataset-we-want-to-create

dataset_name = f'{project_name}-backtesting {start_time.strftime("%Y-%m-%d")}-{end_time.strftime("%Y-%m-%d")}'

---

## Name of the experiment we want to create from the historical runs

**URL:** llms-txt#name-of-the-experiment-we-want-to-create-from-the-historical-runs

baseline_experiment_name = f"prod-baseline-gpt-3.5-turbo-{str(uuid4())[:4]}"

---

## Next steps

**URL:** llms-txt#next-steps

Now that you've created a prompt, you can use it in your application code. See [how to pull a prompt programmatically](/langsmith/manage-prompts-programmatically#pull-a-prompt).

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/create-a-prompt.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Nodes

**URL:** llms-txt#nodes

def orchestrator(state: State):
    """Orchestrator that generates a plan for the report"""

# Generate queries
    report_sections = planner.invoke(
        [
            SystemMessage(content="Generate a plan for the report."),
            HumanMessage(content=f"Here is the report topic: {state['topic']}"),
        ]
    )

return {"sections": report_sections.sections}

def llm_call(state: WorkerState):
    """Worker writes a section of the report"""

# Generate section
    section = llm.invoke(
        [
            SystemMessage(
                content="Write a report section following the provided name and description. Include no preamble for each section. Use markdown formatting."
            ),
            HumanMessage(
                content=f"Here is the section name: {state['section'].name} and description: {state['section'].description}"
            ),
        ]
    )

# Write the updated section to completed sections
    return {"completed_sections": [section.content]}

def synthesizer(state: State):
    """Synthesize full report from sections"""

# List of completed sections
    completed_sections = state["completed_sections"]

# Format completed section to str to use as context for final sections
    completed_report_sections = "\n\n---\n\n".join(completed_sections)

return {"final_report": completed_report_sections}

---

## node_2 accepts private data from node_1, whereas

**URL:** llms-txt#node_2-accepts-private-data-from-node_1,-whereas

---

## Node 2 input only requests the private data available after node_1

**URL:** llms-txt#node-2-input-only-requests-the-private-data-available-after-node_1

class Node2Input(TypedDict):
    private_data: str

def node_2(state: Node2Input) -> OverallState:
    output = {"a": "set by node_2"}
    print(f"Entered node `node_2`:\n\tInput: {state}.\n\tReturned: {output}")
    return output

---

## node_3 does not see the private data.

**URL:** llms-txt#node_3-does-not-see-the-private-data.

builder = StateGraph(OverallState).add_sequence([node_1, node_2, node_3])
builder.add_edge(START, "node_1")
graph = builder.compile()

---

## Node 3 only has access to the overall state (no access to private data from node_1)

**URL:** llms-txt#node-3-only-has-access-to-the-overall-state-(no-access-to-private-data-from-node_1)

def node_3(state: OverallState) -> OverallState:
    output = {"a": "set by node_3"}
    print(f"Entered node `node_3`:\n\tInput: {state}.\n\tReturned: {output}")
    return output

---

## Node for making sure the 'followup' key is set before our agent run completes.

**URL:** llms-txt#node-for-making-sure-the-'followup'-key-is-set-before-our-agent-run-completes.

def compile_followup(state: State) -> dict:
    """Set the followup to be the last message if it hasn't explicitly been set."""
    if not state.get("followup"):
        return {"followup": state["messages"][-1].content}
    return {}

---

## Node for routing.

**URL:** llms-txt#node-for-routing.

async def intent_classifier(
    state: State,
) -> Command[Literal["refund_agent", "question_answering_agent"]]:
    response = router_llm.invoke(
        [{"role": "system", "content": route_instructions}, *state["messages"]]
    )
    return Command(goto=response["intent"] + "_agent")

---

## Node-style: logging before model calls

**URL:** llms-txt#node-style:-logging-before-model-calls

@before_model
def log_before_model(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:
    print(f"About to call model with {len(state['messages'])} messages")
    return None

---

## Node-style: validation after model calls

**URL:** llms-txt#node-style:-validation-after-model-calls

@after_model(can_jump_to=["end"])
def validate_output(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:
    last_message = state["messages"][-1]
    if "BLOCKED" in last_message.content:
        return {
            "messages": [AIMessage("I cannot respond to that request.")],
            "jump_to": "end"
        }
    return None

---

## Node that updates instructions

**URL:** llms-txt#node-that-updates-instructions

**Contents:**
  - Writing memories
  - Memory storage

def update_instructions(state: State, store: BaseStore):
    namespace = ("instructions",)
    instructions = store.search(namespace)[0]
    # Memory logic
    prompt = prompt_template.format(instructions=instructions.value["instructions"], conversation=state["messages"])
    output = llm.invoke(prompt)
    new_instructions = output['new_instructions']
    store.put(("agent_instructions",), "agent_a", {"instructions": new_instructions})
    ...
python  theme={null}
from langgraph.store.memory import InMemoryStore

def embed(texts: list[str]) -> list[list[float]]:
    # Replace with an actual embedding function or LangChain embeddings object
    return [[1.0, 2.0] * len(texts)]

**Examples:**

Example 1 (unknown):
```unknown
<img src="https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/update-instructions.png?fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=13644c954ed79a45b8a1a762b3e39da1" alt="" data-og-width="493" width="493" data-og-height="515" height="515" data-path="oss/images/update-instructions.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/update-instructions.png?w=280&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=90632c71febee5777be6ae2c338f0880 280w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/update-instructions.png?w=560&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=aefcc771a030a2d6a89f815b87e60fd4 560w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/update-instructions.png?w=840&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=9115490b76daffe987e3867bc9176386 840w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/update-instructions.png?w=1100&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=0df26f2e6f669f2fbea59a9a49482fb4 1100w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/update-instructions.png?w=1650&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=132e7b1f377e0b57c03ab31c6d788df4 1650w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/update-instructions.png?w=2500&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=651cd1bb14e445a972a671a196b6a893 2500w" />

### Writing memories

There are two primary methods for agents to write memories: ["in the hot path"](#in-the-hot-path) and ["in the background"](#in-the-background).

<img src="https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/hot_path_vs_background.png?fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=edd006d6189dc29a2edcba57c41fd744" alt="" data-og-width="842" width="842" data-og-height="418" height="418" data-path="oss/images/hot_path_vs_background.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/hot_path_vs_background.png?w=280&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=3efd9962012347a64b596d1d36925b33 280w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/hot_path_vs_background.png?w=560&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=add54b5469d7b4a8f22d7da250c19ddf 560w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/hot_path_vs_background.png?w=840&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=1e763d0f2ee8aa4f5b302ad44bc19d2f 840w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/hot_path_vs_background.png?w=1100&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=ce84a68af250e53a4693332d39179136 1100w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/hot_path_vs_background.png?w=1650&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=5f807accb9c63dae57c27c9a1d17f29a 1650w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/hot_path_vs_background.png?w=2500&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=ecef974859d58f691dbb22a4a1cc1572 2500w" />

#### In the hot path

Creating memories during runtime offers both advantages and challenges. On the positive side, this approach allows for real-time updates, making new memories immediately available for use in subsequent interactions. It also enables transparency, as users can be notified when memories are created and stored.

However, this method also presents challenges. It may increase complexity if the agent requires a new tool to decide what to commit to memory. In addition, the process of reasoning about what to save to memory can impact agent latency. Finally, the agent must multitask between memory creation and its other responsibilities, potentially affecting the quantity and quality of memories created.

As an example, ChatGPT uses a [save\_memories](https://openai.com/index/memory-and-new-controls-for-chatgpt/) tool to upsert memories as content strings, deciding whether and how to use this tool with each user message. See our [memory-agent](https://github.com/langchain-ai/memory-agent) template as an reference implementation.

#### In the background

Creating memories as a separate background task offers several advantages. It eliminates latency in the primary application, separates application logic from memory management, and allows for more focused task completion by the agent. This approach also provides flexibility in timing memory creation to avoid redundant work.

However, this method has its own challenges. Determining the frequency of memory writing becomes crucial, as infrequent updates may leave other threads without new context. Deciding when to trigger memory formation is also important. Common strategies include scheduling after a set time period (with rescheduling if new events occur), using a cron schedule, or allowing manual triggers by users or the application logic.

See our [memory-service](https://github.com/langchain-ai/memory-template) template as an reference implementation.

### Memory storage

LangGraph stores long-term memories as JSON documents in a [store](/oss/python/langgraph/persistence#memory-store). Each memory is organized under a custom `namespace` (similar to a folder) and a distinct `key` (like a file name). Namespaces often include user or org IDs or other labels that makes it easier to organize information. This structure enables hierarchical organization of memories. Cross-namespace searching is then supported through content filters.
```

---

## Node that *uses* the instructions

**URL:** llms-txt#node-that-*uses*-the-instructions

def call_model(state: State, store: BaseStore):
    namespace = ("agent_instructions", )
    instructions = store.get(namespace, key="agent_a")[0]
    # Application logic
    prompt = prompt_template.format(instructions=instructions.value["instructions"])
    ...

---

## Note that here we inspect the runtime config for an "env" variable.

**URL:** llms-txt#note-that-here-we-inspect-the-runtime-config-for-an-"env"-variable.

---

## Note that providers implement different scores; the score here

**URL:** llms-txt#note-that-providers-implement-different-scores;-the-score-here

---

## Note that we're (optionally) passing the memory when compiling the graph

**URL:** llms-txt#note-that-we're-(optionally)-passing-the-memory-when-compiling-the-graph

**Contents:**
  - Create a dataset
  - Create an evaluator
  - Run evaluations

app = workflow.compile()
python  theme={null}
from langsmith import Client

questions = [
    "what's the weather in sf",
    "whats the weather in san fran",
    "whats the weather in tangier"
]

answers = [
    "It's 60 degrees and foggy.",
    "It's 60 degrees and foggy.",
    "It's 90 degrees and sunny.",
]

ls_client = Client()
dataset = ls_client.create_dataset(
    "weather agent",
    inputs=[{"question": q} for q in questions],
    outputs=[{"answers": a} for a in answers],
)
python  theme={null}
judge_llm = init_chat_model("gpt-4o")

async def correct(outputs: dict, reference_outputs: dict) -> bool:
    instructions = (
        "Given an actual answer and an expected answer, determine whether"
        " the actual answer contains all of the information in the"
        " expected answer. Respond with 'CORRECT' if the actual answer"
        " does contain all of the expected information and 'INCORRECT'"
        " otherwise. Do not include anything else in your response."
    )
    # Our graph outputs a State dictionary, which in this case means
    # we'll have a 'messages' key and the final message should
    # be our actual answer.
    actual_answer = outputs["messages"][-1].content
    expected_answer = reference_outputs["answer"]
    user_msg = (
        f"ACTUAL ANSWER: {actual_answer}"
        f"\n\nEXPECTED ANSWER: {expected_answer}"
    )
    response = await judge_llm.ainvoke(
        [
            {"role": "system", "content": instructions},
            {"role": "user", "content": user_msg}
        ]
    )
    return response.content.upper() == "CORRECT"
python  theme={null}
from langsmith import aevaluate

def example_to_state(inputs: dict) -> dict:
  return {"messages": [{"role": "user", "content": inputs['question']}]}

**Examples:**

Example 1 (unknown):
```unknown
### Create a dataset

Let's create a simple dataset of questions and expected responses:
```

Example 2 (unknown):
```unknown
### Create an evaluator

And a simple evaluator:

Requires `langsmith>=0.2.0`
```

Example 3 (unknown):
```unknown
### Run evaluations

Now we can run our evaluations and explore the results. We'll just need to wrap our graph function so that it can take inputs in the format they're stored on our example:

<Note>
  If all of your graph nodes are defined as sync functions then you can use `evaluate` or `aevaluate`. If any of you nodes are defined as async, you'll need to use `aevaluate`
</Note>

Requires `langsmith>=0.2.0`
```

---

## NOTE: there are no edges between nodes A, B and C!

**URL:** llms-txt#note:-there-are-no-edges-between-nodes-a,-b-and-c!

**Contents:**
  - Navigate to a node in a parent graph
  - Use inside tools
- Visualize your graph
  - Mermaid
  - PNG

graph = builder.compile()
python  theme={null}
from IPython.display import display, Image

display(Image(graph.get_graph().draw_mermaid_png()))
python  theme={null}
graph.invoke({"foo": ""})

Called A
Called C
python  theme={null}
def my_node(state: State) -> Command[Literal["my_other_node"]]:
    return Command(
        update={"foo": "bar"},
        goto="other_subgraph",  # where `other_subgraph` is a node in the parent graph
        graph=Command.PARENT
    )
python  theme={null}
import operator
from typing_extensions import Annotated

class State(TypedDict):
    # NOTE: we define a reducer here
    foo: Annotated[str, operator.add]  # [!code highlight]

def node_a(state: State):
    print("Called A")
    value = random.choice(["a", "b"])
    # this is a replacement for a conditional edge function
    if value == "a":
        goto = "node_b"
    else:
        goto = "node_c"

# note how Command allows you to BOTH update the graph state AND route to the next node
    return Command(
        update={"foo": value},
        goto=goto,
        # this tells LangGraph to navigate to node_b or node_c in the parent graph
        # NOTE: this will navigate to the closest parent graph relative to the subgraph
        graph=Command.PARENT,  # [!code highlight]
    )

subgraph = StateGraph(State).add_node(node_a).add_edge(START, "node_a").compile()

def node_b(state: State):
    print("Called B")
    # NOTE: since we've defined a reducer, we don't need to manually append
    # new characters to existing 'foo' value. instead, reducer will append these
    # automatically (via operator.add)
    return {"foo": "b"}  # [!code highlight]

def node_c(state: State):
    print("Called C")
    return {"foo": "c"}  # [!code highlight]

builder = StateGraph(State)
builder.add_edge(START, "subgraph")
builder.add_node("subgraph", subgraph)
builder.add_node(node_b)
builder.add_node(node_c)

graph = builder.compile()
python  theme={null}
graph.invoke({"foo": ""})

Called A
Called C
python  theme={null}
@tool
def lookup_user_info(tool_call_id: Annotated[str, InjectedToolCallId], config: RunnableConfig):
    """Use this to look up user information to better assist them with their questions."""
    user_info = get_user_info(config.get("configurable", {}).get("user_id"))
    return Command(
        update={
            # update the state keys
            "user_info": user_info,
            # update the message history
            "messages": [ToolMessage("Successfully looked up user information", tool_call_id=tool_call_id)]
        }
    )
python  theme={null}
import random
from typing import Annotated, Literal
from typing_extensions import TypedDict
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages

class State(TypedDict):
    messages: Annotated[list, add_messages]

class MyNode:
    def __init__(self, name: str):
        self.name = name
    def __call__(self, state: State):
        return {"messages": [("assistant", f"Called node {self.name}")]}

def route(state) -> Literal["entry_node", END]:
    if len(state["messages"]) > 10:
        return END
    return "entry_node"

def add_fractal_nodes(builder, current_node, level, max_level):
    if level > max_level:
        return
    # Number of nodes to create at this level
    num_nodes = random.randint(1, 3)  # Adjust randomness as needed
    for i in range(num_nodes):
        nm = ["A", "B", "C"][i]
        node_name = f"node_{current_node}_{nm}"
        builder.add_node(node_name, MyNode(node_name))
        builder.add_edge(current_node, node_name)
        # Recursively add more nodes
        r = random.random()
        if r > 0.2 and level + 1 < max_level:
            add_fractal_nodes(builder, node_name, level + 1, max_level)
        elif r > 0.05:
            builder.add_conditional_edges(node_name, route, node_name)
        else:
            # End
            builder.add_edge(node_name, END)

def build_fractal_graph(max_level: int):
    builder = StateGraph(State)
    entry_point = "entry_node"
    builder.add_node(entry_point, MyNode(entry_point))
    builder.add_edge(START, entry_point)
    add_fractal_nodes(builder, entry_point, 1, max_level)
    # Optional: set a finish point if required
    builder.add_edge(entry_point, END)  # or any specific node
    return builder.compile()

app = build_fractal_graph(3)
python  theme={null}
print(app.get_graph().draw_mermaid())

%%{init: {'flowchart': {'curve': 'linear'}}}%%
graph TD;
    tart__([<p>__start__</p>]):::first
    ry_node(entry_node)
    e_entry_node_A(node_entry_node_A)
    e_entry_node_B(node_entry_node_B)
    e_node_entry_node_B_A(node_node_entry_node_B_A)
    e_node_entry_node_B_B(node_node_entry_node_B_B)
    e_node_entry_node_B_C(node_node_entry_node_B_C)
    nd__([<p>__end__</p>]):::last
    tart__ --> entry_node;
    ry_node --> __end__;
    ry_node --> node_entry_node_A;
    ry_node --> node_entry_node_B;
    e_entry_node_B --> node_node_entry_node_B_A;
    e_entry_node_B --> node_node_entry_node_B_B;
    e_entry_node_B --> node_node_entry_node_B_C;
    e_entry_node_A -.-> entry_node;
    e_entry_node_A -.-> __end__;
    e_node_entry_node_B_A -.-> entry_node;
    e_node_entry_node_B_A -.-> __end__;
    e_node_entry_node_B_B -.-> entry_node;
    e_node_entry_node_B_B -.-> __end__;
    e_node_entry_node_B_C -.-> entry_node;
    e_node_entry_node_B_C -.-> __end__;
    ssDef default fill:#f2f0ff,line-height:1.2
    ssDef first fill-opacity:0
    ssDef last fill:#bfb6fc
python  theme={null}
from IPython.display import Image, display
from langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeStyles

display(Image(app.get_graph().draw_mermaid_png()))
python  theme={null}
import nest_asyncio

nest_asyncio.apply()  # Required for Jupyter Notebook to run async functions

display(
    Image(
        app.get_graph().draw_mermaid_png(
            curve_style=CurveStyle.LINEAR,
            node_colors=NodeStyles(first="#ffdfba", last="#baffc9", default="#fad7de"),
            wrap_label_n_words=9,
            output_file_path=None,
            draw_method=MermaidDrawMethod.PYPPETEER,
            background_color="white",
            padding=10,
        )
    )
)
python  theme={null}
try:
    display(Image(app.get_graph().draw_png()))
except ImportError:
    print(
        "You likely need to install dependencies for pygraphviz, see more here https://github.com/pygraphviz/pygraphviz/blob/main/INSTALL.txt"
    )
```

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/use-graph-api.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
<Warning>
  You might have noticed that we used [`Command`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.Command) as a return type annotation, e.g. `Command[Literal["node_b", "node_c"]]`. This is necessary for the graph rendering and tells LangGraph that `node_a` can navigate to `node_b` and `node_c`.
</Warning>
```

Example 2 (unknown):
```unknown
<img src="https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_11.png?fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=f11e5cddedbf2760d40533f294c44aea" alt="Command-based graph navigation" data-og-width="232" width="232" data-og-height="333" height="333" data-path="oss/images/graph_api_image_11.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_11.png?w=280&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=c1b27d92b257a6c4ac57f34f007d0ee1 280w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_11.png?w=560&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=695d0062e5fb8ebea5525379edbba476 560w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_11.png?w=840&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=7bd3f779df628beba60a397674f85b59 840w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_11.png?w=1100&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=85a9194e8b4d9df2d01d10784dcf75d0 1100w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_11.png?w=1650&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=efd9118d4bcd6d1eb92760c573645fbd 1650w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_11.png?w=2500&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=1eb2a132386a64d18582af6978e4ac24 2500w" />

If we run the graph multiple times, we'd see it take different paths (A -> B or A -> C) based on the random choice in node A.
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
### Navigate to a node in a parent graph

If you are using [subgraphs](/oss/python/langgraph/use-subgraphs), you might want to navigate from a node within a subgraph to a different subgraph (i.e. a different node in the parent graph). To do so, you can specify `graph=Command.PARENT` in `Command`:
```

---

## Note: This example requires the `requests` and `requests_toolbelt` libraries.

**URL:** llms-txt#note:-this-example-requires-the-`requests`-and-`requests_toolbelt`-libraries.

---

## No longer supported

**URL:** llms-txt#no-longer-supported

model_with_tools = ChatOpenAI().bind_tools([some_tool])
agent = create_agent(model_with_tools, tools=[])

---

## null

**URL:** llms-txt#null

**Contents:**
- Interface
- By category
  - File loaders
  - Web loaders
- All document loaders

Source: https://docs.langchain.com/oss/javascript/integrations/document_loaders/index

Document loaders provide a **standard interface** for reading data from different sources (such as Slack, Notion, or Google Drive) into LangChain's [Document](https://v03.api.js.langchain.com/classes/_langchain_core.documents.Document.html) format.
This ensures that data can be handled consistently regardless of the source.

All document loaders implement the [BaseLoader](https://v03.api.js.langchain.com/classes/_langchain_core.document_loaders_base.BaseDocumentLoader.html) interface.

Each document loader may define its own parameters, but they share a common API:

* `.load()`: Loads all documents at once.
* `.loadAndSplit()`: Loads all documents at once and splits them into smaller documents.

LangChain.js categorizes document loaders in two different ways:

* [File loaders](/oss/javascript/integrations/document_loaders/file_loaders/), which load data into LangChain formats from your local filesystem.
* [Web loaders](/oss/javascript/integrations/document_loaders/web_loaders/), which load data from remote sources.

<Info>
  If you'd like to contribute an integration, see [Contributing integrations](/oss/javascript/contributing#add-a-new-integration).
</Info>

| Document Loader                                                             | Description                              | Package/API |
| --------------------------------------------------------------------------- | ---------------------------------------- | ----------- |
| [PDFLoader](/oss/javascript/integrations/document_loaders/file_loaders/pdf) | Load and parse PDF files using pdf-parse | Package     |

#### Common File Types

| Document Loader                                                                   | Description                                                  | Package/API |
| --------------------------------------------------------------------------------- | ------------------------------------------------------------ | ----------- |
| [CSV](/oss/javascript/integrations/document_loaders/file_loaders/csv)             | Load data from CSV files with configurable column extraction | Package     |
| [JSON](/oss/javascript/integrations/document_loaders/file_loaders/json)           | Load JSON files using JSON pointer to target specific keys   | Package     |
| [JSONLines](/oss/javascript/integrations/document_loaders/file_loaders/jsonlines) | Load data from JSONLines/JSONL files                         | Package     |
| [Text](/oss/javascript/integrations/document_loaders/file_loaders/text)           | Load plain text files                                        | Package     |
| [DOCX](/oss/javascript/integrations/document_loaders/file_loaders/docx)           | Load Microsoft Word documents (.docx and .doc formats)       | Package     |
| [EPUB](/oss/javascript/integrations/document_loaders/file_loaders/epub)           | Load EPUB files with optional chapter splitting              | Package     |
| [PPTX](/oss/javascript/integrations/document_loaders/file_loaders/pptx)           | Load PowerPoint presentations                                | Package     |
| [Subtitles](/oss/javascript/integrations/document_loaders/file_loaders/subtitles) | Load subtitle files (.srt format)                            | Package     |

#### Specialized File Loaders

| Document Loader                                                                                         | Description                                                 | Package/API |
| ------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------- | ----------- |
| [DirectoryLoader](/oss/javascript/integrations/document_loaders/file_loaders/directory)                 | Load all files from a directory with custom loader mappings | Package     |
| [UnstructuredLoader](/oss/javascript/integrations/document_loaders/file_loaders/unstructured)           | Load multiple file types using Unstructured API             | API         |
| [MultiFileLoader](/oss/javascript/integrations/document_loaders/file_loaders/multi_file)                | Load data from multiple individual file paths               | Package     |
| [ChatGPT](/oss/javascript/integrations/document_loaders/file_loaders/chatgpt)                           | Load ChatGPT conversation exports                           | Package     |
| [Notion Markdown](/oss/javascript/integrations/document_loaders/file_loaders/notion_markdown)           | Load Notion pages exported as Markdown                      | Package     |
| [OpenAI Whisper Audio](/oss/javascript/integrations/document_loaders/file_loaders/openai_whisper_audio) | Transcribe audio files using OpenAI Whisper API             | API         |

| Document Loader                                                                                      | Description                                                            | Web Support | Package/API |
| ---------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------- | :---------: | ----------- |
| [Cheerio](/oss/javascript/integrations/document_loaders/web_loaders/web_cheerio)                     | Load webpages using Cheerio (lightweight, no JavaScript execution)     |      ✅      | Package     |
| [Playwright](/oss/javascript/integrations/document_loaders/web_loaders/web_playwright)               | Load dynamic webpages using Playwright (supports JavaScript rendering) |      ❌      | Package     |
| [Puppeteer](/oss/javascript/integrations/document_loaders/web_loaders/web_puppeteer)                 | Load dynamic webpages using Puppeteer (headless Chrome)                |      ❌      | Package     |
| [FireCrawl](/oss/javascript/integrations/document_loaders/web_loaders/firecrawl)                     | Crawl and convert websites into LLM-ready markdown                     |      ✅      | API         |
| [Spider](/oss/javascript/integrations/document_loaders/web_loaders/spider)                           | Fast crawler that converts websites into HTML, markdown, or text       |      ✅      | API         |
| [RecursiveUrlLoader](/oss/javascript/integrations/document_loaders/web_loaders/recursive_url_loader) | Recursively load webpages following links                              |      ❌      | Package     |
| [Sitemap](/oss/javascript/integrations/document_loaders/web_loaders/sitemap)                         | Load all pages from a sitemap.xml                                      |      ✅      | Package     |
| [Browserbase](/oss/javascript/integrations/document_loaders/web_loaders/browserbase)                 | Load webpages using managed headless browsers with stealth mode        |      ✅      | API         |
| [WebPDFLoader](/oss/javascript/integrations/document_loaders/web_loaders/pdf)                        | Load PDF files in web environments                                     |      ✅      | Package     |

| Document Loader                                                                                                        | Description                                        | Web Support | Package/API |
| ---------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------- | :---------: | ----------- |
| [S3](/oss/javascript/integrations/document_loaders/web_loaders/s3)                                                     | Load files from AWS S3 buckets                     |      ❌      | Package     |
| [Azure Blob Storage Container](/oss/javascript/integrations/document_loaders/web_loaders/azure_blob_storage_container) | Load all files from Azure Blob Storage container   |      ❌      | Package     |
| [Azure Blob Storage File](/oss/javascript/integrations/document_loaders/web_loaders/azure_blob_storage_file)           | Load individual files from Azure Blob Storage      |      ❌      | Package     |
| [Google Cloud Storage](/oss/javascript/integrations/document_loaders/web_loaders/google_cloud_storage)                 | Load files from Google Cloud Storage buckets       |      ❌      | Package     |
| [Google Cloud SQL for PostgreSQL](/oss/javascript/integrations/document_loaders/web_loaders/google_cloudsql_pg)        | Load documents from Cloud SQL PostgreSQL databases |      ✅      | Package     |

#### Productivity Tools

| Document Loader                                                                    | Description                             | Web Support | Package/API |
| ---------------------------------------------------------------------------------- | --------------------------------------- | :---------: | ----------- |
| [Notion API](/oss/javascript/integrations/document_loaders/web_loaders/notionapi)  | Load Notion pages and databases via API |      ✅      | API         |
| [Figma](/oss/javascript/integrations/document_loaders/web_loaders/figma)           | Load Figma file data                    |      ✅      | API         |
| [Confluence](/oss/javascript/integrations/document_loaders/web_loaders/confluence) | Load pages from Confluence spaces       |      ❌      | API         |
| [GitHub](/oss/javascript/integrations/document_loaders/web_loaders/github)         | Load files from GitHub repositories     |      ✅      | API         |
| [GitBook](/oss/javascript/integrations/document_loaders/web_loaders/gitbook)       | Load GitBook documentation pages        |      ✅      | Package     |
| [Jira](/oss/javascript/integrations/document_loaders/web_loaders/jira)             | Load issues from Jira projects          |      ❌      | API         |
| [Airtable](/oss/javascript/integrations/document_loaders/web_loaders/airtable)     | Load records from Airtable bases        |      ✅      | API         |
| [Taskade](/oss/javascript/integrations/document_loaders/web_loaders/taskade)       | Load Taskade project data               |      ✅      | API         |

#### Search & Data APIs

| Document Loader                                                                          | Description                                                    | Web Support | Package/API |
| ---------------------------------------------------------------------------------------- | -------------------------------------------------------------- | :---------: | ----------- |
| [SearchAPI](/oss/javascript/integrations/document_loaders/web_loaders/searchapi)         | Load web search results from SearchAPI (Google, YouTube, etc.) |      ✅      | API         |
| [SerpAPI](/oss/javascript/integrations/document_loaders/web_loaders/serpapi)             | Load web search results from SerpAPI                           |      ✅      | API         |
| [Apify Dataset](/oss/javascript/integrations/document_loaders/web_loaders/apify_dataset) | Load scraped data from Apify platform                          |      ✅      | API         |

| Document Loader                                                                                        | Description                                           | Web Support | Package/API |
| ------------------------------------------------------------------------------------------------------ | ----------------------------------------------------- | :---------: | ----------- |
| [YouTube](/oss/javascript/integrations/document_loaders/web_loaders/youtube)                           | Load YouTube video transcripts                        |      ✅      | Package     |
| [AssemblyAI](/oss/javascript/integrations/document_loaders/web_loaders/assemblyai_audio_transcription) | Transcribe audio and video files using AssemblyAI API |      ✅      | API         |
| [Sonix](/oss/javascript/integrations/document_loaders/web_loaders/sonix_audio_transcription)           | Transcribe audio files using Sonix API                |      ❌      | API         |

| Document Loader                                                                                        | Description                                                | Web Support | Package/API |
| ------------------------------------------------------------------------------------------------------ | ---------------------------------------------------------- | :---------: | ----------- |
| [Couchbase](/oss/javascript/integrations/document_loaders/web_loaders/couchbase)                       | Load documents from Couchbase database using SQL++ queries |      ✅      | Package     |
| [LangSmith](/oss/javascript/integrations/document_loaders/web_loaders/langsmith)                       | Load datasets and traces from LangSmith                    |      ✅      | API         |
| [Hacker News](/oss/javascript/integrations/document_loaders/web_loaders/hn)                            | Load Hacker News threads and comments                      |      ✅      | Package     |
| [IMSDB](/oss/javascript/integrations/document_loaders/web_loaders/imsdb)                               | Load movie scripts from Internet Movie Script Database     |      ✅      | Package     |
| [College Confidential](/oss/javascript/integrations/document_loaders/web_loaders/college_confidential) | Load college information from College Confidential         |      ✅      | Package     |
| [Blockchain Data](/oss/javascript/integrations/document_loaders/web_loaders/sort_xyz_blockchain)       | Load blockchain data (NFTs, transactions) via Sort.xyz API |      ✅      | API         |

## All document loaders

<Columns cols={3}>
  <Card title="Airtable" icon="link" href="/oss/javascript/integrations/document_loaders/web_loaders/airtable" arrow="true" cta="View guide" />

<Card title="Apify Dataset" icon="link" href="/oss/javascript/integrations/document_loaders/web_loaders/apify_dataset" arrow="true" cta="View guide" />

<Card title="AssemblyAI Audio Transcript" icon="link" href="/oss/javascript/integrations/document_loaders/web_loaders/assemblyai_audio_transcription" arrow="true" cta="View guide" />

<Card title="Azure Blob Storage Container" icon="link" href="/oss/javascript/integrations/document_loaders/web_loaders/azure_blob_storage_container" arrow="true" cta="View guide" />

<Card title="Azure Blob Storage File" icon="link" href="/oss/javascript/integrations/document_loaders/web_loaders/azure_blob_storage_file" arrow="true" cta="View guide" />

<Card title="Blockchain Data" icon="link" href="/oss/javascript/integrations/document_loaders/web_loaders/sort_xyz_blockchain" arrow="true" cta="View guide" />

<Card title="Browserbase" icon="link" href="/oss/javascript/integrations/document_loaders/web_loaders/browserbase" arrow="true" cta="View guide" />

<Card title="ChatGPT" icon="link" href="/oss/javascript/integrations/document_loaders/file_loaders/chatgpt" arrow="true" cta="View guide" />

<Card title="Cheerio" icon="link" href="/oss/javascript/integrations/document_loaders/web_loaders/web_cheerio" arrow="true" cta="View guide" />

<Card title="College Confidential" icon="link" href="/oss/javascript/integrations/document_loaders/web_loaders/college_confidential" arrow="true" cta="View guide" />

<Card title="Confluence" icon="link" href="/oss/javascript/integrations/document_loaders/web_loaders/confluence" arrow="true" cta="View guide" />

<Card title="Couchbase" icon="link" href="/oss/javascript/integrations/document_loaders/web_loaders/couchbase" arrow="true" cta="View guide" />

<Card title="CSV" icon="link" href="/oss/javascript/integrations/document_loaders/file_loaders/csv" arrow="true" cta="View guide" />

<Card title="DirectoryLoader" icon="link" href="/oss/javascript/integrations/document_loaders/file_loaders/directory" arrow="true" cta="View guide" />

<Card title="DOCX" icon="link" href="/oss/javascript/integrations/document_loaders/file_loaders/docx" arrow="true" cta="View guide" />

<Card title="EPUB" icon="link" href="/oss/javascript/integrations/document_loaders/file_loaders/epub" arrow="true" cta="View guide" />

<Card title="Figma" icon="link" href="/oss/javascript/integrations/document_loaders/web_loaders/figma" arrow="true" cta="View guide" />

<Card title="FireCrawl" icon="link" href="/oss/javascript/integrations/document_loaders/web_loaders/firecrawl" arrow="true" cta="View guide" />

<Card title="GitHub" icon="link" href="/oss/javascript/integrations/document_loaders/web_loaders/github" arrow="true" cta="View guide" />

<Card title="GitBook" icon="link" href="/oss/javascript/integrations/document_loaders/web_loaders/gitbook" arrow="true" cta="View guide" />

<Card title="Google Cloud SQL for PostgreSQL" icon="link" href="/oss/javascript/integrations/document_loaders/web_loaders/google_cloudsql_pg" arrow="true" cta="View guide" />

<Card title="Google Cloud Storage" icon="link" href="/oss/javascript/integrations/document_loaders/web_loaders/google_cloud_storage" arrow="true" cta="View guide" />

<Card title="Hacker News" icon="link" href="/oss/javascript/integrations/document_loaders/web_loaders/hn" arrow="true" cta="View guide" />

<Card title="IMSDB" icon="link" href="/oss/javascript/integrations/document_loaders/web_loaders/imsdb" arrow="true" cta="View guide" />

<Card title="Jira" icon="link" href="/oss/javascript/integrations/document_loaders/web_loaders/jira" arrow="true" cta="View guide" />

<Card title="JSON" icon="link" href="/oss/javascript/integrations/document_loaders/file_loaders/json" arrow="true" cta="View guide" />

<Card title="JSONLines" icon="link" href="/oss/javascript/integrations/document_loaders/file_loaders/jsonlines" arrow="true" cta="View guide" />

<Card title="LangSmith" icon="link" href="/oss/javascript/integrations/document_loaders/web_loaders/langsmith" arrow="true" cta="View guide" />

<Card title="MultiFileLoader" icon="link" href="/oss/javascript/integrations/document_loaders/file_loaders/multi_file" arrow="true" cta="View guide" />

<Card title="Notion API" icon="link" href="/oss/javascript/integrations/document_loaders/web_loaders/notionapi" arrow="true" cta="View guide" />

<Card title="Notion Markdown" icon="link" href="/oss/javascript/integrations/document_loaders/file_loaders/notion_markdown" arrow="true" cta="View guide" />

<Card title="OpenAI Whisper Audio" icon="link" href="/oss/javascript/integrations/document_loaders/file_loaders/openai_whisper_audio" arrow="true" cta="View guide" />

<Card title="PDFLoader" icon="link" href="/oss/javascript/integrations/document_loaders/file_loaders/pdf" arrow="true" cta="View guide" />

<Card title="Playwright" icon="link" href="/oss/javascript/integrations/document_loaders/web_loaders/web_playwright" arrow="true" cta="View guide" />

<Card title="PPTX" icon="link" href="/oss/javascript/integrations/document_loaders/file_loaders/pptx" arrow="true" cta="View guide" />

<Card title="Puppeteer" icon="link" href="/oss/javascript/integrations/document_loaders/web_loaders/web_puppeteer" arrow="true" cta="View guide" />

<Card title="RecursiveUrlLoader" icon="link" href="/oss/javascript/integrations/document_loaders/web_loaders/recursive_url_loader" arrow="true" cta="View guide" />

<Card title="S3" icon="link" href="/oss/javascript/integrations/document_loaders/web_loaders/s3" arrow="true" cta="View guide" />

<Card title="SearchAPI" icon="link" href="/oss/javascript/integrations/document_loaders/web_loaders/searchapi" arrow="true" cta="View guide" />

<Card title="SerpAPI" icon="link" href="/oss/javascript/integrations/document_loaders/web_loaders/serpapi" arrow="true" cta="View guide" />

<Card title="Sitemap" icon="link" href="/oss/javascript/integrations/document_loaders/web_loaders/sitemap" arrow="true" cta="View guide" />

<Card title="Sonix Audio" icon="link" href="/oss/javascript/integrations/document_loaders/web_loaders/sonix_audio_transcription" arrow="true" cta="View guide" />

<Card title="Spider" icon="link" href="/oss/javascript/integrations/document_loaders/web_loaders/spider" arrow="true" cta="View guide" />

<Card title="Subtitles" icon="link" href="/oss/javascript/integrations/document_loaders/file_loaders/subtitles" arrow="true" cta="View guide" />

<Card title="Taskade" icon="link" href="/oss/javascript/integrations/document_loaders/web_loaders/taskade" arrow="true" cta="View guide" />

<Card title="Text" icon="link" href="/oss/javascript/integrations/document_loaders/file_loaders/text" arrow="true" cta="View guide" />

<Card title="UnstructuredLoader" icon="link" href="/oss/javascript/integrations/document_loaders/file_loaders/unstructured" arrow="true" cta="View guide" />

<Card title="WebPDFLoader" icon="link" href="/oss/javascript/integrations/document_loaders/web_loaders/pdf" arrow="true" cta="View guide" />

<Card title="YouTube" icon="link" href="/oss/javascript/integrations/document_loaders/web_loaders/youtube" arrow="true" cta="View guide" />
</Columns>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/javascript/integrations/document_loaders/index.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Observability concepts

**URL:** llms-txt#observability-concepts

**Contents:**
- Runs
- Traces
- Threads
- Projects
- Feedback
- Tags
- Metadata
- Data storage and retention
- Deleting traces from LangSmith

Source: https://docs.langchain.com/langsmith/observability-concepts

This page covers key concepts that are important to understand when logging traces to LangSmith.

A [*trace*](#traces) records the sequence of steps your application takes—from receiving an input, through intermediate processing, to producing a final output. Each step within a trace is represented by a [*run*](#runs). Multiple traces are grouped together within a [*project*](#projects), and traces from multi-turn conversations can be linked together as a [*thread*](#threads).

The following diagram displays these concepts in the context of a simple RAG app, which retrieves documents from an index and generates an answer.

<div style={{ textAlign: 'center' }}>
  <img className="block dark:hidden" src="https://mintcdn.com/langchain-5e9cc07a/Tf5b6pnNY9Uj6Vtl/langsmith/images/primitives.png?fit=max&auto=format&n=Tf5b6pnNY9Uj6Vtl&q=85&s=50c5f4d966f8fe4f8ae0be0beaf11bc4" alt="Primitives of LangSmith Project, Trace, Run in the context of a question and answer RAG app." data-og-width="2701" width="2701" data-og-height="1739" height="1739" data-path="langsmith/images/primitives.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Tf5b6pnNY9Uj6Vtl/langsmith/images/primitives.png?w=280&fit=max&auto=format&n=Tf5b6pnNY9Uj6Vtl&q=85&s=e0b6083af11ec78c1650c907a6b8649a 280w, https://mintcdn.com/langchain-5e9cc07a/Tf5b6pnNY9Uj6Vtl/langsmith/images/primitives.png?w=560&fit=max&auto=format&n=Tf5b6pnNY9Uj6Vtl&q=85&s=9054d8620c453c520e161c1ba8fb1fdc 560w, https://mintcdn.com/langchain-5e9cc07a/Tf5b6pnNY9Uj6Vtl/langsmith/images/primitives.png?w=840&fit=max&auto=format&n=Tf5b6pnNY9Uj6Vtl&q=85&s=8ece2e0b84019b722446e9bfdbf067f9 840w, https://mintcdn.com/langchain-5e9cc07a/Tf5b6pnNY9Uj6Vtl/langsmith/images/primitives.png?w=1100&fit=max&auto=format&n=Tf5b6pnNY9Uj6Vtl&q=85&s=c4e3700fe862b539954b8c6c0124bfac 1100w, https://mintcdn.com/langchain-5e9cc07a/Tf5b6pnNY9Uj6Vtl/langsmith/images/primitives.png?w=1650&fit=max&auto=format&n=Tf5b6pnNY9Uj6Vtl&q=85&s=f9dfe46dbb576c1faeb1f4ad52324527 1650w, https://mintcdn.com/langchain-5e9cc07a/Tf5b6pnNY9Uj6Vtl/langsmith/images/primitives.png?w=2500&fit=max&auto=format&n=Tf5b6pnNY9Uj6Vtl&q=85&s=9137d815270a89f9d4df799381a88aac 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/langchain-5e9cc07a/Tf5b6pnNY9Uj6Vtl/langsmith/images/primitives-dark.png?fit=max&auto=format&n=Tf5b6pnNY9Uj6Vtl&q=85&s=3ca35a8a6cec65b9a5139e7ac8cac470" alt="Primitives of LangSmith Project, Trace, Run in the context of a question and answer RAG app." data-og-width="2919" width="2919" data-og-height="1752" height="1752" data-path="langsmith/images/primitives-dark.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Tf5b6pnNY9Uj6Vtl/langsmith/images/primitives-dark.png?w=280&fit=max&auto=format&n=Tf5b6pnNY9Uj6Vtl&q=85&s=41bff3be3eabb386bd11347b77908625 280w, https://mintcdn.com/langchain-5e9cc07a/Tf5b6pnNY9Uj6Vtl/langsmith/images/primitives-dark.png?w=560&fit=max&auto=format&n=Tf5b6pnNY9Uj6Vtl&q=85&s=1e7d10f532b2a099ca74cd80f1f0f90d 560w, https://mintcdn.com/langchain-5e9cc07a/Tf5b6pnNY9Uj6Vtl/langsmith/images/primitives-dark.png?w=840&fit=max&auto=format&n=Tf5b6pnNY9Uj6Vtl&q=85&s=10e2a103221b086ed8985a8478c9992c 840w, https://mintcdn.com/langchain-5e9cc07a/Tf5b6pnNY9Uj6Vtl/langsmith/images/primitives-dark.png?w=1100&fit=max&auto=format&n=Tf5b6pnNY9Uj6Vtl&q=85&s=a82db1fc02ffd0db0222ed0a14e48a86 1100w, https://mintcdn.com/langchain-5e9cc07a/Tf5b6pnNY9Uj6Vtl/langsmith/images/primitives-dark.png?w=1650&fit=max&auto=format&n=Tf5b6pnNY9Uj6Vtl&q=85&s=70a1cd6dd3f30b5863bba85e92a30733 1650w, https://mintcdn.com/langchain-5e9cc07a/Tf5b6pnNY9Uj6Vtl/langsmith/images/primitives-dark.png?w=2500&fit=max&auto=format&n=Tf5b6pnNY9Uj6Vtl&q=85&s=33a6bfeb3258ced236732b0be95b6ab3 2500w" />
</div>

A *run* is a span representing a single unit of work or operation within your LLM application. This could be anything from a single call to an LLM or chain, to a prompt formatting call, to a runnable lambda invocation. If you are familiar with [OpenTelemetry](https://opentelemetry.io/), you can think of a run as a span.

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/run.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=a692d614eb441aef6e3a1f02f4a37e8a" alt="Run" data-og-width="1830" width="1830" data-og-height="1527" height="1527" data-path="langsmith/images/run.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/run.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=911a09dc4da0d94015cc2a22f95efce6 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/run.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=6ae06bcbba3e3b240cfa41950f4e451d 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/run.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=2e886c505427057c726f30b495fc7c4c 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/run.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=48dfe76850483f46da1bbe4bcccb5f8b 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/run.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=eff65c5910eaa68456e26712dfa64d1d 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/run.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=d99e6ba31cd3f4039ad86b0ade0b78b4 2500w" />

A *trace* is a collection of runs for a single operation. For example, if you have a user request that triggers a chain, and that chain makes a call to an LLM, then to an output parser, and so on, all of these runs would be part of the same trace. If you are familiar with [OpenTelemetry](https://opentelemetry.io/), you can think of a LangSmith trace as a collection of spans. Runs are bound to a trace by a unique trace ID.

<img src="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=dd692f6433fbcf0413a4516c170062f2" alt="Trace" data-og-width="1830" width="1830" data-og-height="1527" height="1527" data-path="langsmith/images/trace.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=fea7f898d3f25c76bed212af157b1a1d 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=5a75129cb5ab5fab0a6618d0c3160ec2 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=3102493a51a280b38004ff8628231ad6 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=f0e4d194f6a6b18b4505a2b4a4f40fc3 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=d1f0bb5ae6e5dec024c5e3726b848394 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=f5d14619201af89c134fd1fb99b7cdf0 2500w" />

A *thread* is a sequence of traces representing a single conversation. Many LLM applications have a chatbot-like interface in which the user and the LLM application engage in a multi-turn conversation. Each turn in the conversation is represented as its own trace, but these traces are linked together by being part of the same thread. The most recent trace in a thread is the latest message exchange.

To group traces into threads, you pass a special metadata key (`session_id`, `thread_id`, or `conversation_id`) with a unique identifier value that links the traces together.

[Learn how to configure threads](/langsmith/threads).

<img className="block dark:hidden" src="https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-light.png?fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=f7af4c3904073d5f58f28c656603ca19" alt="Thread representing a sequence of traces in a multi-turn conversation." data-og-width="1273" width="1273" data-og-height="757" height="757" data-path="langsmith/images/thread-overview-light.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-light.png?w=280&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=cd769088ab3ab2dae09982915f23772d 280w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-light.png?w=560&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=70ae6b5a6b8edb83ba3604d4c6e0262e 560w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-light.png?w=840&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=61d89d8077072221373490edac65363c 840w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-light.png?w=1100&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=ad8159fe12f056dbc561c612e3797b97 1100w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-light.png?w=1650&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=3611f7bcc95c45bcb91c093ca36ef348 1650w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-light.png?w=2500&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=1c0426d7e83562e1d76e079959bda186 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-dark.png?fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=f738de4cac932ed2b8657e8f3b706b77" alt="Thread representing a sequence of traces in a multi-turn conversation." data-og-width="1273" width="1273" data-og-height="753" height="753" data-path="langsmith/images/thread-overview-dark.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-dark.png?w=280&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=9bc9dd49c63661dceb981899c5f0332b 280w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-dark.png?w=560&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=01713d47cf762f99be1a1143b01582e8 560w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-dark.png?w=840&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=cfc77e449d0ce27cdfa51b2f7c6ed655 840w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-dark.png?w=1100&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=d84f671a8f2c1207dbb98c72a37d1832 1100w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-dark.png?w=1650&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=d592bd4c8671b3d7f19a49471888a901 1650w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-dark.png?w=2500&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=2405eaef2af227dd5e5efac85fc9e623 2500w" />

A *project* is a collection of traces. You can think of a project as a container for all the traces that are related to a single application or service. You can have multiple projects, and each project can have multiple traces.

<img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/project.png?fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=2426200ab2e619674636e41f11246c0d" alt="Project" data-og-width="1830" width="1830" data-og-height="1527" height="1527" data-path="langsmith/images/project.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/project.png?w=280&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=97510cd2501c9d2dc529680172664219 280w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/project.png?w=560&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=8624fbec9c58a1999d702aed79a606f6 560w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/project.png?w=840&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=5d918966500e84ce29550591755d98f6 840w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/project.png?w=1100&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=d9a6a4b2365de4e4cf80506d3fc0a467 1100w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/project.png?w=1650&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=11761179be51c8cfc58193d6d0215269 1650w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/project.png?w=2500&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=779669a7f2f2106946d1f7eb3f427ed5 2500w" />

*Feedback* allows you to score an individual run based on certain criteria. Each feedback entry consists of a feedback tag and feedback score, and is bound to a run by a unique run ID. Feedback can be continuous or discrete (categorical), and you can reuse feedback tags across different runs within an organization.

You can collect feedback on runs in a number of ways:

1. [Sent up along with a trace](/langsmith/attach-user-feedback) from the LLM application.
2. Generated by a user in the app [inline](/langsmith/annotate-traces-inline) or in an [annotation queue](/langsmith/annotation-queues).
3. Generated by an automatic evaluator during [offline evaluation](/langsmith/evaluate-llm-application).
4. Generated by an [online evaluator](/langsmith/online-evaluations).

To learn more about how feedback is stored in the application, refer to the [Feedback data format guide](/langsmith/feedback-data-format).

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/feedback.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=13fa69590caa5ba050e3cda9dbb6a336" alt="Feedback" data-og-width="1830" width="1830" data-og-height="1527" height="1527" data-path="langsmith/images/feedback.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/feedback.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=ef7168bb4e77c0b70f5cc6b7062be002 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/feedback.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=75059818a510d95abea177fb5e3a1b4e 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/feedback.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=a29438f9e3126eb0aa01023e02c0b621 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/feedback.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=904b4e5e7aa4dcfd49121b82fb269c5f 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/feedback.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=f5d68cfe719b0cc0a9d5e2b0fefb12ae 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/feedback.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=3be4d77ff49c91b49effcceeb96b38d3 2500w" />

*Tags* are collections of strings that can be attached to runs. You can use tags to do the following in the LangSmith UI:

* Categorize runs for easier search.
* Filter runs.
* Group runs together for analysis.

[Learn how to attach tags to your traces](/langsmith/add-metadata-tags).

<img src="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tags.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=44e6b6242fd76e811dcc28d88c5c6db5" alt="Tags" data-og-width="1830" width="1830" data-og-height="1527" height="1527" data-path="langsmith/images/tags.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tags.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=35d294b4a295130dfb7f23a8237fe53d 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tags.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=ab7e4b06eced56da2b51eb9f88738e6f 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tags.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=ecd67b36985d21a275b3b6aa34922a84 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tags.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=dfc774fcd984d08202f178eb75cc14e6 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tags.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=9e5c4c571cbaf45fdab26f21e854bc29 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tags.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=b792075475d671a84e3efb1ca47d527b 2500w" />

*Metadata* is a collection of key-value pairs that you can attach to runs. You can use metadata to store additional information about a run, such as the version of the application that generated the run, the environment in which the run was generated, or any other information that you want to associate with a run. Similarly to tags, you can use metadata to filter runs in the LangSmith UI or group runs together for analysis.

[Learn how to add metadata to your traces](/langsmith/add-metadata-tags).

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/metadata.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=e48730c02b7974035bbb312734e86a92" alt="Metadata" data-og-width="1830" width="1830" data-og-height="1527" height="1527" data-path="langsmith/images/metadata.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/metadata.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=6fbe88a76fd74d9fb36e89465ef068cc 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/metadata.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=7e3c71b02884707ca26863a88e4d04f6 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/metadata.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=8799298d81f72725c01a937c549fdb81 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/metadata.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=6dbb9c568c187cbd8bd14a471f6977b3 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/metadata.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=be48eaaa028a65c6ca6698c27cf9da6b 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/metadata.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=810f9462214e87aba38b724a9b5083af 2500w" />

## Data storage and retention

For traces ingested on or after Wednesday, May 22, 2024, LangSmith (SaaS) retains trace data for a maximum of 400 days past the date and time the trace was inserted into the LangSmith trace database.

After 400 days, the traces are permanently deleted from LangSmith, with a limited amount of metadata retained for the purpose of showing accurate statistics, such as historic usage and cost.

<Note>
  If you wish to keep tracing data longer than the data retention period, you can add it to a dataset. A [dataset](/langsmith/manage-datasets) allows you to store the trace inputs and outputs (e.g., as a key-value dataset), and will persist indefinitely, even after the trace gets deleted.
</Note>

## Deleting traces from LangSmith

If you need to remove a trace from LangSmith before its expiration date, you can do so by deleting the project that contains it.

You can delete a project with one of the following ways:

* In the [LangSmith UI](https://smith.langchain.com), select the **Delete** option on the project's overflow menu.
* With the [`delete_tracer_sessions`](https://api.smith.langchain.com/redoc#tag/tracer-sessions/operation/delete_tracer_session_api_v1_sessions__session_id__delete) API endpoint
* With the `delete_project()` ([Python](/langsmith/smith-python-sdk)) or `deleteProject()` ([JS/TS](/langsmith/smith-js-ts-sdk)) in the LangSmith SDK.

LangSmith does not support self-service deletion of individual traces.

If you have a need to delete a single trace (or set of traces) from LangSmith project before its expiration date, the account owner should reach out to [LangSmith Support](mailto:support@langchain.dev) with the organization ID and trace IDs.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/observability-concepts.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Observability

**URL:** llms-txt#observability

**Contents:**
- Prerequisites
- Enable tracing
- Trace selectively

Source: https://docs.langchain.com/oss/javascript/langgraph/observability

Traces are a series of steps that your application takes to go from input to output. Each of these individual steps is represented by a run. You can use [LangSmith](https://smith.langchain.com/) to visualize these execution steps. To use it, [enable tracing for your application](/langsmith/trace-with-langgraph). This enables you to do the following:

* [Debug a locally running application](/langsmith/observability-studio#debug-langsmith-traces).
* [Evaluate the application performance](/oss/javascript/langchain/evals).
* [Monitor the application](/langsmith/dashboards).

Before you begin, ensure you have the following:

* A [LangSmith account](https://smith.langchain.com/) (free to sign up)

To enable tracing for your application, set the following environment variables:

By default, the trace will be logged to the project with the name `default`. To configure a custom project name, see [Log to a project](#log-to-a-project).

For more information, see [Trace with LangGraph](/langsmith/trace-with-langgraph).

You may opt to trace specific invocations or parts of your application using LangSmith's `tracing_context` context manager:

```python  theme={null}
import langsmith as ls

**Examples:**

Example 1 (unknown):
```unknown
By default, the trace will be logged to the project with the name `default`. To configure a custom project name, see [Log to a project](#log-to-a-project).

For more information, see [Trace with LangGraph](/langsmith/trace-with-langgraph).

## Trace selectively

You may opt to trace specific invocations or parts of your application using LangSmith's `tracing_context` context manager:
```

---

## Observability in Studio

**URL:** llms-txt#observability-in-studio

**Contents:**
- Iterate on prompts
  - Direct node editing
  - Graph configuration
  - Playground
- Run experiments over a dataset
  - Prerequisites
  - Experiment setup
- Debug LangSmith traces
  - Open deployed threads
  - Testing local agents with remote traces

Source: https://docs.langchain.com/langsmith/observability-studio

LangSmith [Studio](/langsmith/studio) provides tools to inspect, debug, and improve your app beyond execution. By working with traces, datasets, and prompts, you can see how your application behaves in detail, measure its performance, and refine its outputs:

* [Iterate on prompts](#iterate-on-prompts): Modify prompts inside graph nodes directly or with the LangSmith playground.
* [Run experiments over a dataset](#run-experiments-over-a-dataset): Execute your assistant over a LangSmith dataset to score and compare results.
* [Debug LangSmith traces](#debug-langsmith-traces): Import traced runs into Studio and optionally clone them into your local agent.
* [Add a node to a dataset](#add-node-to-dataset): Turn parts of thread history into dataset examples for evaluation or further analysis.

## Iterate on prompts

Studio supports the following methods for modifying prompts in your graph:

* [Direct node editing](#direct-node-editing)
* [Playground interface](#playground)

### Direct node editing

Studio allows you to edit prompts used inside individual nodes, directly from the graph interface.

### Graph configuration

Define your [configuration](/oss/python/langgraph/use-graph-api#add-runtime-configuration) to specify prompt fields and their associated nodes using `langgraph_nodes` and `langgraph_type` keys.

#### `langgraph_nodes`

* **Description**: Specifies which nodes of the graph a configuration field is associated with.
* **Value Type**: Array of strings, where each string is the name of a node in your graph.
* **Usage Context**: Include in the `json_schema_extra` dictionary for Pydantic models or the `metadata["json_schema_extra"]` dictionary for dataclasses.
* **Example**:

#### `langgraph_type`

* **Description**: Specifies the type of configuration field, which determines how it's handled in the UI.
* **Value Type**: String
* **Supported Values**:
  * `"prompt"`: Indicates the field contains prompt text that should be treated specially in the UI.
* **Usage Context**: Include in the `json_schema_extra` dictionary for Pydantic models or the `metadata["json_schema_extra"]` dictionary for dataclasses.
* **Example**:

<Accordion title="Full example configuration">
  
</Accordion>

#### Editing prompts in the UI

1. Locate the gear icon on nodes with associated configuration fields.
2. Click to open the configuration modal.
3. Edit the values.
4. Save to update the current assistant version or create a new one.

The [playground](/langsmith/create-a-prompt) interface allows testing individual LLM calls without running the full graph:

1. Select a thread.
2. Click **View LLM Runs** on a node. This lists all the LLM calls (if any) made inside the node.
3. Select an LLM run to open in the playground.
4. Modify prompts and test different model and tool settings.
5. Copy updated prompts back to your graph.

## Run experiments over a dataset

Studio lets you run [evaluations](/langsmith/evaluation-concepts) by executing your assistant against a predefined LangSmith [dataset](/langsmith/evaluation-concepts#datasets). This allows you to test performance across a variety of inputs, compare outputs to reference answers, and score results with configured [evaluators](/langsmith/evaluation-concepts#evaluators).

This guide shows you how to run a full end-to-end experiment directly from Studio.

Before running an experiment, ensure you have the following:

* **A LangSmith dataset**: Your dataset should contain the inputs you want to test and optionally, reference outputs for comparison. The schema for the inputs must match the required input schema for the assistant. For more information on schemas, see [here](/oss/python/langgraph/use-graph-api#schema). For more on creating datasets, refer to [How to Manage Datasets](/langsmith/manage-datasets-in-application#set-up-your-dataset).
* **(Optional) Evaluators**: You can attach evaluators (e.g., LLM-as-a-Judge, heuristics, or custom functions) to your dataset in LangSmith. These will run automatically after the graph has processed all inputs.
* **A running application**: The experiment can be run against:
  * An application deployed on [LangSmith](/langsmith/deployments).
  * A locally running application started via the [langgraph-cli](/langsmith/local-server).

1. Launch the experiment. Click the **Run experiment** button in the top right corner of the Studio page.
2. Select your dataset. In the modal that appears, select the dataset (or a specific dataset split) to use for the experiment and click **Start**.
3. Monitor the progress. All of the inputs in the dataset will now be run against the active assistant. Monitor the experiment's progress via the badge in the top right corner.
4. You can continue to work in Studio while the experiment runs in the background. Click the arrow icon button at any time to navigate to LangSmith and view the detailed experiment results.

## Debug LangSmith traces

This guide explains how to open LangSmith traces in Studio for interactive investigation and debugging.

### Open deployed threads

1. Open the LangSmith trace, selecting the root run.
2. Click **Run in Studio**.

This will open Studio connected to the associated deployment with the trace's parent thread selected.

### Testing local agents with remote traces

This section explains how to test a local agent against remote traces from LangSmith. This enables you to use production traces as input for local testing, allowing you to debug and verify agent modifications in your development environment.

* A LangSmith traced thread
* A [locally running agent](/langsmith/local-server#local-development-server).

<Info>
  **Local agent requirements**

* langgraph>=0.3.18
  * langgraph-api>=0.0.32
  * Contains the same set of nodes present in the remote trace
</Info>

1. Open the LangSmith trace, selecting the root run.
2. Click the dropdown next to **Run in Studio**.
3. Enter your local agent's URL.
4. Select **Clone thread locally**.
5. If multiple graphs exist, select the target graph.

A new thread will be created in your local agent with the thread history inferred and copied from the remote thread, and you will be navigated to Studio for your locally running application.

## Add node to dataset

Add [examples](/langsmith/evaluation-concepts#examples) to [LangSmith datasets](/langsmith/manage-datasets) from nodes in the thread log. This is useful to evaluate individual steps of the agent.

1. Select a thread.
2. Click **Add to Dataset**.
3. Select nodes whose input/output you want to add to a dataset.
4. For each selected node, select the target dataset to create the example in. By default a dataset for the specific assistant and node will be selected. If this dataset does not yet exist, it will be created.
5. Edit the example's input/output as needed before adding it to the dataset.
6. Select **Add to dataset** at the bottom of the page to add all selected nodes to their respective datasets.

For more details, refer to [How to evaluate an application's intermediate steps](/langsmith/evaluate-on-intermediate-steps).

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/observability-studio.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
#### `langgraph_type`

* **Description**: Specifies the type of configuration field, which determines how it's handled in the UI.
* **Value Type**: String
* **Supported Values**:
  * `"prompt"`: Indicates the field contains prompt text that should be treated specially in the UI.
* **Usage Context**: Include in the `json_schema_extra` dictionary for Pydantic models or the `metadata["json_schema_extra"]` dictionary for dataclasses.
* **Example**:
```

Example 2 (unknown):
```unknown
<Accordion title="Full example configuration">
```

---

## Ollama

**URL:** llms-txt#ollama

**Contents:**
- Model interfaces

Source: https://docs.langchain.com/oss/python/integrations/providers/ollama

This page covers all LangChain integrations with [Ollama](https://ollama.com/).

Ollama allows you to run open-source models (like [`gpt-oss`](https://ollama.com/library/gpt-oss)) locally.

For a complete list of supported models and variants, see the [Ollama model library](https://ollama.ai/library).

<Columns cols={2}>
  <Card title="ChatOllama" href="/oss/python/integrations/chat/ollama" cta="Get started" icon="message" arrow>
    Ollama chat models.
  </Card>

<Card title="OllamaLLM" href="/oss/python/integrations/llms/ollama" cta="Get started" icon="i-cursor" arrow>
    (Legacy) Ollama text completion models.
  </Card>

<Card title="OllamaEmbeddings" href="/oss/python/integrations/text_embedding/ollama" cta="Get started" icon="microsoft" arrow>
    Ollama embedding models.
  </Card>
</Columns>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/providers/ollama.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Once these are set, start Claude Code, and events will be traced to LangSmith

**URL:** llms-txt#once-these-are-set,-start-claude-code,-and-events-will-be-traced-to-langsmith

<Note>
  Claude Code emits [open telemetry standard events](https://docs.claude.com/en/docs/claude-code/monitoring-usage#events) for monitoring usage, but this does not include the actual prompts and messages that go to the LLM.
</Note>

<Note>
  If you're self-hosting LangSmith, replace the base endpoint with your LangSmith api endpoint and append `/api/v1`. For example: `OTEL_EXPORTER_OTLP_LOGS_ENDPOINT=https://ai-company.com/api/v1/otel/v1/claude_code`
</Note>

<img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/claude-code-trace.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=2a6e8481d9451ba0775d1dd175ad893e" alt="Claude Code Trace" data-og-width="3406" width="3406" data-og-height="1972" height="1972" data-path="langsmith/images/claude-code-trace.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/claude-code-trace.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=3a00d0f6cbcd7fb5b69121e4a69abb48 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/claude-code-trace.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=8b9c25576a05a142506b30ce068bddd4 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/claude-code-trace.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=d5a3a902b15e71eb551eb394d5caf641 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/claude-code-trace.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=cfa9e38c765e191020680f34010d4fb8 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/claude-code-trace.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=ee44593b5516434c6ea05c1d49306d3d 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/claude-code-trace.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=e037152a80fdb00763f7f64532521950 2500w" />

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/trace-claude-code.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Only keep post title, headers, and content from the full HTML.

**URL:** llms-txt#only-keep-post-title,-headers,-and-content-from-the-full-html.

**Contents:**
  - Splitting documents
  - Storing documents
- 2. Retrieval and Generation
  - RAG agents

bs4_strainer = bs4.SoupStrainer(class_=("post-title", "post-header", "post-content"))
loader = WebBaseLoader(
    web_paths=("https://lilianweng.github.io/posts/2023-06-23-agent/",),
    bs_kwargs={"parse_only": bs4_strainer},
)
docs = loader.load()

assert len(docs) == 1
print(f"Total characters: {len(docs[0].page_content)}")
output  theme={null}
Total characters: 43131
python  theme={null}
print(docs[0].page_content[:500])
output  theme={null}
      LLM Powered Autonomous Agents

Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng

Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.
Agent System Overview#
In
python  theme={null}
from langchain_text_splitters import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,  # chunk size (characters)
    chunk_overlap=200,  # chunk overlap (characters)
    add_start_index=True,  # track index in original document
)
all_splits = text_splitter.split_documents(docs)

print(f"Split blog post into {len(all_splits)} sub-documents.")
output  theme={null}
Split blog post into 66 sub-documents.
python  theme={null}
document_ids = vector_store.add_documents(documents=all_splits)

print(document_ids[:3])
output  theme={null}
['07c18af6-ad58-479a-bfb1-d508033f9c64', '9000bf8e-1993-446f-8d4d-f4e507ba4b8f', 'ba3b5d14-bed9-4f5f-88be-44c88aedc2e6']
python  theme={null}
from langchain.tools import tool

@tool(response_format="content_and_artifact")
def retrieve_context(query: str):
    """Retrieve information to help answer a query."""
    retrieved_docs = vector_store.similarity_search(query, k=2)
    serialized = "\n\n".join(
        (f"Source: {doc.metadata}\nContent: {doc.page_content}")
        for doc in retrieved_docs
    )
    return serialized, retrieved_docs
python  theme={null}
  from typing import Literal

def retrieve_context(query: str, section: Literal["beginning", "middle", "end"]):
  python  theme={null}
from langchain.agents import create_agent

tools = [retrieve_context]

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
**Go deeper**

`DocumentLoader`: Object that loads data from a source as list of `Documents`.

* [Integrations](/oss/python/integrations/document_loaders/): 160+ integrations to choose from.
* [`BaseLoader`](https://reference.langchain.com/python/langchain_core/document_loaders/#langchain_core.document_loaders.BaseLoader): API reference for the base interface.

### Splitting documents

Our loaded document is over 42k characters which is too long to fit into the context window of many models. Even for those models that could fit the full post in their context window, models can struggle to find information in very long inputs.

To handle this we'll split the [`Document`](https://reference.langchain.com/python/langchain_core/documents/#langchain_core.documents.base.Document) into chunks for embedding and vector storage. This should help us retrieve only the most relevant parts of the blog post at run time.

As in the [semantic search tutorial](/oss/python/langchain/knowledge-base), we use a `RecursiveCharacterTextSplitter`, which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases.
```

---

## OpenAIEmbeddings

**URL:** llms-txt#openaiembeddings

**Contents:**
- Overview
  - Integration details
- Setup
  - Credentials

Source: https://docs.langchain.com/oss/javascript/integrations/text_embedding/openai

This will help you get started with OpenAIEmbeddings [embedding models](/oss/javascript/integrations/text_embedding) using LangChain. For detailed documentation on `OpenAIEmbeddings` features and configuration options, please refer to the [API reference](https://api.js.langchain.com/classes/langchain_openai.OpenAIEmbeddings.html).

### Integration details

| Class                                                                                           | Package                                                                         | Local | [Py support](https://python.langchain.com/docs/integrations/text_embedding/openai/) |                                             Downloads                                             |                                             Version                                            |
| :---------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------ | :---: | :---------------------------------------------------------------------------------: | :-----------------------------------------------------------------------------------------------: | :--------------------------------------------------------------------------------------------: |
| [OpenAIEmbeddings](https://api.js.langchain.com/classes/langchain_openai.OpenAIEmbeddings.html) | [@langchain/openai](https://api.js.langchain.com/modules/langchain_openai.html) |   ❌   |                                          ✅                                          | ![NPM - Downloads](https://img.shields.io/npm/dm/@langchain/openai?style=flat-square\&label=%20&) | ![NPM - Version](https://img.shields.io/npm/v/@langchain/openai?style=flat-square\&label=%20&) |

To access OpenAIEmbeddings embedding models you'll need to create an OpenAI account, get an API key, and install the `@langchain/openai` integration package.

Head to [platform.openai.com](https://platform.openai.com) to sign up to OpenAI and generate an API key. Once you've done this set the `OPENAI_API_KEY` environment variable:

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:

```bash  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.smith.langchain.com/) API key by uncommenting below:
```

---

## OpenAI

**URL:** llms-txt#openai

**Contents:**
- Model interfaces
- Tools and toolkits
- Retrievers
- Document loaders
- Other

Source: https://docs.langchain.com/oss/python/integrations/providers/openai

This page covers all LangChain integrations with [OpenAI](https://en.wikipedia.org/wiki/OpenAI)

<Columns cols={2}>
  <Card title="ChatOpenAI" href="/oss/python/integrations/chat/openai" cta="Get started" icon="message" arrow>
    OpenAI chat models.
  </Card>

<Card title="AzureChatOpenAI" href="/oss/python/integrations/chat/azure_chat_openai" cta="Get started" icon="microsoft" arrow>
    Wrapper for OpenAI chat models hosted on Azure.
  </Card>

<Card title="OpenAI" href="/oss/python/integrations/llms/openai" cta="Get started" icon="i-cursor" arrow>
    (Legacy) OpenAI text completion models.
  </Card>

<Card title="AzureOpenAI" href="/oss/python/integrations/llms/azure_openai" cta="Get started" icon="microsoft" arrow>
    Wrapper for (legacy) OpenAI text completion models hosted on Azure.
  </Card>

<Card title="OpenAIEmbeddings" href="/oss/python/integrations/text_embedding/openai" cta="Get started" icon="layer-group" arrow>
    OpenAI embedding models.
  </Card>

<Card title="AzureOpenAIEmbeddings" href="/oss/python/integrations/text_embedding/azure_openai" cta="Get started" icon="microsoft" arrow>
    Wrapper for OpenAI embedding models hosted on Azure.
  </Card>
</Columns>

## Tools and toolkits

<Columns cols={2}>
  <Card title="Dall-E Image Generator" href="/oss/python/integrations/tools/dalle_image_generator" cta="Get started" icon="image" arrow>
    Text-to-image generation using OpenAI's Dall-E models.
  </Card>
</Columns>

<Columns cols={2}>
  <Card title="ChatGPTPluginRetriever" href="/oss/python/integrations/retrievers/chatgpt-plugin" cta="Get started" icon="download" arrow>
    Retrieve real-time information; e.g., sports scores, stock prices, the latest news, etc.
  </Card>
</Columns>

<Columns cols={2}>
  <Card title="ChatGPTLoader" href="/oss/python/integrations/document_loaders/chatgpt_loader" cta="Get started" icon="file" arrow>
    Load `conversations.json` from your ChatGPT data export folder.
  </Card>
</Columns>

<Columns cols={2}>
  <Card title="Adapter" href="/oss/python/integrations/adapters/openai" cta="Get started" icon="arrows-left-right" arrow>
    Adapt LangChain models to OpenAI APIs.
  </Card>

<Card title="OpenAIModerationChain" href="https://python.langchain.com/v0.1/docs/guides/productionization/safety/moderation" cta="Get started" icon="link" arrow>
    Detect text that could be hateful, violent, etc.
  </Card>
</Columns>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/providers/openai.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Optimize a classifier

**URL:** llms-txt#optimize-a-classifier

**Contents:**
- The objective
- Getting started
- Set up automations
- Update the application
  - NEW CODE ###

Source: https://docs.langchain.com/langsmith/optimize-classifier

This tutorial walks through optimizing a classifier based on user a feedback. Classifiers are great to optimize because its generally pretty simple to collect the desired output, which makes it easy to create few shot examples based on user feedback. That is exactly what we will do in this example.

In this example, we will build a bot that classify GitHub issues based on their title. It will take in a title and classify it into one of many different classes. Then, we will start to collect user feedback and use that to shape how this classifier performs.

To get started, we will first set it up so that we send all traces to a specific project. We can do this by setting an environment variable:

We can then create our initial application. This will be a really simple function that just takes in a GitHub issue title and tries to label it.

We can then start to interact with it. When interacting with it, we will generate the LangSmith run id ahead of time and pass that into this function. We do this so we can attach feedback later on.

Here's how we can invoke the application:

Here's how we can attach feedback after. We can collect feedback in two forms.

First, we can collect "positive" feedback - this is for examples that the model got right.

Next, we can focus on collecting feedback that corresponds to a "correction" to the generation. In this example the model will classify it as a bug, whereas I really want this to be classified as documentation.

## Set up automations

We can now set up automations to move examples with feedback of some form into a dataset. We will set up two automations, one for positive feedback and the other for negative feedback.

The first will take all runs with positive feedback and automatically add them to a dataset. The logic behind this is that any run with positive feedback we can use as a good example in future iterations. Let's create a dataset called `classifier-github-issues` to add this data to.

<img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/class-optimization-neg.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=e36c57f7e0e224ff1ea29bcfbe9891fc" alt="Optimization Negative" data-og-width="1033" width="1033" data-og-height="558" height="558" data-path="langsmith/images/class-optimization-neg.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/class-optimization-neg.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=4091f7ae7d447eab035b32b66788eaee 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/class-optimization-neg.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=9971db3fd8992d31d94b58095381e575 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/class-optimization-neg.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=96f35883e764b0f207692ce1fba46d08 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/class-optimization-neg.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=f1aa5d6d49137a2f8d254ad2d327dabf 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/class-optimization-neg.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=c9d20165b113f53fe5638c66ff47eddb 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/class-optimization-neg.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=0109460c47883c4225fd54115acadb2b 2500w" />

The second will take all runs with a correction and use a webhook to add them to a dataset. When creating this webhook, we will select the option to "Use Corrections". This option will make it so that when creating a dataset from a run, rather than using the output of the run as the gold-truth output of the datapoint, it will use the correction.

<img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/class-optimization-pos.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=6485ca961ed1c29d33f25f75f90ba939" alt="Optimization Positive" data-og-width="1038" width="1038" data-og-height="506" height="506" data-path="langsmith/images/class-optimization-pos.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/class-optimization-pos.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=07ee0c534c8d8ce3e34e7c17058af5c0 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/class-optimization-pos.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=d14fc77148d349e02b363af96f0752ad 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/class-optimization-pos.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=56592428dbb8d34492bbbc2bee911500 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/class-optimization-pos.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=81a4d0d24ca94ad2adb5b4463bb3f776 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/class-optimization-pos.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=1c3086f1fa332b846e7b6f668084fada 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/class-optimization-pos.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=41ba1a118c1f220018e619edf2fecee2 2500w" />

## Update the application

We can now update our code to pull down the dataset we are sending runs to. Once we pull it down, we can create a string with the examples in it. We can then put this string as part of the prompt!

```python  theme={null}
### NEW CODE ###

**Examples:**

Example 1 (unknown):
```unknown
We can then create our initial application. This will be a really simple function that just takes in a GitHub issue title and tries to label it.
```

Example 2 (unknown):
```unknown
We can then start to interact with it. When interacting with it, we will generate the LangSmith run id ahead of time and pass that into this function. We do this so we can attach feedback later on.

Here's how we can invoke the application:
```

Example 3 (unknown):
```unknown
Here's how we can attach feedback after. We can collect feedback in two forms.

First, we can collect "positive" feedback - this is for examples that the model got right.
```

Example 4 (unknown):
```unknown
Next, we can focus on collecting feedback that corresponds to a "correction" to the generation. In this example the model will classify it as a bug, whereas I really want this to be classified as documentation.
```

---

## Optionally add the 'traceable' decorator to trace the inputs/outputs of this function.

**URL:** llms-txt#optionally-add-the-'traceable'-decorator-to-trace-the-inputs/outputs-of-this-function.

**Contents:**
- UI
  - Pre-built evaluators
- Customize your LLM-as-a-judge evaluator
  - Select/create the evaluator
  - Configure the evaluator
  - Save the evaluator

@traceable
def dummy_app(inputs: dict) -> dict:
    return {"answer": "hmm i'm not sure", "reasoning": "i didn't understand the question"}

ls_client = Client()
dataset = ls_client.create_dataset("big questions")
examples = [
    {"inputs": {"question": "how will the universe end"}},
    {"inputs": {"question": "are we alone"}},
]
ls_client.create_examples(dataset_id=dataset.id, examples=examples)

results = evaluate(
    dummy_app,
    data=dataset,
    evaluators=[valid_reasoning]
)
```

See [here](/langsmith/code-evaluator) for more on how to write a custom evaluator.

### Pre-built evaluators

Pre-built evaluators are a useful starting point when setting up evaluations. The LangSmith UI supports the following pre-built evaluators:

* **Hallucination**: Detect factually incorrect outputs. Requires a reference output.
* **Correctness**: Check semantic similarity to a reference.
* **Conciseness**: Evaluate whether an answer is a concise response to a question.
* **Code checker**: Verify correctness of code answers.

You can configure these evaluators::

* When running an evaluation using the [playground](/langsmith/observability-concepts#prompt-playground)
* As part of a dataset to [automatically run evaluations on experiments](/langsmith/bind-evaluator-to-dataset)
* When running an [online evaluation](/langsmith/online-evaluations#configure-llm-as-judge-evaluators)

## Customize your LLM-as-a-judge evaluator

Add specific instructions for your LLM-as-a-judge evalutor prompt and configure which parts of the input/output/reference output should be passed to the evaluator.

### Select/create the evaluator

* In the playground or from a dataset: Select the **+Evaluator** button
* From a tracing project: Select **Add rules**, configure your rule and select **Apply evaluator**

Select the **Create your own evaluator option**. Alternatively, you may start by selecting a pre-built evaluator and editing it.

### Configure the evaluator

Create a new prompt, or choose an existing prompt from the [prompt hub](/langsmith/prompt-engineering-quickstart).

* **Create your own prompt**: Create a custom prompt inline.

* **Pull a prompt from the prompt hub**: Use the **Select a prompt** dropdown to select from an existing prompt. You can't edit these prompts directly within the prompt editor, but you can view the prompt and the schema it uses. To make changes, edit the prompt in the playground and commit the version, and then pull in your new prompt in the evaluator.

Select the desired model from the provided options.

#### Mapping variables

Use variable mapping to indicate the variables that are passed into your evaluator prompt from your run or example. To aid with variable mapping, an example (or run) is provided for reference. Click on the the variables in your prompt and use the dropdown to map them to the relevant parts of the input, output, or reference output.

To add prompt variables type the variable with double curly brackets `{{prompt_var}}` if using mustache formatting (the default) or single curly brackets `{prompt_var}` if using f-string formatting.

You may remove variables as needed. For example if you are evaluating a metric such as conciseness, you typically don't need a reference output so you may remove that variable.

Previewing the prompt will show you of what the formatted prompt will look like using the reference run and dataset example shown on the right.

#### Improve your evaluator with few-shot examples

To better align the LLM-as-a-judge evaluator to human preferences, LangSmith allows you to collect [human corrections](/langsmith/create-few-shot-evaluators#make-corrections) on evaluator scores. With this selection enabled, corrections are then inserted automatically as few-shot examples into your prompt.

Learn [how to set up few-shot examples and make corrections](/langsmith/create-few-shot-evaluators).

#### Feedback configuration

Feedback configuration is the scoring criteria that your LLM-as-a-judge evaluator will use. Think of this as the rubric that your evaluator will grade based on. Scores will be added as [feedback](/langsmith/observability-concepts#feedback) to a run or example. Defining feedback for your evaluator:

1. **Name the feedback key**: This is the name that will appear when viewing evaluation results. Names should be unique across experiments.

2. **Add a description**: Describe what the feedback represents.

3. **Choose a feedback type**:

* **Boolean**: True/false feedback.
* **Categorical**: Select from predefined categories.
* **Continuous**: Numerical scoring within a specified range.

Behind the scenes, feedback configuration is added as [structured output](https://python.langchain.com/docs/concepts/structured_outputs/) to the LLM-as-a-judge prompt. If you're using an existing prompt from the hub, you must add an output schema to the prompt before configuring an evaluator to use it. Each top-level key in the output schema will be treated as a separate piece of feedback.

### Save the evaluator

Once your are finished configuring, save your changes.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/llm-as-judge.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Optionally wrap the OpenAI client to trace all model calls.

**URL:** llms-txt#optionally-wrap-the-openai-client-to-trace-all-model-calls.

oai_client = wrappers.wrap_openai(OpenAI())

def valid_reasoning(inputs: dict, outputs: dict) -> bool:
    """Use an LLM to judge if the reasoning and the answer are consistent."""
    instructions = """
Given the following question, answer, and reasoning, determine if the reasoning
for the answer is logically valid and consistent with the question and the answer."""

class Response(BaseModel):
        reasoning_is_valid: bool

msg = f"Question: {inputs['question']}\nAnswer: {outputs['answer']}\nReasoning: {outputs['reasoning']}"
    response = oai_client.beta.chat.completions.parse(
        model="gpt-4o",
        messages=[{"role": "system", "content": instructions,}, {"role": "user", "content": msg}],
        response_format=Response
    )
    return response.choices[0].message.parsed.reasoning_is_valid

---

## Optional. You can swap OpenAI for any other tool-calling chat model.

**URL:** llms-txt#optional.-you-can-swap-openai-for-any-other-tool-calling-chat-model.

os.environ["OPENAI_API_KEY"] = "YOUR OPENAI API KEY"

---

## Optional. You can swap Tavily for the free DuckDuckGo search tool if preferred.

**URL:** llms-txt#optional.-you-can-swap-tavily-for-the-free-duckduckgo-search-tool-if-preferred.

---

## Or if you'd like a token that can be used by any agent, set agent_scoped=False

**URL:** llms-txt#or-if-you'd-like-a-token-that-can-be-used-by-any-agent,-set-agent_scoped=false

auth_result = await client.authenticate(
    provider="{provider_id}",
    scopes=["scopeA"],
    user_id="your_user_id",
    agent_scoped=False
)
python  theme={null}
token = auth_result.token
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
During execution, if authentication is required, the SDK will throw an [interrupt](https://langchain-ai.github.io/langgraph/how-tos/human_in_the_loop/add-human-in-the-loop/#pause-using-interrupt). The agent execution pauses and presents the OAuth URL to the user:

<img src="https://mintcdn.com/langchain-5e9cc07a/Xbr8HuVd9jPi6qTU/images/langgraph-auth-interrupt.png?fit=max&auto=format&n=Xbr8HuVd9jPi6qTU&q=85&s=94f84dd7ec822ca69f9a27b4458dca9f" alt="Studio interrupt showing OAuth URL" data-og-width="1197" width="1197" data-og-height="530" height="530" data-path="images/langgraph-auth-interrupt.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Xbr8HuVd9jPi6qTU/images/langgraph-auth-interrupt.png?w=280&fit=max&auto=format&n=Xbr8HuVd9jPi6qTU&q=85&s=8e2f6ddeb7ae2b7e3f349a23ed69270a 280w, https://mintcdn.com/langchain-5e9cc07a/Xbr8HuVd9jPi6qTU/images/langgraph-auth-interrupt.png?w=560&fit=max&auto=format&n=Xbr8HuVd9jPi6qTU&q=85&s=ed5f6697e44784a6a937f6bfd3248780 560w, https://mintcdn.com/langchain-5e9cc07a/Xbr8HuVd9jPi6qTU/images/langgraph-auth-interrupt.png?w=840&fit=max&auto=format&n=Xbr8HuVd9jPi6qTU&q=85&s=bb34295ee4128adb77cdf6dd1a76d88a 840w, https://mintcdn.com/langchain-5e9cc07a/Xbr8HuVd9jPi6qTU/images/langgraph-auth-interrupt.png?w=1100&fit=max&auto=format&n=Xbr8HuVd9jPi6qTU&q=85&s=09df9030e048467ca35ab70bf73b2272 1100w, https://mintcdn.com/langchain-5e9cc07a/Xbr8HuVd9jPi6qTU/images/langgraph-auth-interrupt.png?w=1650&fit=max&auto=format&n=Xbr8HuVd9jPi6qTU&q=85&s=ebfe20351ac52045b30713007da5ba61 1650w, https://mintcdn.com/langchain-5e9cc07a/Xbr8HuVd9jPi6qTU/images/langgraph-auth-interrupt.png?w=2500&fit=max&auto=format&n=Xbr8HuVd9jPi6qTU&q=85&s=ff3b2fcebfdb6fc76e7269d8aef34077 2500w" />

After the user completes OAuth authentication and we receive the callback from the provider, they will see the auth success page.

<img src="https://mintcdn.com/langchain-5e9cc07a/Xbr8HuVd9jPi6qTU/images/github-auth-success.png?fit=max&auto=format&n=Xbr8HuVd9jPi6qTU&q=85&s=72e6492f074507bc8888804066205fcb" alt="GitHub OAuth success page" data-og-width="447" width="447" data-og-height="279" height="279" data-path="images/github-auth-success.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Xbr8HuVd9jPi6qTU/images/github-auth-success.png?w=280&fit=max&auto=format&n=Xbr8HuVd9jPi6qTU&q=85&s=031b2f9d30e4da4240059cb25fba6d15 280w, https://mintcdn.com/langchain-5e9cc07a/Xbr8HuVd9jPi6qTU/images/github-auth-success.png?w=560&fit=max&auto=format&n=Xbr8HuVd9jPi6qTU&q=85&s=eb4d01516b4691158a47a8b2632d22e3 560w, https://mintcdn.com/langchain-5e9cc07a/Xbr8HuVd9jPi6qTU/images/github-auth-success.png?w=840&fit=max&auto=format&n=Xbr8HuVd9jPi6qTU&q=85&s=e49b04f99e4c2f485769443da039bca1 840w, https://mintcdn.com/langchain-5e9cc07a/Xbr8HuVd9jPi6qTU/images/github-auth-success.png?w=1100&fit=max&auto=format&n=Xbr8HuVd9jPi6qTU&q=85&s=930aee5e270d2fcb4d6bdfb001150d81 1100w, https://mintcdn.com/langchain-5e9cc07a/Xbr8HuVd9jPi6qTU/images/github-auth-success.png?w=1650&fit=max&auto=format&n=Xbr8HuVd9jPi6qTU&q=85&s=3b5dc841251462c3ed140800564c0ad8 1650w, https://mintcdn.com/langchain-5e9cc07a/Xbr8HuVd9jPi6qTU/images/github-auth-success.png?w=2500&fit=max&auto=format&n=Xbr8HuVd9jPi6qTU&q=85&s=0e53fbc4c56b16bf1db88c98ab2e631d 2500w" />

The agent then resumes execution from the point it left off at, and the token can be used for any API calls. We store and refresh OAuth tokens so that future uses of the service by either the user or agent do not require an OAuth flow.
```

Example 2 (unknown):
```unknown
#### Outside LangGraph context

Provide the `auth_url` to the user for out-of-band OAuth flows.
```

---

## Our SQL queries will only work if we filter on the exact string values that are in the DB.

**URL:** llms-txt#our-sql-queries-will-only-work-if-we-filter-on-the-exact-string-values-that-are-in-the-db.

---

## 'outputs' will come from your target function.

**URL:** llms-txt#'outputs'-will-come-from-your-target-function.

**Contents:**
- Example: Single LLM call
- Example: Non-LLM component
- Example: Application or agent

def evaluator_one(inputs: dict, outputs: dict) -> bool:
    return outputs["foo"] == 2

def evaluator_two(inputs: dict, outputs: dict) -> bool:
    return len(outputs["bar"]) < 3

client = Client()
results = client.evaluate(
    dummy_target,  # <-- target function
    data="your-dataset-name",
    evaluators=[evaluator_one, evaluator_two],
    ...
)
python Python theme={null}
  from langsmith import wrappers
  from openai import OpenAI

# Optionally wrap the OpenAI client to automatically
  # trace all model calls.
  oai_client = wrappers.wrap_openai(OpenAI())

def target(inputs: dict) -> dict:
    # This assumes your dataset has inputs with a 'messages' key.
    # You can update to match your dataset schema.
    messages = inputs["messages"]
    response = oai_client.chat.completions.create(
        messages=messages,
        model="gpt-4o-mini",
    )
    return {"answer": response.choices[0].message.content}
  typescript TypeScript theme={null}
  import OpenAI from 'openai';
  import { wrapOpenAI } from "langsmith/wrappers";

const client = wrapOpenAI(new OpenAI());

// This is the function you will evaluate.
  const target = async(inputs) => {
    // This assumes your dataset has inputs with a `messages` key
    const messages = inputs.messages;
    const response = await client.chat.completions.create({
        messages: messages,
        model: 'gpt-4o-mini',
    });
    return { answer: response.choices[0].message.content };
  }
  python Python (LangChain) theme={null}
  from langchain.chat_models import init_chat_model

model = init_chat_model("gpt-4o-mini")

def target(inputs: dict) -> dict:
    # This assumes your dataset has inputs with a `messages` key
    messages = inputs["messages"]
    response = model.invoke(messages)
    return {"answer": response.content}
  typescript TypeScript (LangChain) theme={null}
  import { ChatOpenAI } from '@langchain/openai';

// This is the function you will evaluate.
  const target = async(inputs) => {
    // This assumes your dataset has inputs with a `messages` key
    const messages = inputs.messages;
    const model = new ChatOpenAI({ model: "gpt-4o-mini" });
    const response = await model.invoke(messages);
    return {"answer": response.content};
  }
  python Python theme={null}
  from langsmith import traceable

# Optionally decorate with '@traceable' to trace all invocations of this function.
  @traceable
  def calculator_tool(operation: str, number1: float, number2: float) -> str:
    if operation == "add":
        return str(number1 + number2)
    elif operation == "subtract":
        return str(number1 - number2)
    elif operation == "multiply":
        return str(number1 * number2)
    elif operation == "divide":
        return str(number1 / number2)
    else:
        raise ValueError(f"Unrecognized operation: {operation}.")

# This is the function you will evaluate.
  def target(inputs: dict) -> dict:
    # This assumes your dataset has inputs with `operation`, `num1`, and `num2` keys.
    operation = inputs["operation"]
    number1 = inputs["num1"]
    number2 = inputs["num2"]
    result = calculator_tool(operation, number1, number2)
    return {"result": result}
  typescript TypeScript theme={null}
  import { traceable } from "langsmith/traceable";

// Optionally wrap in 'traceable' to trace all invocations of this function.
  const calculatorTool = traceable(async ({ operation, number1, number2 }) => {
  // Functions must return strings
  if (operation === "add") {
    return (number1 + number2).toString();
  } else if (operation === "subtract") {
    return (number1 - number2).toString();
  } else if (operation === "multiply") {
    return (number1 * number2).toString();
  } else if (operation === "divide") {
    return (number1 / number2).toString();
  } else {
    throw new Error("Invalid operation.");
  }
  });

// This is the function you will evaluate.
  const target = async (inputs) => {
  // This assumes your dataset has inputs with `operation`, `num1`, and `num2` keys
  const result = await calculatorTool.invoke({
    operation: inputs.operation,
    number1: inputs.num1,
    number2: inputs.num2,
  });
  return { result };
  }
  python Python theme={null}
  from my_agent import agent

# This is the function you will evaluate.
  def target(inputs: dict) -> dict:
    # This assumes your dataset has inputs with a `messages` key
    messages = inputs["messages"]
    # Replace `invoke` with whatever you use to call your agent
    response = agent.invoke({"messages": messages})
    # This assumes your agent output is in the right format
    return response
  typescript TypeScript theme={null}
  import { agent } from 'my_agent';

// This is the function you will evaluate.
  const target = async(inputs) => {
  // This assumes your dataset has inputs with a `messages` key
  const messages = inputs.messages;
  // Replace `invoke` with whatever you use to call your agent
  const response = await agent.invoke({ messages });
  // This assumes your agent output is in the right format
  return response;
  }
  python  theme={null}
  from my_agent import agent
  from langsmith import Client
  client = Client()
  client.evaluate(agent, ...)
  ```
</Check>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/define-target-function.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
<Check>
  `evaluate()` will automatically trace your target function. This means that if you run any traceable code within your target function, this will also be traced as child runs of the target trace.
</Check>

## Example: Single LLM call

<CodeGroup>
```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown

```

---

## Output from node_1 contains private data that is not part of the overall state

**URL:** llms-txt#output-from-node_1-contains-private-data-that-is-not-part-of-the-overall-state

class Node1Output(TypedDict):
    private_data: str

---

## Overview

**URL:** llms-txt#overview

**Contents:**
- Resource Hierarchy
  - Organizations
  - Workspaces
  - Resource tags
- User Management and RBAC
  - Users
  - API keys
  - Organization roles
  - Workspace roles (RBAC)
- Best Practices

Source: https://docs.langchain.com/langsmith/administration-overview

This overview covers topics related to managing users, organizations, and workspaces within LangSmith.

## Resource Hierarchy

An organization is a logical grouping of users within LangSmith with its own billing configuration. Typically, there is one organization per company. An organization can have multiple workspaces. For more details, see the [setup guide](/langsmith/set-up-a-workspace#set-up-an-organization).

When you log in for the first time, a personal organization will be created for you automatically. If you'd like to collaborate with others, you can create a separate organization and invite your team members to join. There are a few important differences between your personal organization and shared organizations:

| Feature             | Personal            | Shared                                                                                       |
| ------------------- | ------------------- | -------------------------------------------------------------------------------------------- |
| Maximum workspaces  | 1                   | Variable, depending on plan (see [pricing page](https://www.langchain.com/pricing-langsmith) |
| Collaboration       | Cannot invite users | Can invite users                                                                             |
| Billing: paid plans | Developer plan only | All other plans available                                                                    |

<Info>
  Workspaces were formerly called Tenants. Some code and APIs may still reference the old name for a period of time during the transition.
</Info>

A workspace is a logical grouping of users and resources within an organization. A workspace separates trust boundaries for resources and access control. Users may have permissions in a workspace that grant them access to the resources in that workspace, including tracing projects, datasets, annotation queues, and prompts. For more details, see the [setup guide](/langsmith/set-up-a-workspace).

It is recommended to create a separate workspace for each team within your organization. To organize resources even further, you can use [Resource Tags](#resource-tags) to group resources within a workspace.

The following image shows a sample workspace settings page: <img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/sample-workspace.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=a2af70dec53a98f502132de92445aed2" alt="Sample Workspace" data-og-width="3008" width="3008" data-og-height="956" height="956" data-path="langsmith/images/sample-workspace.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/sample-workspace.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=439802e412d8f96c45a4af9829659f5a 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/sample-workspace.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=0ce5170a93ee736faf115533f46467f2 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/sample-workspace.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=e3d47902f66ad077ae9a893be183c348 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/sample-workspace.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=b5af7b353e10c44b844f60a677740eb7 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/sample-workspace.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=6acccca51bf1962f2abcd82ab3dca96e 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/sample-workspace.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=213d4b9372f31ca2babcbe02e69a0e3e 2500w" />

The following diagram explains the relationship between organizations, workspaces, and the different resources scoped to and within a workspace: <img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/resource-hierarchy.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=bc51967d88ce39120547d174ab6a442e" alt="Resource Hierarchy" data-og-width="1403" width="1403" data-og-height="272" height="272" data-path="langsmith/images/resource-hierarchy.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/resource-hierarchy.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=a679974443061378a17aef96f8118bc2 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/resource-hierarchy.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=15fa6c782376f97d79a2c2f87a13d5a7 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/resource-hierarchy.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=f6ff24a91b5c00e17c44c07bd5868a06 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/resource-hierarchy.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=7eaab6991daaca255d41da41fad268a9 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/resource-hierarchy.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=fd2c292577912b0cf04c6d14ba636fe7 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/resource-hierarchy.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=da5879f0ae4b12f5e8b64d447ddee8fe 2500w" />

See the table below for details on which features are available in which scope (organization or workspace):

| Resource/Setting                                                            | Scope            |
| --------------------------------------------------------------------------- | ---------------- |
| Trace Projects                                                              | Workspace        |
| Annotation Queues                                                           | Workspace        |
| Deployments                                                                 | Workspace        |
| Datasets & Experiments                                                      | Workspace        |
| Prompts                                                                     | Workspace        |
| Resource Tags                                                               | Workspace        |
| API Keys                                                                    | Workspace        |
| Settings including Secrets, Feedback config, Models, Rules, and Shared URLs | Workspace        |
| User management: Invite User to Workspace                                   | Workspace        |
| RBAC: Assigning Workspace Roles                                             | Workspace        |
| Data Retention, Usage Limits                                                | Workspace\*      |
| Plans and Billing, Credits, Invoices                                        | Organization     |
| User management: Invite User to Organization                                | Organization\*\* |
| Adding Workspaces                                                           | Organization     |
| Assigning Organization Roles                                                | Organization     |
| RBAC: Creating/Editing/Deleting Custom Roles                                | Organization     |

\* Data retention settings and usage limits will be available soon for the organization level as well \*\* Self-hosted installations may enable workspace-level invites of users to the organization via a feature flag. See the [self-hosted user management docs](/langsmith/self-host-user-management) for details.

Resource tags allow you to organize resources within a workspace. Each tag is a key-value pair that can be assigned to a resource. Tags can be used to filter workspace-scoped resources in the UI and API: Projects, Datasets, Annotation Queues, Deployments, and Experiments.

Each new workspace comes with two default tag keys: `Application` and `Environment`; as the names suggest, these tags can be used to categorize resources based on the application and environment they belong to. More tags can be added as needed.

LangSmith resource tags are very similar to tags in cloud services like [AWS](https://docs.aws.amazon.com/tag-editor/latest/userguide/tagging.html).

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/resource-tags.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=99d640e56b981397219e3ad1a5d3f3b2" alt="Sample Resource Tags" data-og-width="1152" width="1152" data-og-height="233" height="233" data-path="langsmith/images/resource-tags.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/resource-tags.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=d819b9e316cd0ac5cd065f51999cbfe5 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/resource-tags.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=d2389e8878ee353d730f13ad63f59482 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/resource-tags.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=bbdafff1f16e1036d0f243e8aa75481d 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/resource-tags.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=5f02b3dcfd6c6c842ad8899cd19d43ee 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/resource-tags.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=5d362a1f024b376c19c0dc03d06406ac 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/resource-tags.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=0f5fc4688726e872dbb53a1d3d4f0b00 2500w" />

## User Management and RBAC

A user is a person who has access to LangSmith. Users can be members of one or more organizations and workspaces within those organizations.

Organization members are managed in organization settings:

<img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/org-members-settings.png?fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=93249dc8c55c6d8001b350e497b2a0ac" alt="Sample Organization Members" data-og-width="3020" width="3020" data-og-height="1246" height="1246" data-path="langsmith/images/org-members-settings.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/org-members-settings.png?w=280&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=d2c0bbf572906894c972ea1047ff1c3a 280w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/org-members-settings.png?w=560&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=fe38d538de5f0436f05fa1fa0679f612 560w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/org-members-settings.png?w=840&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=cf5e551ff7c68793da8a3f5ecdef7e49 840w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/org-members-settings.png?w=1100&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=a401368eebe3fc19a5bec603ad9bb9c8 1100w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/org-members-settings.png?w=1650&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=be6d182043d052dece018f54bd546916 1650w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/org-members-settings.png?w=2500&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=450b385d226c53c1f327612e44abc5d1 2500w" />

And workspace members are managed in workspace settings:

<img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/org-settings-workspaces-tab.png?fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=678c90095209d5e1440e452297478dc2" alt="Sample Workspace Members" data-og-width="3016" width="3016" data-og-height="1248" height="1248" data-path="langsmith/images/org-settings-workspaces-tab.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/org-settings-workspaces-tab.png?w=280&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=a1a6827b0fc3fdc068b98a02bbb58a05 280w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/org-settings-workspaces-tab.png?w=560&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=79c438ec6b20a407d607a56b66dfa431 560w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/org-settings-workspaces-tab.png?w=840&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=1c622e1fcd3afdff3f001fe4f642a9cf 840w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/org-settings-workspaces-tab.png?w=1100&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=bda1583680e7f19c2cc65a72a5158fae 1100w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/org-settings-workspaces-tab.png?w=1650&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=c80bcd52cb503a6652978dad756123f0 1650w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/org-settings-workspaces-tab.png?w=2500&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=d8350437e643a858e138674d38ed8751 2500w" />

<Warning>
  We ended support for legacy API keys prefixed with `ls__` on October 22, 2024 in favor of personal access tokens (PATs) and service keys. We require using PATs and service keys for all new integrations. API keys prefixed with `ls__` will no longer work as of October 22, 2024.
</Warning>

#### Expiration Dates

When you create an API key, you have the option to set an expiration date. Adding an expiration date to keys enhances security and minimizes the risk of unauthorized access. For example, you may set expiration dates on keys for temporary tasks that require elevated access.

By default, keys never expire. Once expired, an API key is no longer valid and cannot be reactivated or have its expiration modified.

#### Personal Access Tokens (PATs)

Personal Access Tokens (PATs) are used to authenticate requests to the LangSmith API. They are created by users and scoped to a user. The PAT will have the same permissions as the user that created it. We recommend not using these to authenticate requests from your application, but rather using them for personal scripts or tools that interact with the LangSmith API. If the user associated with the PAT is removed from the organization, the PAT will no longer work.

PATs are prefixed with `lsv2_pt_`

Service keys are similar to PATs, but are used to authenticate requests to the LangSmith API on behalf of a service account. Only admins can create service keys. We recommend using these for applications / services that need to interact with the LangSmith API, such as LangGraph agents or other integrations. Service keys may be scoped to a single workspace, multiple workspaces, or the entire organization, and can be used to authenticate requests to the LangSmith API for whichever workspace(s) it has access to.

Service keys are prefixed with `lsv2_sk_`

<Warning>
  Use the `X-Tenant-Id` header to specify the target workspace.

* **When using PATs**: If this header is omitted, requests will run against the default workspace associated with the key.
  * **When using organization-scoped service keys**: You must include the `X-Tenant-Id` header when accessing workspace-scoped resources. Without it, the request will fail with a `403 Forbidden` error.
</Warning>

<Note>
  To see how to create a service key or Personal Access Token, see the [setup guide](/langsmith/create-account-api-key)
</Note>

### Organization roles

Organization roles are distinct from the Enterprise feature (RBAC) below and are used in the context of multiple [workspaces](#workspaces). Your organization role determines your workspace membership characteristics and your organization-level permissions. See the [organization setup guide](/langsmith/set-up-a-workspace#organization-roles) for more information.

The organization role selected also impacts workspace membership as described here:

* `Organization Admin` grants full access to manage all organization configuration, users, billing, and workspaces. **An `Organization Admin` has `Admin` access to all workspaces in an organization**
* `Organization User` may read organization information but cannot execute any write actions at the organization level. An `Organization User` may create Personal Access Tokens. **An `Organization User` can be added to a subset of workspaces and assigned workspace roles as usual (if RBAC is enabled), which specify permissions at the workspace level.**
* `Organization Viewer` is equivalent to `Organization User`, but **cannot** create Personal Access Tokens. (for self-hosted, available in Helm chart version 0.11.25+)

<Info>
  The `Organization User` and `Organization Viewer` roles are only available in organizations on plans with multiple workspaces. In organizations limited to a single workspace, all users are `Organization Admins`. Custom organization-scoped roles are not available yet.

See [security settings](/langsmith/manage-organization-by-api#security-settings) for instructions on how to disable PAT creation for the entire organization.
</Info>

See the table below for all organization permissions:

|                                             | Organization Viewer | Organization User | Organization Admin |
| ------------------------------------------- | ------------------- | ----------------- | ------------------ |
| View organization configuration             | ✅                   | ✅                 | ✅                  |
| View organization roles                     | ✅                   | ✅                 | ✅                  |
| View organization members                   | ✅                   | ✅                 | ✅                  |
| View data retention settings                | ✅                   | ✅                 | ✅                  |
| View usage limits                           | ✅                   | ✅                 | ✅                  |
| Create personal access tokens (PATs)        | ❌                   | ✅                 | ✅                  |
| Admin access to all workspaces              | ❌                   | ❌                 | ✅                  |
| Manage billing settings                     | ❌                   | ❌                 | ✅                  |
| Create workspaces                           | ❌                   | ❌                 | ✅                  |
| Create, edit, and delete organization roles | ❌                   | ❌                 | ✅                  |
| Invite new users to organization            | ❌                   | ❌                 | ✅                  |
| Delete user invites                         | ❌                   | ❌                 | ✅                  |
| Remove users from an organization           | ❌                   | ❌                 | ✅                  |
| Update data retention settings              | ❌                   | ❌                 | ✅                  |
| Update usage limits                         | ❌                   | ❌                 | ✅                  |

### Workspace roles (RBAC)

<Note>
  RBAC (Role-Based Access Control) is a feature that is only available to Enterprise customers. If you are interested in this feature, [contact our sales team](https://www.langchain.com/contact-sales). Other plans default to using the Admin role for all users.
</Note>

Roles are used to define the set of permissions that a user has within a workspace. There are three built-in system roles that cannot be edited:

* `Admin` - has full access to all resources within the workspace
* `Viewer` - has read-only access to all resources within the workspace
* `Editor` - has full permissions except for workspace management (adding/removing users, changing roles, configuring service keys)

Organization admins can also create/edit custom roles with specific permissions for different resources.

Roles can be managed in organization settings under the `Roles` tab:

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/roles-tab-rbac.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=8c9f1e2b8e18b6f40b59ecc3c18d0e5f" alt="Roles" data-og-width="3018" width="3018" data-og-height="1546" height="1546" data-path="langsmith/images/roles-tab-rbac.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/roles-tab-rbac.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=94ab2c109a053547c6bbe9ac0364d870 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/roles-tab-rbac.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=fd6bc62163b03e32aca8fd40d831111f 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/roles-tab-rbac.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=f85073d5302051fd6febed8762615018 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/roles-tab-rbac.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=5c1f79814e3b54e747cc33d7096364e5 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/roles-tab-rbac.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=78bfc784d69847b1c08d40671e30d64e 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/roles-tab-rbac.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=a4651067818a7de28a672307d4ecdbee 2500w" />

For more details on assigning and creating roles, see the [access control setup guide](/langsmith/user-management).

### Environment Separation

Use [resource tags](#resource-tags) to organize resources by environment using the default tag key `Environment` and different values for the environment (e.g., `dev`, `staging`, `prod`). We do not recommend using separate workspaces for environment separation because resources cannot be shared across workspaces, which would prevent you from promoting resources (like prompts) between environments.

<Note>
  **Resource tags vs. commit tags for prompt management**

While both types of tags can use environment terminology like `dev`, `staging`, and `prod`, they serve different purposes:

* **Resource tags** (`Environment: prod`): Use these to *organize and filter* resources across your workspace. Apply resource tags to tracing projects, datasets, and other resources (including prompts) to group them by environment, which enables filtering in the UI.
  * [Commit tags](/langsmith/manage-prompts#commit-tags) (`prod` tag): Use these to manage which [prompt version](/langsmith/prompt-engineering) your code references. Commit tags are labels that point to specific commits in a prompt's history. When your code pulls a prompt by tag name (e.g., `client.pull_prompt("prompt-name:prod")`), it retrieves whichever commit that tag currently points to. To promote a prompt from `staging` to `prod`, move the commit tag to point to the desired version.

Resource tags organize **which resources** belong to an environment. Commit tags let you control **which version** of a prompt your code references without changing the code itself.
</Note>

This section covers how data retention works and how it's priced in LangSmith.

#### Why retention matters

* **Privacy**: Many data privacy regulations, such as GDPR in Europe or CCPA in California, require organizations to delete personal data once it's no longer necessary for the purposes for which it was collected. Setting retention periods aids in compliance with such regulations.
* **Cost**: LangSmith charges less for traces that have low data retention. See our tutorial on how to [optimize spend](/langsmith/billing#optimize-your-tracing-spend) for details.

LangSmith has two tiers of traces based on Data Retention with the following characteristics:

|                      | Base              | Extended        |
| -------------------- | ----------------- | --------------- |
| **Price**            | \$.50 / 1k traces | \$5 / 1k traces |
| **Retention Period** | 14 days           | 400 days        |

**Data deletion after retention ends**

After the specified retention period, traces are no longer accessible in the tracing project UI or via the API. All user data associated with the trace (e.g. inputs and outputs) is deleted from our internal systems within a day thereafter. Some metadata associated with each trace may be retained indefinitely for analytics and billing purposes.

**Data retention auto-upgrades**

<Warning>
  Auto upgrades can have an impact on your bill. Please read this section carefully to fully understand your estimated LangSmith tracing costs.
</Warning>

When you use certain features with `base` tier traces, their data retention will be automatically upgraded to `extended` tier. This will increase both the retention period, and the cost of the trace.

The complete list of scenarios in which a trace will upgrade when:

* **Feedback** is added to any run on the trace (or any trace in the thread), whether through [manual annotation](/langsmith/annotate-traces-inline#annotate-traces-and-runs-inline), automatically with [an online evaluator](/langsmith/online-evaluations), or programmatically [via the SDK](/langsmith/attach-user-feedback#log-user-feedback-using-the-sdk).
* An **[annotation queue](/langsmith/annotation-queues#assign-runs-to-an-annotation-queue)** receives any run from the trace.
* An **[automation rule](/langsmith/rules#set-up-automation-rules)** matches any run within a trace.

**Why auto-upgrade traces?**

We have two reasons behind the auto-upgrade model for tracing:

1. We think that traces that match any of these conditions are fundamentally more interesting than other traces, and therefore it is good for users to be able to keep them around longer.
2. We philosophically want to charge customers an order of magnitude lower for traces that may not be interacted with meaningfully. We think auto-upgrades align our pricing model with the value that LangSmith brings, where only traces with meaningful interaction are charged at a higher rate.

If you have questions or concerns about our pricing model, please feel free to reach out to [support@langchain.dev](mailto:support@langchain.dev) and let us know your thoughts!

**How does data retention affect downstream features?**

* **Annotation Queues, Run Rules, and Feedback**: Traces that use these features will be [auto-upgraded](#data-retention-auto-upgrades).
* **Monitoring**: The monitoring tab will continue to work even after a base tier trace's data retention period ends. It is powered by trace metadata that exists for >30 days, meaning that your monitoring graphs will continue to stay accurate even on `base` tier traces.
* **Datasets**: Datasets have an indefinite data retention period. Restated differently, if you add a trace's inputs and outputs to a dataset, they will never be deleted. We suggest that if you are using LangSmith for data collection, you take advantage of the datasets feature.

On your LangSmith invoice, you will see two metrics that we charge for:

* LangSmith Traces (Base Charge)
* LangSmith Traces (Extended Data Retention Upgrades).

The first metric includes all traces, regardless of tier. The second metric just counts the number of extended retention traces.

**Why measure all traces + upgrades instead of base and extended traces?**

A natural question to ask when considering our pricing is why not just show the number of `base` tier and `extended` tier traces directly on the invoice?

While we understand this would be more straightforward, it doesn't fit trace upgrades properly. Consider a `base` tier trace that was recorded on June 30, and upgraded to `extended` tier on July 3. The `base` tier trace occurred in the June billing period, but the upgrade occurred in the July billing period. Therefore, we need to be able to measure these two events independently to properly bill our customers.

If your trace was recorded as an extended retention trace, then the `base` and `extended` metrics will both be recorded with the same timestamp.

The Base Charge for a trace is .05¢ per trace. We priced the upgrade such that an `extended` retention trace costs 10x the price of a base tier trace (.50¢ per trace) including both metrics. Thus, each upgrade costs .45¢.

LangSmith has rate limits which are designed to ensure the stability of the service for all users.

To ensure access and stability, LangSmith will respond with HTTP Status Code 429 indicating that rate or usage limits have been exceeded under the following circumstances:

#### Temporary throughput limit over a 1 minute period at our application load balancer

This 429 is the the result of exceeding a fixed number of API calls over a 1 minute window on a per API key/access token basis. The start of the window will vary slightly — it is not guaranteed to start at the start of a clock minute — and may change depending on application deployment events.

After the max events are received we will respond with a 429 until 60 seconds from the start of the evaluation window has been reached and then the process repeats.

This 429 is thrown by our application load balancer and is a mechanism in place for all LangSmith users independent of plan tier to ensure continuity of service for all users.

| Method            | Endpoints     | Limit | Window   |
| ----------------- | ------------- | ----- | -------- |
| `DELETE`          | `/sessions*`  | 30    | 1 minute |
| `POST` OR `PATCH` | `/runs*`      | 5000  | 1 minute |
| `GET`             | `/runs/:id`   | 30    | 1 minute |
| `POST`            | `/feedbacks*` | 5000  | 1 minute |
| `*`               | `*`           | 2000  | 1 minute |

<Note>
  The LangSmith SDK takes steps to minimize the likelihood of reaching these limits on run-related endpoints by batching up to 100 runs from a single session ID into a single API call.
</Note>

#### Plan-level hourly trace event limit

This 429 is the result of reaching your maximum hourly events ingested and is evaluated in a fixed window starting at the beginning of each clock hour in UTC and resets at the top of each new hour.

An event in this context is the creation or update of a run. So if run is created, then subsequently updated in the same hourly window, that will count as 2 events against this limit.

This is thrown by our application and varies by plan tier, with organizations on our Startup/Plus and Enterprise plan tiers having higher hourly limits than our Free and Developer Plan Tiers which are designed for personal use.

| Plan                             | Limit          | Window |
| -------------------------------- | -------------- | ------ |
| Developer (no payment on file)   | 50,000 events  | 1 hour |
| Developer (with payment on file) | 250,000 events | 1 hour |
| Startup/Plus                     | 500,000 events | 1 hour |
| Enterprise                       | Custom         | Custom |

#### Plan-level hourly trace data ingest limit

This 429 is the result of reaching the maximum amount of data ingested across your trace inputs, outputs, and metadata and is evaluated in a fixed window starting at the beginning of each clock hour in UTC and resets at the top of each new hour.

Typically, inputs, outputs, and metadata are send on both run creation and update events. So if a run is created and is 2.0MB in size at creation, and 3.0MB in size when updated in the same hourly window, that will count as 5.0MB of storage against this limit.

This is thrown by our application and varies by plan tier, with organizations on our Startup/Plus and Enterprise plan tiers having higher hourly limits than our Free and Developer Plan Tiers which are designed for personal use.

| Plan                             | Limit  | Window |
| -------------------------------- | ------ | ------ |
| Developer (no payment on file)   | 500MB  | 1 hour |
| Developer (with payment on file) | 2.5GB  | 1 hour |
| Startup/Plus                     | 5.0GB  | 1 hour |
| Enterprise                       | Custom | Custom |

#### Plan-level monthly unique traces limit

This 429 is the result of reaching your maximum monthly traces ingested and is evaluated in a fixed window starting at the beginning of each calendar month in UTC and resets at the beginning of each new month.

This is thrown by our application and applies only to the Developer Plan Tier when there is no payment method on file.

| Plan                           | Limit        | Window  |
| ------------------------------ | ------------ | ------- |
| Developer (no payment on file) | 5,000 traces | 1 month |

#### Self-configured monthly usage limits

This 429 is the result of reaching your usage limit as configured by your organization admin and is evaluated in a fixed window starting at the beginning of each calendar month in UTC and resets at the beginning of each new month.

This is thrown by our application and varies by organization based on their configured settings.

#### Handling 429s responses in your application

Since some 429 responses are temporary and may succeed on a successive call, if you are directly calling the LangSmith API in your application we recommend implementing retry logic with exponential backoff and jitter.

For convenience, LangChain applications built with the LangSmith SDK has this capability built-in.

<Note>
  It is important to note that if you are saturating the endpoints for extended periods of time, retries may not be effective as your application will eventually run large enough backlogs to exhaust all retries.

If that is the case, we would like to discuss your needs more specifically. Please reach out to [LangSmith Support](mailto:support@langchain.dev) with details about your applications throughput needs and sample code and we can work with you to better understand whether the best approach is fixing a bug, changes to your application code, or a different LangSmith plan.
</Note>

LangSmith lets you configure usage limits on tracing. Note that these are *usage* limits, not *spend* limits, which mean they let you limit the quantity of occurrences of some event rather than the total amount you will spend.

LangSmith lets you set two different monthly limits, mirroring our Billable Metrics discussed in the aforementioned data retention guide:

* All traces limit
* Extended data retention traces limit

These let you limit the number of total traces, and extended data retention traces respectively.

#### Properties of usage limiting

Usage limiting is approximate, meaning that we do not guarantee the exactness of the limit. In rare cases, there may be a small period of time where additional traces are processed above the limit threshold before usage limiting begins to apply.

#### Side effects of extended data retention traces limit

The extended data retention traces limit has side effects. If the limit is already reached, any feature that could cause an auto-upgrade of tracing tiers becomes inaccessible. This is because an auto-upgrade of a trace would cause another extended retention trace to be created, which in turn should not be allowed by the limit. Therefore, you can no longer:

1. match run rules
2. add feedback to traces
3. add runs to annotation queues

Each of these features may cause an auto upgrade, so we shut them off when the limit is reached.

#### Updating usage limits

Usage limits can be updated from the `Settings` page under `Usage and Billing`. Limit values are cached, so it may take a minute or two before the new limits apply.

* Tutorial on how to [optimize spend](/langsmith/billing#optimize-your-tracing-spend)

## Additional Resources

* **[Release Versions](/langsmith/release-versions)**: Learn about LangSmith's version support policy, including Active, Critical, End of Life, and Deprecated support levels.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/administration-overview.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## over the generic handler for all actions on the "threads" resource

**URL:** llms-txt#over-the-generic-handler-for-all-actions-on-the-"threads"-resource

@auth.on.threads
async def on_thread(
    ctx: Auth.types.AuthContext,
    value: Auth.types.threads.create.value
):
    # Setting metadata on the thread being created
    # will ensure that the resource contains an "owner" field
    # Then any time a user tries to access this thread or runs within the thread,
    # we can filter by owner
    metadata = value.setdefault("metadata", {})
    metadata["owner"] = ctx.user.identity
    return {"owner": ctx.user.identity}

---

## Parent graph

**URL:** llms-txt#parent-graph

**Contents:**
- View subgraph state
- Stream subgraph outputs

builder = StateGraph(State)
builder.add_node("node_1", subgraph)
builder.add_edge(START, "node_1")

checkpointer = MemorySaver()
graph = builder.compile(checkpointer=checkpointer)
python  theme={null}
subgraph_builder = StateGraph(...)
subgraph = subgraph_builder.compile(checkpointer=True)
python  theme={null}
  from langgraph.graph import START, StateGraph
  from langgraph.checkpoint.memory import MemorySaver
  from langgraph.types import interrupt, Command
  from typing_extensions import TypedDict

class State(TypedDict):
      foo: str

def subgraph_node_1(state: State):
      value = interrupt("Provide value:")
      return {"foo": state["foo"] + value}

subgraph_builder = StateGraph(State)
  subgraph_builder.add_node(subgraph_node_1)
  subgraph_builder.add_edge(START, "subgraph_node_1")

subgraph = subgraph_builder.compile()

builder = StateGraph(State)
  builder.add_node("node_1", subgraph)
  builder.add_edge(START, "node_1")

checkpointer = MemorySaver()
  graph = builder.compile(checkpointer=checkpointer)

config = {"configurable": {"thread_id": "1"}}

graph.invoke({"foo": ""}, config)
  parent_state = graph.get_state(config)

# This will be available only when the subgraph is interrupted.
  # Once you resume the graph, you won't be able to access the subgraph state.
  subgraph_state = graph.get_state(config, subgraphs=True).tasks[0].state

# resume the subgraph
  graph.invoke(Command(resume="bar"), config)
  python  theme={null}
for chunk in graph.stream(
    {"foo": "foo"},
    subgraphs=True, # [!code highlight]
    stream_mode="updates",
):
    print(chunk)
python  theme={null}
  from typing_extensions import TypedDict
  from langgraph.graph.state import StateGraph, START

# Define subgraph
  class SubgraphState(TypedDict):
      foo: str
      bar: str

def subgraph_node_1(state: SubgraphState):
      return {"bar": "bar"}

def subgraph_node_2(state: SubgraphState):
      # note that this node is using a state key ('bar') that is only available in the subgraph
      # and is sending update on the shared state key ('foo')
      return {"foo": state["foo"] + state["bar"]}

subgraph_builder = StateGraph(SubgraphState)
  subgraph_builder.add_node(subgraph_node_1)
  subgraph_builder.add_node(subgraph_node_2)
  subgraph_builder.add_edge(START, "subgraph_node_1")
  subgraph_builder.add_edge("subgraph_node_1", "subgraph_node_2")
  subgraph = subgraph_builder.compile()

# Define parent graph
  class ParentState(TypedDict):
      foo: str

def node_1(state: ParentState):
      return {"foo": "hi! " + state["foo"]}

builder = StateGraph(ParentState)
  builder.add_node("node_1", node_1)
  builder.add_node("node_2", subgraph)
  builder.add_edge(START, "node_1")
  builder.add_edge("node_1", "node_2")
  graph = builder.compile()

for chunk in graph.stream(
      {"foo": "foo"},
      stream_mode="updates",
      subgraphs=True, # [!code highlight]
  ):
      print(chunk)
  
  ((), {'node_1': {'foo': 'hi! foo'}})
  (('node_2:e58e5673-a661-ebb0-70d4-e298a7fc28b7',), {'subgraph_node_1': {'bar': 'bar'}})
  (('node_2:e58e5673-a661-ebb0-70d4-e298a7fc28b7',), {'subgraph_node_2': {'foo': 'hi! foobar'}})
  ((), {'node_2': {'foo': 'hi! foobar'}})
  ```
</Accordion>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/use-subgraphs.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
If you want the subgraph to **have its own memory**, you can compile it with the appropriate checkpointer option. This is useful in [multi-agent](/oss/python/langchain/multi-agent) systems, if you want agents to keep track of their internal message histories:
```

Example 2 (unknown):
```unknown
## View subgraph state

When you enable [persistence](/oss/python/langgraph/persistence), you can [inspect the graph state](/oss/python/langgraph/persistence#checkpoints) (checkpoint) via the appropriate method. To view the subgraph state, you can use the subgraphs option.

You can inspect the graph state via `graph.get_state(config)`. To view the subgraph state, you can use `graph.get_state(config, subgraphs=True)`.

<Warning>
  **Available **only** when interrupted**
  Subgraph state can only be viewed **when the subgraph is interrupted**. Once you resume the graph, you won't be able to access the subgraph state.
</Warning>

<Accordion title="View interrupted subgraph state">
```

Example 3 (unknown):
```unknown
1. This will be available only when the subgraph is interrupted. Once you resume the graph, you won't be able to access the subgraph state.
</Accordion>

## Stream subgraph outputs

To include outputs from subgraphs in the streamed outputs, you can set the subgraphs option in the stream method of the parent graph. This will stream outputs from both the parent graph and any subgraphs.
```

Example 4 (unknown):
```unknown
<Accordion title="Stream from subgraphs">
```

---

## pass the thread ID to persist agent outputs for future interactions

**URL:** llms-txt#pass-the-thread-id-to-persist-agent-outputs-for-future-interactions

**Contents:**
- 4. Prepare for deployment
- 5. Deploy to LangSmith

config = {"configurable": {"thread_id": "1"}}

for chunk in graph.stream(
    {
        "messages": [
            {
                "role": "user",
                "content": "Find numbers between 10 and 30 in fibonacci sequence",
            }
        ]
    },
    config,
):
    print(chunk)

user_proxy (to assistant):

Find numbers between 10 and 30 in fibonacci sequence

--------------------------------------------------------------------------------
assistant (to user_proxy):

To find numbers between 10 and 30 in the Fibonacci sequence, we can generate the Fibonacci sequence and check which numbers fall within this range. Here's a plan:

1. Generate Fibonacci numbers starting from 0.
2. Continue generating until the numbers exceed 30.
3. Collect and print the numbers that are between 10 and 30.

...
python {highlight={10}} theme={null}
for chunk in graph.stream(
    {
        "messages": [
            {
                "role": "user",
                "content": "Multiply the last number by 3",
            }
        ]
    },
    config,
):
    print(chunk)

user_proxy (to assistant):

Multiply the last number by 3
Context:
Find numbers between 10 and 30 in fibonacci sequence
The Fibonacci numbers between 10 and 30 are 13 and 21.

These numbers are part of the Fibonacci sequence, which is generated by adding the two preceding numbers to get the next number, starting from 0 and 1.

The sequence goes: 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, ...

As you can see, 13 and 21 are the only numbers in this sequence that fall between 10 and 30.

--------------------------------------------------------------------------------
assistant (to user_proxy):

The last number in the Fibonacci sequence between 10 and 30 is 21. Multiplying 21 by 3 gives:

--------------------------------------------------------------------------------
{'call_autogen_agent': {'messages': {'role': 'assistant', 'content': 'The last number in the Fibonacci sequence between 10 and 30 is 21. Multiplying 21 by 3 gives:\n\n21 * 3 = 63\n\nTERMINATE'}}}

my-autogen-agent/
├── agent.py          # Your main agent code
├── requirements.txt  # Python dependencies
└── langgraph.json   # LangGraph configuration
python  theme={null}
    import os
    import autogen
    from langchain_core.messages import convert_to_openai_messages
    from langgraph.graph import StateGraph, MessagesState, START
    from langgraph.checkpoint.memory import MemorySaver

# AutoGen configuration
    config_list = [{"model": "gpt-4o", "api_key": os.environ["OPENAI_API_KEY"]}]

llm_config = {
        "timeout": 600,
        "cache_seed": 42,
        "config_list": config_list,
        "temperature": 0,
    }

# Create AutoGen agents
    autogen_agent = autogen.AssistantAgent(
        name="assistant",
        llm_config=llm_config,
    )

user_proxy = autogen.UserProxyAgent(
        name="user_proxy",
        human_input_mode="NEVER",
        max_consecutive_auto_reply=10,
        is_termination_msg=lambda x: x.get("content", "").rstrip().endswith("TERMINATE"),
        code_execution_config={
            "work_dir": "/tmp/autogen_work",
            "use_docker": False,
        },
        llm_config=llm_config,
        system_message="Reply TERMINATE if the task has been solved at full satisfaction.",
    )

def call_autogen_agent(state: MessagesState):
        """Node function that calls the AutoGen agent"""
        messages = convert_to_openai_messages(state["messages"])
        last_message = messages[-1]
        carryover = messages[:-1] if len(messages) > 1 else []

response = user_proxy.initiate_chat(
            autogen_agent,
            message=last_message,
            carryover=carryover
        )

final_content = response.chat_history[-1]["content"]
        return {"messages": {"role": "assistant", "content": final_content}}

# Create and compile the graph
    def create_graph():
        checkpointer = MemorySaver()
        builder = StateGraph(MessagesState)
        builder.add_node("autogen", call_autogen_agent)
        builder.add_edge(START, "autogen")
        return builder.compile(checkpointer=checkpointer)

# Export the graph for LangSmith
    graph = create_graph()
    
    langgraph>=0.1.0
    pyautogen>=0.2.0
    langchain-core>=0.1.0
    langchain-openai>=0.0.5
    json  theme={null}
    {
    "dependencies": ["."],
    "graphs": {
        "autogen_agent": "./agent.py:graph"
    },
    "env": ".env"
    }
    bash pip theme={null}
  pip install -U langgraph-cli
  bash uv theme={null}
  uv add langgraph-cli
  
langgraph deploy --config langgraph.json
```

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/autogen-integration.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
**Output:**
```

Example 2 (unknown):
```unknown
Since we're leveraging LangGraph's [persistence](/oss/python/langgraph/persistence) features we can now continue the conversation using the same thread ID -- LangGraph will automatically pass previous history to the AutoGen agent:
```

Example 3 (unknown):
```unknown
**Output:**
```

Example 4 (unknown):
```unknown
## 4. Prepare for deployment

To deploy to LangSmith, create a file structure like the following:
```

---

## Pass your API key and desired tracing project through headers

**URL:** llms-txt#pass-your-api-key-and-desired-tracing-project-through-headers

export OTEL_EXPORTER_OTLP_HEADERS="x-api-key=<api-key>,Langsmith-Project=<project-name>"

---

## path/to/embedding_function.py

**URL:** llms-txt#path/to/embedding_function.py

**Contents:**
- Querying via the API

from openai import AsyncOpenAI

client = AsyncOpenAI()

async def aembed_texts(texts: list[str]) -> list[list[float]]:
    """Custom embedding function that must:
    1. Be async
    2. Accept a list of strings
    3. Return a list of float arrays (embeddings)
    """
    response = await client.embeddings.create(
        model="text-embedding-3-small",
        input=texts
    )
    return [e.embedding for e in response.data]
python  theme={null}
from langgraph_sdk import get_client

async def search_store():
    client = get_client()
    results = await client.store.search_items(
        ("memory", "facts"),
        query="your search query",
        limit=3  # number of results to return
    )
    return results

**Examples:**

Example 1 (unknown):
```unknown
## Querying via the API

You can also query the store using the LangGraph SDK. Since the SDK uses async operations:
```

---

## Persistence

**URL:** llms-txt#persistence

**Contents:**
- Threads
- Checkpoints
  - Get state
  - Get state history
  - Replay
  - Update state
- Memory Store
  - Basic Usage
  - Semantic Search
  - Using in LangGraph

Source: https://docs.langchain.com/oss/javascript/langgraph/persistence

LangGraph has a built-in persistence layer, implemented through checkpointers. When you compile a graph with a checkpointer, the checkpointer saves a `checkpoint` of the graph state at every super-step. Those checkpoints are saved to a `thread`, which can be accessed after graph execution. Because `threads` allow access to graph's state after execution, several powerful capabilities including human-in-the-loop, memory, time travel, and fault-tolerance are all possible. Below, we'll discuss each of these concepts in more detail.

<img src="https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/checkpoints.jpg?fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=966566aaae853ed4d240c2d0d067467c" alt="Checkpoints" data-og-width="2316" width="2316" data-og-height="748" height="748" data-path="oss/images/checkpoints.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/checkpoints.jpg?w=280&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=7bb8525bfcd22b3903b3209aa7497f47 280w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/checkpoints.jpg?w=560&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=e8d07fc2899b9a13c7b00eb9b259c3c9 560w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/checkpoints.jpg?w=840&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=46a2f9ed3b131a7c78700711e8c314d6 840w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/checkpoints.jpg?w=1100&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=c339bd49757810dad226e1846f066c94 1100w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/checkpoints.jpg?w=1650&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=8333dfdb9d766363f251132f2dfa08a1 1650w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/checkpoints.jpg?w=2500&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=33ba13937eed043ba4a7a87b36d3046f 2500w" />

<Info>
  **LangGraph API handles checkpointing automatically**
  When using the LangGraph API, you don't need to implement or configure checkpointers manually. The API handles all persistence infrastructure for you behind the scenes.
</Info>

A thread is a unique ID or thread identifier assigned to each checkpoint saved by a checkpointer. It contains the accumulated state of a sequence of [runs](/langsmith/assistants#execution). When a run is executed, the [state](/oss/javascript/langgraph/graph-api#state) of the underlying graph of the assistant will be persisted to the thread.

When invoking a graph with a checkpointer, you **must** specify a `thread_id` as part of the `configurable` portion of the config.

A thread's current and historical state can be retrieved. To persist state, a thread must be created prior to executing a run. The LangSmith API provides several endpoints for creating and managing threads and thread state. See the [API reference](https://langchain-ai.github.io/langgraph/cloud/reference/api/) for more details.

The state of a thread at a particular point in time is called a checkpoint. Checkpoint is a snapshot of the graph state saved at each super-step and is represented by `StateSnapshot` object with the following key properties:

* `config`: Config associated with this checkpoint.
* `metadata`: Metadata associated with this checkpoint.
* `values`: Values of the state channels at this point in time.
* `next` A tuple of the node names to execute next in the graph.
* `tasks`: A tuple of `PregelTask` objects that contain information about next tasks to be executed. If the step was previously attempted, it will include error information. If a graph was interrupted [dynamically](/oss/javascript/langgraph/interrupts#pause-using-interrupt) from within a node, tasks will contain additional data associated with interrupts.

Checkpoints are persisted and can be used to restore the state of a thread at a later time.

Let's see what checkpoints are saved when a simple graph is invoked as follows:

After we run the graph, we expect to see exactly 4 checkpoints:

* Empty checkpoint with [`START`](https://langchain-ai.github.io/langgraphjs/reference/variables/langgraph.START.html) as the next node to be executed
* Checkpoint with the user input `{'foo': '', 'bar': []}` and `nodeA` as the next node to be executed
* Checkpoint with the outputs of `nodeA` `{'foo': 'a', 'bar': ['a']}` and `nodeB` as the next node to be executed
* Checkpoint with the outputs of `nodeB` `{'foo': 'b', 'bar': ['a', 'b']}` and no next nodes to be executed

Note that the `bar` channel values contain outputs from both nodes as we have a reducer for the `bar` channel.

When interacting with the saved graph state, you **must** specify a [thread identifier](#threads). You can view the *latest* state of the graph by calling `graph.getState(config)`. This will return a `StateSnapshot` object that corresponds to the latest checkpoint associated with the thread ID provided in the config or a checkpoint associated with a checkpoint ID for the thread, if provided.

In our example, the output of `getState` will look like this:

### Get state history

You can get the full history of the graph execution for a given thread by calling `graph.getStateHistory(config)`. This will return a list of `StateSnapshot` objects associated with the thread ID provided in the config. Importantly, the checkpoints will be ordered chronologically with the most recent checkpoint / `StateSnapshot` being the first in the list.

In our example, the output of `getStateHistory` will look like this:

<img src="https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/get_state.jpg?fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=38ffff52be4d8806b287836295a3c058" alt="State" data-og-width="2692" width="2692" data-og-height="1056" height="1056" data-path="oss/images/get_state.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/get_state.jpg?w=280&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=e932acac5021614d0eb99b90e54be004 280w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/get_state.jpg?w=560&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=2eaf153fd49ba728e1d679c12bb44b6f 560w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/get_state.jpg?w=840&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=0ac091c7dbe8b1f0acff97615a3683ee 840w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/get_state.jpg?w=1100&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=9921a482f1c4f86316fca23a5150b153 1100w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/get_state.jpg?w=1650&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=9412cd906f6d67a9fe1f50a5d4f4c674 1650w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/get_state.jpg?w=2500&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=ccc5118ed85926bda3715c81ce728fcc 2500w" />

It's also possible to play-back a prior graph execution. If we `invoke` a graph with a `thread_id` and a `checkpoint_id`, then we will *re-play* the previously executed steps *before* a checkpoint that corresponds to the `checkpoint_id`, and only execute the steps *after* the checkpoint.

* `thread_id` is the ID of a thread.
* `checkpoint_id` is an identifier that refers to a specific checkpoint within a thread.

You must pass these when invoking the graph as part of the `configurable` portion of the config:

Importantly, LangGraph knows whether a particular step has been executed previously. If it has, LangGraph simply *re-plays* that particular step in the graph and does not re-execute the step, but only for the steps *before* the provided `checkpoint_id`. All of the steps *after* `checkpoint_id` will be executed (i.e., a new fork), even if they have been executed previously. See this [how to guide on time-travel to learn more about replaying](/oss/javascript/langgraph/use-time-travel).

<img src="https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/re_play.png?fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=d7b34b85c106e55d181ae1f4afb50251" alt="Replay" data-og-width="2276" width="2276" data-og-height="986" height="986" data-path="oss/images/re_play.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/re_play.png?w=280&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=627d1fb4cb0ce3e5734784cc4a841cca 280w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/re_play.png?w=560&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=ab462e9559619778d1bdfced578ee0ba 560w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/re_play.png?w=840&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=7cc304a2a0996e22f783e9a5f7a69f89 840w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/re_play.png?w=1100&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=b322f66ef96d6734dcac38213104f080 1100w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/re_play.png?w=1650&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=922f1b014b33fae4fda1e576d57a9983 1650w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/re_play.png?w=2500&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=efae9196c69a2908846c9d23ad117a90 2500w" />

In addition to re-playing the graph from specific `checkpoints`, we can also *edit* the graph state. We do this using `graph.updateState()`. This method accepts three different arguments:

The config should contain `thread_id` specifying which thread to update. When only the `thread_id` is passed, we update (or fork) the current state. Optionally, if we include `checkpoint_id` field, then we fork that selected checkpoint.

These are the values that will be used to update the state. Note that this update is treated exactly as any update from a node is treated. This means that these values will be passed to the [reducer](/oss/javascript/langgraph/graph-api#reducers) functions, if they are defined for some of the channels in the graph state. This means that [`update_state`](https://langchain-ai.github.io/langgraphjs/reference/classes/langgraph.CompiledStateGraph.html#updateState) does NOT automatically overwrite the channel values for every channel, but only for the channels without reducers. Let's walk through an example.

Let's assume you have defined the state of your graph with the following schema (see full example above):

Let's now assume the current state of the graph is

If you update the state as below:

Then the new state of the graph will be:

The `foo` key (channel) is completely changed (because there is no reducer specified for that channel, so `updateState` overwrites it). However, there is a reducer specified for the `bar` key, and so it appends `"b"` to the state of `bar`.

The final thing you can optionally specify when calling `updateState` is `asNode`. If you provide it, the update will be applied as if it came from node `asNode`. If `asNode` is not provided, it will be set to the last node that updated the state, if not ambiguous. The reason this matters is that the next steps to execute depend on the last node to have given an update, so this can be used to control which node executes next. See this [how to guide on time-travel to learn more about forking state](/oss/javascript/langgraph/use-time-travel).

<img src="https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/checkpoints_full_story.jpg?fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=a52016b2c44b57bd395d6e1eac47aa36" alt="Update" data-og-width="3705" width="3705" data-og-height="2598" height="2598" data-path="oss/images/checkpoints_full_story.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/checkpoints_full_story.jpg?w=280&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=06de1669d4d62f0e8013c4ffef021437 280w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/checkpoints_full_story.jpg?w=560&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=b149bed4f842c4f179e55247a426befe 560w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/checkpoints_full_story.jpg?w=840&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=58cfc0341a77e179ce443a89d667784c 840w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/checkpoints_full_story.jpg?w=1100&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=29776799d5a22c3aec7d4a45f675ba14 1100w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/checkpoints_full_story.jpg?w=1650&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=5600d9dd7c52dda79e4eb240c344f84a 1650w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/checkpoints_full_story.jpg?w=2500&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=e428c9c4fc060579c0b7fead1d4a54cb 2500w" />

<img src="https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/shared_state.png?fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=354526fb48c5eb11b4b2684a2df40d6c" alt="Model of shared state" data-og-width="1482" width="1482" data-og-height="777" height="777" data-path="oss/images/shared_state.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/shared_state.png?w=280&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=1965b83f077aea6301b95b59a9a1e318 280w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/shared_state.png?w=560&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=02898a7498e355e04919ac4121678179 560w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/shared_state.png?w=840&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=4ef92e64d1151922511c78afde7abdca 840w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/shared_state.png?w=1100&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=abddd799a170aa9af9145574e46cff6f 1100w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/shared_state.png?w=1650&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=14025324ecb0c462ee1919033d2ae9c5 1650w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/shared_state.png?w=2500&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=a4f7989c4392a7ba8160f559d6fd8942 2500w" />

A [state schema](/oss/javascript/langgraph/graph-api#schema) specifies a set of keys that are populated as a graph is executed. As discussed above, state can be written by a checkpointer to a thread at each graph step, enabling state persistence.

But, what if we want to retain some information *across threads*? Consider the case of a chatbot where we want to retain specific information about the user across *all* chat conversations (e.g., threads) with that user!

With checkpointers alone, we cannot share information across threads. This motivates the need for the [`Store`](https://python.langchain.com/api_reference/langgraph/index.html#module-langgraph.store) interface. As an illustration, we can define an `InMemoryStore` to store information about a user across threads. We simply compile our graph with a checkpointer, as before, and with our new `in_memory_store` variable.

<Info>
  **LangGraph API handles stores automatically**
  When using the LangGraph API, you don't need to implement or configure stores manually. The API handles all storage infrastructure for you behind the scenes.
</Info>

First, let's showcase this in isolation without using LangGraph.

Memories are namespaced by a `tuple`, which in this specific example will be `(<user_id>, "memories")`. The namespace can be any length and represent anything, does not have to be user specific.

We use the `store.put` method to save memories to our namespace in the store. When we do this, we specify the namespace, as defined above, and a key-value pair for the memory: the key is simply a unique identifier for the memory (`memory_id`) and the value (a dictionary) is the memory itself.

We can read out memories in our namespace using the `store.search` method, which will return all memories for a given user as a list. The most recent memory is the last in the list.

The attributes it has are:

* `value`: The value of this memory
* `key`: A unique key for this memory in this namespace
* `namespace`: A list of strings, the namespace of this memory type
* `createdAt`: Timestamp for when this memory was created
* `updatedAt`: Timestamp for when this memory was updated

Beyond simple retrieval, the store also supports semantic search, allowing you to find memories based on meaning rather than exact matches. To enable this, configure the store with an embedding model:

Now when searching, you can use natural language queries to find relevant memories:

You can control which parts of your memories get embedded by configuring the `fields` parameter or by specifying the `index` parameter when storing memories:

### Using in LangGraph

With this all in place, we use the `memoryStore` in LangGraph. The `memoryStore` works hand-in-hand with the checkpointer: the checkpointer saves state to threads, as discussed above, and the `memoryStore` allows us to store arbitrary information for access *across* threads. We compile the graph with both the checkpointer and the `memoryStore` as follows.

We invoke the graph with a `thread_id`, as before, and also with a `user_id`, which we'll use to namespace our memories to this particular user as we showed above.

We can access the `memoryStore` and the `user_id` in *any node* by accessing `config` and `store` as node arguments. Here's how we might use semantic search in a node to find relevant memories:

As we showed above, we can also access the store in any node and use the `store.search` method to get memories. Recall the memories are returned as a list of objects that can be converted to a dictionary.

We can access the memories and use them in our model call.

If we create a new thread, we can still access the same memories so long as the `user_id` is the same.

When we use the LangSmith, either locally (e.g., in [Studio](/langsmith/studio)) or [hosted with LangSmith](/langsmith/platform-setup), the base store is available to use by default and does not need to be specified during graph compilation. To enable semantic search, however, you **do** need to configure the indexing settings in your `langgraph.json` file. For example:

See the [deployment guide](/langsmith/semantic-search) for more details and configuration options.

## Checkpointer libraries

Under the hood, checkpointing is powered by checkpointer objects that conform to [`BaseCheckpointSaver`](https://langchain-ai.github.io/langgraphjs/reference/classes/checkpoint.BaseCheckpointSaver.html) interface. LangGraph provides several checkpointer implementations, all implemented via standalone, installable libraries:

* `@langchain/langgraph-checkpoint`: The base interface for checkpointer savers ([`BaseCheckpointSaver`](https://langchain-ai.github.io/langgraphjs/reference/classes/checkpoint.BaseCheckpointSaver.html)) and serialization/deserialization interface ([`SerializerProtocol`](https://langchain-ai.github.io/langgraphjs/reference/interfaces/checkpoint.SerializerProtocol.html)). Includes in-memory checkpointer implementation ([`MemorySaver`](https://langchain-ai.github.io/langgraphjs/reference/classes/checkpoint.MemorySaver.html)) for experimentation. LangGraph comes with `@langchain/langgraph-checkpoint` included.
* `@langchain/langgraph-checkpoint-sqlite`: An implementation of LangGraph checkpointer that uses SQLite database ([`SqliteSaver`](https://langchain-ai.github.io/langgraphjs/reference/classes/checkpoint_sqlite.SqliteSaver.html)). Ideal for experimentation and local workflows. Needs to be installed separately.
* `@langchain/langgraph-checkpoint-postgres`: An advanced checkpointer that uses Postgres database ([`PostgresSaver`](https://langchain-ai.github.io/langgraphjs/reference/classes/checkpoint_postgres.PostgresSaver.html)), used in LangSmith. Ideal for using in production. Needs to be installed separately.

### Checkpointer interface

Each checkpointer conforms to the [`BaseCheckpointSaver`](https://langchain-ai.github.io/langgraphjs/reference/classes/checkpoint.BaseCheckpointSaver.html) interface and implements the following methods:

* `.put` - Store a checkpoint with its configuration and metadata.
* `.putWrites` - Store intermediate writes linked to a checkpoint (i.e. [pending writes](#pending-writes)).
* `.getTuple` - Fetch a checkpoint tuple using for a given configuration (`thread_id` and `checkpoint_id`). This is used to populate `StateSnapshot` in `graph.getState()`.
* `.list` - List checkpoints that match a given configuration and filter criteria. This is used to populate state history in `graph.getStateHistory()`

When checkpointers save the graph state, they need to serialize the channel values in the state. This is done using serializer objects.

`@langchain/langgraph-checkpoint` defines protocol for implementing serializers and provides a default implementation that handles a wide variety of types, including LangChain and LangGraph primitives, datetimes, enums and more.

### Human-in-the-loop

First, checkpointers facilitate [human-in-the-loop workflows](/oss/javascript/langgraph/interrupts) workflows by allowing humans to inspect, interrupt, and approve graph steps. Checkpointers are needed for these workflows as the human has to be able to view the state of a graph at any point in time, and the graph has to be to resume execution after the human has made any updates to the state. See [the how-to guides](/oss/javascript/langgraph/interrupts) for examples.

Second, checkpointers allow for ["memory"](/oss/javascript/concepts/memory) between interactions. In the case of repeated human interactions (like conversations) any follow up messages can be sent to that thread, which will retain its memory of previous ones. See [Add memory](/oss/javascript/langgraph/add-memory) for information on how to add and manage conversation memory using checkpointers.

Third, checkpointers allow for ["time travel"](/oss/javascript/langgraph/use-time-travel), allowing users to replay prior graph executions to review and / or debug specific graph steps. In addition, checkpointers make it possible to fork the graph state at arbitrary checkpoints to explore alternative trajectories.

Lastly, checkpointing also provides fault-tolerance and error recovery: if one or more nodes fail at a given superstep, you can restart your graph from the last successful step. Additionally, when a graph node fails mid-execution at a given superstep, LangGraph stores pending checkpoint writes from any other nodes that completed successfully at that superstep, so that whenever we resume graph execution from that superstep we don't re-run the successful nodes.

Additionally, when a graph node fails mid-execution at a given superstep, LangGraph stores pending checkpoint writes from any other nodes that completed successfully at that superstep, so that whenever we resume graph execution from that superstep we don't re-run the successful nodes.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/persistence.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
A thread's current and historical state can be retrieved. To persist state, a thread must be created prior to executing a run. The LangSmith API provides several endpoints for creating and managing threads and thread state. See the [API reference](https://langchain-ai.github.io/langgraph/cloud/reference/api/) for more details.

## Checkpoints

The state of a thread at a particular point in time is called a checkpoint. Checkpoint is a snapshot of the graph state saved at each super-step and is represented by `StateSnapshot` object with the following key properties:

* `config`: Config associated with this checkpoint.
* `metadata`: Metadata associated with this checkpoint.
* `values`: Values of the state channels at this point in time.
* `next` A tuple of the node names to execute next in the graph.
* `tasks`: A tuple of `PregelTask` objects that contain information about next tasks to be executed. If the step was previously attempted, it will include error information. If a graph was interrupted [dynamically](/oss/javascript/langgraph/interrupts#pause-using-interrupt) from within a node, tasks will contain additional data associated with interrupts.

Checkpoints are persisted and can be used to restore the state of a thread at a later time.

Let's see what checkpoints are saved when a simple graph is invoked as follows:
```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
After we run the graph, we expect to see exactly 4 checkpoints:

* Empty checkpoint with [`START`](https://langchain-ai.github.io/langgraphjs/reference/variables/langgraph.START.html) as the next node to be executed
* Checkpoint with the user input `{'foo': '', 'bar': []}` and `nodeA` as the next node to be executed
* Checkpoint with the outputs of `nodeA` `{'foo': 'a', 'bar': ['a']}` and `nodeB` as the next node to be executed
* Checkpoint with the outputs of `nodeB` `{'foo': 'b', 'bar': ['a', 'b']}` and no next nodes to be executed

Note that the `bar` channel values contain outputs from both nodes as we have a reducer for the `bar` channel.

### Get state

When interacting with the saved graph state, you **must** specify a [thread identifier](#threads). You can view the *latest* state of the graph by calling `graph.getState(config)`. This will return a `StateSnapshot` object that corresponds to the latest checkpoint associated with the thread ID provided in the config or a checkpoint associated with a checkpoint ID for the thread, if provided.
```

Example 4 (unknown):
```unknown
In our example, the output of `getState` will look like this:
```

---

## Persistent file (survives across threads)

**URL:** llms-txt#persistent-file-(survives-across-threads)

**Contents:**
- Cross-thread persistence

agent.invoke({
    "messages": [{"role": "user", "content": "Save final report to /memories/report.txt"}]
})
python  theme={null}
import uuid

**Examples:**

Example 1 (unknown):
```unknown
## Cross-thread persistence

Files in `/memories/` can be accessed from any thread:
```

---

## Philosophy

**URL:** llms-txt#philosophy

**Contents:**
- History

Source: https://docs.langchain.com/oss/javascript/langchain/philosophy

LangChain exists to be the easiest place to start building with LLMs, while also being flexible and production-ready.

LangChain is driven by a few core beliefs:

* Large Language Models (LLMs) are great, powerful new technology.
* LLMs are even better when you combine them with external sources of data.
* LLMs will transform what the applications of the future look like. Specifically, the applications of the future will look more and more agentic.
* It is still very early on in that transformation.
* While it's easy to build a prototype of those agentic applications, it's still really hard to build agents that are reliable enough to put into production.

With LangChain, we have two core focuses:

<Steps>
  <Step title="We want to enable developers to build with the best models.">
    Different providers expose different APIs, with different model parameters and different message formats.
    Standardizing these model inputs and outputs is a core focus, making it easy for developer to easily change to the most recent state-of-the-art model, avoiding lock-in.
  </Step>

<Step title="We want to make it easy to use models to orchestrate more complex flows that interact with other data and computation.">
    Models should be used for more than just *text generation* - they should also be used to orchestrate more complex flows that interact with other data. LangChain makes it easy to define [tools](/oss/javascript/langchain/tools) that LLMs can use dynamically, as well as help with parsing of and access to unstructured data.
  </Step>
</Steps>

Given the constant rate of change in the field, LangChain has also evolved over time. Below is a brief timeline of how LangChain has changed over the years, evolving alongside what it means to build with LLMs:

<Update label="2022-10-24" description="v0.0.1">
  A month before ChatGPT, **LangChain was launched as a Python package**. It consisted of two main components:

* LLM abstractions
  * "Chains", or predetermined steps of computation to run, for common use cases. For example - RAG: run a retrieval step, then run a generation step.

The name LangChain comes from "Language" (like Language models) and "Chains".
</Update>

<Update label="2022-12">
  The first general purpose agents were added to LangChain.

These general purpose agents were based on the [ReAct paper](https://arxiv.org/abs/2210.03629) (ReAct standing for Reasoning and Acting). They used LLMs to generate JSON that represented tool calls, and then parsed that JSON to determine what tools to call.
</Update>

<Update label="2023-01">
  OpenAI releases a 'Chat Completion' API.

Previously, models took in strings and returned a string. In the ChatCompletions API, they evolved to take in a list of messages and return a message. Other model providers followed suit, and LangChain updated to work with lists of messages.
</Update>

<Update label="2023-01">
  LangChain releases a JavaScript version.

LLMs and agents will change how applications are built and JavaScript is the language of application developers.
</Update>

<Update label="2023-02">
  **LangChain Inc. was formed as a company** around the open source LangChain project.

The main goal was to "make intelligent agents ubiquitous". The team recognized that while LangChain was a key part (LangChain made it simple to get started with LLMs), there was also a need for other components.
</Update>

<Update label="2023-03">
  OpenAI releases 'function calling' in their API.

This allowed the API to explicitly generate payloads that represented tool calls. Other model providers followed suit, and LangChain was updated to use this as the preferred method for tool calling (rather than parsing JSON).
</Update>

<Update label="2023-06">
  **LangSmith is released** as closed source platform by LangChain Inc., providing observability and evals

The main issue with building agents is getting them to be reliable, and LangSmith, which provides observability and evals, was built to solve that need. LangChain was updated to integrate seamlessly with LangSmith.
</Update>

<Update label="2024-01" description="v0.1.0">
  **LangChain releases 0.1.0**, its first non-0.0.x.

The industry matured from prototypes to production, and as such, LangChain increased its focus on stability.
</Update>

<Update label="2024-02">
  **LangGraph is released** as an open-source library.

The original LangChain had two focuses: LLM abstractions, and high-level interfaces for getting started with common applications; however, it was missing a low-level orchestration layer that allowed developers to control the exact flow of their agent. Enter: LangGraph.

When building LangGraph, we learned from lessons when building LangChain and added functionality we discovered was needed: streaming, durable execution, short-term memory, human-in-the-loop, and more.
</Update>

<Update label="2024-06">
  **LangChain has over 700 integrations.**

Integrations were split out of the core LangChain package, and either moved into their own standalone packages (for the core integrations) or `@langchain/community`.
</Update>

<Update label="2024-10">
  LangGraph becomes the preferred way to build any AI application that is more than a single LLM call.

As developers tried to improve the reliability of their applications, they needed more control than the high-level interfaces provided. LangGraph provided that low-level flexibility. Most chains and agents were marked as deprecated in LangChain with guides on how to migrate them to LangGraph. There is still one high-level abstraction created in LangGraph: an agent abstraction. It is built on top of low-level LangGraph and has the same interface as the ReAct agents from LangChain.
</Update>

<Update label="2025-04">
  Model APIs become more multimodal.

Models started to accept files, images, videos, and more. We updated the `@langchain/core` message format accordingly to allow developers to specify these multimodal inputs in a standard way.
</Update>

<Update label="2025-10-20" description="v1.0.0">
  **LangChain releases 1.0** with two major changes:

1. Complete revamp of all chains and agents in `langchain`. All chains and agents are now replaced with only one high level abstraction: an agent abstraction built on top of LangGraph. This was the high-level abstraction that was originally created in LangGraph, but just moved to LangChain.

For users still using old LangChain chains/agents who do NOT want to upgrade (note: we recommend you do), you can continue using old LangChain by installing the `@langchain/classic` package.

2. A standard message content format: Model APIs evolved from returning messages with a simple content string to more complex output types - reasoning blocks, citations, server-side tool calls, etc. LangChain evolved its message formats to standardize these across providers.
</Update>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/philosophy.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Pick a dataset id. In this case, we are using the dataset we created above.

**URL:** llms-txt#pick-a-dataset-id.-in-this-case,-we-are-using-the-dataset-we-created-above.

---

## pip install -qU "langchain[anthropic]" to call the model

**URL:** llms-txt#pip-install--qu-"langchain[anthropic]"-to-call-the-model

from langchain.agents import create_agent

def get_weather(city: str) -> str:
    """Get weather for a given city."""
    return f"It's always sunny in {city}!"

agent = create_agent(
    model="claude-sonnet-4-5-20250929",
    tools=[get_weather],
    system_prompt="You are a helpful assistant",
)

---

## pip install requests requests_toolbelt

**URL:** llms-txt#pip-install-requests-requests_toolbelt

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/trace-with-api.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Prevent logging of sensitive data in traces

**URL:** llms-txt#prevent-logging-of-sensitive-data-in-traces

**Contents:**
- Rule-based masking of inputs and outputs
- Processing Inputs & Outputs for a Single Function
- Quick starts
  - Regex

Source: https://docs.langchain.com/langsmith/mask-inputs-outputs

In some situations, you may need to prevent the inputs and outputs of your traces from being logged for privacy or security reasons. LangSmith provides a way to filter the inputs and outputs of your traces before they are sent to the LangSmith backend.

If you want to completely hide the inputs and outputs of your traces, you can set the following environment variables when running your application:

This works for both the LangSmith SDK (Python and TypeScript) and LangChain.

You can also customize and override this behavior for a given `Client` instance. This can be done by setting the `hide_inputs` and `hide_outputs` parameters on the `Client` object (`hideInputs` and `hideOutputs` in TypeScript).

For the example below, we will simply return an empty object for both `hide_inputs` and `hide_outputs`, but you can customize this to your needs.

## Rule-based masking of inputs and outputs

<Info>
  This feature is available in the following LangSmith SDK versions:

* Python: 0.1.81 and above
  * TypeScript: 0.1.33 and above
</Info>

To mask specific data in inputs and outputs, you can use the `create_anonymizer` / `createAnonymizer` function and pass the newly created anonymizer when instantiating the client. The anonymizer can be either constructed from a list of regex patterns and the replacement values or from a function that accepts and returns a string value.

The anonymizer will be skipped for inputs if `LANGSMITH_HIDE_INPUTS = true`. Same applies for outputs if `LANGSMITH_HIDE_OUTPUTS = true`.

However, if inputs or outputs are to be sent to client, the `anonymizer` method will take precedence over functions found in `hide_inputs` and `hide_outputs`. By default, the `create_anonymizer` will only look at maximum of 10 nesting levels deep, which can be configured via the `max_depth` parameter.

Please note, that using the anonymizer might incur a performance hit with complex regular expressions or large payloads, as the anonymizer serializes the payload to JSON before processing.

<Note>
  Improving the performance of `anonymizer` API is on our roadmap! If you are encountering performance issues, please contact us at [support@langchain.dev](mailto:support@langchain.dev).
</Note>

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/hide-inputs-outputs.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=ac9ba9a6729029a7fa38da03e1466a1a" alt="" data-og-width="1708" width="1708" data-og-height="717" height="717" data-path="langsmith/images/hide-inputs-outputs.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/hide-inputs-outputs.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=7ded12c0345f47d55e9802083c5032d0 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/hide-inputs-outputs.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=8cbe74d09660d8c65e8a75dd78cdb24e 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/hide-inputs-outputs.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=8cb8c0b5c926e46522b9539b0262ee7a 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/hide-inputs-outputs.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=9b8ef244796fad943ec76b0aa5733f80 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/hide-inputs-outputs.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=87f35d63f42f05c49a220d5b8a87787a 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/hide-inputs-outputs.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=5173e30032c065646c13e9b9c6a95fb5 2500w" />

Older versions of LangSmith SDKs can use the `hide_inputs` and `hide_outputs` parameters to achieve the same effect. You can also use these parameters to process the inputs and outputs more efficiently as well.

## Processing Inputs & Outputs for a Single Function

<Info>
  The `process_outputs` parameter is available in LangSmith SDK version 0.1.98 and above for Python.
</Info>

In addition to client-level input and output processing, LangSmith provides function-level processing through the `process_inputs` and `process_outputs` parameters of the `@traceable` decorator.

These parameters accept functions that allow you to transform the inputs and outputs of a specific function before they are logged to LangSmith. This is useful for reducing payload size, removing sensitive information, or customizing how an object should be serialized and represented in LangSmith for a particular function.

Here's an example of how to use `process_inputs` and `process_outputs`:

In this example, `process_inputs` creates a new dictionary with processed input data, and `process_outputs` transforms the output into a specific format before logging to LangSmith.

<Warning>
  It's recommended to avoid mutating the source objects in the processor functions. Instead, create and return new objects with the processed data.
</Warning>

For asynchronous functions, the usage is similar:

These function-level processors take precedence over client-level processors (`hide_inputs` and `hide_outputs`) when both are defined.

You can combine rule-based masking with various anonymizers to scrub sensitive information from inputs and outputs. In this how-to-guide, we'll cover working with regex, Microsoft Presidio, and Amazon Comprehend.

<Info>
  The implementation below is not exhaustive and may miss some formats or edge cases. Test any implementation thoroughly before using it in production.
</Info>

You can use regex to mask inputs and outputs before they are sent to LangSmith. The implementation below masks email addresses, phone numbers, full names, credit card numbers, and SSNs.

```python  theme={null}
import re
import openai
from langsmith import Client
from langsmith.wrappers import wrap_openai

**Examples:**

Example 1 (unknown):
```unknown
This works for both the LangSmith SDK (Python and TypeScript) and LangChain.

You can also customize and override this behavior for a given `Client` instance. This can be done by setting the `hide_inputs` and `hide_outputs` parameters on the `Client` object (`hideInputs` and `hideOutputs` in TypeScript).

For the example below, we will simply return an empty object for both `hide_inputs` and `hide_outputs`, but you can customize this to your needs.

<CodeGroup>
```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
</CodeGroup>

## Rule-based masking of inputs and outputs

<Info>
  This feature is available in the following LangSmith SDK versions:

  * Python: 0.1.81 and above
  * TypeScript: 0.1.33 and above
</Info>

To mask specific data in inputs and outputs, you can use the `create_anonymizer` / `createAnonymizer` function and pass the newly created anonymizer when instantiating the client. The anonymizer can be either constructed from a list of regex patterns and the replacement values or from a function that accepts and returns a string value.

The anonymizer will be skipped for inputs if `LANGSMITH_HIDE_INPUTS = true`. Same applies for outputs if `LANGSMITH_HIDE_OUTPUTS = true`.

However, if inputs or outputs are to be sent to client, the `anonymizer` method will take precedence over functions found in `hide_inputs` and `hide_outputs`. By default, the `create_anonymizer` will only look at maximum of 10 nesting levels deep, which can be configured via the `max_depth` parameter.

<CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Pricing plans

**URL:** llms-txt#pricing-plans

Source: https://docs.langchain.com/langsmith/pricing-plans

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/pricing-plans.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Print the agent's response

**URL:** llms-txt#print-the-agent's-response

**Contents:**
- What happened?
- Next steps

print(result["messages"][-1].content)
```

Your deep agent automatically:

1. **Planned its approach**: Used the built-in `write_todos` tool to break down the research task
2. **Conducted research**: Called the `internet_search` tool to gather information
3. **Managed context**: Used file system tools (`write_file`, `read_file`) to offload large search results
4. **Spawned subagents** (if needed): Delegated complex subtasks to specialized subagents
5. **Synthesized a report**: Compiled findings into a coherent response

Now that you've built your first deep agent:

* **Customize your agent**: Learn about [customization options](/oss/python/deepagents/customization), including custom system prompts, tools, and subagents.
* **Understand middleware**: Dive into the [middleware architecture](/oss/python/deepagents/middleware) that powers deep agents.
* **Add long-term memory**: Enable [persistent memory](/oss/python/deepagents/long-term-memory) across conversations.
* **Deploy to production**: Learn about [deployment options](/oss/python/langgraph/deploy) for LangGraph applications.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/deepagents/quickstart.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Proactive Approach (recommended)

**URL:** llms-txt#proactive-approach-(recommended)

def agent_with_monitoring(state: dict, config: RunnableConfig) -> dict:
    """Proactively monitor and handle recursion within the graph"""
    current_step = config["metadata"]["langgraph_step"]
    recursion_limit = config["recursion_limit"]

# Early detection - route to internal handling
    if current_step >= recursion_limit - 2:  # 2 steps before limit
        return {
            **state,
            "status": "recursion_limit_approaching",
            "final_answer": "Reached iteration limit, returning partial result"
        }

# Normal processing
    return {"messages": state["messages"] + [f"Step {current_step}"]}

---

## Process final result

**URL:** llms-txt#process-final-result

**Contents:**
- Multiple tool calls
- Edit tool arguments
- Subagent interrupts
- Best practices
  - Always use a checkpointer
  - Use the same thread ID

print(result["messages"][-1]["content"])
python  theme={null}
config = {"configurable": {"thread_id": str(uuid.uuid4())}}

result = agent.invoke({
    "messages": [{
        "role": "user",
        "content": "Delete temp.txt and send an email to admin@example.com"
    }]
}, config=config)

if result.get("__interrupt__"):
    interrupts = result["__interrupt__"][0].value
    action_requests = interrupts["action_requests"]

# Two tools need approval
    assert len(action_requests) == 2

# Provide decisions in the same order as action_requests
    decisions = [
        {"type": "approve"},  # First tool: delete_file
        {"type": "reject"}    # Second tool: send_email
    ]

result = agent.invoke(
        Command(resume={"decisions": decisions}),
        config=config
    )
python  theme={null}
if result.get("__interrupt__"):
    interrupts = result["__interrupt__"][0].value
    action_request = interrupts["action_requests"][0]

# Original args from the agent
    print(action_request["args"])  # {"to": "everyone@company.com", ...}

# User decides to edit the recipient
    decisions = [{
        "type": "edit",
        "edited_action": {
            "name": action_request["name"],  # Must include the tool name
            "args": {"to": "team@company.com", "subject": "...", "body": "..."}
        }
    }]

result = agent.invoke(
        Command(resume={"decisions": decisions}),
        config=config
    )
python  theme={null}
agent = create_deep_agent(
    tools=[delete_file, read_file],
    interrupt_on={
        "delete_file": True,
        "read_file": False,
    },
    subagents=[{
        "name": "file-manager",
        "description": "Manages file operations",
        "system_prompt": "You are a file management assistant.",
        "tools": [delete_file, read_file],
        "interrupt_on": {
            # Override: require approval for reads in this subagent
            "delete_file": True,
            "read_file": True,  # Different from main agent!
        }
    }],
    checkpointer=checkpointer
)
python  theme={null}
from langgraph.checkpoint.memory import MemorySaver

checkpointer = MemorySaver()
agent = create_deep_agent(
    tools=[...],
    interrupt_on={...},
    checkpointer=checkpointer  # Required for HITL
)
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
## Multiple tool calls

When the agent calls multiple tools that require approval, all interrupts are batched together in a single interrupt. You must provide decisions for each one in order.
```

Example 2 (unknown):
```unknown
## Edit tool arguments

When `"edit"` is in the allowed decisions, you can modify the tool arguments before execution:
```

Example 3 (unknown):
```unknown
## Subagent interrupts

Each subagent can have its own `interrupt_on` configuration that overrides the main agent's settings:
```

Example 4 (unknown):
```unknown
When a subagent triggers an interrupt, the handling is the same – check for `__interrupt__` and resume with `Command`.

## Best practices

### Always use a checkpointer

Human-in-the-loop requires a checkpointer to persist agent state between the interrupt and resume:
```

---

## Prompt engineering concepts

**URL:** llms-txt#prompt-engineering-concepts

**Contents:**
- Why prompt engineering?
- Prompts vs. prompt templates
- Prompts in LangSmith
  - Chat vs Completion
  - F-string vs. mustache
  - Tools
  - Structured output
  - Model
- Prompt versioning
  - Commits

Source: https://docs.langchain.com/langsmith/prompt-engineering-concepts

While traditional software applications are built by writing code, AI applications often derive their logic from prompts.

This guide will walk through the key concepts of prompt engineering in LangSmith.

## Why prompt engineering?

A prompt sets the stage for the model, like an audience member at an improv show directing the actor's next performance - it guides the model's behavior without changing its underlying capabilities. Just as telling an actor to "be a pirate" determines how they act, a prompt provides instructions, examples, and context that shape how the model responds.

Prompt engineering is important because it allows you to change the way the model behaves. While there are other ways to change the model's behavior (like fine-tuning), prompt engineering is usually the simplest to get started with and often provides the highest ROI.

We often see that prompt engineering is multi-disciplinary. Sometimes the best prompt engineer is not the software engineer who is building the application, but rather the product manager or another domain expert. It is important to have the proper tooling and infrastructure to support this cross-disciplinary building.

## Prompts vs. prompt templates

Although we often use these terms interchangably, it is important to understand the difference between "prompts" and "prompt templates".

Prompts refer to the messages that are passed into the language model.

Prompt Templates refer to a way of formatting information to get that prompt to hold the information that you want. Prompt templates can include variables for few shot examples, outside context, or any other external data that is needed in your prompt.

<img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-vs-prompt-template.png?fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=714ad4962c85cfb8847ebbf01559c217" alt="" data-og-width="1084" width="1084" data-og-height="450" height="450" data-path="langsmith/images/prompt-vs-prompt-template.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-vs-prompt-template.png?w=280&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=81497578f297dd5a7311de6f1c06ef85 280w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-vs-prompt-template.png?w=560&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=d13dfcf34430d876cdc1a2b77a5fd3c4 560w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-vs-prompt-template.png?w=840&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=feec2461df390de635a2534b50eab8e6 840w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-vs-prompt-template.png?w=1100&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=e1851de62aa377fc3d70ec62dd91c5b2 1100w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-vs-prompt-template.png?w=1650&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=6bccce7c02cb065f9ef8964fbe3112b6 1650w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-vs-prompt-template.png?w=2500&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=c3494cef2e70686d55493ded764cc8c0 2500w" />

## Prompts in LangSmith

You can store and version prompts templates in LangSmith. There are few key aspects of a prompt template to understand.

### Chat vs Completion

There are two different types of prompts: `chat` style prompts and `completion` style prompts.

Chat style prompts are a **list of messages**. This is the prompting style supported by most model APIs these days, and so this should generally be preferred.

Completion style prompts are just a string. This is an older style of prompting, and so mostly exists for legacy reasons.

### F-string vs. mustache

You can format your prompt with input variables using either [f-string](https://realpython.com/python-f-strings/) or [mustache](https://mustache.github.io/mustache.5.html) format. Here is an example prompt with f-string format:

And here is one with mustache:

To add a conditional mustache prompt:

* The playground UI will pick up `is_logged_in` variable, but any nested variables you'll need to specify yourself. Paste the following into inputs to ensure the above conditional prompt works:

<Check>
  The LangSmith Playground uses `f-string` as the default template format, but you can switch to `mustache` format in the prompt settings/template format section. `mustache` gives you more flexibility around conditional variables, loops, and nested keys. For conditional variables, you'll need to manually add json variables in the 'inputs' section. Read [the documentation](https://mustache.github.io/mustache.5.html)
</Check>

Tools are interfaces the LLM can use to interact with the outside world. Tools consist of a name, description, and JSON schema of arguments used to call the tool.

### Structured output

Structured output is a feature of most state of the art LLMs, wherein instead of producing raw text as output they stick to a specified schema. This may or may not use [Tools](#tools) under the hood.

<Check>
  Structured outputs are similar to tools, but different in a few key ways. With tools, the LLM choose which tool to call (or may choose not to call any); with structured output, the LLM **always** responds in this format. With tools, the LLM may select **multiple** tools; with structured output, only one response is generate.
</Check>

Optionally, you can store a model configuration alongside a prompt template. This includes the name of the model and any other parameters (temperature, etc).

Verisioning is a key part of iterating and collaborating on your different prompts.

Every saved update to a prompt creates a new commit with a unique commit hash. This allows you to:

* View the full history of changes to a prompt.
* Review earlier versions.
* Revert to a previous state if needed.
* Reference specific versions in your code using the commit hash (e.g., `client.pull_prompt("prompt_name:commit_hash")`).

In the UI, you can compare a commit with its previous version by toggling **Show diff** in the top-right corner of the **Commits** tab.

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/commit-diff.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=a045b239f92dc1c616c7e42ae282c2b9" alt="The commit hashes list for a prompt with the diff of one commit." data-og-width="2884" width="2884" data-og-height="1426" height="1426" data-path="langsmith/images/commit-diff.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/commit-diff.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=5f1f1e66762226af9ec78e7dcd88a40e 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/commit-diff.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=9170a56b51a619a3aea586942c6d1825 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/commit-diff.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=198ae20ca965b50d42178bb241bddf46 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/commit-diff.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=5e56c9323edf28650c4c667def63d7d1 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/commit-diff.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=25b8995161ed4523c09526eee44aef8a 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/commit-diff.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=ac495d702a4df63f35ac47e5584a1c4b 2500w" />

Commit tags are human-readable labels that point to specific commits in your prompt's history. Unlike commit hashes, tags can be moved to point to different commits, allowing you to update which version your code references without changing the code itself.

Use cases for commit tags can include:

* **Environment-specific tags**: Mark commits for `production` or `staging` environments, which allows you to switch between different versions without changing your code.
* **Version control**: Mark stable versions of your prompts, for example, `v1`, `v2`, which lets you reference specific versions in your code and track changes over time.
* **Collaboration**: Mark versions ready for review, which enables you to share specific versions with collaborators and get feedback.

<Note>
  **Not to be confused with resource tags**: Commit tags reference specific prompt versions. [Resource tags](/langsmith/set-up-resource-tags) are key-value pairs used to organize workspace resources.
</Note>

For detailed information on creating and managing commit tags, see [Manage prompts](/langsmith/manage-prompts#commit-tags).

The prompt playground makes the process of iterating and testing your prompts seamless. You can enter the playground from the sidebar or directly from a saved prompt.

In the playground you can:

* Change the model being used
* Change prompt template being used
* Change the output schema
* Change the tools available
* Enter the input variables to run through the prompt template
* Run the prompt through the model
* Observe the outputs

## Testing multiple prompts

You can add more prompts to your playground to easily compare outputs and decide which version is better:

<img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-prompt-to-playground.gif?s=1c6f0c32b45a3f480b16d704c09570fc" alt="" data-og-width="1000" width="1000" data-og-height="539" height="539" data-path="langsmith/images/add-prompt-to-playground.gif" data-optimize="true" data-opv="3" />

## Testing over a dataset

To test over a dataset, you simply select the dataset from the top right and press Start. You can modify whether the results are streamed back as well as how many repitions there are in the test.

<img src="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/test-over-dataset-in-playground.gif?s=aaf0f90a0c61934a928f81d5e11e2c35" alt="" data-og-width="1000" width="1000" data-og-height="539" height="539" data-path="langsmith/images/test-over-dataset-in-playground.gif" data-optimize="true" data-opv="3" />

You can click on the "View Experiment" button to dive deeper into the results of the test.

<iframe className="w-full aspect-video rounded-xl" src="https://www.youtube.com/embed/h4f6bIWGkog?si=IVJFfhldC7M3HL4G" title="YouTube video player" frameBorder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowFullScreen />

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/prompt-engineering-concepts.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
And here is one with mustache:
```

Example 2 (unknown):
```unknown
To add a conditional mustache prompt:
```

Example 3 (unknown):
```unknown
* The playground UI will pick up `is_logged_in` variable, but any nested variables you'll need to specify yourself. Paste the following into inputs to ensure the above conditional prompt works:
```

---

## Prompt engineering

**URL:** llms-txt#prompt-engineering

Source: https://docs.langchain.com/langsmith/prompt-engineering

The following sections help you create, manage, and optimize your prompts:

<Columns cols={3}>
  <Card title="Review core concepts" icon="circle-info" href="/langsmith/prompt-engineering-concepts" arrow="true">
    Read definitions and key terminology for prompt engineering in LangSmith.
  </Card>

<Card title="Create and update prompts" icon="pen-to-square" href="/langsmith/create-a-prompt" arrow="true">
    Build prompts via the UI or SDK, configure settings, use tools, add multimodal inputs, and connect model providers.
  </Card>

<Card title="Manage prompts" icon="tags" href="/langsmith/manage-prompts" arrow="true">
    Organize with tags, commit changes, trigger webhooks, and share through the public prompt hub.
  </Card>

<Card title="Explore the prompt hub" icon="folder-tree" href="/langsmith/manage-prompts#public-prompt-hub" arrow="true">
    Browse and manage prompt tags and discover community prompts from the LangChain Hub.
  </Card>

<Card title="Open the prompt playground" icon="vial" href="/langsmith/prompt-engineering-concepts#prompt-playground" arrow="true">
    Test and experiment with prompts using custom endpoints and model configurations.
  </Card>

<Card title="Follow tutorials" icon="book-open" href="/langsmith/optimize-classifier" arrow="true">
    Learn step-by-step techniques, like optimizing classifiers and advanced prompt engineering.
  </Card>
</Columns>

<Note>
  To set up a LangSmith instance, visit the [Platform setup section](/langsmith/platform-setup) to choose between cloud, hybrid, or self-hosted. All options include observability, evaluation, prompt engineering, and deployment.
</Note>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/prompt-engineering.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Prompt engineering quickstart

**URL:** llms-txt#prompt-engineering-quickstart

**Contents:**
- Prerequisites
- Next steps
- Video guide

Source: https://docs.langchain.com/langsmith/prompt-engineering-quickstart

Prompts guide the behavior of large language models (LLM). [*Prompt engineering*](/langsmith/prompt-engineering-concepts) is the process of crafting, testing, and refining the instructions you give to an LLM so it produces reliable and useful responses.

LangSmith provides tools to create, version, test, and collaborate on prompts. You’ll also encounter common concepts like [*prompt templates*](/langsmith/prompt-engineering-concepts#prompts-vs-prompt-templates), which let you reuse structured prompts, and [*variables*](/langsmith/prompt-engineering-concepts#f-string-vs-mustache), which allow you to dynamically insert values (such as a user’s question) into a prompt.

In this quickstart, you’ll create, test, and improve prompts using either the UI or the SDK. This quickstart will use OpenAI as the example LLM provider, but the same workflow applies across other providers.

<Tip>
  If you prefer to watch a video on getting started with prompt engineering, refer to the quickstart [Video guide](#video-guide).
</Tip>

Before you begin, make sure you have:

* **A LangSmith account**: Sign up or log in at [smith.langchain.com](https://smith.langchain.com).
* **A LangSmith API key**: Follow the [Create an API key](/langsmith/create-account-api-key#create-an-api-key) guide.
* **An OpenAI API key**: Generate this from the [OpenAI dashboard](https://platform.openai.com/account/api-keys).

Select the tab for UI or SDK workflows:

<Tabs>
  <Tab title="UI" icon="window">
    ## 1. Set workspace secret

In the [LangSmith UI](https://smith.langchain.com), ensure that your OpenAI API key is set as a [workspace secret](/langsmith/administration-overview#workspace-secrets).

1. Navigate to <Icon icon="gear" /> **Settings** and then move to the **Secrets** tab.
    2. Select **Add secret** and enter the `OPENAI_API_KEY` and your API key as the **Value**.
    3. Select **Save secret**.

<Note> When adding workspace secrets in the LangSmith UI, make sure the secret keys match the environment variable names expected by your model provider.</Note>

## 2. Create a prompt

1. In the [LangSmith UI](https://smith.langchain.com), navigate to the **Prompts** section in the left-hand menu.
    2. Click on **+ Prompt** to create a prompt.
    3. Modify the prompt by editing or adding prompts and input variables as needed.

<div style={{ textAlign: 'center' }}>
      <img className="block dark:hidden" src="https://mintcdn.com/langchain-5e9cc07a/t6ucb6rQa27Wd6Te/langsmith/images/create-a-prompt-light.png?fit=max&auto=format&n=t6ucb6rQa27Wd6Te&q=85&s=0cafd7b1330fd88caa7403772068a50d" alt="Prompt playground with the system prompt ready for editing." data-og-width="951" width="951" data-og-height="412" height="412" data-path="langsmith/images/create-a-prompt-light.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/t6ucb6rQa27Wd6Te/langsmith/images/create-a-prompt-light.png?w=280&fit=max&auto=format&n=t6ucb6rQa27Wd6Te&q=85&s=86270ac274480c09b4b772c79835c96a 280w, https://mintcdn.com/langchain-5e9cc07a/t6ucb6rQa27Wd6Te/langsmith/images/create-a-prompt-light.png?w=560&fit=max&auto=format&n=t6ucb6rQa27Wd6Te&q=85&s=0bba28323e21330632a3368603cfd436 560w, https://mintcdn.com/langchain-5e9cc07a/t6ucb6rQa27Wd6Te/langsmith/images/create-a-prompt-light.png?w=840&fit=max&auto=format&n=t6ucb6rQa27Wd6Te&q=85&s=7c6ec959c52230f4a9c1153e05c2a257 840w, https://mintcdn.com/langchain-5e9cc07a/t6ucb6rQa27Wd6Te/langsmith/images/create-a-prompt-light.png?w=1100&fit=max&auto=format&n=t6ucb6rQa27Wd6Te&q=85&s=3c76f683eb2ef9c71dc1a3d7e337fffb 1100w, https://mintcdn.com/langchain-5e9cc07a/t6ucb6rQa27Wd6Te/langsmith/images/create-a-prompt-light.png?w=1650&fit=max&auto=format&n=t6ucb6rQa27Wd6Te&q=85&s=b900411f7aa605f20f6c5182834bf4c3 1650w, https://mintcdn.com/langchain-5e9cc07a/t6ucb6rQa27Wd6Te/langsmith/images/create-a-prompt-light.png?w=2500&fit=max&auto=format&n=t6ucb6rQa27Wd6Te&q=85&s=b3b11c7507cc57b2fc178b251c0173dc 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/langchain-5e9cc07a/t6ucb6rQa27Wd6Te/langsmith/images/create-a-prompt-dark.png?fit=max&auto=format&n=t6ucb6rQa27Wd6Te&q=85&s=16f217eb0e1c0b02ad0d7658f1a53f4d" alt="Prompt playground with the system prompt ready for editing." data-og-width="937" width="937" data-og-height="402" height="402" data-path="langsmith/images/create-a-prompt-dark.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/t6ucb6rQa27Wd6Te/langsmith/images/create-a-prompt-dark.png?w=280&fit=max&auto=format&n=t6ucb6rQa27Wd6Te&q=85&s=401ae88da122905ab2e820fc22ce1b37 280w, https://mintcdn.com/langchain-5e9cc07a/t6ucb6rQa27Wd6Te/langsmith/images/create-a-prompt-dark.png?w=560&fit=max&auto=format&n=t6ucb6rQa27Wd6Te&q=85&s=61c573f773dec1485545395c1fd37525 560w, https://mintcdn.com/langchain-5e9cc07a/t6ucb6rQa27Wd6Te/langsmith/images/create-a-prompt-dark.png?w=840&fit=max&auto=format&n=t6ucb6rQa27Wd6Te&q=85&s=0fd582cf184a36f1f2ce7dc21d7be9b9 840w, https://mintcdn.com/langchain-5e9cc07a/t6ucb6rQa27Wd6Te/langsmith/images/create-a-prompt-dark.png?w=1100&fit=max&auto=format&n=t6ucb6rQa27Wd6Te&q=85&s=32408b98f4df25413ba0e78b85c7b483 1100w, https://mintcdn.com/langchain-5e9cc07a/t6ucb6rQa27Wd6Te/langsmith/images/create-a-prompt-dark.png?w=1650&fit=max&auto=format&n=t6ucb6rQa27Wd6Te&q=85&s=102ecf375a8d66ac339b12bc163588e7 1650w, https://mintcdn.com/langchain-5e9cc07a/t6ucb6rQa27Wd6Te/langsmith/images/create-a-prompt-dark.png?w=2500&fit=max&auto=format&n=t6ucb6rQa27Wd6Te&q=85&s=3543f050b1e4e0e581c4386304284039 2500w" />
    </div>

1. Under the **Prompts** heading select the gear <Icon icon="gear" iconType="solid" /> icon next to the model name, which will launch the **Prompt Settings** window on the **Model Configuration** tab.

2. Set the [model configuration](/langsmith/managing-model-configurations) you want to use. The **Provider** and **Model** you select will determine the parameters that are configurable on this configuration page. Once set, click **Save as**.

<div style={{ textAlign: 'center' }}>
         <img className="block dark:hidden" src="https://mintcdn.com/langchain-5e9cc07a/6r3GRtwWCl4ozaHW/langsmith/images/model-config-light.png?fit=max&auto=format&n=6r3GRtwWCl4ozaHW&q=85&s=6c0f7d7012b1e5295fe545149f955e6b" alt="Model Configuration window in the LangSmith UI, settings for Provider, Model, Temperature, Max Output Tokens, Top P, Presence Penalty, Frequency Penalty, Reasoning Effort, etc." data-og-width="886" width="886" data-og-height="689" height="689" data-path="langsmith/images/model-config-light.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/6r3GRtwWCl4ozaHW/langsmith/images/model-config-light.png?w=280&fit=max&auto=format&n=6r3GRtwWCl4ozaHW&q=85&s=4e3b9ad92f6f14f4e0523bef50199318 280w, https://mintcdn.com/langchain-5e9cc07a/6r3GRtwWCl4ozaHW/langsmith/images/model-config-light.png?w=560&fit=max&auto=format&n=6r3GRtwWCl4ozaHW&q=85&s=e538eb740495a8afa8bfc552b13ae294 560w, https://mintcdn.com/langchain-5e9cc07a/6r3GRtwWCl4ozaHW/langsmith/images/model-config-light.png?w=840&fit=max&auto=format&n=6r3GRtwWCl4ozaHW&q=85&s=ebe73264e977153c869fd04d1552d09b 840w, https://mintcdn.com/langchain-5e9cc07a/6r3GRtwWCl4ozaHW/langsmith/images/model-config-light.png?w=1100&fit=max&auto=format&n=6r3GRtwWCl4ozaHW&q=85&s=2eeb01882056046bc73cc019d674af7e 1100w, https://mintcdn.com/langchain-5e9cc07a/6r3GRtwWCl4ozaHW/langsmith/images/model-config-light.png?w=1650&fit=max&auto=format&n=6r3GRtwWCl4ozaHW&q=85&s=8f28fe2fe8054cf0623fb9d17f91966f 1650w, https://mintcdn.com/langchain-5e9cc07a/6r3GRtwWCl4ozaHW/langsmith/images/model-config-light.png?w=2500&fit=max&auto=format&n=6r3GRtwWCl4ozaHW&q=85&s=cf9ad39be3623e73322d123699e73f19 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/langchain-5e9cc07a/ppc8uxWc01j4q7Ia/langsmith/images/model-config-dark.png?fit=max&auto=format&n=ppc8uxWc01j4q7Ia&q=85&s=2e9da272c3fc8f7ac958c6e6d1da85e3" alt="Model Configuration window in the LangSmith UI, settings for Provider, Model, Temperature, Max Output Tokens, Top P, Presence Penalty, Frequency Penalty, Reasoning Effort, etc." data-og-width="881" width="881" data-og-height="732" height="732" data-path="langsmith/images/model-config-dark.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ppc8uxWc01j4q7Ia/langsmith/images/model-config-dark.png?w=280&fit=max&auto=format&n=ppc8uxWc01j4q7Ia&q=85&s=652fb75a4682cfc813743a1260764e59 280w, https://mintcdn.com/langchain-5e9cc07a/ppc8uxWc01j4q7Ia/langsmith/images/model-config-dark.png?w=560&fit=max&auto=format&n=ppc8uxWc01j4q7Ia&q=85&s=02c980a8387f3d69a5870660b1668080 560w, https://mintcdn.com/langchain-5e9cc07a/ppc8uxWc01j4q7Ia/langsmith/images/model-config-dark.png?w=840&fit=max&auto=format&n=ppc8uxWc01j4q7Ia&q=85&s=ee633c06056fa7ad46ea58a179afa169 840w, https://mintcdn.com/langchain-5e9cc07a/ppc8uxWc01j4q7Ia/langsmith/images/model-config-dark.png?w=1100&fit=max&auto=format&n=ppc8uxWc01j4q7Ia&q=85&s=f62a35ed726b5f89c156a40c9ea76f2c 1100w, https://mintcdn.com/langchain-5e9cc07a/ppc8uxWc01j4q7Ia/langsmith/images/model-config-dark.png?w=1650&fit=max&auto=format&n=ppc8uxWc01j4q7Ia&q=85&s=18114575db8e6c7ce928763ddcb88c12 1650w, https://mintcdn.com/langchain-5e9cc07a/ppc8uxWc01j4q7Ia/langsmith/images/model-config-dark.png?w=2500&fit=max&auto=format&n=ppc8uxWc01j4q7Ia&q=85&s=ab24dc4975def52db55c4896ead5b77c 2500w" />
       </div>

3. Specify the input variables you would like to test in the **Inputs** box and then click <Icon icon="circle-play" iconType="solid" /> **Start**.

<div style={{ textAlign: 'center' }}>
         <img className="block dark:hidden" src="https://mintcdn.com/langchain-5e9cc07a/8DPu7MR3QecByOI5/langsmith/images/set-input-start-light.png?fit=max&auto=format&n=8DPu7MR3QecByOI5&q=85&s=bd86e76180c022a110ca0f0d9d19a198" alt="The input box with a question entered. The output box contains the response to the prompt." data-og-width="702" width="702" data-og-height="763" height="763" data-path="langsmith/images/set-input-start-light.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/8DPu7MR3QecByOI5/langsmith/images/set-input-start-light.png?w=280&fit=max&auto=format&n=8DPu7MR3QecByOI5&q=85&s=dc5f16c448685e182a0001b9dbcb1afd 280w, https://mintcdn.com/langchain-5e9cc07a/8DPu7MR3QecByOI5/langsmith/images/set-input-start-light.png?w=560&fit=max&auto=format&n=8DPu7MR3QecByOI5&q=85&s=372a5f62109cd09b57ba2ca11d73c65b 560w, https://mintcdn.com/langchain-5e9cc07a/8DPu7MR3QecByOI5/langsmith/images/set-input-start-light.png?w=840&fit=max&auto=format&n=8DPu7MR3QecByOI5&q=85&s=1fcd9d131af34af4b5e10ab5fea2a9f8 840w, https://mintcdn.com/langchain-5e9cc07a/8DPu7MR3QecByOI5/langsmith/images/set-input-start-light.png?w=1100&fit=max&auto=format&n=8DPu7MR3QecByOI5&q=85&s=f30ae237c4d2cf55918918eca39bb5e9 1100w, https://mintcdn.com/langchain-5e9cc07a/8DPu7MR3QecByOI5/langsmith/images/set-input-start-light.png?w=1650&fit=max&auto=format&n=8DPu7MR3QecByOI5&q=85&s=26326dd6839418f161f6393118f8c441 1650w, https://mintcdn.com/langchain-5e9cc07a/8DPu7MR3QecByOI5/langsmith/images/set-input-start-light.png?w=2500&fit=max&auto=format&n=8DPu7MR3QecByOI5&q=85&s=2654906d801046f11e6f96bfa7c7a59e 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/langchain-5e9cc07a/8DPu7MR3QecByOI5/langsmith/images/set-input-start-dark.png?fit=max&auto=format&n=8DPu7MR3QecByOI5&q=85&s=bfd369e7426a57fc0cad75df8dd6942d" alt="The input box with a question entered. The output box contains the response to the prompt." data-og-width="698" width="698" data-og-height="769" height="769" data-path="langsmith/images/set-input-start-dark.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/8DPu7MR3QecByOI5/langsmith/images/set-input-start-dark.png?w=280&fit=max&auto=format&n=8DPu7MR3QecByOI5&q=85&s=9538e3e173a2f19994f08865a389f247 280w, https://mintcdn.com/langchain-5e9cc07a/8DPu7MR3QecByOI5/langsmith/images/set-input-start-dark.png?w=560&fit=max&auto=format&n=8DPu7MR3QecByOI5&q=85&s=fc8dc91b4564531bb5a27b971ca27c3e 560w, https://mintcdn.com/langchain-5e9cc07a/8DPu7MR3QecByOI5/langsmith/images/set-input-start-dark.png?w=840&fit=max&auto=format&n=8DPu7MR3QecByOI5&q=85&s=ae9db3c1ec55c4653ad86b95addeec12 840w, https://mintcdn.com/langchain-5e9cc07a/8DPu7MR3QecByOI5/langsmith/images/set-input-start-dark.png?w=1100&fit=max&auto=format&n=8DPu7MR3QecByOI5&q=85&s=0f169783a075952a0066de46aeb3bdc7 1100w, https://mintcdn.com/langchain-5e9cc07a/8DPu7MR3QecByOI5/langsmith/images/set-input-start-dark.png?w=1650&fit=max&auto=format&n=8DPu7MR3QecByOI5&q=85&s=686196a3493642cf70ff89aa235a6715 1650w, https://mintcdn.com/langchain-5e9cc07a/8DPu7MR3QecByOI5/langsmith/images/set-input-start-dark.png?w=2500&fit=max&auto=format&n=8DPu7MR3QecByOI5&q=85&s=0532f0c8cf0ff37ed7c9da7a67bf6700 2500w" />
       </div>

To learn about more options for configuring your prompt in the Playground, refer to [Configure prompt settings](/langsmith/managing-model-configurations).

4. After testing and refining your prompt, click **Save** to store it for future use.

## 4. Iterate on a prompt

LangSmith allows for team-based prompt iteration. [Workspace](/langsmith/administration-overview#workspaces) members can experiment with prompts in the playground and save their changes as a new [*commit*](/langsmith/prompt-engineering-concepts#commits) when ready.

To improve your prompts:

* Reference the documentation provided by your model provider for best practices in prompt creation, such as:
      * [Best practices for prompt engineering with the OpenAI API](https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-the-openai-api)
      * [Gemini's Introduction to prompt design](https://ai.google.dev/gemini-api/docs/prompting-intro)
    * Build and refine your prompts with the Prompt Canvas—an interactive tool in LangSmith. Learn more in the [Prompt Canvas guide](/langsmith/write-prompt-with-ai).
    * Tag specific commits to mark important moments in your commit history.

1. To create a commit, navigate to the **Playground** and select **Commit**. Choose the prompt to commit changes to and then **Commit**.
      2. Navigate to **Prompts** in the left-hand menu. Select the prompt. Once on the prompt's detail page, move to the **Commits** tab. Find the tag icon <Icon icon="tag" iconType="solid" /> to **Add a Commit Tag**.

<div style={{ textAlign: 'center' }}>
        <img className="block dark:hidden" src="https://mintcdn.com/langchain-5e9cc07a/7n0eM_8e3pn_DFNx/langsmith/images/add-commit-tag-light.png?fit=max&auto=format&n=7n0eM_8e3pn_DFNx&q=85&s=80e7b49cb4036e15369c9e417d1d63ad" alt="The tag, the commit tag box with the commit label, and the commit tag name box to create the tag." data-og-width="702" width="702" data-og-height="226" height="226" data-path="langsmith/images/add-commit-tag-light.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/7n0eM_8e3pn_DFNx/langsmith/images/add-commit-tag-light.png?w=280&fit=max&auto=format&n=7n0eM_8e3pn_DFNx&q=85&s=bbed6d72fa9986d211195f7cb6e3bbc3 280w, https://mintcdn.com/langchain-5e9cc07a/7n0eM_8e3pn_DFNx/langsmith/images/add-commit-tag-light.png?w=560&fit=max&auto=format&n=7n0eM_8e3pn_DFNx&q=85&s=93623e1b7ae7b4c9826c7a76694dd43d 560w, https://mintcdn.com/langchain-5e9cc07a/7n0eM_8e3pn_DFNx/langsmith/images/add-commit-tag-light.png?w=840&fit=max&auto=format&n=7n0eM_8e3pn_DFNx&q=85&s=c9ccfe378c982af55b6781691b0c6a1b 840w, https://mintcdn.com/langchain-5e9cc07a/7n0eM_8e3pn_DFNx/langsmith/images/add-commit-tag-light.png?w=1100&fit=max&auto=format&n=7n0eM_8e3pn_DFNx&q=85&s=d9464f4655a0ba67b81916eb9e69cdbc 1100w, https://mintcdn.com/langchain-5e9cc07a/7n0eM_8e3pn_DFNx/langsmith/images/add-commit-tag-light.png?w=1650&fit=max&auto=format&n=7n0eM_8e3pn_DFNx&q=85&s=43b3ad05600b7c9e0883541acccb1cc5 1650w, https://mintcdn.com/langchain-5e9cc07a/7n0eM_8e3pn_DFNx/langsmith/images/add-commit-tag-light.png?w=2500&fit=max&auto=format&n=7n0eM_8e3pn_DFNx&q=85&s=69019a8aad0d4733333f0f69ad468171 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/langchain-5e9cc07a/7n0eM_8e3pn_DFNx/langsmith/images/add-commit-tag-dark.png?fit=max&auto=format&n=7n0eM_8e3pn_DFNx&q=85&s=80883a09fbaea8c892d0a7de88f7a6ca" alt="The tag, the commit tag box with the commit label, and the commit tag name box to create the tag." data-og-width="698" width="698" data-og-height="221" height="221" data-path="langsmith/images/add-commit-tag-dark.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/7n0eM_8e3pn_DFNx/langsmith/images/add-commit-tag-dark.png?w=280&fit=max&auto=format&n=7n0eM_8e3pn_DFNx&q=85&s=0c7eb9c966673a410811a5a08a39cbf7 280w, https://mintcdn.com/langchain-5e9cc07a/7n0eM_8e3pn_DFNx/langsmith/images/add-commit-tag-dark.png?w=560&fit=max&auto=format&n=7n0eM_8e3pn_DFNx&q=85&s=9976856ffeca88d9f8d856e0d7766613 560w, https://mintcdn.com/langchain-5e9cc07a/7n0eM_8e3pn_DFNx/langsmith/images/add-commit-tag-dark.png?w=840&fit=max&auto=format&n=7n0eM_8e3pn_DFNx&q=85&s=9a79984ca88bd3fa74b512ce8b26430b 840w, https://mintcdn.com/langchain-5e9cc07a/7n0eM_8e3pn_DFNx/langsmith/images/add-commit-tag-dark.png?w=1100&fit=max&auto=format&n=7n0eM_8e3pn_DFNx&q=85&s=82c269188447b9ecf9f7ff17fa9805ec 1100w, https://mintcdn.com/langchain-5e9cc07a/7n0eM_8e3pn_DFNx/langsmith/images/add-commit-tag-dark.png?w=1650&fit=max&auto=format&n=7n0eM_8e3pn_DFNx&q=85&s=83e03a2d056fc81440e6f11acfa1479c 1650w, https://mintcdn.com/langchain-5e9cc07a/7n0eM_8e3pn_DFNx/langsmith/images/add-commit-tag-dark.png?w=2500&fit=max&auto=format&n=7n0eM_8e3pn_DFNx&q=85&s=cc7d694fc1cc6c4a76c7dca4a9ed9dc4 2500w" />
      </div>
  </Tab>

<Tab title="SDK" icon="code">
    ## 1. Set up your environment

1. In your terminal, prepare your environment:

2. Set your API keys:

## 2. Create a prompt

To create a prompt, you'll define a list of messages that you want in your prompt and then push to LangSmith.

Use the language-specific constructor and push method:

* Python: [`ChatPromptTemplate`](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) → [`client.push_prompt(...)`](https://docs.smith.langchain.com/reference/python/client/langsmith.client.Client#langsmith.client.Client.push_prompt)
    * TypeScript: [`ChatPromptTemplate.fromMessages(...)`](https://v03.api.js.langchain.com/classes/_langchain_core.prompts.ChatPromptTemplate.html#fromMessages) → [`client.pushPrompt(...)`](https://langsmith-docs-7jgx2bq8f-langchain.vercel.app/reference/js/classes/client.Client#pushprompt)

1. Add the following code to a `create_prompt` file:

This creates an ordered list of messages, wraps them in `ChatPromptTemplate`, and then pushes the prompt by name to your [workspace](/langsmith/administration-overview#workspaces) for versioning and reuse.

2. Run `create_prompt`:

Follow the resulting link to view the newly created Prompt Hub prompt in the LangSmith UI.

In this step, you'll pull the prompt you created in [step 2](#2-create-a-prompt) by name (`"prompt-quickstart"`), format it with a test input, convert it to OpenAI’s chat format, and call the OpenAI Chat Completions API.

Then, you'll iterate on the prompt by creating a new version. Members of your workspace can open an existing prompt, experiment with changes in the [UI](https://smith.langchain.com), and save those changes as a new commit on the same prompt, which preserves history for the whole team.

1. Add the following to a `test_prompt` file:

This loads the prompt by name using `pull` for the latest committed version of the prompt that you're testing. You can also specify a specific commit by passing the commit hash `"<prompt-name>:<commit-hash>"`

2. Run `test_prompt` :

3. To create a new version of a prompt, call the same push method you used initially with the same prompt name and your updated template. LangSmith will record it as a new commit and preserve prior versions.

Copy the following code to an `iterate_prompt` file:

4. Run `iterate_prompt` :

Now your prompt will contain two commits.

To improve your prompts:

* Reference the documentation provided by your model provider for best practices in prompt creation, such as:
      * [Best practices for prompt engineering with the OpenAI API](https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-the-openai-api)
      * [Gemini's Introduction to prompt design](https://ai.google.dev/gemini-api/docs/prompting-intro)
    * Build and refine your prompts with the Prompt Canvas—an interactive tool in LangSmith. Learn more in the [Prompt Canvas guide](/langsmith/write-prompt-with-ai).
  </Tab>
</Tabs>

* Learn more about how to store and manage prompts using the Prompt Hub in the [Create a prompt guide](/langsmith/create-a-prompt).
* Learn how to set up the Playground to [Test multi-turn conversations](/langsmith/multiple-messages) in this tutorial.
* Learn how to test your prompt's performance over a dataset instead of individual examples, refer to [Run an evaluation from the Prompt Playground](/langsmith/run-evaluation-from-prompt-playground).

<iframe className="w-full aspect-video rounded-xl" src="https://www.youtube.com/embed/h4f6bIWGkog?si=IVJFfhldC7M3HL4G" title="YouTube video player" frameBorder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowFullScreen />

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/prompt-engineering-quickstart.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

    2. Set your API keys:
```

Example 3 (unknown):
```unknown
## 2. Create a prompt

    To create a prompt, you'll define a list of messages that you want in your prompt and then push to LangSmith.

    Use the language-specific constructor and push method:

    * Python: [`ChatPromptTemplate`](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) → [`client.push_prompt(...)`](https://docs.smith.langchain.com/reference/python/client/langsmith.client.Client#langsmith.client.Client.push_prompt)
    * TypeScript: [`ChatPromptTemplate.fromMessages(...)`](https://v03.api.js.langchain.com/classes/_langchain_core.prompts.ChatPromptTemplate.html#fromMessages) → [`client.pushPrompt(...)`](https://langsmith-docs-7jgx2bq8f-langchain.vercel.app/reference/js/classes/client.Client#pushprompt)

    1. Add the following code to a `create_prompt` file:

       <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Property access

**URL:** llms-txt#property-access

---

## Provider-native format (e.g., OpenAI)

**URL:** llms-txt#provider-native-format-(e.g.,-openai)

human_message = HumanMessage(content=[
    {"type": "text", "text": "Hello, how are you?"},
    {"type": "image_url", "image_url": {"url": "https://example.com/image.jpg"}}
])

---

## Publicly available test files

**URL:** llms-txt#publicly-available-test-files

pdf_url = "https://www.w3.org/WAI/ER/tests/xhtml/testfiles/resources/pdf/dummy.pdf"
wav_url = "https://openaiassets.blob.core.windows.net/$web/API/docs/audio/alloy.wav"
img_url = "https://www.w3.org/Graphics/PNG/nurbcup2si.png"

---

## Publish an integration

**URL:** llms-txt#publish-an-integration

Source: https://docs.langchain.com/oss/javascript/contributing/publish-langgraph

**Make your integration available to the community.**

Now that your package is implemented and tested, you can publish it and add documentation to make it discoverable by the community.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/contributing/publish-langgraph.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Pull the images from the public registry

**URL:** llms-txt#pull-the-images-from-the-public-registry

**Contents:**
- Configuration

docker pull langchain/langsmith-backend:latest
docker tag langchain/langsmith-backend:latest <your-registry>/langsmith-backend:latest
docker push <your-registry>/langsmith-backend:latest
yaml Helm theme={null}
  images:
    imagePullSecrets: [] # Add your image pull secrets here if needed
    registry: "" # Set this to your registry URL if you mirrored all images to the same registry using our script. Then you can remove the repository prefix from the images below.
    aceBackendImage:
      repository: "(your-registry)/langchain/langsmith-ace-backend"
      pullPolicy: IfNotPresent
      tag: "0.10.66"
    backendImage:
      repository: "(your-registry)/langchain/langsmith-backend"
      pullPolicy: IfNotPresent
      tag: "0.10.66"
    frontendImage:
      repository: "(your-registry)/langchain/langsmith-frontend"
      pullPolicy: IfNotPresent
      tag: "0.10.66"
    hostBackendImage:
      repository: "(your-registry)/langchain/hosted-langserve-backend"
      pullPolicy: IfNotPresent
      tag: "0.10.66"
    operatorImage:
      repository: "(your-registry)/langchain/langgraph-operator"
      pullPolicy: IfNotPresent
      tag: "6cc83a8"
    platformBackendImage:
      repository: "(your-registry)/langchain/langsmith-go-backend"
      pullPolicy: IfNotPresent
      tag: "0.10.66"
    playgroundImage:
      repository: "(your-registry)/langchain/langsmith-playground"
      pullPolicy: IfNotPresent
      tag: "0.10.66"
    postgresImage:
      repository: "(your-registry)/postgres"
      pullPolicy: IfNotPresent
      tag: "14.7"
    redisImage:
      repository: "(your-registry)/redis"
      pullPolicy: IfNotPresent
      tag: "7"
    clickhouseImage:
      repository: "(your-registry)/clickhouse/clickhouse-server"
      pullPolicy: Always
      tag: "24.8"
  bash Docker theme={null}
  # In your .env file
  _REGISTRY=your-registry # Set this to your registry URL if you mirrored all images to the same registry using our script. Otherwise you will need to manually set the repository for each image in the compose file.
  ```
</CodeGroup>

Once configured, you will need to update your LangSmith installation. You can follow our upgrade guide here: [Upgrading LangSmith](/langsmith/self-host-upgrades).If your upgrade is successful, your LangSmith instance should now be using the mirrored images from your Docker registry.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/self-host-mirroring-images.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
You will need to repeat this for each image that you want to mirror.

## Configuration

Once the images are mirrored, you will need to configure your LangSmith installation to use the mirrored images. You can do this by modifying the `values.yaml` file for your LangSmith Helm Chart installation or the `.env` file for your Docker installation. Replace tag with the version you want to use, e.g. `0.10.66` for the latest version at the time of writing.

<CodeGroup>
```

Example 2 (unknown):
```unknown

```

---

## Push to your container registry

**URL:** llms-txt#push-to-your-container-registry

**Contents:**
  - Connect to Your Deployed Agent
  - Environment configuration

docker push my-agent:latest
bash  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
You can push to any container registry (Docker Hub, AWS ECR, Azure ACR, Google GCR, etc.) that your deployment environment has access to.

**Supported deployments:**

* <Icon icon="cloud" /> **Cloud LangSmith**: Use the Control Plane API to create deployments from your GitHub repository
* <Icon icon="server" /> **Self-Hosted/Hybrid LangSmith**: Use the Control Plane API to create deployments from your container registry

See the [LangGraph CLI build documentation](/langsmith/cli#build) for more details.

### Connect to Your Deployed Agent

* <Icon icon="code" /> **[LangGraph SDK](https://langchain-ai.github.io/langgraph/cloud/reference/sdk/python_sdk_ref/#langgraph-sdk-python)**: Use the LangGraph SDK for programmatic integration.
* <Icon icon="project-diagram" /> **[RemoteGraph](/langsmith/use-remote-graph)**: Connect using RemoteGraph for remote graph connections (to use your graph in other graphs).
* <Icon icon="globe" /> **[REST API](/langsmith/server-api-ref)**: Use HTTP-based interactions with your deployed agent.
* <Icon icon="desktop" /> **[Studio](/langsmith/studio)**: Access the visual interface for testing and debugging.

### Environment configuration

#### Database & cache configuration

By default, LangSmith Deployments create PostgreSQL and Redis instances for you. To use external services, set the following environment variables in your new deployment or revision:
```

---

## Python >= 3.11 is required.

**URL:** llms-txt#python->=-3.11-is-required.

**Contents:**
  - 2. Prepare your agent
  - 3. Environment variables
  - 4. Create a LangGraph config file
  - 5. Install dependencies
  - 6. View your agent in Studio

pip install --upgrade "langgraph-cli[inmem]"
python title="agent.py" theme={null}
from langchain.agents import create_agent

def send_email(to: str, subject: str, body: str):
    """Send an email"""
    email = {
        "to": to,
        "subject": subject,
        "body": body
    }
    # ... email sending logic

return f"Email sent to {to}"

agent = create_agent(
    "gpt-4o",
    tools=[send_email],
    system_prompt="You are an email assistant. Always use the send_email tool.",
)
bash .env theme={null}
LANGSMITH_API_KEY=lsv2...
json title="langgraph.json" theme={null}
{
  "dependencies": ["."],
  "graphs": {
    "agent": "./src/agent.py:agent"
  },
  "env": ".env"
}
bash  theme={null}
my-app/
├── src
│   └── agent.py
├── .env
└── langgraph.json
shell pip theme={null}
  pip install -e .
  shell uv theme={null}
  uv sync
  shell  theme={null}
langgraph dev
```

<Warning>
  Safari blocks `localhost` connections to Studio. To work around this, run the above command with `--tunnel` to access Studio via a secure tunnel.
</Warning>

Your agent will be accessible via API (`http://127.0.0.1:2024`) and the Studio UI `https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024`:

<Frame>
    <img src="https://mintcdn.com/langchain-5e9cc07a/TCDks4pdsHdxWmuJ/oss/images/studio_create-agent.png?fit=max&auto=format&n=TCDks4pdsHdxWmuJ&q=85&s=ebd259e9fa24af7d011dfcc568f74be2" alt="Agent view in the Studio UI" data-og-width="2836" width="2836" data-og-height="1752" height="1752" data-path="oss/images/studio_create-agent.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/TCDks4pdsHdxWmuJ/oss/images/studio_create-agent.png?w=280&fit=max&auto=format&n=TCDks4pdsHdxWmuJ&q=85&s=cf9c05bdd08661d4d546c540c7a28cbe 280w, https://mintcdn.com/langchain-5e9cc07a/TCDks4pdsHdxWmuJ/oss/images/studio_create-agent.png?w=560&fit=max&auto=format&n=TCDks4pdsHdxWmuJ&q=85&s=484b2fd56957d048bd89280ce97065a0 560w, https://mintcdn.com/langchain-5e9cc07a/TCDks4pdsHdxWmuJ/oss/images/studio_create-agent.png?w=840&fit=max&auto=format&n=TCDks4pdsHdxWmuJ&q=85&s=92991302ac24604022ab82ac22729f68 840w, https://mintcdn.com/langchain-5e9cc07a/TCDks4pdsHdxWmuJ/oss/images/studio_create-agent.png?w=1100&fit=max&auto=format&n=TCDks4pdsHdxWmuJ&q=85&s=ed366abe8dabc42a9d7c300a591e1614 1100w, https://mintcdn.com/langchain-5e9cc07a/TCDks4pdsHdxWmuJ/oss/images/studio_create-agent.png?w=1650&fit=max&auto=format&n=TCDks4pdsHdxWmuJ&q=85&s=d5865d3c4b0d26e9d72e50d474547a63 1650w, https://mintcdn.com/langchain-5e9cc07a/TCDks4pdsHdxWmuJ/oss/images/studio_create-agent.png?w=2500&fit=max&auto=format&n=TCDks4pdsHdxWmuJ&q=85&s=6b254add2df9cc3c10ac0c2bcb3a589c 2500w" />
</Frame>

Studio makes each step of your agent easily observable. Replay any input and inspect the exact prompt, tool arguments, return values, and token/latency metrics. If a tool throws an exception, Studio records it with surrounding state so you can spend less time debugging.

Keep your dev server running, edit prompts or tool signatures, and watch Studio hot-reload. Re-run the conversation thread from any step to verify behavior changes. See [Manage threads](/langsmith/use-studio#edit-thread-history) for more details.

As your agent grows, the same view scales from a single-tool demo to multi-node graphs, keeping decisions legible and reproducible.

<Tip>
  For an in-depth look at Studio, check out the [overview page](/langsmith/studio).
</Tip>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/studio.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
### 2. Prepare your agent

We'll use the following simple agent as an example:
```

Example 2 (unknown):
```unknown
### 3. Environment variables

Create a `.env` file in the root of your project and fill in the necessary API keys. We'll need to set the `LANGSMITH_API_KEY` environment variable to the API key you get from [LangSmith](https://smith.langchain.com/settings).

<Warning>
  Be sure not to commit your `.env` to version control systems such as Git!
</Warning>
```

Example 3 (unknown):
```unknown
### 4. Create a LangGraph config file

Inside your app's directory, create a configuration file `langgraph.json`:
```

Example 4 (unknown):
```unknown
[`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent) automatically returns a compiled LangGraph graph that we can pass to the `graphs` key in our configuration file.

<Info>
  See the [LangGraph configuration file reference](/langsmith/cli#configuration-file) for detailed explanations of each key in the JSON object of the configuration file.
</Info>

So far, our project structure looks like this:
```

---

## Query traces (SDK)

**URL:** llms-txt#query-traces-(sdk)

**Contents:**
- Use filter arguments
  - List all runs in a project
  - List LLM and Chat runs in the last 24 hours
  - List root runs in a project
  - List runs without errors
  - List runs by run ID
- Use filter query language
  - List all root runs in a conversational thread
  - List all runs called "extractor" whose root of the trace was assigned feedback "user\_score" score of 1
  - List runs with "star\_rating" key whose score is greater than 4

Source: https://docs.langchain.com/langsmith/export-traces

<Tip>
  **Recommended Reading**

Before diving into this content, it might be helpful to read the following:

* [Run (span) data format](/langsmith/run-data-format)
  * <RegionalUrl type="api" suffix="/redoc" text="LangSmith API Reference" />
  * [LangSmith trace query syntax](/langsmith/trace-query-syntax)
</Tip>

<Note>
  **If you are looking to export a large volume of traces, we recommend that you use the [Bulk Data Export](./data-export) functionality, as it will better handle large data volumes and will support automatic retries and parallelization across partitions.**
</Note>

The recommended way to query runs (the span data in LangSmith traces) is to use the `list_runs` method in the SDK or `/runs/query` endpoint in the API.

LangSmith stores traces in a simple format that is specified in the [Run (span) data format](/langsmith/run-data-format).

## Use filter arguments

For simple queries, you don't have to rely on our query syntax. You can use the filter arguments specified in the [filter arguments reference](/langsmith/trace-query-syntax#filter-arguments).

<Warning>
  **Prerequisites**

Initialize the client before running the below code snippets.
</Warning>

Below are some examples of ways to list runs using keyword arguments:

### List all runs in a project

### List LLM and Chat runs in the last 24 hours

### List root runs in a project

Root runs are runs that have no parents. These are assigned a value of `True` for `is_root`. You can use this to filter for root runs.

### List runs without errors

### List runs by run ID

<Warning>
  **Ignores Other Arguments**

If you provide a list of run IDs in the way described above, it will ignore all other filtering arguments like `project_name`, `run_type`, etc. and directly return the runs matching the given IDs.
</Warning>

If you have a list of run IDs, you can list them directly:

## Use filter query language

For more complex queries, you can use the query language described in the [filter query language reference](/langsmith/trace-query-syntax#filter-query-language).

### List all root runs in a conversational thread

This is the way to fetch runs in a conversational thread. For more information on setting up threads, refer to our [how-to guide on setting up threads](./threads).
Threads are grouped by setting a shared thread ID. The LangSmith UI lets you use any one of the following three metadata keys: `session_id`, `conversation_id`, or `thread_id`. The session ID is also known as the tracing project ID. The following query matches on any of them.

### List all runs called "extractor" whose root of the trace was assigned feedback "user\_score" score of 1

### List runs with "star\_rating" key whose score is greater than 4

### List runs that took longer than 5 seconds to complete

### List all runs that have "error" not equal to null

### List all runs where start\_time is greater than a specific timestamp

### List all runs that contain the string "substring"

### List all runs that are tagged with the git hash "2aa1cf4"

### List all runs that started after a specific timestamp and either have "error" not equal to null or a "Correctness" feedback score equal to 0

### Complex query: List all runs where tags include "experimental" or "beta" and latency is greater than 2 seconds

### Search trace trees by full text

You can use the `search()` function without any specific field to do a full text search across all string fields in a run. This allows you to quickly find traces that match a search term.

### Check for presence of metadata

If you want to check for the presence of metadata, you can use the `eq` operator, optionally with an `and` statement to match by value. This is useful if you want to log more structured information about your runs.

### Check for environment details in metadata

A common pattern is to add environment information to your traces via metadata. If you want to filter for runs containing environment metadata, you can use the same pattern as above:

### Check for conversation ID in metadata

Another common way to associate traces in the same conversation is by using a shared conversation ID. If you want to filter runs based on a conversation ID in this way, you can search for that ID in the metadata.

### Negative filtering on key-value pairs

You can use negative filtering on metadata, input, and output key-value pairs to exclude specific runs from your results. Here are some examples for metadata key-value pairs but the same logic applies to input and output key-value pairs.

### Combine multiple filters

If you want to combine multiple conditions to refine your search, you can use the `and` operator along with other filtering functions. Here's how you can search for runs named "ChatOpenAI" that also have a specific `conversation_id` in their metadata:

List all runs named "RetrieveDocs" whose root run has a "user\_score" feedback of 1 and any run in the full trace is named "ExpandQuery".

This type of query is useful if you want to extract a specific run conditional on various states or steps being reached within the trace.

### Advanced: export flattened trace view with child tool usage

The following Python example demonstrates how to export a flattened view of traces, including information on the tools (from nested runs) used by the agent within each trace.
This can be used to analyze the behavior of your agents across multiple traces.

This example queries all tool runs within a specified number of days and groups them by their parent (root) run ID. It then fetches the relevant information for each root run, such as the run name, inputs, outputs, and combines that information with the child run information.

To optimize the query, the example:

1. Selects only the necessary fields when querying tool runs to reduce query time.
2. Fetches root runs in batches while processing tool runs concurrently.

<CodeGroup>
  
</CodeGroup>

### Advanced: export retriever IO for traces with feedback

This query is useful if you want to fine-tune embeddings or diagnose end-to-end system performance issues based on retriever behavior.
The following Python example demonstrates how to export retriever inputs and outputs within traces that have a specific feedback score.

<CodeGroup>
  
</CodeGroup>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/export-traces.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

Below are some examples of ways to list runs using keyword arguments:

### List all runs in a project

<CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>

### List LLM and Chat runs in the last 24 hours

<CodeGroup>
```

---

## Quickstart

**URL:** llms-txt#quickstart

Source: https://docs.langchain.com/oss/javascript/langgraph/quickstart

This quickstart demonstrates how to build a calculator agent using the LangGraph Graph API or the Functional API.

* [Use the Graph API](#use-the-graph-api) if you prefer to define your agent as a graph of nodes and edges.
* [Use the Functional API](#use-the-functional-api) if you prefer to define your agent as a single function.

For conceptual information, see [Graph API overview](/oss/javascript/langgraph/graph-api) and [Functional API overview](/oss/javascript/langgraph/functional-api).

<Info>
  For this example, you will need to set up a [Claude (Anthropic)](https://www.anthropic.com/) account and get an API key. Then, set the `ANTHROPIC_API_KEY` environment variable in your terminal.
</Info>

<Tabs>
  <Tab title="Use the Graph API">
    ## 1. Define tools and model

In this example, we'll use the Claude Sonnet 4.5 model and define tools for addition, multiplication, and division.

The graph's state is used to store the messages and the number of LLM calls.

<Tip>
      State in LangGraph persists throughout the agent's execution.

The `Annotated` type with `operator.add` ensures that new messages are appended to the existing list rather than replacing it.
    </Tip>

## 3. Define model node

The model node is used to call the LLM and decide whether to call a tool or not.

## 4. Define tool node

The tool node is used to call the tools and return the results.

## 5. Define end logic

The conditional edge function is used to route to the tool node or end based upon whether the LLM made a tool call.

## 6. Build and compile the agent

The agent is built using the [`StateGraph`](https://langchain-ai.github.io/langgraphjs/reference/classes/langgraph.StateGraph.html) class and compiled using the @\[`compile`]\[StateGraph.compile] method.

<Tip>
      To learn how to trace your agent with LangSmith, see the [LangSmith documentation](/langsmith/trace-with-langgraph).
    </Tip>

Congratulations! You've built your first agent using the LangGraph Graph API.

<Accordion title="Full code example">
      
    </Accordion>
  </Tab>

<Tab title="Use the Functional API">
    ## 1. Define tools and model

In this example, we'll use the Claude Sonnet 4.5 model and define tools for addition, multiplication, and division.

## 2. Define model node

The model node is used to call the LLM and decide whether to call a tool or not.

<Tip>
      The @\[`@task`] decorator marks a function as a task that can be executed as part of the agent. Tasks can be called synchronously or asynchronously within your entrypoint function.
    </Tip>

## 3. Define tool node

The tool node is used to call the tools and return the results.

The agent is built using the @\[`@entrypoint`] function.

<Note>
      In the Functional API, instead of defining nodes and edges explicitly, you write standard control flow logic (loops, conditionals) within a single function.
    </Note>

<Tip>
      To learn how to trace your agent with LangSmith, see the [LangSmith documentation](/langsmith/trace-with-langgraph).
    </Tip>

Congratulations! You've built your first agent using the LangGraph Functional API.

<Accordion title="Full code example" icon="code">
      
    </Accordion>
  </Tab>
</Tabs>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/quickstart.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
## 2. Define state

    The graph's state is used to store the messages and the number of LLM calls.

    <Tip>
      State in LangGraph persists throughout the agent's execution.

      The `Annotated` type with `operator.add` ensures that new messages are appended to the existing list rather than replacing it.
    </Tip>
```

Example 2 (unknown):
```unknown
## 3. Define model node

    The model node is used to call the LLM and decide whether to call a tool or not.
```

Example 3 (unknown):
```unknown
## 4. Define tool node

    The tool node is used to call the tools and return the results.
```

Example 4 (unknown):
```unknown
## 5. Define end logic

    The conditional edge function is used to route to the tool node or end based upon whether the LLM made a tool call.
```

---

## Reactive Approach (fallback)

**URL:** llms-txt#reactive-approach-(fallback)

**Contents:**
- Visualization

try:
    result = graph.invoke(initial_state, {"recursion_limit": 10})
except GraphRecursionError as e:
    # Handle externally after graph execution fails
    result = fallback_handler(initial_state)
python  theme={null}
def inspect_metadata(state: dict, config: RunnableConfig) -> dict:
    metadata = config["metadata"]

print(f"Step: {metadata['langgraph_step']}")
    print(f"Node: {metadata['langgraph_node']}")
    print(f"Triggers: {metadata['langgraph_triggers']}")
    print(f"Path: {metadata['langgraph_path']}")
    print(f"Checkpoint NS: {metadata['langgraph_checkpoint_ns']}")

It's often nice to be able to visualize graphs, especially as they get more complex. LangGraph comes with several built-in ways to visualize graphs. See [this how-to guide](/oss/python/langgraph/use-graph-api#visualize-your-graph) for more info.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/graph-api.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
The key differences between these approaches are:

| Approach                                  | Detection            | Handling                             | Control Flow                       |
| ----------------------------------------- | -------------------- | ------------------------------------ | ---------------------------------- |
| Proactive (using `langgraph_step`)        | Before limit reached | Inside graph via conditional routing | Graph continues to completion node |
| Reactive (catching `GraphRecursionError`) | After limit exceeded | Outside graph in try/catch           | Graph execution terminated         |

**Proactive advantages:**

* Graceful degradation within the graph
* Can save intermediate state in checkpoints
* Better user experience with partial results
* Graph completes normally (no exception)

**Reactive advantages:**

* Simpler implementation
* No need to modify graph logic
* Centralized error handling

#### Other available metadata

Along with `langgraph_step`, the following metadata is also available in `config["metadata"]`:
```

---

## Reading a thread. Since this is also more specific than the generic @auth.on handler, and the @auth.on.threads handler,

**URL:** llms-txt#reading-a-thread.-since-this-is-also-more-specific-than-the-generic-@auth.on-handler,-and-the-@auth.on.threads-handler,

---

## Rebuild graph at runtime

**URL:** llms-txt#rebuild-graph-at-runtime

**Contents:**
- Prerequisites
- Define graphs
  - No rebuild
  - Rebuild

Source: https://docs.langchain.com/langsmith/graph-rebuild

You might need to rebuild your graph with a different configuration for a new run. For example, you might need to use a different graph state or graph structure depending on the config. This guide shows how you can do this.

<Note>
  **Note**
  In most cases, customizing behavior based on the config should be handled by a single graph where each node can read a config and change its behavior based on it
</Note>

Make sure to check out [this how-to guide](/langsmith/setup-app-requirements-txt) on setting up your app for deployment first.

Let's say you have an app with a simple graph that calls an LLM and returns the response to the user. The app file directory looks like the following:

where the graph is defined in `openai_agent.py`.

In the standard LangGraph API configuration, the server uses the compiled graph instance that's defined at the top level of `openai_agent.py`, which looks like the following:

To make the server aware of your graph, you need to specify a path to the variable that contains the [`CompiledStateGraph`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.CompiledStateGraph) instance in your LangGraph API configuration (`langgraph.json`), e.g.:

To make your graph rebuild on each new run with custom configuration, you need to rewrite `openai_agent.py` to instead provide a *function* that takes a config and returns a graph (or compiled graph) instance. Let's say we want to return our existing graph for user ID '1', and a tool-calling agent for other users. We can modify `openai_agent.py` as follows:

```python  theme={null}
from typing import Annotated
from typing_extensions import TypedDict
from langchain_openai import ChatOpenAI
from langgraph.graph import END, START, MessageGraph
from langgraph.graph.state import StateGraph
from langgraph.graph.message import add_messages
from langchain.tools import tool
from langgraph.prebuilt import ToolNode
from langchain.messages import BaseMessage
from langchain_core.runnables import RunnableConfig

class State(TypedDict):
    messages: Annotated[list[BaseMessage], add_messages]

model = ChatOpenAI(temperature=0)

def make_default_graph():
    """Make a simple LLM agent"""
    graph_workflow = StateGraph(State)
    def call_model(state):
        return {"messages": [model.invoke(state["messages"])]}

graph_workflow.add_node("agent", call_model)
    graph_workflow.add_edge("agent", END)
    graph_workflow.add_edge(START, "agent")

agent = graph_workflow.compile()
    return agent

def make_alternative_graph():
    """Make a tool-calling agent"""

@tool
    def add(a: float, b: float):
        """Adds two numbers."""
        return a + b

tool_node = ToolNode([add])
    model_with_tools = model.bind_tools([add])
    def call_model(state):
        return {"messages": [model_with_tools.invoke(state["messages"])]}

def should_continue(state: State):
        if state["messages"][-1].tool_calls:
            return "tools"
        else:
            return END

graph_workflow = StateGraph(State)

graph_workflow.add_node("agent", call_model)
    graph_workflow.add_node("tools", tool_node)
    graph_workflow.add_edge("tools", "agent")
    graph_workflow.add_edge(START, "agent")
    graph_workflow.add_conditional_edges("agent", should_continue)

agent = graph_workflow.compile()
    return agent

**Examples:**

Example 1 (unknown):
```unknown
my-app/
|-- requirements.txt
|-- .env
|-- openai_agent.py     # code for your graph
```

Example 2 (unknown):
```unknown
To make the server aware of your graph, you need to specify a path to the variable that contains the [`CompiledStateGraph`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.CompiledStateGraph) instance in your LangGraph API configuration (`langgraph.json`), e.g.:
```

Example 3 (unknown):
```unknown
### Rebuild

To make your graph rebuild on each new run with custom configuration, you need to rewrite `openai_agent.py` to instead provide a *function* that takes a config and returns a graph (or compiled graph) instance. Let's say we want to return our existing graph for user ID '1', and a tool-calling agent for other users. We can modify `openai_agent.py` as follows:
```

---

## Receivers

**URL:** llms-txt#receivers

**Contents:**
- Logs
- Metrics
  - Traces
- Processors
  - Recommended OTEL Processors
- Exporters

This is an example for a ***Sidecar*** collector to read logs from its own pod, excluding logs from non domain-specific containers. A Sidecar configuration is useful here because we require access to every container's filesystem. A DaemonSet can also be used.

<Info>
  **This configuration requires 'get', 'list', and 'watch' permissions on pods in the given namespace.**
</Info>

Metrics can be scraped using the Prometheus endpoints. A single instance ***Gateway*** collector can be be used to avoid duplication of queries when fetching metrics. The following config scrapes all of the default named LangSmith services:

<Info>
  **This configuration requires 'get', 'list', and 'watch' permissions on pods, services and endpoints in the given namespace.**
</Info>

For traces, you need to enable the OTLP receiver. The following configuration can be used to listen to HTTP traces on port 4318, and GRPC on port 4317:

### Recommended OTEL Processors

The following processors are recommended when using the OTel collector:

* [Batch Processor](https://github.com/open-telemetry/opentelemetry-collector/blob/main/processor/batchprocessor/README.md): Groups the data into batches before sending to exporters.
* [Memory Limiter](https://github.com/open-telemetry/opentelemetry-collector/blob/main/processor/memorylimiterprocessor/README.md): Prevents the collector from using too much memory and crashing. When the soft limit is crossed, the collector stops accepting new data.
* [Kubernetes Attributes Processor](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/k8sattributesprocessor): Adds Kubernetes metadata such as pod name into the telemetry data.

Exporters just need to point to an external endpoint of your liking. The following configuration allows you to configure a separate endpoint for logs, metrics and traces:

<Note>
  **The OTel Collector also supports exporting directly to a [Datadog](https://docs.datadoghq.com/opentelemetry/setup/collector_exporter) endpoint.**
</Note>

**Examples:**

Example 1 (unknown):
```unknown
<Info>
  **This configuration requires 'get', 'list', and 'watch' permissions on pods in the given namespace.**
</Info>

## Metrics

Metrics can be scraped using the Prometheus endpoints. A single instance ***Gateway*** collector can be be used to avoid duplication of queries when fetching metrics. The following config scrapes all of the default named LangSmith services:
```

Example 2 (unknown):
```unknown
<Info>
  **This configuration requires 'get', 'list', and 'watch' permissions on pods, services and endpoints in the given namespace.**
</Info>

### Traces

For traces, you need to enable the OTLP receiver. The following configuration can be used to listen to HTTP traces on port 4318, and GRPC on port 4317:
```

Example 3 (unknown):
```unknown
## Processors

### Recommended OTEL Processors

The following processors are recommended when using the OTel collector:

* [Batch Processor](https://github.com/open-telemetry/opentelemetry-collector/blob/main/processor/batchprocessor/README.md): Groups the data into batches before sending to exporters.
* [Memory Limiter](https://github.com/open-telemetry/opentelemetry-collector/blob/main/processor/memorylimiterprocessor/README.md): Prevents the collector from using too much memory and crashing. When the soft limit is crossed, the collector stops accepting new data.
* [Kubernetes Attributes Processor](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/k8sattributesprocessor): Adds Kubernetes metadata such as pod name into the telemetry data.

## Exporters

Exporters just need to point to an external endpoint of your liking. The following configuration allows you to configure a separate endpoint for logs, metrics and traces:
```

---

## Reference

**URL:** llms-txt#reference

**Contents:**
- Reference sites

Source: https://docs.langchain.com/oss/javascript/reference/overview

Comprehensive API reference documentation for the LangChain and LangGraph Python and TypeScript libraries.

<CardGroup cols={2}>
  <Card title="LangChain" icon="link" href="https://reference.langchain.com/javascript/modules/langchain.html">
    Complete API reference for LangChain JavaScript/TypeScript, including chat models, tools, agents, and more.
  </Card>

<Card title="LangGraph" icon="diagram-project" href="https://reference.langchain.com/javascript/modules/_langchain_langgraph.html">
    Complete API reference for LangGraph JavaScript/TypeScript, including graph APIs, state management, checkpointing, and more.
  </Card>
</CardGroup>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/reference/overview.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Regions FAQ

**URL:** llms-txt#regions-faq

**Contents:**
- Legal and compliance
- Features
- Plans and pricing

Source: https://docs.langchain.com/langsmith/regions-faq

<Note>
  See the [cloud architecture reference](/langsmith/cloud#architecture) for additional details.
</Note>

## Legal and compliance

#### *What privacy and data protection frameworks does LangSmith, including its EU instance, comply with?*

LangSmith complies with the General Data Protection Regulation (GDPR) and other laws and regulations applicable to the LangSmith service. We are also SOC 2 Type 2 certified and are HIPAA compliant. You can request more information about our security policies and posture at [trust.langchain.com](https://trust.langchain.com). If you would like to sign a Data Processing Addendum (DPA) with us, please reach out to [support@langchain.dev](mailto:support@langchain.dev). Please note we only enter into Business Associate Agreements (BAAs) with customers on our Enterprise plan.

#### *My company isn't based in the EU, can I still have my data hosted there?*

Yes, you can host your LangSmith data in the EU instance independent of your location.

#### *Do you have a legal entity in the EU that we can contract with?*

We do not have a legal entity in the EU for customer contracting today.

#### *Do different legal terms apply if I choose the EU region?*

The terms are the same for the EU and US regions.

#### *How do I use the EU instance?*

Follow the instructions [here](/langsmith/create-account-api-key) to create an account and an API key (make sure to change the region to EU in the dropdown)

#### *Are there any functional differences between US and EU cloud-managed LangSmith?*

There may be a small delay between launches to each region depending on the feature. Besides that, they are functionally equivalent - all features supported in the US are supported in the EU and vice versa.

#### *Can an organization have workspaces in different regions?*

LangSmith does not support this at the moment, but if you are interested, please contact [support@langchain.dev](mailto:support@langchain.dev) and share your use case.

#### *Can I connect an EU organization to a US organization and share billing?*

LangSmith does not support this at the moment, but if you are interested, please contact [support@langchain.dev](mailto:support@langchain.dev) and share your use case.

#### *What data will be stored in my selected region?*

See the [cloud architecture reference](/langsmith/cloud#architecture) for details.

#### *How can I see my organization's region?*

Check your URL - any organizations on [https://eu.smith.langchain.com](https://eu.smith.langchain.com) are in the EU, and any on [https://smith.langchain.com](https://smith.langchain.com) are in the US.

#### *Can I switch my organization from the US to EU or vice versa?*

We do not support migration between regions at this time, but if you are interested in this feature, please reach out to [support@langchain.dev](mailto:support@langchain.dev).

#### *Is the EU region available on all LangSmith plans?*

Yes, you can sign up for the EU region on all plans including free plans.

#### *Is pricing different for the EU region compared to the US region?*

No, pricing is the same for the EU and US regions.

#### *What currency is used for payment if I use the EU region?*

All LangSmith plans are paid in USD.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/regions-faq.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Reject Concurrent

**URL:** llms-txt#reject-concurrent

**Contents:**
- Setup
- Create runs
- View run results

Source: https://docs.langchain.com/langsmith/reject-concurrent

This guide assumes knowledge of what double-texting is, which you can learn about in the [double-texting conceptual guide](/langsmith/double-texting).

The guide covers the `reject` option for double texting, which rejects the new run of the graph by throwing an error and continues with the original run until completion. Below is a quick example of using the `reject` option.

First, we will define a quick helper function for printing out JS and CURL model outputs (you can skip this if using Python):

<Tabs>
  <Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

Now, let's import our required packages and instantiate our client, assistant, and thread.

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

Now we can run a thread and try to run a second one with the "reject" option, which should fail since we have already started a run:

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

We can verify that the original thread finished executing:

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/reject-concurrent.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
</Tab>

  <Tab title="CURL">
```

Example 2 (unknown):
```unknown
</Tab>
</Tabs>

Now, let's import our required packages and instantiate our client, assistant, and thread.

<Tabs>
  <Tab title="Python">
```

Example 3 (unknown):
```unknown
</Tab>

  <Tab title="Javascript">
```

Example 4 (unknown):
```unknown
</Tab>

  <Tab title="CURL">
```

---

## Release policy

**URL:** llms-txt#release-policy

Source: https://docs.langchain.com/oss/javascript/release-policy

This page explains the LangChain and LangGraph release policies. Click on the tabs below to view the release policies for each:

<Tabs>
  <Tab title="LangChain">
    The LangChain ecosystem is composed of different component packages (e.g., `@langchain/core`, `langchain`, `@langchain/community`, partner packages, etc.)

With the release of LangChain 1.0, **minor** releases (e.g., from `1.0.x` to `1.1.0`) of `langchain` and `@langchain/core` follow semantic versioning and may be released frequently. Minor releases contain new features and improvements but do not include breaking changes.

Patch versions are released frequently, up to a few times per week, as they contain bug fixes and minor improvements.

The development of LLM applications is a rapidly evolving field, and we are constantly learning from our users and the community. As such, we expect that the APIs in `langchain` and `@langchain/core` will continue to evolve to better serve the needs of our users.

With LangChain 1.0's adoption of semantic versioning:

* Breaking changes to the public API will only occur in major version releases (e.g., `2.0.0`)
    * Minor version bumps (e.g., `1.0.0` to `1.1.0`) add new features without breaking changes
    * Patch version bumps (e.g., `1.0.0` to `1.0.1`) contain bug fixes and minor improvements

We will generally try to avoid making unnecessary changes, and will provide a deprecation policy for features that are being removed.

### Stability of other packages

The stability of other packages in the LangChain ecosystem may vary:

* **Partner packages maintained by LangChain** (such as `langchain-openai` and `langchain-anthropic`) follow semantic versioning and are expected to be stable post 1.0. Other partner packages may follow different stability and versioning policies, and users should refer to the documentation of those packages for more information.

* **`@langchain/community`** is a community maintained package that contains 3rd party integrations. Due to the nature of community contributions and third-party dependencies, `@langchain/community` does not follow the same strict semantic versioning policy as `langchain` and `@langchain/core`. See the "Special considerations" section under Long-term support for more details.

## Deprecation policy

We will generally avoid deprecating features until a better alternative is available.

With LangChain 1.0's semantic versioning approach, deprecated features will continue to work throughout the entire 1.x release series. Breaking changes, including the removal of deprecated features, will only occur in major version releases (e.g., 2.0).

When a feature is deprecated in `langchain` or `@langchain/core`, we will:

* Clearly mark it as deprecated in the code and documentation
    * Provide migration guidance to the recommended alternative
    * Provide security updates for the deprecated feature through all 1.x minor releases

In some situations, we may allow deprecated features to remain in the code base even longer if they are not causing maintenance issues, to further reduce the burden on users.

## Long-term support (LTS)

LangChain follows a long-term support (LTS) policy to provide stability for production applications:

### Release status definitions

Packages are marked with one of the following statuses:

* **ACTIVE**: Current active development, includes bug fixes, security patches, and new features
    * **MAINTENANCE**: Receives all security patches and critical bug fixes, but no new features

### Current LTS releases

**LangChain 1.0** is designated as an LTS release:

* **Status**: ACTIVE until the release of 2.0
    * **Support period**: After 2.0 is released, 1.0 will enter MAINTENANCE mode for at least 1 year
    * **Semver compliance**: Users can upgrade between minor versions (e.g., 1.0 to 1.1) without breaking changes

### Legacy version support

* **Status**: MAINTENANCE mode
    * **Support period**: Until December 2026
    * **Support includes**: Security patches and critical bug fixes

### Special considerations

**@langchain/community 1.0**: Due to the nature of community contributions and third-party integrations, `@langchain/community` may have breaking changes on minor releases.
  </Tab>

<Tab title="LangGraph">
    LangGraph follows a structured release policy to ensure stability and predictability for users building production applications.

We expect to space out **major** releases by at least 6-12 months to provide stability for production applications.

**Minor** releases are typically released every 1-2 months with new features and improvements.

**Patch** releases are released as needed, often weekly, to address bugs and security issues.

All APIs without special prefixes are considered stable and ready for production use. We maintain backward compatibility for stable features within a major version.

Features marked as `beta` in the documentation are:

* Feature-complete and tested
    * Safe for production use with the understanding they may change
    * Subject to minor API adjustments based on user feedback

### Experimental features

Features marked as `experimental` or `alpha`:

* Are under active development
    * May change significantly or be removed
    * Should be used with caution in production

APIs prefixed with underscore (`_`) or explicitly marked as internal:

* Are not part of the public API
    * May change without notice
    * Should not be used directly

## Deprecation policy

When deprecating features:

1. **Deprecation Notice**: Features are marked as deprecated with clear migration guidance
    2. **Grace Period**: Deprecated features remain functional for at least one minor version
    3. **Removal**: Features are removed only in major version releases
    4. **Migration Support**: We provide migration guides and, when possible, automated tools

## Platform compatibility

### JavaScript/TypeScript support

* We support Node.js LTS versions
    * TypeScript definitions are provided for all public APIs
    * Browser compatibility is documented for client-side components

Breaking changes are only introduced in major versions and include:

* Removal of deprecated APIs
    * Changes to required parameters
    * Changes to default behavior that affect existing applications
    * Minimum Python/Node.js version updates

For major version upgrades, we provide:

* Comprehensive migration guides
    * Automated migration scripts when feasible
    * Extended support period for the previous major version
    * Clear documentation of all breaking changes

## Long-term support (LTS)

LangGraph follows a long-term support (LTS) policy to provide stability for production applications:

### Release status definitions

Packages are marked with one of the following statuses:

* **ACTIVE**: Current active development, includes bug fixes, security patches, and new features
    * **MAINTENANCE**: Receives all security patches and critical bug fixes, but no new features

### Current LTS releases

**LangGraph 1.0** is designated as an LTS release:

* **Status**: ACTIVE until the release of 2.0
    * **Support period**: After 2.0 is released, 1.0 will enter MAINTENANCE mode for at least 1 year
    * **Semver compliance**: Users can upgrade between minor versions (e.g., 1.0 to 1.1) without breaking changes

### Legacy version support

* **Status**: MAINTENANCE mode
    * **Support period**: Until December 2026
    * **Support includes**: All security patches and critical bug fixes

* [Versioning](/oss/javascript/versioning) - Version numbering and support details
    * [Releases](/oss/javascript/releases) - Version-specific release notes and migration guides
  </Tab>
</Tabs>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/release-policy.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Release versions

**URL:** llms-txt#release-versions

**Contents:**
- Support levels
  - Active
  - Critical
  - End of life (EOL)
  - Deprecated
- Version support policy
  - Minor version support
  - Patch releases
- Recommendations
- Version compatibility

Source: https://docs.langchain.com/langsmith/release-versions

export const product_0 = "LangSmith"

{product_0} provides different support levels for different versions, which may include new features, bug fixes, or security patches.

There are four support levels:

* Active
* Critical
* End of life (EOL)
* Deprecated

Where N represents the latest minor version (e.g., 0.3, 0.4, etc.).

The current minor version (N) receives full support, including:

* New features and capabilities
* Bug fixes and regressions
* Security patches
* Quality-of-life improvements
* High confidence changes that are narrowly scoped

The previous minor version (N-1) receives limited support:

* Critical security fixes
* Installation fixes
* No new features or general bug fixes
* Transitioned from Active when a newer minor version is released

### End of life (EOL)

Versions older than N-2 (N-2, N-3, etc.) receive no support:

* No new patch releases
* No bug fixes, including known bugs
* No security updates
* Users should upgrade to a supported version

Versions that are no longer maintained:

* All versions prior to the first stable release
* Versions that have been explicitly deprecated
* No support or maintenance provided

## Version support policy

{product_0} follows an N-2 support policy for minor versions:

* **N (Current)**: Active support
* **N-1**: Critical support
* **N-2 and older**: End of Life

### Minor version support

Minor versions include new features and capabilities and are supported according to the N-2 policy. When we refer to a minor version, such as v0.3, we always mean its latest available patch release (v0.3.x).

During the support window for each version:

* **Active Support**: Regular patch releases with bug fixes, regressions, and new features
* **Critical Support**: Security-only releases for critical fixes related to security and installation
* **End of Life**: No new patches released

* **Stay Current**: We recommend upgrading to the latest minor version to receive full support and access to new features
* **Plan Upgrades**: Monitor the changelog for upcoming version changes and plan upgrades accordingly
* **Security**: Critical security fixes are only provided for Active and Critical support versions
* **Testing**: Test your applications with newer versions before upgrading in production

## Version compatibility

When upgrading between minor versions:

* Review the changelog for breaking changes
* Test your applications thoroughly
* Follow the upgrade guides provided in the documentation
* Consider the support timeline for your current version

## Current version support

To check the current supported versions and their support levels, refer to the [Agent Server Changelog](/langsmith/agent-server-changelog) for the latest release information.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/release-versions.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Remember that langgraph graphs are also langchain runnables.

**URL:** llms-txt#remember-that-langgraph-graphs-are-also-langchain-runnables.

**Contents:**
- Evaluating intermediate steps
- Running and evaluating individual nodes
- Related
- Reference code

target = example_to_state | app

experiment_results = await aevaluate(
    target,
    data="weather agent",
    evaluators=[correct],
    max_concurrency=4,  # optional
    experiment_prefix="claude-3.5-baseline",  # optional
)
python  theme={null}
def right_tool(outputs: dict) -> bool:
    tool_calls = outputs["messages"][1].tool_calls
    return bool(tool_calls and tool_calls[0]["name"] == "search")

experiment_results = await aevaluate(
    target,
    data="weather agent",
    evaluators=[correct, right_tool],
    max_concurrency=4,  # optional
    experiment_prefix="claude-3.5-baseline",  # optional
)
python  theme={null}
from langsmith.schemas import Run, Example

def right_tool_from_run(run: Run, example: Example) -> dict:
    # Get documents and answer
    first_model_run = next(run for run in root_run.child_runs if run.name == "agent")
    tool_calls = first_model_run.outputs["messages"][-1].tool_calls
    right_tool = bool(tool_calls and tool_calls[0]["name"] == "search")
    return {"key": "right_tool", "value": right_tool}

experiment_results = await aevaluate(
    target,
    data="weather agent",
    evaluators=[correct, right_tool_from_run],
    max_concurrency=4,  # optional
    experiment_prefix="claude-3.5-baseline",  # optional
)
python  theme={null}
node_target = example_to_state | app.nodes["agent"]

node_experiment_results = await aevaluate(
    node_target,
    data="weather agent",
    evaluators=[right_tool_from_run],
    max_concurrency=4,  # optional
    experiment_prefix="claude-3.5-model-node",  # optional
)
python  theme={null}
  from typing import Annotated, Literal, TypedDict
  from langchain.chat_models import init_chat_model
  from langchain.tools import tool
  from langgraph.prebuilt import ToolNode
  from langgraph.graph import END, START, StateGraph
  from langgraph.graph.message import add_messages
  from langsmith import Client, aevaluate

# Define a graph
  class State(TypedDict):
      # Messages have the type "list". The 'add_messages' function
      # in the annotation defines how this state key should be updated
      # (in this case, it appends messages to the list, rather than overwriting them)
      messages: Annotated[list, add_messages]

# Define the tools for the agent to use
  @tool
  def search(query: str) -> str:
      """Call to surf the web."""
      # This is a placeholder, but don't tell the LLM that...
      if "sf" in query.lower() or "san francisco" in query.lower():
          return "It's 60 degrees and foggy."
      return "It's 90 degrees and sunny."

tools = [search]
  tool_node = ToolNode(tools)
  model = init_chat_model("claude-sonnet-4-5-20250929").bind_tools(tools)

# Define the function that determines whether to continue or not
  def should_continue(state: State) -> Literal["tools", END]:
      messages = state['messages']
      last_message = messages[-1]

# If the LLM makes a tool call, then we route to the "tools" node
      if last_message.tool_calls:
          return "tools"

# Otherwise, we stop (reply to the user)
      return END

# Define the function that calls the model
  def call_model(state: State):
      messages = state['messages']
      response = model.invoke(messages)
      # We return a list, because this will get added to the existing list
      return {"messages": [response]}

# Define a new graph
  workflow = StateGraph(State)

# Define the two nodes we will cycle between
  workflow.add_node("agent", call_model)
  workflow.add_node("tools", tool_node)

# Set the entrypoint as 'agent'
  # This means that this node is the first one called
  workflow.add_edge(START, "agent")

# We now add a conditional edge
  workflow.add_conditional_edges(
      # First, we define the start node. We use 'agent'.
      # This means these are the edges taken after the 'agent' node is called.
      "agent",
      # Next, we pass in the function that will determine which node is called next.
      should_continue,
  )

# We now add a normal edge from 'tools' to 'agent'.
  # This means that after 'tools' is called, 'agent' node is called next.
  workflow.add_edge("tools", 'agent')

# Finally, we compile it!
  # This compiles it into a LangChain Runnable,
  # meaning you can use it as you would any other runnable.
  # Note that we're (optionally) passing the memory when compiling the graph
  app = workflow.compile()

questions = [
      "what's the weather in sf",
      "whats the weather in san fran",
      "whats the weather in tangier"
  ]

answers = [
      "It's 60 degrees and foggy.",
      "It's 60 degrees and foggy.",
      "It's 90 degrees and sunny.",
  ]

# Create a dataset
  ls_client = Client()
  dataset = ls_client.create_dataset(
      "weather agent",
      inputs=[{"question": q} for q in questions],
      outputs=[{"answers": a} for a in answers],
  )

# Define evaluators
  async def correct(outputs: dict, reference_outputs: dict) -> bool:
      instructions = (
          "Given an actual answer and an expected answer, determine whether"
          " the actual answer contains all of the information in the"
          " expected answer. Respond with 'CORRECT' if the actual answer"
          " does contain all of the expected information and 'INCORRECT'"
          " otherwise. Do not include anything else in your response."
      )
      # Our graph outputs a State dictionary, which in this case means
      # we'll have a 'messages' key and the final message should
      # be our actual answer.
      actual_answer = outputs["messages"][-1].content
      expected_answer = reference_outputs["answer"]
      user_msg = (
          f"ACTUAL ANSWER: {actual_answer}"
          f"\n\nEXPECTED ANSWER: {expected_answer}"
      )
      response = await judge_llm.ainvoke(
          [
              {"role": "system", "content": instructions},
              {"role": "user", "content": user_msg}
          ]
      )
      return response.content.upper() == "CORRECT"

def right_tool(outputs: dict) -> bool:
      tool_calls = outputs["messages"][1].tool_calls
      return bool(tool_calls and tool_calls[0]["name"] == "search")

# Run evaluation
  experiment_results = await aevaluate(
      target,
      data="weather agent",
      evaluators=[correct, right_tool],
      max_concurrency=4,  # optional
      experiment_prefix="claude-3.5-baseline",  # optional
  )
  ```
</Accordion>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/evaluate-graph.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
## Evaluating intermediate steps

Often it is valuable to evaluate not only the final output of an agent but also the intermediate steps it has taken. What's nice about `langgraph` is that the output of a graph is a state object that often already carries information about the intermediate steps taken. Usually we can evaluate whatever we're interested in just by looking at the messages in our state. For example, we can look at the messages to assert that the model invoked the 'search' tool upon as a first step.

Requires `langsmith>=0.2.0`
```

Example 2 (unknown):
```unknown
If we need access to information about intermediate steps that isn't in state, we can look at the Run object. This contains the full traces for all node inputs and outputs:

<Check>
  See more about what arguments you can pass to custom evaluators in this [how-to guide](/langsmith/code-evaluator).
</Check>
```

Example 3 (unknown):
```unknown
## Running and evaluating individual nodes

Sometimes you want to evaluate a single node directly to save time and costs. `langgraph` makes it easy to do this. In this case we can even continue using the evaluators we've been using.
```

Example 4 (unknown):
```unknown
## Related

* [`langgraph` evaluation docs](https://langchain-ai.github.io/langgraph/tutorials/#evaluation)

## Reference code

<Accordion title="Click to see a consolidated code snippet">
```

---

## RemoteGraph

**URL:** llms-txt#remotegraph

Source: https://docs.langchain.com/langsmith/remote-graph

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/remote-graph.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## required environment variables

**URL:** llms-txt#required-environment-variables

CONTROL_PLANE_HOST = os.getenv("CONTROL_PLANE_HOST")
LANGSMITH_API_KEY = os.getenv("LANGSMITH_API_KEY")
WORKSPACE_ID = os.getenv("WORKSPACE_ID")
INTEGRATION_ID = os.getenv("INTEGRATION_ID")
MAX_WAIT_TIME = 1800  # 30 mins

def get_headers() -> dict:
    """Return common headers for requests to the control plane API."""
    return {
        "X-Api-Key": LANGSMITH_API_KEY,
        "X-Tenant-Id": WORKSPACE_ID,
    }

def create_deployment() -> str:
    """Create deployment. Return deployment ID."""
    headers = get_headers()
    headers["Content-Type"] = "application/json"

deployment_name = "my_deployment"

request_body = {
        "name": deployment_name,
        "source": "github",
        "source_config": {
            "integration_id": INTEGRATION_ID,
            "repo_url": "https://github.com/langchain-ai/langgraph-example",
            "deployment_type": "dev",
            "build_on_push": False,
            "custom_url": None,
            "resource_spec": None,
        },
        "source_revision_config": {
            "repo_ref": "main",
            "langgraph_config_path": "langgraph.json",
            "image_uri": None,
        },
        "secrets": [
            {
                "name": "OPENAI_API_KEY",
                "value": "test_openai_api_key",
            },
            {
                "name": "ANTHROPIC_API_KEY",
                "value": "test_anthropic_api_key",
            },
            {
                "name": "TAVILY_API_KEY",
                "value": "test_tavily_api_key",
            },
        ],
    }

response = requests.post(
        url=f"{CONTROL_PLANE_HOST}/v2/deployments",
        headers=headers,
        json=request_body,
    )

if response.status_code != 201:
        raise Exception(f"Failed to create deployment: {response.text}")

deployment_id = response.json()["id"]
    print(f"Created deployment {deployment_name} ({deployment_id})")
    return deployment_id

def get_deployment(deployment_id: str) -> dict:
    """Get deployment."""
    response = requests.get(
        url=f"{CONTROL_PLANE_HOST}/v2/deployments/{deployment_id}",
        headers=get_headers(),
    )

if response.status_code != 200:
        raise Exception(f"Failed to get deployment ID {deployment_id}: {response.text}")

return response.json()

def list_revisions(deployment_id: str) -> list[dict]:
    """List revisions.

Return list is sorted by created_at in descending order (latest first).
    """
    response = requests.get(
        url=f"{CONTROL_PLANE_HOST}/v2/deployments/{deployment_id}/revisions",
        headers=get_headers(),
    )

if response.status_code != 200:
        raise Exception(
            f"Failed to list revisions for deployment ID {deployment_id}: {response.text}"
        )

return response.json()

def get_revision(
    deployment_id: str,
    revision_id: str,
) -> dict:
    """Get revision."""
    response = requests.get(
        url=f"{CONTROL_PLANE_HOST}/v2/deployments/{deployment_id}/revisions/{revision_id}",
        headers=get_headers(),
    )

if response.status_code != 200:
        raise Exception(f"Failed to get revision ID {revision_id}: {response.text}")

return response.json()

def patch_deployment(deployment_id: str) -> None:
    """Patch deployment."""
    headers = get_headers()
    headers["Content-Type"] = "application/json"

# This creates a new revision because source_revision_config is included
    response = requests.patch(
        url=f"{CONTROL_PLANE_HOST}/v2/deployments/{deployment_id}",
        headers=headers,
        json={
            "source_config": {
                "build_on_push": True,
            },
            "source_revision_config": {
                "repo_ref": "main",
                "langgraph_config_path": "langgraph.json",
            },
        },
    )

if response.status_code != 200:
        raise Exception(f"Failed to patch deployment: {response.text}")

print(f"Patched deployment ID {deployment_id}")

def wait_for_deployment(deployment_id: str, revision_id: str) -> None:
    """Wait for revision status to be DEPLOYED."""
    start_time = time.time()
    revision, status = None, None
    while time.time() - start_time < MAX_WAIT_TIME:
        revision = get_revision(deployment_id, revision_id)
        status = revision["status"]
        if status == "DEPLOYED":
            break
        elif "FAILED" in status:
            raise Exception(f"Revision ID {revision_id} failed: {revision}")

print(f"Waiting for revision ID {revision_id} to be DEPLOYED...")
        time.sleep(60)

if status != "DEPLOYED":
        raise Exception(
            f"Timeout waiting for revision ID {revision_id} to be DEPLOYED: {revision}"
        )

def delete_deployment(deployment_id: str) -> None:
    """Delete deployment."""
    response = requests.delete(
        url=f"{CONTROL_PLANE_HOST}/v2/deployments/{deployment_id}",
        headers=get_headers(),
    )

if response.status_code != 204:
        raise Exception(
            f"Failed to delete deployment ID {deployment_id}: {response.text}"
        )

print(f"Deployment ID {deployment_id} deleted")

if __name__ == "__main__":
    # create deployment and get the latest revision
    deployment_id = create_deployment()
    revisions = list_revisions(deployment_id)
    latest_revision = revisions["resources"][0]
    latest_revision_id = latest_revision["id"]

# wait for latest revision to be DEPLOYED
    wait_for_deployment(deployment_id, latest_revision_id)

# patch the deployment and get the latest revision
    patch_deployment(deployment_id)
    revisions = list_revisions(deployment_id)
    latest_revision = revisions["resources"][0]
    latest_revision_id = latest_revision["id"]

# wait for latest revision to be DEPLOYED
    wait_for_deployment(deployment_id, latest_revision_id)

# delete the deployment
    delete_deployment(deployment_id)
```

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/api-ref-control-plane.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## results in `Weather(temperature=70.0, condition='sunny')`

**URL:** llms-txt#results-in-`weather(temperature=70.0,-condition='sunny')`

**Contents:**
- Standard content blocks

python  theme={null}
from langchain_anthropic import ChatAnthropic

model = ChatAnthropic(model="claude-sonnet-4-5-20250929")
response = model.invoke("What's the capital of France?")

**Examples:**

Example 1 (unknown):
```unknown
**Error handling**: Control error handling via the `handle_errors` parameter to `ToolStrategy`:

* **Parsing errors**: Model generates data that doesn't match desired structure
* **Multiple tool calls**: Model generates 2+ tool calls for structured output schemas

***

## Standard content blocks

<Note>
  Content block support is currently only available for the following integrations:

  * [`langchain-anthropic`](https://pypi.org/project/langchain-anthropic/)
  * [`langchain-aws`](https://pypi.org/project/langchain-aws/)
  * [`langchain-openai`](https://pypi.org/project/langchain-openai/)
  * [`langchain-google-genai`](https://pypi.org/project/langchain-google-genai/)
  * [`langchain-ollama`](https://pypi.org/project/langchain-ollama/)

  Broader support for content blocks will be rolled out gradually across more providers.
</Note>

The new [`content_blocks`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.messages.BaseMessage.content_blocks) property introduces a standard representation for message content that works across providers:
```

---

## Resume (use same config)

**URL:** llms-txt#resume-(use-same-config)

**Contents:**
  - Match decision order to actions
  - Tailor configurations by risk

result = agent.invoke(Command(resume={...}), config=config)
python  theme={null}
if result.get("__interrupt__"):
    interrupts = result["__interrupt__"][0].value
    action_requests = interrupts["action_requests"]

# Create one decision per action, in order
    decisions = []
    for action in action_requests:
        decision = get_user_decision(action)  # Your logic
        decisions.append(decision)

result = agent.invoke(
        Command(resume={"decisions": decisions}),
        config=config
    )
python  theme={null}
interrupt_on = {
    # High risk: full control (approve, edit, reject)
    "delete_file": {"allowed_decisions": ["approve", "edit", "reject"]},
    "send_email": {"allowed_decisions": ["approve", "edit", "reject"]},

# Medium risk: no editing allowed
    "write_file": {"allowed_decisions": ["approve", "reject"]},

# Low risk: no interrupts
    "read_file": False,
    "list_files": False,
}
```

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/deepagents/human-in-the-loop.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
### Match decision order to actions

The decisions list must match the order of `action_requests`:
```

Example 2 (unknown):
```unknown
### Tailor configurations by risk

Configure different tools based on their risk level:
```

---

## Resume with approval decision

**URL:** llms-txt#resume-with-approval-decision

**Contents:**
  - Decision types
- Execution lifecycle
- Custom HITL logic

agent.invoke(
    Command( # [!code highlight]
        resume={"decisions": [{"type": "approve"}]}  # or "edit", "reject" [!code highlight]
    ), # [!code highlight]
    config=config # Same thread ID to resume the paused conversation
)
python  theme={null}
    agent.invoke(
        Command(
            # Decisions are provided as a list, one per action under review.
            # The order of decisions must match the order of actions
            # listed in the `__interrupt__` request.
            resume={
                "decisions": [
                    {
                        "type": "approve",
                    }
                ]
            }
        ),
        config=config  # Same thread ID to resume the paused conversation
    )
    python  theme={null}
    agent.invoke(
        Command(
            # Decisions are provided as a list, one per action under review.
            # The order of decisions must match the order of actions
            # listed in the `__interrupt__` request.
            resume={
                "decisions": [
                    {
                        "type": "edit",
                        # Edited action with tool name and args
                        "edited_action": {
                            # Tool name to call.
                            # Will usually be the same as the original action.
                            "name": "new_tool_name",
                            # Arguments to pass to the tool.
                            "args": {"key1": "new_value", "key2": "original_value"},
                        }
                    }
                ]
            }
        ),
        config=config  # Same thread ID to resume the paused conversation
    )
    python  theme={null}
    agent.invoke(
        Command(
            # Decisions are provided as a list, one per action under review.
            # The order of decisions must match the order of actions
            # listed in the `__interrupt__` request.
            resume={
                "decisions": [
                    {
                        "type": "reject",
                        # An explanation about why the action was rejected
                        "message": "No, this is wrong because ..., instead do this ...",
                    }
                ]
            }
        ),
        config=config  # Same thread ID to resume the paused conversation
    )
    python  theme={null}
    {
        "decisions": [
            {"type": "approve"},
            {
                "type": "edit",
                "edited_action": {
                    "name": "tool_name",
                    "args": {"param": "new_value"}
                }
            },
            {
                "type": "reject",
                "message": "This action is not allowed"
            }
        ]
    }
    ```
  </Tab>
</Tabs>

## Execution lifecycle

The middleware defines an `after_model` hook that runs after the model generates a response but before any tool calls are executed:

1. The agent invokes the model to generate a response.
2. The middleware inspects the response for tool calls.
3. If any calls require human input, the middleware builds a `HITLRequest` with `action_requests` and `review_configs` and calls [interrupt](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt).
4. The agent waits for human decisions.
5. Based on the `HITLResponse` decisions, the middleware executes approved or edited calls, synthesizes [ToolMessage](https://reference.langchain.com/python/langchain/messages/#langchain.messages.ToolMessage)'s for rejected calls, and resumes execution.

For more specialized workflows, you can build custom HITL logic directly using the [interrupt](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) primitive and [middleware](/oss/python/langchain/middleware) abstraction.

Review the [execution lifecycle](#execution-lifecycle) above to understand how to integrate interrupts into the agent's operation.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/human-in-the-loop.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
### Decision types

<Tabs>
  <Tab title="✅ approve">
    Use `approve` to approve the tool call as-is and execute it without changes.
```

Example 2 (unknown):
```unknown
</Tab>

  <Tab title="✏️ edit">
    Use `edit` to modify the tool call before execution.
    Provide the edited action with the new tool name and arguments.
```

Example 3 (unknown):
```unknown
<Tip>
      When **editing** tool arguments, make changes conservatively. Significant modifications to the original arguments may cause the model to re-evaluate its approach and potentially execute the tool multiple times or take unexpected actions.
    </Tip>
  </Tab>

  <Tab title="❌ reject">
    Use `reject` to reject the tool call and provide feedback instead of execution.
```

Example 4 (unknown):
```unknown
The `message` is added to the conversation as feedback to help the agent understand why the action was rejected and what it should do instead.

    ***

    ### Multiple decisions

    When multiple actions are under review, provide a decision for each action in the same order as they appear in the interrupt:
```

---

## Resume with the human's response

**URL:** llms-txt#resume-with-the-human's-response

---

## Retrieval

**URL:** llms-txt#retrieval

**Contents:**
- Building a knowledge base
  - From retrieval to RAG
  - Retrieval Pipeline
  - Building Blocks
- RAG Architectures
  - 2-step RAG
  - Agentic RAG
  - Hybrid RAG

Source: https://docs.langchain.com/oss/javascript/langchain/retrieval

Large language models (LLMs) are powerful, but they have two key limitations:

* **Finite context** — they can’t ingest entire corpora at once.
* **Static knowledge** — their training data is frozen at a point in time.

Retrieval addresses these problems by fetching relevant external knowledge at query time. This is the foundation of **Retrieval-Augmented Generation (RAG)**: enhancing an LLM’s answers with context-specific information.

## Building a knowledge base

A **knowledge base** is a repository of documents or structured data used during retrieval.

If you need a custom knowledge base, you can use LangChain’s document loaders and vector stores to build one from your own data.

<Note>
  If you already have a knowledge base (e.g., a SQL database, CRM, or internal documentation system), you do **not** need to rebuild it. You can:

* Connect it as a **tool** for an agent in Agentic RAG.
  * Query it and supply the retrieved content as context to the LLM [(2-Step RAG)](#2-step-rag).
</Note>

See the following tutorial to build a searchable knowledge base and minimal RAG workflow:

<Card title="Tutorial: Semantic search" icon="database" href="/oss/javascript/langchain/knowledge-base" arrow cta="Learn more">
  Learn how to create a searchable knowledge base from your own data using LangChain’s document loaders, embeddings, and vector stores.
  In this tutorial, you’ll build a search engine over a PDF, enabling retrieval of passages relevant to a query. You’ll also implement a minimal RAG workflow on top of this engine to see how external knowledge can be integrated into LLM reasoning.
</Card>

### From retrieval to RAG

Retrieval allows LLMs to access relevant context at runtime. But most real-world applications go one step further: they **integrate retrieval with generation** to produce grounded, context-aware answers.

This is the core idea behind **Retrieval-Augmented Generation (RAG)**. The retrieval pipeline becomes a foundation for a broader system that combines search with generation.

### Retrieval Pipeline

A typical retrieval workflow looks like this:

Each component is modular: you can swap loaders, splitters, embeddings, or vector stores without rewriting the app’s logic.

<Columns cols={2}>
  <Card title="Document loaders" icon="file-import" href="/oss/javascript/integrations/document_loaders" arrow cta="Learn more">
    Ingest data from external sources (Google Drive, Slack, Notion, etc.), returning standardized [`Document`](https://v03.api.js.langchain.com/classes/_langchain_core.documents.Document.html) objects.
  </Card>

<Card title="Embedding models" icon="diagram-project" href="/oss/javascript/integrations/text_embedding" arrow cta="Learn more">
    An embedding model turns text into a vector of numbers so that texts with similar meaning land close together in that vector space.
  </Card>

<Card title="Vector stores" icon="database" href="/oss/javascript/integrations/vectorstores/" arrow cta="Learn more">
    Specialized databases for storing and searching embeddings.
  </Card>

<Card title="Retrievers" icon="binoculars" href="/oss/javascript/integrations/retrievers/" arrow cta="Learn more">
    A retriever is an interface that returns documents given an unstructured query.
  </Card>
</Columns>

RAG can be implemented in multiple ways, depending on your system's needs. We outline each type in the sections below.

| Architecture    | Description                                                                | Control   | Flexibility | Latency    | Example Use Case                                  |
| --------------- | -------------------------------------------------------------------------- | --------- | ----------- | ---------- | ------------------------------------------------- |
| **2-Step RAG**  | Retrieval always happens before generation. Simple and predictable         | ✅ High    | ❌ Low       | ⚡ Fast     | FAQs, documentation bots                          |
| **Agentic RAG** | An LLM-powered agent decides *when* and *how* to retrieve during reasoning | ❌ Low     | ✅ High      | ⏳ Variable | Research assistants with access to multiple tools |
| **Hybrid**      | Combines characteristics of both approaches with validation steps          | ⚖️ Medium | ⚖️ Medium   | ⏳ Variable | Domain-specific Q\&A with quality validation      |

<Info>
  **Latency**: Latency is generally more **predictable** in **2-Step RAG**, as the maximum number of LLM calls is known and capped. This predictability assumes that LLM inference time is the dominant factor. However, real-world latency may also be affected by the performance of retrieval steps—such as API response times, network delays, or database queries—which can vary based on the tools and infrastructure in use.
</Info>

In **2-Step RAG**, the retrieval step is always executed before the generation step. This architecture is straightforward and predictable, making it suitable for many applications where the retrieval of relevant documents is a clear prerequisite for generating an answer.

<Card title="Tutorial: Retrieval-Augmented Generation (RAG)" icon="robot" href="/oss/javascript/langchain/rag#rag-chains" arrow cta="Learn more">
  See how to build a Q\&A chatbot that can answer questions grounded in your data using Retrieval-Augmented Generation.
  This tutorial walks through two approaches:

* A **RAG agent** that runs searches with a flexible tool—great for general-purpose use.
  * A **2-step RAG** chain that requires just one LLM call per query—fast and efficient for simpler tasks.
</Card>

**Agentic Retrieval-Augmented Generation (RAG)** combines the strengths of Retrieval-Augmented Generation with agent-based reasoning. Instead of retrieving documents before answering, an agent (powered by an LLM) reasons step-by-step and decides **when** and **how** to retrieve information during the interaction.

<Tip>
  The only thing an agent needs to enable RAG behavior is access to one or more **tools** that can fetch external knowledge — such as documentation loaders, web APIs, or database queries.
</Tip>

<Expandable title="Extended example: Agentic RAG for LangGraph's llms.txt">
  This example implements an **Agentic RAG system** to assist users in querying LangGraph documentation. The agent begins by loading [llms.txt](https://llmstxt.org/), which lists available documentation URLs, and can then dynamically use a `fetch_documentation` tool to retrieve and process the relevant content based on the user’s question.

<Card title="Tutorial: Retrieval-Augmented Generation (RAG)" icon="robot" href="/oss/javascript/langchain/rag" arrow cta="Learn more">
  See how to build a Q\&A chatbot that can answer questions grounded in your data using Retrieval-Augmented Generation.
  This tutorial walks through two approaches:

* A **RAG agent** that runs searches with a flexible tool—great for general-purpose use.
  * A **2-step RAG** chain that requires just one LLM call per query—fast and efficient for simpler tasks.
</Card>

Hybrid RAG combines characteristics of both 2-Step and Agentic RAG. It introduces intermediate steps such as query preprocessing, retrieval validation, and post-generation checks. These systems offer more flexibility than fixed pipelines while maintaining some control over execution.

Typical components include:

* **Query enhancement**: Modify the input question to improve retrieval quality. This can involve rewriting unclear queries, generating multiple variations, or expanding queries with additional context.
* **Retrieval validation**: Evaluate whether retrieved documents are relevant and sufficient. If not, the system may refine the query and retrieve again.
* **Answer validation**: Check the generated answer for accuracy, completeness, and alignment with source content. If needed, the system can regenerate or revise the answer.

The architecture often supports multiple iterations between these steps:

This architecture is suitable for:

* Applications with ambiguous or underspecified queries
* Systems that require validation or quality control steps
* Workflows involving multiple sources or iterative refinement

<Card title="Tutorial: Agentic RAG with Self-Correction" icon="robot" href="/oss/javascript/langgraph/agentic-rag" arrow cta="Learn more">
  An example of **Hybrid RAG** that combines agentic reasoning with retrieval and self-correction.
</Card>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/retrieval.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
Each component is modular: you can swap loaders, splitters, embeddings, or vector stores without rewriting the app’s logic.

### Building Blocks

<Columns cols={2}>
  <Card title="Document loaders" icon="file-import" href="/oss/javascript/integrations/document_loaders" arrow cta="Learn more">
    Ingest data from external sources (Google Drive, Slack, Notion, etc.), returning standardized [`Document`](https://v03.api.js.langchain.com/classes/_langchain_core.documents.Document.html) objects.
  </Card>

  <Card title="Embedding models" icon="diagram-project" href="/oss/javascript/integrations/text_embedding" arrow cta="Learn more">
    An embedding model turns text into a vector of numbers so that texts with similar meaning land close together in that vector space.
  </Card>

  <Card title="Vector stores" icon="database" href="/oss/javascript/integrations/vectorstores/" arrow cta="Learn more">
    Specialized databases for storing and searching embeddings.
  </Card>

  <Card title="Retrievers" icon="binoculars" href="/oss/javascript/integrations/retrievers/" arrow cta="Learn more">
    A retriever is an interface that returns documents given an unstructured query.
  </Card>
</Columns>

## RAG Architectures

RAG can be implemented in multiple ways, depending on your system's needs. We outline each type in the sections below.

| Architecture    | Description                                                                | Control   | Flexibility | Latency    | Example Use Case                                  |
| --------------- | -------------------------------------------------------------------------- | --------- | ----------- | ---------- | ------------------------------------------------- |
| **2-Step RAG**  | Retrieval always happens before generation. Simple and predictable         | ✅ High    | ❌ Low       | ⚡ Fast     | FAQs, documentation bots                          |
| **Agentic RAG** | An LLM-powered agent decides *when* and *how* to retrieve during reasoning | ❌ Low     | ✅ High      | ⏳ Variable | Research assistants with access to multiple tools |
| **Hybrid**      | Combines characteristics of both approaches with validation steps          | ⚖️ Medium | ⚖️ Medium   | ⏳ Variable | Domain-specific Q\&A with quality validation      |

<Info>
  **Latency**: Latency is generally more **predictable** in **2-Step RAG**, as the maximum number of LLM calls is known and capped. This predictability assumes that LLM inference time is the dominant factor. However, real-world latency may also be affected by the performance of retrieval steps—such as API response times, network delays, or database queries—which can vary based on the tools and infrastructure in use.
</Info>

### 2-step RAG

In **2-Step RAG**, the retrieval step is always executed before the generation step. This architecture is straightforward and predictable, making it suitable for many applications where the retrieval of relevant documents is a clear prerequisite for generating an answer.
```

Example 2 (unknown):
```unknown
<Card title="Tutorial: Retrieval-Augmented Generation (RAG)" icon="robot" href="/oss/javascript/langchain/rag#rag-chains" arrow cta="Learn more">
  See how to build a Q\&A chatbot that can answer questions grounded in your data using Retrieval-Augmented Generation.
  This tutorial walks through two approaches:

  * A **RAG agent** that runs searches with a flexible tool—great for general-purpose use.
  * A **2-step RAG** chain that requires just one LLM call per query—fast and efficient for simpler tasks.
</Card>

### Agentic RAG

**Agentic Retrieval-Augmented Generation (RAG)** combines the strengths of Retrieval-Augmented Generation with agent-based reasoning. Instead of retrieving documents before answering, an agent (powered by an LLM) reasons step-by-step and decides **when** and **how** to retrieve information during the interaction.

<Tip>
  The only thing an agent needs to enable RAG behavior is access to one or more **tools** that can fetch external knowledge — such as documentation loaders, web APIs, or database queries.
</Tip>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
<Expandable title="Extended example: Agentic RAG for LangGraph's llms.txt">
  This example implements an **Agentic RAG system** to assist users in querying LangGraph documentation. The agent begins by loading [llms.txt](https://llmstxt.org/), which lists available documentation URLs, and can then dynamically use a `fetch_documentation` tool to retrieve and process the relevant content based on the user’s question.
```

---

## Retrievers

**URL:** llms-txt#retrievers

**Contents:**
- All retrievers

Source: https://docs.langchain.com/oss/javascript/integrations/retrievers/index

A [retriever](/oss/javascript/langchain/retrieval) is an interface that returns documents given an unstructured query.
It is more general than a vector store.
A retriever does not need to be able to store documents, only to return (or retrieve) them.

Retrievers accept a string query as input and return a list of Documents.

For specifics on how to use retrievers, see the [relevant how-to guides here](/oss/javascript/langchain/retrieval).

Note that all [vector stores](/oss/javascript/integrations/vectorstores) can be [cast to retrievers](/oss/javascript/langchain/retrieval).
Refer to the vector store [integration docs](/oss/javascript/integrations/vectorstores/) for available vector store retrievers.

<Columns cols={3}>
  <Card title="Alchemyst AI Retriever" icon="link" href="/oss/javascript/integrations/retrievers/alchemystai-retriever" arrow="true" cta="View guide" />

<Card title="ArxivRetriever" icon="link" href="/oss/javascript/integrations/retrievers/arxiv-retriever" arrow="true" cta="View guide" />

<Card title="Azion EdgeSQL" icon="link" href="/oss/javascript/integrations/retrievers/azion-edgesql" arrow="true" cta="View guide" />

<Card title="Knowledge Bases for Amazon Bedrock" icon="link" href="/oss/javascript/integrations/retrievers/bedrock-knowledge-bases" arrow="true" cta="View guide" />

<Card title="BM25" icon="link" href="/oss/javascript/integrations/retrievers/bm25" arrow="true" cta="View guide" />

<Card title="Chaindesk Retriever" icon="link" href="/oss/javascript/integrations/retrievers/chaindesk-retriever" arrow="true" cta="View guide" />

<Card title="Dria Retriever" icon="link" href="/oss/javascript/integrations/retrievers/dria" arrow="true" cta="View guide" />

<Card title="Exa" icon="link" href="/oss/javascript/integrations/retrievers/exa" arrow="true" cta="View guide" />

<Card title="HyDE Retriever" icon="link" href="/oss/javascript/integrations/retrievers/hyde" arrow="true" cta="View guide" />

<Card title="Amazon Kendra Retriever" icon="link" href="/oss/javascript/integrations/retrievers/kendra-retriever" arrow="true" cta="View guide" />

<Card title="Metal Retriever" icon="link" href="/oss/javascript/integrations/retrievers/metal-retriever" arrow="true" cta="View guide" />

<Card title="Supabase Hybrid Search" icon="link" href="/oss/javascript/integrations/retrievers/supabase-hybrid" arrow="true" cta="View guide" />

<Card title="Tavily Search API" icon="link" href="/oss/javascript/integrations/retrievers/tavily" arrow="true" cta="View guide" />

<Card title="Time-Weighted Retriever" icon="link" href="/oss/javascript/integrations/retrievers/time-weighted-retriever" arrow="true" cta="View guide" />

<Card title="Vespa Retriever" icon="link" href="/oss/javascript/integrations/retrievers/vespa-retriever" arrow="true" cta="View guide" />

<Card title="Zep Cloud Retriever" icon="link" href="/oss/javascript/integrations/retrievers/zep-cloud-retriever" arrow="true" cta="View guide" />

<Card title="Zep Open Source Retriever" icon="link" href="/oss/javascript/integrations/retrievers/zep-retriever" arrow="true" cta="View guide" />
</Columns>

<Info>
  If you'd like to contribute an integration, see [Contributing integrations](/oss/javascript/contributing#add-a-new-integration).
</Info>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/javascript/integrations/retrievers/index.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Rollback Concurrent

**URL:** llms-txt#rollback-concurrent

**Contents:**
- Setup
- Create runs
- View run results

Source: https://docs.langchain.com/langsmith/rollback-concurrent

This guide assumes knowledge of what double-texting is, which you can learn about in the [double-texting conceptual guide](/langsmith/double-texting).

The guide covers the `rollback` option for double texting, which interrupts the prior run of the graph and starts a new one with the double-text. This option is very similar to the `interrupt` option, but in this case the first run is completely deleted from the database and cannot be restarted. Below is a quick example of using the `rollback` option.

First, we will define a quick helper function for printing out JS and CURL model outputs (you can skip this if using Python):

<Tabs>
  <Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

Now, let's import our required packages and instantiate our client, assistant, and thread.

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

Now let's run a thread with the multitask parameter set to "rollback":

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

We can see that the thread has data only from the second run

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

Verify that the original, rolled back run was deleted

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>
</Tabs>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/rollback-concurrent.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
</Tab>

  <Tab title="CURL">
```

Example 2 (unknown):
```unknown
</Tab>
</Tabs>

Now, let's import our required packages and instantiate our client, assistant, and thread.

<Tabs>
  <Tab title="Python">
```

Example 3 (unknown):
```unknown
</Tab>

  <Tab title="Javascript">
```

Example 4 (unknown):
```unknown
</Tab>

  <Tab title="CURL">
```

---

## Routing model with structured output

**URL:** llms-txt#routing-model-with-structured-output

router_llm = init_chat_model("gpt-4o-mini").with_structured_output(
    UserIntent, method="json_schema", strict=True
)

---

## Runtime

**URL:** llms-txt#runtime

**Contents:**
- Overview
- Access
  - Inside tools
  - Inside middleware

Source: https://docs.langchain.com/oss/javascript/langchain/runtime

LangChain's `createAgent` runs on LangGraph's runtime under the hood.

LangGraph exposes a [`Runtime`](https://reference.langchain.com/javascript/modules/langgraph.index.Runtime.html) object with the following information:

1. **Context**: static information like user id, db connections, or other dependencies for an agent invocation
2. **Store**: a [BaseStore](https://langchain-ai.github.io/langgraphjs/reference/classes/checkpoint.BaseStore.html) instance used for [long-term memory](/oss/javascript/langchain/long-term-memory)
3. **Stream writer**: an object used for streaming information via the `"custom"` stream mode

<Tip>
  The runtime context is how you thread data through your agent. Rather than storing things in global state, you can attach values — like a database connection, user session, or configuration — to the context and access them inside tools and middleware. This keeps things stateless, testable, and reusable.
</Tip>

You can access the runtime information within [tools](#inside-tools) and [middleware](#inside-middleware).

When creating an agent with `createAgent`, you can specify a `contextSchema` to define the structure of the `context` stored in the agent [`Runtime`](https://reference.langchain.com/javascript/modules/langgraph.index.Runtime.html).

When invoking the agent, pass the `context` argument with the relevant configuration for the run:

You can access the runtime information inside tools to:

* Access the context
* Read or write long-term memory
* Write to the [custom stream](/oss/javascript/langchain/streaming#custom-updates) (ex, tool progress / updates)

Use the `runtime` parameter to access the [`Runtime`](https://reference.langchain.com/javascript/modules/langgraph.index.Runtime.html) object inside a tool.

### Inside middleware

You can access runtime information in middleware to create dynamic prompts, modify messages, or control agent behavior based on user context.

Use the `runtime` parameter to access the [`Runtime`](https://reference.langchain.com/javascript/modules/langgraph.index.Runtime.html) object inside middleware.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/runtime.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
### Inside tools

You can access the runtime information inside tools to:

* Access the context
* Read or write long-term memory
* Write to the [custom stream](/oss/javascript/langchain/streaming#custom-updates) (ex, tool progress / updates)

Use the `runtime` parameter to access the [`Runtime`](https://reference.langchain.com/javascript/modules/langgraph.index.Runtime.html) object inside a tool.
```

Example 2 (unknown):
```unknown
### Inside middleware

You can access runtime information in middleware to create dynamic prompts, modify messages, or control agent behavior based on user context.

Use the `runtime` parameter to access the [`Runtime`](https://reference.langchain.com/javascript/modules/langgraph.index.Runtime.html) object inside middleware.
```

---

## Run all tests

**URL:** llms-txt#run-all-tests

uv run --group test pytest tests/unit_tests/
uv run --group test --group test_integration pytest -n auto tests/integration_tests/

---

## Run an evaluation from the prompt playground

**URL:** llms-txt#run-an-evaluation-from-the-prompt-playground

**Contents:**
- Create an experiment in the prompt playground[​](#create-an-experiment-in-the-prompt-playground "Direct link to Create an experiment in the prompt playground")
- Add evaluation scores to the experiment[​](#add-evaluation-scores-to-the-experiment "Direct link to Add evaluation scores to the experiment")

Source: https://docs.langchain.com/langsmith/run-evaluation-from-prompt-playground

LangSmith allows you to run evaluations directly in the UI. The [**Prompt Playground**](/langsmith/prompt-engineering#prompt-playground) allows you to test your prompt or model configuration over a series of inputs to see how well it scores across different contexts or scenarios, without having to write any code.

Before you run an evaluation, you need to have an [existing dataset](/langsmith/evaluation-concepts#datasets). Learn how to [create a dataset from the UI](/langsmith/manage-datasets-in-application#set-up-your-dataset).

If you prefer to run experiments in code, visit [run an evaluation using the SDK](/langsmith/evaluate-llm-application).

<img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/playground-experiment.gif?s=05ec3d2b1aa6590c443a033924fc6141" alt="" data-og-width="1358" width="1358" data-og-height="720" height="720" data-path="langsmith/images/playground-experiment.gif" data-optimize="true" data-opv="3" />

## Create an experiment in the prompt playground[​](#create-an-experiment-in-the-prompt-playground "Direct link to Create an experiment in the prompt playground")

1. **Navigate to the playground** by clicking **Playground** in the sidebar.
2. **Add a prompt** by selecting an existing saved a prompt or creating a new one.
3. **Select a dataset** from the **Test over dataset** dropdown

* Note that the keys in the dataset input must match the input variables of the prompt. For example, in the above video the selected dataset has inputs with the key "blog", which correctly match the input variable of the prompt.
* There is a maximum of 15 input variables allowed in the prompt playground.

4. **Start the experiment** by clicking on the **Start** or CMD+Enter. This will run the prompt over all the examples in the dataset and create an entry for the experiment in the dataset details page. We recommend committing the prompt to the prompt hub before starting the experiment so that it can be easily referenced later when reviewing your experiment.
5. **View the full results** by clicking **View full experiment**. This will take you to the experiment details page where you can see the results of the experiment.

## Add evaluation scores to the experiment[​](#add-evaluation-scores-to-the-experiment "Direct link to Add evaluation scores to the experiment")

Evaluate your experiment over specific critera by adding evaluators. Add LLM-as-a-judge or custom code evaluators in the playground using the **+Evaluator** button.

To learn more about adding evaluators in via UI, visit [how to define an LLM-as-a-judge evaluator](/langsmith/llm-as-judge).

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/run-evaluation-from-prompt-playground.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Run an evaluation with multimodal content

**URL:** llms-txt#run-an-evaluation-with-multimodal-content

**Contents:**
- SDK
  - 1. Create examples with attachments

Source: https://docs.langchain.com/langsmith/evaluate-with-attachments

LangSmith lets you create dataset examples with file attachments—like images, audio files, or documents—so you can reference them when evaluating an application that uses multimodal inputs or outputs.

While you can include multimodal data in your examples by base64 encoding it, this approach is inefficient - the encoded data takes up more space than the original binary files, resulting in slower transfers to and from LangSmith. Using attachments instead provides two key benefits:

1. Faster upload and download speeds due to more efficient binary file transfers
2. Enhanced visualization of different file types in the LangSmith UI

### 1. Create examples with attachments

To upload examples with attachments using the SDK, use the [create\_examples](https://docs.smith.langchain.com/reference/python/client/langsmith.client.Client#langsmith.client.Client.create_examples) / [update\_examples](https://docs.smith.langchain.com/reference/python/client/langsmith.client.Client#langsmith.client.Client.update_examples) Python methods or the [uploadExamplesMultipart](https://docs.smith.langchain.com/reference/js/classes/client.Client#uploadexamplesmultipart) / [updateExamplesMultipart](https://docs.smith.langchain.com/reference/js/classes/client.Client#updateexamplesmultipart) TypeScript methods.

Requires `langsmith>=0.3.13`

```python  theme={null}
import requests
import uuid
from pathlib import Path
from langsmith import Client

---

## Run a LangGraph app locally

**URL:** llms-txt#run-a-langgraph-app-locally

**Contents:**
- Prerequisites
- 1. Install the LangGraph CLI
- 2. Create a LangGraph app 🌱
- 3. Install dependencies
- 4. Create a `.env` file
- 5. Launch Agent Server 🚀
- 6. Test the API
- Next steps

Source: https://docs.langchain.com/langsmith/local-server

This quickstart shows you how to set up a LangGraph application locally for testing and development.

Before you begin, ensure you have an API key for [LangSmith](https://smith.langchain.com/settings) (free to sign up).

## 1. Install the LangGraph CLI

<Tabs>
  <Tab title="Python server">
    
  </Tab>

<Tab title="Node server">
    
  </Tab>
</Tabs>

## 2. Create a LangGraph app 🌱

Create a new app from the [`new-langgraph-project-python` template](https://github.com/langchain-ai/new-langgraph-project) or [`new-langgraph-project-js` template](https://github.com/langchain-ai/new-langgraphjs-project). This template demonstrates a single-node application you can extend with your own logic.

<Tabs>
  <Tab title="Python server">
    
  </Tab>

<Tab title="Node server">
    
  </Tab>
</Tabs>

<Tip>
  **Additional templates**<br />
  If you use [`langgraph new`](/langsmith/cli) without specifying a template, you will be presented with an interactive menu that will allow you to choose from a list of available templates.
</Tip>

## 3. Install dependencies

In the root of your new LangGraph app, install the dependencies in `edit` mode so your local changes are used by the server:

<Tabs>
  <Tab title="Python server">
    
  </Tab>

<Tab title="Node server">
    
  </Tab>
</Tabs>

## 4. Create a `.env` file

You will find a [`.env.example`](/langsmith/application-structure#configuration-file) in the root of your new LangGraph app. Create a `.env` file in the root of your new LangGraph app and copy the contents of the `.env.example` file into it, filling in the necessary API keys:

## 5. Launch Agent Server 🚀

Start the Agent Server locally:

<Tabs>
  <Tab title="Python server">
    
  </Tab>

<Tab title="Node server">
    
  </Tab>
</Tabs>

The [`langgraph dev`](/langsmith/cli) command starts [Agent Server](/langsmith/agent-server) in an in-memory mode. This mode is suitable for development and testing purposes.

<Tip>
  For production use, deploy Agent Server with a persistent storage backend. For more information, refer to the LangSmith [platform options](/langsmith/platform-setup).
</Tip>

<Tabs>
  <Tab title="Python SDK (async)">
    1. Install the LangGraph Python SDK:

2. Send a message to the assistant (threadless run):

<Tab title="Python SDK (sync)">
    1. Install the LangGraph Python SDK:

2. Send a message to the assistant (threadless run):

<Tab title="Javascript SDK">
    1. Install the LangGraph JS SDK:

2. Send a message to the assistant (threadless run):

<Tab title="Rest API">
    
  </Tab>
</Tabs>

Now that you have a LangGraph app running locally, you're ready to deploy it:

**Choose a hosting option for LangSmith:**

* [**Cloud**](/langsmith/cloud): Fastest setup, fully managed (recommended).
* [**Hybrid**](/langsmith/hybrid): <Tooltip tip="The runtime environment where your Agent Servers and agents execute.">Data plane</Tooltip> in your cloud, <Tooltip tip="The LangSmith UI and APIs for managing deployments.">control plane</Tooltip> managed by LangChain.
* [**Self-hosted**](/langsmith/self-hosted): Full control in your infrastructure.

For more details, refer to the [Platform setup comparison](/langsmith/platform-setup).

**Then deploy your app:**

* [Deploy to Cloud quickstart](/langsmith/deployment-quickstart): Quick setup guide.
* [Full Cloud setup guide](/langsmith/deploy-to-cloud): Comprehensive deployment documentation.

**Explore features:**

* **[Studio](/langsmith/studio)**: Visualize, interact with, and debug your application with the Studio UI. Try the [Studio quickstart](/langsmith/quick-start-studio).
* **API References**: [LangSmith Deployment API](https://langchain-ai.github.io/langgraph/cloud/reference/api/api_ref/), [Python SDK](/langsmith/langgraph-python-sdk), [JS/TS SDK](/langsmith/langgraph-js-ts-sdk)

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/local-server.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
</Tab>

  <Tab title="Node server">
```

Example 2 (unknown):
```unknown
</Tab>
</Tabs>

## 2. Create a LangGraph app 🌱

Create a new app from the [`new-langgraph-project-python` template](https://github.com/langchain-ai/new-langgraph-project) or [`new-langgraph-project-js` template](https://github.com/langchain-ai/new-langgraphjs-project). This template demonstrates a single-node application you can extend with your own logic.

<Tabs>
  <Tab title="Python server">
```

Example 3 (unknown):
```unknown
</Tab>

  <Tab title="Node server">
```

Example 4 (unknown):
```unknown
</Tab>
</Tabs>

<Tip>
  **Additional templates**<br />
  If you use [`langgraph new`](/langsmith/cli) without specifying a template, you will be presented with an interactive menu that will allow you to choose from a list of available templates.
</Tip>

## 3. Install dependencies

In the root of your new LangGraph app, install the dependencies in `edit` mode so your local changes are used by the server:

<Tabs>
  <Tab title="Python server">
```

---

## Run a local server

**URL:** llms-txt#run-a-local-server

**Contents:**
- Prerequisites
- 1. Install the LangGraph CLI
- 2. Create a LangGraph app 🌱
- 3. Install dependencies
- 4. Create a `.env` file
- 5. Launch Agent server 🚀
- 6. Test your application in Studio
- 7. Test the API
- Next steps

Source: https://docs.langchain.com/oss/javascript/langgraph/local-server

This guide shows you how to run a LangGraph application locally.

Before you begin, ensure you have the following:

* An API key for [LangSmith](https://smith.langchain.com/settings) - free to sign up

## 1. Install the LangGraph CLI

## 2. Create a LangGraph app 🌱

Create a new app from the [`new-langgraph-project-js` template](https://github.com/langchain-ai/new-langgraphjs-project). This template demonstrates a single-node application you can extend with your own logic.

## 3. Install dependencies

In the root of your new LangGraph app, install the dependencies in `edit` mode so your local changes are used by the server:

## 4. Create a `.env` file

You will find a `.env.example` in the root of your new LangGraph app. Create a `.env` file in the root of your new LangGraph app and copy the contents of the `.env.example` file into it, filling in the necessary API keys:

## 5. Launch Agent server 🚀

Start the LangGraph API server locally:

The `langgraph dev` command starts Agent Server in an in-memory mode. This mode is suitable for development and testing purposes. For production use, deploy Agent Server with access to a persistent storage backend. For more information, see the [Platform setup overview](/langsmith/platform-setup).

## 6. Test your application in Studio

[Studio](/langsmith/studio) is a specialized UI that you can connect to LangGraph API server to visualize, interact with, and debug your application locally. Test your graph in Studio by visiting the URL provided in the output of the `langgraph dev` command:

For an Agent Server running on a custom host/port, update the baseURL parameter.

<Accordion title="Safari compatibility">
  Use the `--tunnel` flag with your command to create a secure tunnel, as Safari has limitations when connecting to localhost servers:

<Tabs>
  <Tab title="Javascript SDK">
    1. Install the LangGraph JS SDK:

2. Send a message to the assistant (threadless run):

<Tab title="Rest API">
    
  </Tab>
</Tabs>

Now that you have a LangGraph app running locally, take your journey further by exploring deployment and advanced features:

* [Deployment quickstart](/langsmith/deployment-quickstart): Deploy your LangGraph app using LangSmith.

* [LangSmith](/langsmith/home): Learn about foundational LangSmith concepts.

* [JS/TS SDK Reference](https://reference.langchain.com/javascript/modules/_langchain_langgraph-sdk.html): Explore the JS/TS SDK API Reference.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/local-server.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
## 2. Create a LangGraph app 🌱

Create a new app from the [`new-langgraph-project-js` template](https://github.com/langchain-ai/new-langgraphjs-project). This template demonstrates a single-node application you can extend with your own logic.
```

Example 2 (unknown):
```unknown
## 3. Install dependencies

In the root of your new LangGraph app, install the dependencies in `edit` mode so your local changes are used by the server:
```

Example 3 (unknown):
```unknown
## 4. Create a `.env` file

You will find a `.env.example` in the root of your new LangGraph app. Create a `.env` file in the root of your new LangGraph app and copy the contents of the `.env.example` file into it, filling in the necessary API keys:
```

Example 4 (unknown):
```unknown
## 5. Launch Agent server 🚀

Start the LangGraph API server locally:
```

---

## Run a specific test file

**URL:** llms-txt#run-a-specific-test-file

uv run --group test pytest tests/integration_tests/test_chat_models.py

---

## Run a specific test function in a file

**URL:** llms-txt#run-a-specific-test-function-in-a-file

uv run --group test pytest tests/integration_tests/test_chat_models.py::test_chat_completions

---

## Run a specific test function within a class

**URL:** llms-txt#run-a-specific-test-function-within-a-class

**Contents:**
- Troubleshooting

uv run --group test pytest tests/integration_tests/test_chat_models.py::TestChatParrotLinkIntegration::test_chat_completions
```

For a full list of the standard test suites that are available, as well as information on which tests are included and how to troubleshoot common issues, see the [Standard Tests API Reference](https://reference.langchain.com/python/langchain_tests).

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/contributing/standard-tests-langchain.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Run backtests on a new version of an agent

**URL:** llms-txt#run-backtests-on-a-new-version-of-an-agent

**Contents:**
- Setup
  - Configure the environment

Source: https://docs.langchain.com/langsmith/run-backtests-new-agent

Deploying your application is just the beginning of a continuous improvement process. After you deploy to production, you'll want to refine your system by enhancing prompts, language models, tools, and architectures. Backtesting involves assessing new versions of your application using historical data and comparing the new outputs to the original ones. Compared to evaluations using pre-production datasets, backtesting offers a clearer indication of whether the new version of your application is an improvement over the current deployment.

Here are the basic steps for backtesting:

1. Select sample runs from your production tracing project to test against.
2. Transform the run inputs into a dataset and record the run outputs as an initial experiment against that dataset.
3. Execute your new system on the new dataset and compare the results of the experiments.

This process will provide you with a new dataset of representative inputs, which you can version and use for backtesting your models.

<Info>
  Often, you won't have definitive "ground truth" answers available. In such cases, you can manually label the outputs or use evaluators that don't rely on reference data. If your application allows for capturing ground-truth labels, for example by allowing users to leave feedback, we strongly recommend doing so.
</Info>

### Configure the environment

Install and set environment variables. This guide requires `langsmith>=0.2.4`.

<Info>
  For convenience we'll use the LangChain OSS framework in this tutorial, but the LangSmith functionality shown is framework-agnostic.
</Info>

```python  theme={null}
import getpass
import os

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>
```

---

## Run creation, streaming, updates, etc.

**URL:** llms-txt#run-creation,-streaming,-updates,-etc.

---

## Run evaluation

**URL:** llms-txt#run-evaluation

**Contents:**
- Reference code

experiment_results = await client.aevaluate(
    run_intent_classifier,
    data=dataset_name,
    evaluators=[correct],
    experiment_prefix="sql-agent-gpt4o-intent-classifier",
    max_concurrency=4,
)
`python  theme={null}
  import json
  import sqlite3
  from typing import Literal

from langchain.chat_models import init_chat_model
  from langchain.embeddings import init_embeddings
  from langchain_core.runnables import RunnableConfig
  from langchain.tools import tool
  from langchain_core.vectorstores import InMemoryVectorStore
  from langgraph.graph import END, StateGraph
  from langgraph.graph.message import AnyMessage, add_messages
  from langchain.agents import create_agent
  from langgraph.types import Command, interrupt
  from langsmith import Client
  import requests
  from tabulate import tabulate
  from typing_extensions import Annotated, TypedDict

url = "https://storage.googleapis.com/benchmarks-artifacts/chinook/Chinook.db"

response = requests.get(url)

if response.status_code == 200:
      # Open a local file in binary write mode
      with open("chinook.db", "wb") as file:
          # Write the content of the response (the file) to the local file
          file.write(response.content)
      print("File downloaded and saved as Chinook.db")
  else:
      print(f"Failed to download the file. Status code: {response.status_code}")

def _refund(
      invoice_id: int | None, invoice_line_ids: list[int] | None, mock: bool = False
  ) -> float:
      """Given an Invoice ID and/or Invoice Line IDs, delete the relevant Invoice/InvoiceLine records in the Chinook DB.

Args:
          invoice_id: The Invoice to delete.
          invoice_line_ids: The Invoice Lines to delete.
          mock: If True, do not actually delete the specified Invoice/Invoice Lines. Used for testing purposes.

Returns:
          float: The total dollar amount that was deleted (or mock deleted).
      """

if invoice_id is None and invoice_line_ids is None:
          return 0.0

# Connect to the Chinook database
      conn = sqlite3.connect("chinook.db")
      cursor = conn.cursor()

try:
          # If invoice_id is provided, delete entire invoice and its lines
          if invoice_id is not None:
              # First get the total amount for the invoice
              cursor.execute(
                  """
                  SELECT Total
                  FROM Invoice
                  WHERE InvoiceId = ?
              """,
                  (invoice_id,),
              )

result = cursor.fetchone()
              if result:
                  total_refund += result[0]

# Delete invoice lines first (due to foreign key constraints)
              if not mock:
                  cursor.execute(
                      """
                      DELETE FROM InvoiceLine
                      WHERE InvoiceId = ?
                  """,
                      (invoice_id,),
                  )

# Then delete the invoice
                  cursor.execute(
                      """
                      DELETE FROM Invoice
                      WHERE InvoiceId = ?
                  """,
                      (invoice_id,),
                  )

# If specific invoice lines are provided
          if invoice_line_ids is not None:
              # Get the total amount for the specified invoice lines
              placeholders = ",".join(["?" for _ in invoice_line_ids])
              cursor.execute(
                  f"""
                  SELECT SUM(UnitPrice * Quantity)
                  FROM InvoiceLine
                  WHERE InvoiceLineId IN ({placeholders})
              """,
                  invoice_line_ids,
              )

result = cursor.fetchone()
              if result and result[0]:
                  total_refund += result[0]

if not mock:
                  # Delete the specified invoice lines
                  cursor.execute(
                      f"""
                      DELETE FROM InvoiceLine
                      WHERE InvoiceLineId IN ({placeholders})
                  """,
                      invoice_line_ids,
                  )

# Commit the changes
          conn.commit()

except sqlite3.Error as e:
          # Roll back in case of error
          conn.rollback()
          raise e

finally:
          # Close the connection
          conn.close()

return float(total_refund)

def _lookup(
      customer_first_name: str,
      customer_last_name: str,
      customer_phone: str,
      track_name: str | None,
      album_title: str | None,
      artist_name: str | None,
      purchase_date_iso_8601: str | None,
  ) -> list[dict]:
      """Find all of the Invoice Line IDs in the Chinook DB for the given filters.

Returns:
          a list of dictionaries that contain keys: {
              'invoice_line_id',
              'track_name',
              'artist_name',
              'purchase_date',
              'quantity_purchased',
              'price_per_unit'
          }
      """

# Connect to the database
      conn = sqlite3.connect("chinook.db")
      cursor = conn.cursor()

# Base query joining all necessary tables
      query = """
      SELECT
          il.InvoiceLineId,
          t.Name as track_name,
          art.Name as artist_name,
          i.InvoiceDate as purchase_date,
          il.Quantity as quantity_purchased,
          il.UnitPrice as price_per_unit
      FROM InvoiceLine il
      JOIN Invoice i ON il.InvoiceId = i.InvoiceId
      JOIN Customer c ON i.CustomerId = c.CustomerId
      JOIN Track t ON il.TrackId = t.TrackId
      JOIN Album alb ON t.AlbumId = alb.AlbumId
      JOIN Artist art ON alb.ArtistId = art.ArtistId
      WHERE c.FirstName = ?
      AND c.LastName = ?
      AND c.Phone = ?
      """

# Parameters for the query
      params = [customer_first_name, customer_last_name, customer_phone]

# Add optional filters
      if track_name:
          query += " AND t.Name = ?"
          params.append(track_name)

if album_title:
          query += " AND alb.Title = ?"
          params.append(album_title)

if artist_name:
          query += " AND art.Name = ?"
          params.append(artist_name)

if purchase_date_iso_8601:
          query += " AND date(i.InvoiceDate) = date(?)"
          params.append(purchase_date_iso_8601)

# Execute query
      cursor.execute(query, params)

# Fetch results
      results = cursor.fetchall()

# Convert results to list of dictionaries
      output = []
      for row in results:
          output.append(
              {
                  "invoice_line_id": row[0],
                  "track_name": row[1],
                  "artist_name": row[2],
                  "purchase_date": row[3],
                  "quantity_purchased": row[4],
                  "price_per_unit": row[5],
              }
          )

# Close connection
      conn.close()

# Graph state.
  class State(TypedDict):
      """Agent state."""

messages: Annotated[list[AnyMessage], add_messages]
      followup: str | None

invoice_id: int | None
      invoice_line_ids: list[int] | None
      customer_first_name: str | None
      customer_last_name: str | None
      customer_phone: str | None
      track_name: str | None
      album_title: str | None
      artist_name: str | None
      purchase_date_iso_8601: str | None

# Instructions for extracting the user/purchase info from the conversation.
  gather_info_instructions = """You are managing an online music store that sells song tracks. \
  Customers can buy multiple tracks at a time and these purchases are recorded in a database as \
  an Invoice per purchase and an associated set of Invoice Lines for each purchased track.

Your task is to help customers who would like a refund for one or more of the tracks they've \
  purchased. In order for you to be able refund them, the customer must specify the Invoice ID \
  to get a refund on all the tracks they bought in a single transaction, or one or more Invoice \
  Line IDs if they would like refunds on individual tracks.

Often a user will not know the specific Invoice ID(s) or Invoice Line ID(s) for which they \
  would like a refund. In this case you can help them look up their invoices by asking them to \
  specify:
  - Required: Their first name, last name, and phone number.
  - Optionally: The track name, artist name, album name, or purchase date.

If the customer has not specified the required information (either Invoice/Invoice Line IDs \
  or first name, last name, phone) then please ask them to specify it."""

# Extraction schema, mirrors the graph state.
  class PurchaseInformation(TypedDict):
      """All of the known information about the invoice / invoice lines the customer would like refunded. Do not make up values, leave fields as null if you don't know their value."""

invoice_id: int | None
      invoice_line_ids: list[int] | None
      customer_first_name: str | None
      customer_last_name: str | None
      customer_phone: str | None
      track_name: str | None
      album_title: str | None
      artist_name: str | None
      purchase_date_iso_8601: str | None
      followup: Annotated[
          str | None,
          ...,
          "If the user hasn't enough identifying information, please tell them what the required information is and ask them to specify it.",
      ]

# Model for performing extraction.
  info_llm = init_chat_model("gpt-4o-mini").with_structured_output(
      PurchaseInformation, method="json_schema", include_raw=True
  )

# Graph node for extracting user info and routing to lookup/refund/END.
  async def gather_info(state: State) -> Command[Literal["lookup", "refund", END]]:
      info = await info_llm.ainvoke(
          [
              {"role": "system", "content": gather_info_instructions},
              *state["messages"],
          ]
      )
      parsed = info["parsed"]
      if any(parsed[k] for k in ("invoice_id", "invoice_line_ids")):
          goto = "refund"
      elif all(
          parsed[k]
          for k in ("customer_first_name", "customer_last_name", "customer_phone")
      ):
          goto = "lookup"
      else:
          goto = END
      update = {"messages": [info["raw"]], **parsed}
      return Command(update=update, goto=goto)

# Graph node for executing the refund.
  # Note that here we inspect the runtime config for an "env" variable.
  # If "env" is set to "test", then we don't actually delete any rows from our database.
  # This will become important when we're running our evaluations.
  def refund(state: State, config: RunnableConfig) -> dict:
      # Whether to mock the deletion. True if the configurable var 'env' is set to 'test'.
      mock = config.get("configurable", {}).get("env", "prod") == "test"
      refunded = _refund(
          invoice_id=state["invoice_id"],
          invoice_line_ids=state["invoice_line_ids"],
          mock=mock,
      )
      response = f"You have been refunded a total of: ${refunded:.2f}. Is there anything else I can help with?"
      return {
          "messages": [{"role": "assistant", "content": response}],
          "followup": response,
      }

# Graph node for looking up the users purchases
  def lookup(state: State) -> dict:
      args = (
          state[k]
          for k in (
              "customer_first_name",
              "customer_last_name",
              "customer_phone",
              "track_name",
              "album_title",
              "artist_name",
              "purchase_date_iso_8601",
          )
      )
      results = _lookup(*args)
      if not results:
          response = "We did not find any purchases associated with the information you've provided. Are you sure you've entered all of your information correctly?"
          followup = response
      else:
          response = f"Which of the following purchases would you like to be refunded for?\n\n"
          followup = f"Which of the following purchases would you like to be refunded for?\n\n{tabulate(results, headers='keys')}"
      return {
          "messages": [{"role": "assistant", "content": response}],
          "followup": followup,
          "invoice_line_ids": [res["invoice_line_id"] for res in results],
      }

# Building our graph
  graph_builder = StateGraph(State)

graph_builder.add_node(gather_info)
  graph_builder.add_node(refund)
  graph_builder.add_node(lookup)

graph_builder.set_entry_point("gather_info")
  graph_builder.add_edge("lookup", END)
  graph_builder.add_edge("refund", END)

refund_graph = graph_builder.compile()

# Our SQL queries will only work if we filter on the exact string values that are in the DB.
  # To ensure this, we'll create vectorstore indexes for all of the artists, tracks and albums
  # ahead of time and use those to disambiguate the user input. E.g. if a user searches for
  # songs by "prince" and our DB records the artist as "Prince", ideally when we query our
  # artist vectorstore for "prince" we'll get back the value "Prince", which we can then
  # use in our SQL queries.
  def index_fields() -> (
      tuple[InMemoryVectorStore, InMemoryVectorStore, InMemoryVectorStore]
  ):
      """Create an index for all artists, an index for all albums, and an index for all songs."""
      try:
          # Connect to the chinook database
          conn = sqlite3.connect("chinook.db")
          cursor = conn.cursor()

# Fetch all results
          tracks = cursor.execute("SELECT Name FROM Track").fetchall()
          artists = cursor.execute("SELECT Name FROM Artist").fetchall()
          albums = cursor.execute("SELECT Title FROM Album").fetchall()
      finally:
          # Close the connection
          if conn:
              conn.close()

embeddings = init_embeddings("openai:text-embedding-3-small")

track_store = InMemoryVectorStore(embeddings)
      artist_store = InMemoryVectorStore(embeddings)
      album_store = InMemoryVectorStore(embeddings)

track_store.add_texts([t[0] for t in tracks])
      artist_store.add_texts([a[0] for a in artists])
      album_store.add_texts([a[0] for a in albums])
      return track_store, artist_store, album_store

track_store, artist_store, album_store = index_fields()

# Agent tools
  @tool
  def lookup_track(
      track_name: str | None = None,
      album_title: str | None = None,
      artist_name: str | None = None,
  ) -> list[dict]:
      """Lookup a track in Chinook DB based on identifying information about.

Returns:
          a list of dictionaries per matching track that contain keys {'track_name', 'artist_name', 'album_name'}
      """
      conn = sqlite3.connect("chinook.db")
      cursor = conn.cursor()

query = """
      SELECT DISTINCT t.Name as track_name, ar.Name as artist_name, al.Title as album_name
      FROM Track t
      JOIN Album al ON t.AlbumId = al.AlbumId
      JOIN Artist ar ON al.ArtistId = ar.ArtistId
      WHERE 1=1
      """
      params = []

if track_name:
          track_name = track_store.similarity_search(track_name, k=1)[0].page_content
          query += " AND t.Name LIKE ?"
          params.append(f"%{track_name}%")
      if album_title:
          album_title = album_store.similarity_search(album_title, k=1)[0].page_content
          query += " AND al.Title LIKE ?"
          params.append(f"%{album_title}%")
      if artist_name:
          artist_name = artist_store.similarity_search(artist_name, k=1)[0].page_content
          query += " AND ar.Name LIKE ?"
          params.append(f"%{artist_name}%")

cursor.execute(query, params)
      results = cursor.fetchall()

tracks = [
          {"track_name": row[0], "artist_name": row[1], "album_name": row[2]}
          for row in results
      ]

conn.close()
      return tracks

@tool
  def lookup_album(
      track_name: str | None = None,
      album_title: str | None = None,
      artist_name: str | None = None,
  ) -> list[dict]:
      """Lookup an album in Chinook DB based on identifying information about.

Returns:
          a list of dictionaries per matching album that contain keys {'album_name', 'artist_name'}
      """
      conn = sqlite3.connect("chinook.db")
      cursor = conn.cursor()

query = """
      SELECT DISTINCT al.Title as album_name, ar.Name as artist_name
      FROM Album al
      JOIN Artist ar ON al.ArtistId = ar.ArtistId
      LEFT JOIN Track t ON t.AlbumId = al.AlbumId
      WHERE 1=1
      """
      params = []

if track_name:
          query += " AND t.Name LIKE ?"
          params.append(f"%{track_name}%")
      if album_title:
          query += " AND al.Title LIKE ?"
          params.append(f"%{album_title}%")
      if artist_name:
          query += " AND ar.Name LIKE ?"
          params.append(f"%{artist_name}%")

cursor.execute(query, params)
      results = cursor.fetchall()

albums = [{"album_name": row[0], "artist_name": row[1]} for row in results]

conn.close()
      return albums

@tool
  def lookup_artist(
      track_name: str | None = None,
      album_title: str | None = None,
      artist_name: str | None = None,
  ) -> list[str]:
      """Lookup an album in Chinook DB based on identifying information about.

Returns:
          a list of matching artist names
      """
      conn = sqlite3.connect("chinook.db")
      cursor = conn.cursor()

query = """
      SELECT DISTINCT ar.Name as artist_name
      FROM Artist ar
      LEFT JOIN Album al ON al.ArtistId = ar.ArtistId
      LEFT JOIN Track t ON t.AlbumId = al.AlbumId
      WHERE 1=1
      """
      params = []

if track_name:
          query += " AND t.Name LIKE ?"
          params.append(f"%{track_name}%")
      if album_title:
          query += " AND al.Title LIKE ?"
          params.append(f"%{album_title}%")
      if artist_name:
          query += " AND ar.Name LIKE ?"
          params.append(f"%{artist_name}%")

cursor.execute(query, params)
      results = cursor.fetchall()

artists = [row[0] for row in results]

conn.close()
      return artists

# Agent model
  qa_llm = init_chat_model("claude-sonnet-4-5-20250929")
  # The prebuilt ReACT agent only expects State to have a 'messages' key, so the
  # state we defined for the refund agent can also be passed to our lookup agent.
  qa_graph = create_agent(qa_llm, [lookup_track, lookup_artist, lookup_album])

# Schema for routing user intent.
  # We'll use structured outputs to enforce that the model returns only
  # the desired output.
  class UserIntent(TypedDict):
      """The user's current intent in the conversation"""

intent: Literal["refund", "question_answering"]

# Routing model with structured output
  router_llm = init_chat_model("gpt-4o-mini").with_structured_output(
      UserIntent, method="json_schema", strict=True
  )

# Instructions for routing.
  route_instructions = """You are managing an online music store that sells song tracks. \
  You can help customers in two types of ways: (1) answering general questions about \
  tracks sold at your store, (2) helping them get a refund on a purhcase they made at your store.

Based on the following conversation, determine if the user is currently seeking general \
  information about song tracks or if they are trying to refund a specific purchase.

Return 'refund' if they are trying to get a refund and 'question_answering' if they are \
  asking a general music question. Do NOT return anything else. Do NOT try to respond to \
  the user.
  """

# Node for routing.
  async def intent_classifier(
      state: State,
  ) -> Command[Literal["refund_agent", "question_answering_agent"]]:
      response = router_llm.invoke(
          [{"role": "system", "content": route_instructions}, *state["messages"]]
      )
      return Command(goto=response["intent"] + "_agent")

# Node for making sure the 'followup' key is set before our agent run completes.
  def compile_followup(state: State) -> dict:
      """Set the followup to be the last message if it hasn't explicitly been set."""
      if not state.get("followup"):
          return {"followup": state["messages"][-1].content}
      return {}

# Agent definition
  graph_builder = StateGraph(State)
  graph_builder.add_node(intent_classifier)
  # Since all of our subagents have compatible state,
  # we can add them as nodes directly.
  graph_builder.add_node("refund_agent", refund_graph)
  graph_builder.add_node("question_answering_agent", qa_graph)
  graph_builder.add_node(compile_followup)

graph_builder.set_entry_point("intent_classifier")
  graph_builder.add_edge("refund_agent", "compile_followup")
  graph_builder.add_edge("question_answering_agent", "compile_followup")
  graph_builder.add_edge("compile_followup", END)

graph = graph_builder.compile()

# Create a dataset
  examples = [
      {
          "inputs": {
              "question": "How many songs do you have by James Brown"
          },
          "outputs": {
              "response": "We have 20 songs by James Brown",
              "trajectory": ["question_answering_agent", "lookup_tracks"]
          },
      },
      {
          "inputs": {
              "question": "My name is Aaron Mitchell and I'd like a refund.",
          },
          "outputs": {
              "response": "I need some more information to help you with the refund. Please specify your phone number, the invoice ID, or the line item IDs for the purchase you'd like refunded.",
              "trajectory": ["refund_agent"],
          }
      },
      {
          "inputs": {
              "question": "My name is Aaron Mitchell and I'd like a refund on my Led Zeppelin purchases. My number is +1 (204) 452-6452",
          },
          "outputs": {
              "response": "Which of the following purchases would you like to be refunded for?\n\n  invoice_line_id  track_name                        artist_name    purchase_date          quantity_purchased    price_per_unit\n-----------------  --------------------------------  -------------  -------------------  --------------------  ----------------\n              267  How Many More Times               Led Zeppelin   2009-08-06 00:00:00                     1              0.99\n              268  What Is And What Should Never Be  Led Zeppelin   2009-08-06 00:00:00                     1              0.99",
              "trajectory": ["refund_agent", "lookup"],
          },
      },
      {
          "inputs": {
              "question": "Who recorded Wish You Were Here again? What other albums of there's do you have?",
          },
          "outputs": {
              "response": "Wish You Were Here is an album by Pink Floyd",
              "trajectory": ["question_answering_agent", "lookup_album"],
          }
      },
      {
          "inputs": {
              "question": "I want a full refund for invoice 237",
          },
          "outputs": {
              "response": "You have been refunded $2.97.",
              "trajectory": ["refund_agent", "refund"],
          },
      },
  ]

dataset_name = "Chinook Customer Service Bot: E2E"

if not client.has_dataset(dataset_name=dataset_name):
      dataset = client.create_dataset(dataset_name=dataset_name)
      client.create_examples(
          dataset_id=dataset.id,
          examples=examples
      )

# LLM-as-judge instructions
  grader_instructions = """You are a teacher grading a quiz.

You will be given a QUESTION, the GROUND TRUTH (correct) RESPONSE, and the STUDENT RESPONSE.

Here is the grade criteria to follow:
  (1) Grade the student responses based ONLY on their factual accuracy relative to the ground truth answer.
  (2) Ensure that the student response does not contain any conflicting statements.
  (3) It is OK if the student response contains more information than the ground truth response, as long as it is factually accurate relative to the  ground truth response.

Correctness:
  True means that the student's response meets all of the criteria.
  False means that the student's response does not meet all of the criteria.

Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct."""

# LLM-as-judge output schema
  class Grade(TypedDict):
      """Compare the expected and actual answers and grade the actual answer."""

reasoning: Annotated[
          str,
          ...,
          "Explain your reasoning for whether the actual response is correct or not.",
      ]
      is_correct: Annotated[
          bool,
          ...,
          "True if the student response is mostly or exactly correct, otherwise False.",
      ]

# Judge LLM
  grader_llm = init_chat_model("gpt-4o-mini", temperature=0).with_structured_output(
      Grade, method="json_schema", strict=True
  )

# Evaluator function
  async def final_answer_correct(
      inputs: dict, outputs: dict, reference_outputs: dict
  ) -> bool:
      """Evaluate if the final response is equivalent to reference response."""

# Note that we assume the outputs has a 'response' dictionary. We'll need to make sure
      # that the target function we define includes this key.
      user = f"""QUESTION: {inputs['question']}
      GROUND TRUTH RESPONSE: {reference_outputs['response']}
      STUDENT RESPONSE: {outputs['response']}"""

grade = await grader_llm.ainvoke(
          [
              {"role": "system", "content": grader_instructions},
              {"role": "user", "content": user},
          ]
      )
      return grade["is_correct"]

# Target function
  async def run_graph(inputs: dict) -> dict:
      """Run graph and track the trajectory it takes along with the final response."""
      result = await graph.ainvoke(
          {
              "messages": [
                  {"role": "user", "content": inputs["question"]},
              ]
          },
          config={"env": "test"},
      )
      return {"response": result["followup"]}

# Evaluation job and results
  experiment_results = await client.aevaluate(
      run_graph,
      data=dataset_name,
      evaluators=[final_answer_correct],
      experiment_prefix="sql-agent-gpt4o-e2e",
      num_repetitions=1,
      max_concurrency=4,
  )
  experiment_results.to_pandas()

def trajectory_subsequence(outputs: dict, reference_outputs: dict) -> float:
      """Check how many of the desired steps the agent took."""
      if len(reference_outputs["trajectory"]) > len(outputs["trajectory"]):
          return False

i = j = 0
      while i < len(reference_outputs["trajectory"]) and j < len(outputs["trajectory"]):
          if reference_outputs["trajectory"][i] == outputs["trajectory"][j]:
              i += 1
          j += 1

return i / len(reference_outputs["trajectory"])

async def run_graph(inputs: dict) -> dict:
      """Run graph and track the trajectory it takes along with the final response."""
      trajectory = []
      # Set subgraph=True to stream events from subgraphs of the main graph: https://langchain-ai.github.io/langgraph/how-tos/streaming-subgraphs/
      # Set stream_mode="debug" to stream all possible events: https://langchain-ai.github.io/langgra/langsmith/observability-concepts/streaming
      async for namespace, chunk in graph.astream(
          {
              "messages": [
                  {
                      "role": "user",
                      "content": inputs["question"],
                  }
              ]
          },
          subgraphs=True,
          stream_mode="debug",
      ):
          # Event type for entering a node
          if chunk["type"] == "task":
              # Record the node name
              trajectory.append(chunk["payload"]["name"])
              # Given how we defined our dataset, we also need to track when specific tools are
              # called by our question answering ReACT agent. These tool calls can be found
              # when the ToolsNode (named "tools") is invoked by looking at the AIMessage.tool_calls
              # of the latest input message.
              if chunk["payload"]["name"] == "tools" and chunk["type"] == "task":
                  for tc in chunk["payload"]["input"]["messages"][-1].tool_calls:
                      trajectory.append(tc["name"])

return {"trajectory": trajectory}

experiment_results = await client.aevaluate(
      run_graph,
      data=dataset_name,
      evaluators=[trajectory_subsequence],
      experiment_prefix="sql-agent-gpt4o-trajectory",
      num_repetitions=1,
      max_concurrency=4,
  )
  experiment_results.to_pandas()

# Create dataset
  examples = [
      {
          "inputs": {
              "messages": [
                  {
                      "role": "user",
                      "content": "i bought some tracks recently and i dont like them",
                  }
              ],
          }
          "outputs": {"route": "refund_agent"},
      },
      {
          "inputs": {
              "messages": [
                  {
                      "role": "user",
                      "content": "I was thinking of purchasing some Rolling Stones tunes, any recommendations?",
                  }
              ],
          },
          "outputs": {"route": "question_answering_agent"},
      },
      {
          "inputs": {
              "messages": [
                      {"role": "user", "content": "i want a refund on purchase 237"},
                  {
                      "role": "assistant",
                      "content": "I've refunded you a total of $1.98. How else can I help you today?",
                  },
                  {"role": "user", "content": "did prince release any albums in 2000?"},
              ],
          },
          "outputs": {"route": "question_answering_agent"},
      },
      {
          "inputs": {
              "messages": [
                  {
                      "role": "user",
                      "content": "i purchased a cover of Yesterday recently but can't remember who it was by, which versions of it do you have?",
                  }
              ],
          },
          "outputs": {"route": "question_answering_agent"},
      },
  ]

dataset_name = "Chinook Customer Service Bot: Intent Classifier"
  if not client.has_dataset(dataset_name=dataset_name):
      dataset = client.create_dataset(dataset_name=dataset_name)
      client.create_examples(
          dataset_id=dataset.id,
          examples=examples,
      )

# Evaluator
  def correct(outputs: dict, reference_outputs: dict) -> bool:
      """Check if the agent chose the correct route."""
      return outputs["route"] == reference_outputs["route"]

# Target function for running the relevant step
  async def run_intent_classifier(inputs: dict) -> dict:
      # Note that we can access and run the intent_classifier node of our graph directly.
      command = await graph.nodes["intent_classifier"].ainvoke(inputs)
      return {"route": command.goto}

# Run evaluation
  experiment_results = await client.aevaluate(
      run_intent_classifier,
      data=dataset_name,
      evaluators=[correct],
      experiment_prefix="sql-agent-gpt4o-intent-classifier",
      max_concurrency=4,
  )
  experiment_results.to_pandas()
  ````
</Accordion>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/evaluate-complex-agent.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
You can see what these results look like here: [LangSmith link](https://smith.langchain.com/public/f133dae2-8a88-43a0-9bfd-ab45bfa3920b/d).

## Reference code

Here's a consolidated script with all the above code:

<Accordion title="Reference code">
```

---

## Run (span) data format

**URL:** llms-txt#run-(span)-data-format

Source: https://docs.langchain.com/langsmith/run-data-format

<Check>
  Before diving into this content, it might be helpful to read the following:

* [Conceptual guide on tracing and runs](/langsmith/observability-concepts)
</Check>

LangSmith stores and processes trace data in a simple format that is easy to export and import.

Many of these fields are optional or not important to know about but are included for completeness. The **bolded** fields are the most important ones to know about.

| Field Name                    | Type             | Description                                                                                                                   |
| ----------------------------- | ---------------- | ----------------------------------------------------------------------------------------------------------------------------- |
| **id**                        | UUID             | Unique identifier for the span.                                                                                               |
| **name**                      | string           | The name associated with the run.                                                                                             |
| **inputs**                    | object           | A map or set of inputs provided to the run.                                                                                   |
| **run\_type**                 | string           | Type of run, e.g., "llm", "chain", "tool".                                                                                    |
| **start\_time**               | datetime         | Start time of the run.                                                                                                        |
| **end\_time**                 | datetime         | End time of the run.                                                                                                          |
| **extra**                     | object           | Any extra information run.                                                                                                    |
| **error**                     | string           | Error message if the run encountered an error.                                                                                |
| **outputs**                   | object           | A map or set of outputs generated by the run.                                                                                 |
| **events**                    | array of objects | A list of event objects associated with the run. This is relevant for runs executed with streaming.                           |
| **tags**                      | array of strings | Tags or labels associated with the run.                                                                                       |
| **trace\_id**                 | UUID             | Unique identifier for the trace the run is a part of. This is also the `id` field of the root run of the trace                |
| **dotted\_order**             | string           | Ordering string, hierarchical. Format: `run_start_time`Z`run_uuid`.`child_run_start_time`Z`child_run_uuid`...                 |
| **status**                    | string           | Current status of the run execution, e.g., "error", "pending", "success"                                                      |
| **child\_run\_ids**           | array of UUIDs   | List of IDs for all child runs.                                                                                               |
| **direct\_child\_run\_ids**   | array of UUIDs   | List of IDs for direct children of this run.                                                                                  |
| **parent\_run\_ids**          | array of UUIDs   | List of IDs for all parent runs.                                                                                              |
| **feedback\_stats**           | object           | Aggregations of feedback statistics for this run                                                                              |
| **reference\_example\_id**    | UUID             | ID of a reference example associated with the run. This is usually only present for evaluation runs.                          |
| **total\_tokens**             | integer          | Total number of tokens processed by the run.                                                                                  |
| **prompt\_tokens**            | integer          | Number of tokens in the prompt of the run.                                                                                    |
| **completion\_tokens**        | integer          | Number of tokens in the completion of the run.                                                                                |
| **total\_cost**               | string           | Total cost associated with processing the run.                                                                                |
| **prompt\_cost**              | string           | Cost associated with the prompt part of the run.                                                                              |
| **completion\_cost**          | string           | Cost associated with the completion of the run.                                                                               |
| **first\_token\_time**        | datetime         | Time when the first token of a model output was generated. Only applies for runs with `run_type="llm"` and streaming enabled. |
| **session\_id**               | string           | Session identifier for the run, also known as the tracing project ID.                                                         |
| **in\_dataset**               | boolean          | Indicates whether the run is included in a dataset.                                                                           |
| **parent\_run\_id**           | UUID             | Unique identifier of the parent run.                                                                                          |
| execution\_order (deprecated) | integer          | The order in which this run was executed within the trace.                                                                    |
| serialized                    | object           | Serialized state of the object executing the run if applicable.                                                               |
| manifest\_id (deprecated)     | UUID             | Identifier for a manifest associated with the span.                                                                           |
| manifest\_s3\_id              | UUID             | S3 identifier for the manifest.                                                                                               |
| inputs\_s3\_urls              | object           | S3 URLs for the inputs.                                                                                                       |
| outputs\_s3\_urls             | object           | S3 URLs for the outputs.                                                                                                      |
| price\_model\_id              | UUID             | Identifier for the pricing model applied to the run.                                                                          |
| app\_path                     | string           | Application (UI) path for this run.                                                                                           |
| last\_queued\_at              | datetime         | Last time the span was queued.                                                                                                |
| share\_token                  | string           | Token for sharing access to the run's data.                                                                                   |

Here is an example of a JSON representation of a run in the above format:

#### What is `dotted_order`?

A run's dotted order is a sortable key that fully specifies its location within the tracing hierarchy.

Take the following example:

If you print out the IDs at each stage, you may get the following:

Note a few invariants:

* The "id" is equal to the last 36 characters of the dotted order (the suffix after the final "Z"). See `0ec6b845-18b9-4aa1-8f1b-6ba3f9fdefd6` for example in the grandchild.
* The "trace\_id" is equal to the first UUID in the dotted order (i.e., `dotted_order.split('.')[0].split('Z')[1]`)
* If "parent\_run\_id" exists, it is the penultimate UUID in the dotted order. See `a8024e23-5b82-47fd-970e-f6a5ba3f5097` in the grandchild, for an example.
* If you split the dotted\_order on the dots, each segment is formatted as (`<run_start_time>Z<run_id>`)

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/run-data-format.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
#### What is `dotted_order`?

A run's dotted order is a sortable key that fully specifies its location within the tracing hierarchy.

Take the following example:
```

Example 2 (unknown):
```unknown
If you print out the IDs at each stage, you may get the following:
```

---

## Run support queries against ClickHouse

**URL:** llms-txt#run-support-queries-against-clickhouse

**Contents:**
  - Prerequisites
  - Running the query script

Source: https://docs.langchain.com/langsmith/script-running-ch-support-queries

This Helm repository contains queries to produce output that the LangSmith UI does not currently support directly (e.g. obtaining query exception logs from Clickhouse).

This command takes a clickhouse connection string that contains an embedded name and password (which can be passed in from a call to a secrets manager) and executes a query from an input file. In the example below, we are using the `ch_get_query_exceptions.sql` input file in the `support_queries/clickhouse` directory.

Ensure you have the following tools/items ready.

* [https://kubernetes.io/docs/tasks/tools/](https://kubernetes.io/docs/tasks/tools/)

2. Clickhouse database credentials

* Host
   * Port
   * Username
     * If using the bundled version, this is `default`
   * Password
     * If using the bundled version, this is `password`
   * Database name
     * If using the bundled version, this is `default`

3. Connectivity to the Clickhouse database from the machine you will be running the migration script on.

* If you are using the bundled version, you may need to port forward the clickhouse service to your local machine.
   * Run `kubectl port-forward svc/langsmith-clickhouse 8123:8123` to port forward the clickhouse service to your local machine.

4. The script to run a support query

* You can download the script from [here](https://github.com/langchain-ai/helm/blob/main/charts/langsmith/scripts/run_support_query_ch.sh)

### Running the query script

Run the following command to run the desired query:

For example, if you are using the bundled version with port-forwarding, the command might look like:

which will output query logs for all queries that have thrown exceptions in Clickhouse in the last 7 days. To extract this to a file add the flag `--output path/to/file.csv`

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/script-running-ch-support-queries.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
For example, if you are using the bundled version with port-forwarding, the command might look like:
```

---

## Run support queries against PostgreSQL

**URL:** llms-txt#run-support-queries-against-postgresql

**Contents:**
- Prerequisites
- Running the query script
- Export usage data
  - Get customer information

Source: https://docs.langchain.com/langsmith/script-running-pg-support-queries

This Helm repository contains queries to produce output that the LangSmith UI does not currently support directly (e.g. obtaining trace counts for multiple organizations in a single query).

This command takes a postgres connection string that contains an embedded name and password (which can be passed in from a call to a secrets manager) and executes a query from an input file. In the example below, we are using the `pg_get_trace_counts_daily.sql` input file in the `support_queries/postgres` directory.

Ensure you have the following tools/items ready.

* [https://kubernetes.io/docs/tasks/tools/](https://kubernetes.io/docs/tasks/tools/)

* [https://www.postgresql.org/download/](https://www.postgresql.org/download/)

3. PostgreSQL database connection:

* Host
   * Port
   * Username
     * If using the bundled version, this is `postgres`
   * Password
     * If using the bundled version, this is `postgres`
   * Database name
     * If using the bundled version, this is `postgres`

4. Connectivity to the PostgreSQL database from the machine you will be running the migration script on.

* If you are using the bundled version, you may need to port forward the postgresql service to your local machine.
   * Run `kubectl port-forward svc/langsmith-postgres 5432:5432` to port forward the postgresql service to your local machine.

5. The script to run a support query

* You can download the script from [here](https://github.com/langchain-ai/helm/blob/main/charts/langsmith/scripts/run_support_query_pg.sh)

## Running the query script

Run the following command to run the desired query:

For example, if you are using the bundled version with port-forwarding, the command might look like:

which will output the count of daily traces by workspace ID and organization ID. To extract this to a file add the flag `--output path/to/file.csv`

<Note>
  Exporting usage data requires running Helm chart version 0.11.4 or later.
</Note>

### Get customer information

You need to retrieve your customer information from the LangSmith API before running the export scripts. This information is required as input for the export scripts.

```bash  theme={null}
curl https://<langsmith_url>/api/v1/info

**Examples:**

Example 1 (unknown):
```unknown
For example, if you are using the bundled version with port-forwarding, the command might look like:
```

Example 2 (unknown):
```unknown
which will output the count of daily traces by workspace ID and organization ID. To extract this to a file add the flag `--output path/to/file.csv`

## Export usage data

<Note>
  Exporting usage data requires running Helm chart version 0.11.4 or later.
</Note>

### Get customer information

You need to retrieve your customer information from the LangSmith API before running the export scripts. This information is required as input for the export scripts.
```

---

## Run the agent

**URL:** llms-txt#run-the-agent

**Contents:**
- Build a real-world agent

agent.invoke(
    {"messages": [{"role": "user", "content": "what is the weather in sf"}]}
)
python wrap theme={null}
    SYSTEM_PROMPT = """You are an expert weather forecaster, who speaks in puns.

You have access to two tools:

- get_weather_for_location: use this to get the weather for a specific location
    - get_user_location: use this to get the user's location

If a user asks you for the weather, make sure you know the location. If you can tell from the question that they mean wherever they are, use the get_user_location tool to find their location."""
    python  theme={null}
    from dataclasses import dataclass
    from langchain.tools import tool, ToolRuntime

@tool
    def get_weather_for_location(city: str) -> str:
        """Get weather for a given city."""
        return f"It's always sunny in {city}!"

@dataclass
    class Context:
        """Custom runtime context schema."""
        user_id: str

@tool
    def get_user_location(runtime: ToolRuntime[Context]) -> str:
        """Retrieve user information based on user ID."""
        user_id = runtime.context.user_id
        return "Florida" if user_id == "1" else "SF"
    python  theme={null}
    from langchain.chat_models import init_chat_model

model = init_chat_model(
        "claude-sonnet-4-5-20250929",
        temperature=0.5,
        timeout=10,
        max_tokens=1000
    )
    python  theme={null}
    from dataclasses import dataclass

# We use a dataclass here, but Pydantic models are also supported.
    @dataclass
    class ResponseFormat:
        """Response schema for the agent."""
        # A punny response (always required)
        punny_response: str
        # Any interesting information about the weather if available
        weather_conditions: str | None = None
    python  theme={null}
    from langgraph.checkpoint.memory import InMemorySaver

checkpointer = InMemorySaver()
    python  theme={null}
    agent = create_agent(
        model=model,
        system_prompt=SYSTEM_PROMPT,
        tools=[get_user_location, get_weather_for_location],
        context_schema=Context,
        response_format=ResponseFormat,
        checkpointer=checkpointer
    )

# `thread_id` is a unique identifier for a given conversation.
    config = {"configurable": {"thread_id": "1"}}

response = agent.invoke(
        {"messages": [{"role": "user", "content": "what is the weather outside?"}]},
        config=config,
        context=Context(user_id="1")
    )

print(response['structured_response'])
    # ResponseFormat(
    #     punny_response="Florida is still having a 'sun-derful' day! The sunshine is playing 'ray-dio' hits all day long! I'd say it's the perfect weather for some 'solar-bration'! If you were hoping for rain, I'm afraid that idea is all 'washed up' - the forecast remains 'clear-ly' brilliant!",
    #     weather_conditions="It's always sunny in Florida!"
    # )

# Note that we can continue the conversation using the same `thread_id`.
    response = agent.invoke(
        {"messages": [{"role": "user", "content": "thank you!"}]},
        config=config,
        context=Context(user_id="1")
    )

print(response['structured_response'])
    # ResponseFormat(
    #     punny_response="You're 'thund-erfully' welcome! It's always a 'breeze' to help you stay 'current' with the weather. I'm just 'cloud'-ing around waiting to 'shower' you with more forecasts whenever you need them. Have a 'sun-sational' day in the Florida sunshine!",
    #     weather_conditions=None
    # )
    python  theme={null}
  from dataclasses import dataclass

from langchain.agents import create_agent
  from langchain.chat_models import init_chat_model
  from langchain.tools import tool, ToolRuntime
  from langgraph.checkpoint.memory import InMemorySaver

# Define system prompt
  SYSTEM_PROMPT = """You are an expert weather forecaster, who speaks in puns.

You have access to two tools:

- get_weather_for_location: use this to get the weather for a specific location
  - get_user_location: use this to get the user's location

If a user asks you for the weather, make sure you know the location. If you can tell from the question that they mean wherever they are, use the get_user_location tool to find their location."""

# Define context schema
  @dataclass
  class Context:
      """Custom runtime context schema."""
      user_id: str

# Define tools
  @tool
  def get_weather_for_location(city: str) -> str:
      """Get weather for a given city."""
      return f"It's always sunny in {city}!"

@tool
  def get_user_location(runtime: ToolRuntime[Context]) -> str:
      """Retrieve user information based on user ID."""
      user_id = runtime.context.user_id
      return "Florida" if user_id == "1" else "SF"

# Configure model
  model = init_chat_model(
      "claude-sonnet-4-5-20250929",
      temperature=0
  )

# Define response format
  @dataclass
  class ResponseFormat:
      """Response schema for the agent."""
      # A punny response (always required)
      punny_response: str
      # Any interesting information about the weather if available
      weather_conditions: str | None = None

# Set up memory
  checkpointer = InMemorySaver()

# Create agent
  agent = create_agent(
      model=model,
      system_prompt=SYSTEM_PROMPT,
      tools=[get_user_location, get_weather_for_location],
      context_schema=Context,
      response_format=ResponseFormat,
      checkpointer=checkpointer
  )

# Run agent
  # `thread_id` is a unique identifier for a given conversation.
  config = {"configurable": {"thread_id": "1"}}

response = agent.invoke(
      {"messages": [{"role": "user", "content": "what is the weather outside?"}]},
      config=config,
      context=Context(user_id="1")
  )

print(response['structured_response'])
  # ResponseFormat(
  #     punny_response="Florida is still having a 'sun-derful' day! The sunshine is playing 'ray-dio' hits all day long! I'd say it's the perfect weather for some 'solar-bration'! If you were hoping for rain, I'm afraid that idea is all 'washed up' - the forecast remains 'clear-ly' brilliant!",
  #     weather_conditions="It's always sunny in Florida!"
  # )

# Note that we can continue the conversation using the same `thread_id`.
  response = agent.invoke(
      {"messages": [{"role": "user", "content": "thank you!"}]},
      config=config,
      context=Context(user_id="1")
  )

print(response['structured_response'])
  # ResponseFormat(
  #     punny_response="You're 'thund-erfully' welcome! It's always a 'breeze' to help you stay 'current' with the weather. I'm just 'cloud'-ing around waiting to 'shower' you with more forecasts whenever you need them. Have a 'sun-sational' day in the Florida sunshine!",
  #     weather_conditions=None
  # )
  ```
</Expandable>

<Tip>
  To learn how to trace your agent with LangSmith, see the [LangSmith documentation](/langsmith/trace-with-langchain).
</Tip>

Congratulations! You now have an AI agent that can:

* **Understand context** and remember conversations
* **Use multiple tools** intelligently
* **Provide structured responses** in a consistent format
* **Handle user-specific information** through context
* **Maintain conversation state** across interactions

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/quickstart.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
<Tip>
  To learn how to trace your agent with LangSmith, see the [LangSmith documentation](/langsmith/trace-with-langchain).
</Tip>

## Build a real-world agent

Next, build a practical weather forecasting agent that demonstrates key production concepts:

1. **Detailed system prompts** for better agent behavior
2. **Create tools** that integrate with external data
3. **Model configuration** for consistent responses
4. **Structured output** for predictable results
5. **Conversational memory** for chat-like interactions
6. **Create and run the agent** create a fully functional agent

Let's walk through each step:

<Steps>
  <Step title="Define the system prompt">
    The system prompt defines your agent’s role and behavior. Keep it specific and actionable:
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Create tools">
    [Tools](/oss/python/langchain/tools) let a model interact with external systems by calling functions you define.
    Tools can depend on [runtime context](/oss/python/langchain/runtime) and also interact with [agent memory](/oss/python/langchain/short-term-memory).

    Notice below how the `get_user_location` tool uses runtime context:
```

Example 3 (unknown):
```unknown
<Tip>
      Tools should be well-documented: their name, description, and argument names become part of the model's prompt.
      LangChain's [`@tool` decorator](https://reference.langchain.com/python/langchain/tools/#langchain.tools.tool) adds metadata and enables runtime injection via the `ToolRuntime` parameter.
    </Tip>
  </Step>

  <Step title="Configure your model">
    Set up your [language model](/oss/python/langchain/models) with the right [parameters](/oss/python/langchain/models#parameters) for your use case:
```

Example 4 (unknown):
```unknown
</Step>

  <Step title="Define response format">
    Optionally, define a structured response format if you need the agent responses to match
    a specific schema.
```

---

## Run the agent - all steps will be traced automatically

**URL:** llms-txt#run-the-agent---all-steps-will-be-traced-automatically

**Contents:**
- Trace selectively

response = agent.invoke({
    "messages": [{"role": "user", "content": "Search for the latest AI news and email a summary to john@example.com"}]
})
python  theme={null}
import langsmith as ls

**Examples:**

Example 1 (unknown):
```unknown
By default, the trace will be logged to the project with the name `default`. To configure a custom project name, see [Log to a project](#log-to-a-project).

## Trace selectively

You may opt to trace specific invocations or parts of your application using LangSmith's `tracing_context` context manager:
```

---

## Run the export script with customer information as variables

**URL:** llms-txt#run-the-export-script-with-customer-information-as-variables

**Contents:**
  - Status update

sh run_support_query_pg.sh <postgres_url> \
  --input support_queries/postgres/pg_usage_traces_backfill_export.sql \
  --output ls_export.csv \
  -v customer_id=$CUSTOMER_ID \
  -v customer_name=$CUSTOMER_NAME
bash  theme={null}
sh run_support_query_pg.sh <postgres_url> \
  --input support_queries/postgres/pg_usage_nodes_backfill_export.sql \
  --output lgp_export.csv \
  -v customer_id=$CUSTOMER_ID \
  -v customer_name=$CUSTOMER_NAME
bash  theme={null}
sh run_support_query_pg.sh <postgres_url> --input support_queries/postgres/pg_usage_traces_backfill_update.sql --output export.csv -v backfill_id=<backfill_id>
bash  theme={null}
sh run_support_query_pg.sh <postgres_url> --input support_queries/postgres/pg_usage_nodes_backfill_update.sql --output export.csv -v backfill_id=<backfill_id>
```

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/script-running-pg-support-queries.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
To export LangSmith usage:
```

Example 2 (unknown):
```unknown
### Status update

These scripts update the status of usage events in your installation to reflect that the events have been successfully processed by LangChain.

The scripts require passing in the corresponding `backfill_id`, which will be confirmed by your LangChain rep.

To update LangSmith trace usage:
```

Example 3 (unknown):
```unknown
To update LangSmith usage:
```

---

## Run the graph

**URL:** llms-txt#run-the-graph

**Contents:**
- Use user-scoped MCP tools in your deployment
- Session behavior
- Authentication
- Disable MCP

print(graph.invoke({"question": "hi"}))
python  theme={null}
from langchain_mcp_adapters.client import MultiServerMCPClient

def mcp_tools_node(state, config):
    user = config["configurable"].get("langgraph_auth_user")
         , user["github_token"], user["email"], etc.

client = MultiServerMCPClient({
        "github": {
            "transport": "streamable_http", # (1)
            "url": "https://my-github-mcp-server/mcp", # (2)
            "headers": {
                "Authorization": f"Bearer {user['github_token']}"
            }
        }
    })
    tools = await client.get_tools() # (3)

# Your tool-calling logic here

tool_messages = ...
    return {"messages": tool_messages}
json  theme={null}
{
  "http": {
    "disable_mcp": true
  }
}
```

This will prevent the server from exposing the `/mcp` endpoint.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/server-mcp.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
For more details, see the [low-level concepts guide](/oss/python/langgraph/graph-api#state).

## Use user-scoped MCP tools in your deployment

<Tip>
  **Prerequisites**
  You have added your own [custom auth middleware](/langsmith/custom-auth) that populates the `langgraph_auth_user` object, making it accessible through configurable context for every node in your graph.
</Tip>

To make user-scoped tools available to your LangSmith deployment, start with implementing a snippet like the following:
```

Example 2 (unknown):
```unknown
1. MCP only supports adding headers to requests made to `streamable_http` and `sse` `transport` servers.
2. Your MCP server URL.
3. Get available tools from your MCP server.

*This can also be done by [rebuilding your graph at runtime](/langsmith/graph-rebuild) to have a different configuration for a new run*

## Session behavior

The current LangGraph MCP implementation does not support sessions. Each `/mcp` request is stateless and independent.

## Authentication

The `/mcp` endpoint uses the same authentication as the rest of the LangGraph API. Refer to the [authentication guide](/langsmith/auth) for setup details.

## Disable MCP

To disable the MCP endpoint, set `disable_mcp` to `true` in your `langgraph.json` configuration file:
```

---

## Run the graph until the interrupt is hit.

**URL:** llms-txt#run-the-graph-until-the-interrupt-is-hit.

result = agent.invoke(
    {
        "messages": [
            {
                "role": "user",
                "content": "Delete old records from the database",
            }
        ]
    },
    config=config # [!code highlight]
)

---

## Scalability & resilience

**URL:** llms-txt#scalability-&-resilience

**Contents:**
- Server scalability
- Queue scalability
- Resilience
- Postgres resilience
- Redis resilience

Source: https://docs.langchain.com/langsmith/scalability-and-resilience

LangSmith is designed to scale horizontally with your workload. Each instance of the service is stateless, and keeps no resources in memory. The service is designed to gracefully handle new instances being added or removed, including hard shutdown cases.

## Server scalability

As you add more instances to a service, they will share the HTTP load as long as an appropriate load balancer mechanism is placed in front of them. In most deployment modalities we configure a load balancer for the service automatically. In the “self-hosted without control plane” modality it’s your responsibility to add a load balancer. Since the instances are stateless any load balancing strategy will work, no session stickiness is needed, or recommended. Any instance of the server can communicate with any queue instance (through Redis PubSub), meaning that requests to cancel or stream an in-progress run can be handled by any arbitrary instance.

As you add more instances to a service, they will increase run throughput linearly, as each instance is configured to handle a set number of concurrent runs (by default 10). Each attempt for each run will be handled by a single instance, with exactly-once semantics enforced through Postgres’s MVCC model (refer to section below for crash resilience details). Attempts that fail due to transient database errors are retried up to 3 times. We do not make use of long-lived transactions or locks, this enables us to make more efficient use of Postgres resources.

While a run is being handled by a queue instance, a periodic heartbeat timestamp will be recorded in Redis by that queue worker.

When a graceful shutdown request is received (SIGINT) an instance enters shutdown mode, which

* stops accepting new HTTP requests
* gives any in-progress runs a limited number of seconds to finish (if not finished it will be put back in the queue)
* stops the instance from picking up more runs from the queue

If a hard shutdown occurs due to a server crash or an infrastructure failure, any runs that were in progress will be picked up by an internal sweeper task that looks for in-progress runs that have breached their heartbeat window. The sweeper runs every 2 minutes and will put the runs back in the queue for another instance to pick them up.

## Postgres resilience

For deployment modalities where we manage the Postgres database, we have periodic backups and continuously replicated standby replicas for automatic failover. This Postgres configuration is available in the [Cloud deployment option](/langsmith/cloud) for [`Production` deployment types](/langsmith/control-plane#deployment-types) only.

All communication with Postgres implements retries for retry-able errors. If Postgres is momentarily unavailable, such as during a database restart, most/all traffic should continue to succeed. Prolonged failure of Postgres will render the Agent Server unavailable.

All data that requires durable storage is stored in Postgres, not Redis. Redis is used only for ephemeral metadata, and communication between instances. Therefore we place no durability requirements on Redis.

All communication with Redis implements retries for retry-able errors. If Redis is momentarily unavailable, such as during a database restart, most/all traffic should continue to succeed. Prolonged failure of Redis will render the Agent Server unavailable.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/scalability-and-resilience.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Schema for routing user intent.

**URL:** llms-txt#schema-for-routing-user-intent.

---

## Schema for structured output

**URL:** llms-txt#schema-for-structured-output

from pydantic import BaseModel, Field

class SearchQuery(BaseModel):
    search_query: str = Field(None, description="Query that is optimized web search.")
    justification: str = Field(
        None, description="Why this query is relevant to the user's request."
    )

---

## search for "memories" within this namespace, filtering on content equivalence, sorted by vector similarity

**URL:** llms-txt#search-for-"memories"-within-this-namespace,-filtering-on-content-equivalence,-sorted-by-vector-similarity

**Contents:**
- Read long-term memory in tools

items = store.search( # [!code highlight]
    namespace, filter={"my-key": "my-value"}, query="language preferences"
)
python A tool the agent can use to look up user information theme={null}
from dataclasses import dataclass

from langchain_core.runnables import RunnableConfig
from langchain.agents import create_agent
from langchain.tools import tool, ToolRuntime
from langgraph.store.memory import InMemoryStore

@dataclass
class Context:
    user_id: str

**Examples:**

Example 1 (unknown):
```unknown
For more information about the memory store, see the [Persistence](/oss/python/langgraph/persistence#memory-store) guide.

## Read long-term memory in tools
```

---

## Second invocation: the first message is persisted (Sydney location), so the model returns GMT+10 time

**URL:** llms-txt#second-invocation:-the-first-message-is-persisted-(sydney-location),-so-the-model-returns-gmt+10-time

**Contents:**
- Integration Testing
  - Installing AgentEvals
  - Trajectory Match Evaluator
  - LLM-as-Judge Evaluator
  - Async Support
- LangSmith Integration
- Recording & Replaying HTTP Calls

agent.invoke(HumanMessage(content="What's my local time?"))
bash  theme={null}
pip install agentevals
python  theme={null}
  from langchain.agents import create_agent
  from langchain.tools import tool
  from langchain.messages import HumanMessage, AIMessage, ToolMessage
  from agentevals.trajectory.match import create_trajectory_match_evaluator

@tool
  def get_weather(city: str):
      """Get weather information for a city."""
      return f"It's 75 degrees and sunny in {city}."

agent = create_agent("gpt-4o", tools=[get_weather])

evaluator = create_trajectory_match_evaluator(  # [!code highlight]
      trajectory_match_mode="strict",  # [!code highlight]
  )  # [!code highlight]

def test_weather_tool_called_strict():
      result = agent.invoke({
          "messages": [HumanMessage(content="What's the weather in San Francisco?")]
      })

reference_trajectory = [
          HumanMessage(content="What's the weather in San Francisco?"),
          AIMessage(content="", tool_calls=[
              {"id": "call_1", "name": "get_weather", "args": {"city": "San Francisco"}}
          ]),
          ToolMessage(content="It's 75 degrees and sunny in San Francisco.", tool_call_id="call_1"),
          AIMessage(content="The weather in San Francisco is 75 degrees and sunny."),
      ]

evaluation = evaluator(
          outputs=result["messages"],
          reference_outputs=reference_trajectory
      )
      # {
      #     'key': 'trajectory_strict_match',
      #     'score': True,
      #     'comment': None,
      # }
      assert evaluation["score"] is True
  python  theme={null}
  from langchain.agents import create_agent
  from langchain.tools import tool
  from langchain.messages import HumanMessage, AIMessage, ToolMessage
  from agentevals.trajectory.match import create_trajectory_match_evaluator

@tool
  def get_weather(city: str):
      """Get weather information for a city."""
      return f"It's 75 degrees and sunny in {city}."

@tool
  def get_events(city: str):
      """Get events happening in a city."""
      return f"Concert at the park in {city} tonight."

agent = create_agent("gpt-4o", tools=[get_weather, get_events])

evaluator = create_trajectory_match_evaluator(  # [!code highlight]
      trajectory_match_mode="unordered",  # [!code highlight]
  )  # [!code highlight]

def test_multiple_tools_any_order():
      result = agent.invoke({
          "messages": [HumanMessage(content="What's happening in SF today?")]
      })

# Reference shows tools called in different order than actual execution
      reference_trajectory = [
          HumanMessage(content="What's happening in SF today?"),
          AIMessage(content="", tool_calls=[
              {"id": "call_1", "name": "get_events", "args": {"city": "SF"}},
              {"id": "call_2", "name": "get_weather", "args": {"city": "SF"}},
          ]),
          ToolMessage(content="Concert at the park in SF tonight.", tool_call_id="call_1"),
          ToolMessage(content="It's 75 degrees and sunny in SF.", tool_call_id="call_2"),
          AIMessage(content="Today in SF: 75 degrees and sunny with a concert at the park tonight."),
      ]

evaluation = evaluator(
          outputs=result["messages"],
          reference_outputs=reference_trajectory,
      )
      # {
      #     'key': 'trajectory_unordered_match',
      #     'score': True,
      # }
      assert evaluation["score"] is True
  python  theme={null}
  from langchain.agents import create_agent
  from langchain.tools import tool
  from langchain.messages import HumanMessage, AIMessage, ToolMessage
  from agentevals.trajectory.match import create_trajectory_match_evaluator

@tool
  def get_weather(city: str):
      """Get weather information for a city."""
      return f"It's 75 degrees and sunny in {city}."

@tool
  def get_detailed_forecast(city: str):
      """Get detailed weather forecast for a city."""
      return f"Detailed forecast for {city}: sunny all week."

agent = create_agent("gpt-4o", tools=[get_weather, get_detailed_forecast])

evaluator = create_trajectory_match_evaluator(  # [!code highlight]
      trajectory_match_mode="superset",  # [!code highlight]
  )  # [!code highlight]

def test_agent_calls_required_tools_plus_extra():
      result = agent.invoke({
          "messages": [HumanMessage(content="What's the weather in Boston?")]
      })

# Reference only requires get_weather, but agent may call additional tools
      reference_trajectory = [
          HumanMessage(content="What's the weather in Boston?"),
          AIMessage(content="", tool_calls=[
              {"id": "call_1", "name": "get_weather", "args": {"city": "Boston"}},
          ]),
          ToolMessage(content="It's 75 degrees and sunny in Boston.", tool_call_id="call_1"),
          AIMessage(content="The weather in Boston is 75 degrees and sunny."),
      ]

evaluation = evaluator(
          outputs=result["messages"],
          reference_outputs=reference_trajectory,
      )
      # {
      #     'key': 'trajectory_superset_match',
      #     'score': True,
      #     'comment': None,
      # }
      assert evaluation["score"] is True
  python  theme={null}
  from langchain.agents import create_agent
  from langchain.tools import tool
  from langchain.messages import HumanMessage, AIMessage, ToolMessage
  from agentevals.trajectory.llm import create_trajectory_llm_as_judge, TRAJECTORY_ACCURACY_PROMPT

@tool
  def get_weather(city: str):
      """Get weather information for a city."""
      return f"It's 75 degrees and sunny in {city}."

agent = create_agent("gpt-4o", tools=[get_weather])

evaluator = create_trajectory_llm_as_judge(  # [!code highlight]
      model="openai:o3-mini",  # [!code highlight]
      prompt=TRAJECTORY_ACCURACY_PROMPT,  # [!code highlight]
  )  # [!code highlight]

def test_trajectory_quality():
      result = agent.invoke({
          "messages": [HumanMessage(content="What's the weather in Seattle?")]
      })

evaluation = evaluator(
          outputs=result["messages"],
      )
      # {
      #     'key': 'trajectory_accuracy',
      #     'score': True,
      #     'comment': 'The provided agent trajectory is reasonable...'
      # }
      assert evaluation["score"] is True
  python  theme={null}
  evaluator = create_trajectory_llm_as_judge(
      model="openai:o3-mini",
      prompt=TRAJECTORY_ACCURACY_PROMPT_WITH_REFERENCE,
  )
  evaluation = judge_with_reference(
      outputs=result["messages"],
      reference_outputs=reference_trajectory,
  )
  python  theme={null}
  from agentevals.trajectory.llm import create_async_trajectory_llm_as_judge, TRAJECTORY_ACCURACY_PROMPT
  from agentevals.trajectory.match import create_async_trajectory_match_evaluator

async_judge = create_async_trajectory_llm_as_judge(
      model="openai:o3-mini",
      prompt=TRAJECTORY_ACCURACY_PROMPT,
  )

async_evaluator = create_async_trajectory_match_evaluator(
      trajectory_match_mode="strict",
  )

async def test_async_evaluation():
      result = await agent.ainvoke({
          "messages": [HumanMessage(content="What's the weather?")]
      })

evaluation = await async_judge(outputs=result["messages"])
      assert evaluation["score"] is True
  bash  theme={null}
export LANGSMITH_API_KEY="your_langsmith_api_key"
export LANGSMITH_TRACING="true"
python  theme={null}
  import pytest
  from langsmith import testing as t
  from agentevals.trajectory.llm import create_trajectory_llm_as_judge, TRAJECTORY_ACCURACY_PROMPT

trajectory_evaluator = create_trajectory_llm_as_judge(
      model="openai:o3-mini",
      prompt=TRAJECTORY_ACCURACY_PROMPT,
  )

@pytest.mark.langsmith
  def test_trajectory_accuracy():
      result = agent.invoke({
          "messages": [HumanMessage(content="What's the weather in SF?")]
      })

reference_trajectory = [
          HumanMessage(content="What's the weather in SF?"),
          AIMessage(content="", tool_calls=[
              {"id": "call_1", "name": "get_weather", "args": {"city": "SF"}},
          ]),
          ToolMessage(content="It's 75 degrees and sunny in SF.", tool_call_id="call_1"),
          AIMessage(content="The weather in SF is 75 degrees and sunny."),
      ]

# Log inputs, outputs, and reference outputs to LangSmith
      t.log_inputs({})
      t.log_outputs({"messages": result["messages"]})
      t.log_reference_outputs({"messages": reference_trajectory})

trajectory_evaluator(
          outputs=result["messages"],
          reference_outputs=reference_trajectory
      )
  bash  theme={null}
  pytest test_trajectory.py --langsmith-output
  python  theme={null}
  from langsmith import Client
  from agentevals.trajectory.llm import create_trajectory_llm_as_judge, TRAJECTORY_ACCURACY_PROMPT

trajectory_evaluator = create_trajectory_llm_as_judge(
      model="openai:o3-mini",
      prompt=TRAJECTORY_ACCURACY_PROMPT,
  )

def run_agent(inputs):
      """Your agent function that returns trajectory messages."""
      return agent.invoke(inputs)["messages"]

experiment_results = client.evaluate(
      run_agent,
      data="your_dataset_name",
      evaluators=[trajectory_evaluator]
  )
  py conftest.py theme={null}
import pytest

@pytest.fixture(scope="session")
def vcr_config():
    return {
        "filter_headers": [
            ("authorization", "XXXX"),
            ("x-api-key", "XXXX"),
            # ... other headers you want to mask
        ],
        "filter_query_parameters": [
            ("api_key", "XXXX"),
            ("key", "XXXX"),
        ],
    }
ini pytest.ini theme={null}
  [pytest]
  markers =
      vcr: record/replay HTTP via VCR
  addopts = --record-mode=once
  toml pyproject.toml theme={null}
  [tool.pytest.ini_options]
  markers = [
    "vcr: record/replay HTTP via VCR"
  ]
  addopts = "--record-mode=once"
  python  theme={null}
@pytest.mark.vcr()
def test_agent_trajectory():
    # ...
```

The first time you run this test, your agent will make real network calls and pytest will generate a cassette file `test_agent_trajectory.yaml` in the `tests/cassettes` directory. Subsequent runs will use that cassette to mock the real network calls, granted the agent's requests don't change from the previous run. If they do, the test will fail and you'll need to delete the cassette and rerun the test to record fresh interactions.

<Warning>
  When you modify prompts, add new tools, or change expected trajectories, your saved cassettes will become outdated and your existing tests **will fail**. You should delete the corresponding cassette files and rerun the tests to record fresh interactions.
</Warning>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/test.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
## Integration Testing

Many agent behaviors only emerge when using a real LLM, such as which tool the agent decides to call, how it formats responses, or whether a prompt modification affects the entire execution trajectory. LangChain's [`agentevals`](https://github.com/langchain-ai/agentevals) package provides evaluators specifically designed for testing agent trajectories with live models.

AgentEvals lets you easily evaluate the trajectory of your agent (the exact sequence of messages, including tool calls) by performing a **trajectory match** or by using an **LLM judge**:

<Card title="Trajectory match" icon="equals" arrow="true" href="#trajectory-match-evaluator">
  Hard-code a reference trajectory for a given input and validate the run via a step-by-step comparison.

  Ideal for testing well-defined workflows where you know the expected behavior. Use when you have specific expectations about which tools should be called and in what order. This approach is deterministic, fast, and cost-effective since it doesn't require additional LLM calls.
</Card>

<Card title="LLM-as-judge" icon="gavel" arrow="true" href="#llm-as-judge-evaluator">
  Use a LLM to qualitatively validate your agent's execution trajectory. The "judge" LLM reviews the agent's decisions against a prompt rubric (which can include a reference trajectory).

  More flexible and can assess nuanced aspects like efficiency and appropriateness, but requires an LLM call and is less deterministic. Use when you want to evaluate the overall quality and reasonableness of the agent's trajectory without strict tool call or ordering requirements.
</Card>

### Installing AgentEvals
```

Example 2 (unknown):
```unknown
Or, clone the [AgentEvals repository](https://github.com/langchain-ai/agentevals) directly.

### Trajectory Match Evaluator

AgentEvals offers the `create_trajectory_match_evaluator` function to match your agent's trajectory against a reference trajectory. There are four modes to choose from:

| Mode        | Description                                               | Use Case                                                              |
| ----------- | --------------------------------------------------------- | --------------------------------------------------------------------- |
| `strict`    | Exact match of messages and tool calls in the same order  | Testing specific sequences (e.g., policy lookup before authorization) |
| `unordered` | Same tool calls allowed in any order                      | Verifying information retrieval when order doesn't matter             |
| `subset`    | Agent calls only tools from reference (no extras)         | Ensuring agent doesn't exceed expected scope                          |
| `superset`  | Agent calls at least the reference tools (extras allowed) | Verifying minimum required actions are taken                          |

<Accordion title="Strict match">
  The `strict` mode ensures trajectories contain identical messages in the same order with the same tool calls, though it allows for differences in message content. This is useful when you need to enforce a specific sequence of operations, such as requiring a policy lookup before authorizing an action.
```

Example 3 (unknown):
```unknown
</Accordion>

<Accordion title="Unordered match">
  The `unordered` mode allows the same tool calls in any order, which is helpful when you want to verify that specific information was retrieved but don't care about the sequence. For example, an agent might need to check both weather and events for a city, but the order doesn't matter.
```

Example 4 (unknown):
```unknown
</Accordion>

<Accordion title="Subset and superset match">
  The `superset` and `subset` modes match partial trajectories. The `superset` mode verifies that the agent called at least the tools in the reference trajectory, allowing additional tool calls. The `subset` mode ensures the agent did not call any tools beyond those in the reference.
```

---

## Second session: get user info

**URL:** llms-txt#second-session:-get-user-info

agent.invoke({
    "messages": [{"role": "user", "content": "Get user info for user with id 'abc123'"}]
})

---

## Section 1: Prometheus Exporters

**URL:** llms-txt#section-1:-prometheus-exporters

Use this section if you would like to only deploy metrics exporters for the components in your self hosted deployment, which you can then scrape using your telemetry. If you would like a full observability stack deployed for you, go to the [End-to-End Deployment Section](/langsmith/observability-stack#prerequisites).

The helm chart provides a set of Prometheus exporters to expose metrics from [Redis](https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus-redis-exporter), [Postgres](https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus-postgres-exporter), [Nginx](https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus-nginx-exporter), and [Kube state metrics](https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-state-metrics).

1. Create a local file called `langsmith_obs_config.yaml`
2. Copy over the values from this [file](https://github.com/langchain-ai/helm/blob/main/charts/langsmith-observability/examples/metric-exporters-only.yaml) into `langsmith_obs_config.yaml`, making sure to modify the values to match your LangSmith deployment.
3. Find the latest version of the chart by running `helm search repo langchain/langsmith-observability --versions`.
4. Grab the latest version number, and run `helm install langsmith-observability langchain/langsmith-observability --values langsmith_obs_config.yaml --version <version> -n <namespace> --wait --debug`

This will allow you to scrape metrics at the following service endpoints:

* Postgres: `langsmith-observability-postgres-exporter:9187/metrics`
* Redis: `langsmith-observability-redis-exporter:9121/metrics`
* Nginx: `langsmith-observability-nginx-exporter:9113/metrics`
* KubeStateMetrics: `langsmith-observability-kube-state-metrics:8080/metrics`

You should see the following if the installation went through:

And if you run `kubectl get pods -n langsmith-observability`, you should see:

**Examples:**

Example 1 (unknown):
```unknown
And if you run `kubectl get pods -n langsmith-observability`, you should see:
```

---

## Section 2: Full Observability Stack

**URL:** llms-txt#section-2:-full-observability-stack

**Contents:**
- Prerequisites
  - 1. Compute Resources
  - 2. Cert-Manager
  - 3. OpenTelemetry Operator
- Installation
- Post-Installation
  - Enable Logs and Traces in LangSmith
- Grafana Usage

<Warning>
  **This is not a production observability stack. Use this to gain quick insight into logs, metrics and traces for your deployment. This is only made to handle a few dozen GB of data per day.**
</Warning>

This section will show you how to deploy the end-to-end observability stack for LangSmith, using the [Helm Chart](https://github.com/langchain-ai/helm/tree/main/charts/langsmith-observability).

This chart is built around the open-source LGTM Stack from Grafana. It consists of:

* [Loki](https://grafana.com/docs/loki/latest/) for logs.
* [Mimir](https://grafana.com/docs/mimir/latest/) for metrics + alerting.
* [Tempo](https://grafana.com/docs/tempo/latest/) for traces.
* [Grafana](https://grafana.com/docs/grafana/latest/) for monitoring UI.

As well as [OpenTelemetry Collectors](https://opentelemetry.io/docs/collector/) for gathering the telemetry data.

### 1. Compute Resources

The resource requests and limits for each part of the stack can be modified in the helm chart. Here are the current allocations (request/limit):

* Loki: `2vCPU/3vCPU + 2Gi/4Gi`
* Mimir: `1vCPU/2vCPU + 2Gi/4Gi`
* Tempo: `1vCPU/2vCPU + 4Gi/6Gi`

Make sure you have those resources allocated before bringing up the helm chart, or modify the resource values in your helm configuration file.

The helm chart uses the OpenTelemetry Operator to provision collectors. The operator require that you have [cert-manager](https://cert-manager.io/docs/installation/) installed in your Kubernetes cluster.

If you do not have it installed, you can run the following commands:

### 3. OpenTelemetry Operator

Use the following to install the OpenTelemetry Operator:

The following instructions will bring up OTel collectors, the LGTM stack, Grafana and Prometheus exporters.

1. Create a local file called `langsmith_obs_config.yaml`
2. Copy over the values from this [file](https://github.com/langchain-ai/helm/blob/main/charts/langsmith-observability/examples/e2e-stack.yaml) into `langsmith_obs_config.yaml`, making sure to modify the values to match your LangSmith deployment.
3. Find the latest version of the chart by running `helm search repo langchain/langsmith-observability --versions`.
4. Grab the latest version number, and run `helm install langsmith-observability langchain/langsmith-observability --values langsmith_obs_config.yaml --version <version> -n <namespace> --wait --debug`

<Note>
  **You can selectively collect logs, metrics or traces by modifying the boolean values under `otelCollector` in your config file. You can also selectively bring up each respective piece of the backend (Loki, Mimir, Tempo).**
</Note>

You should see the following if the install went through:

And if you run `kubectl get pods -n langsmith-observability`, you should see:

### Enable Logs and Traces in LangSmith

Once you have installed the observability helm chart, you need to set the following values in your *LangSmith* helm configuration file to enable collection of logs and traces.

<Info>
  1. To get `${LANGSMITH_OTEL_CRD_NAME}`, you can run `kubectl get opentelemetrycollectors -n ${LANGSMITH_OBS_NAMESPACE}` and select the name of the one with MODE = `sidecar`
  2. To get `${GATEWAY_COLLECTOR_SERVICE_NAME}` name, run `kubectl get services -n ${LANGSMITH_OBS_NAMESPACE}` and select the one with Ports 4317/4318 AND a ClusterIP set. It should be something like `langsmith-observability-collector-gateway-collector`
</Info>

Now run `helm upgrade langsmith langchain/langsmith --values langsmith_config.yaml -n <langsmith-namespace> --wait --debug`

Once upgraded, if you run `kubectl get pods -n <langsmith-namespace>` you should see the following (note the 2/2 for sidecar collectors):

Once everything is installed, do the following: to get your Grafana password:

Then port-forward into the `langsmith-observability-grafana` container at port 3000, and open your browser as `localhost:3000`. Use the username `admin` and the password from the secret above to log into Grafana.

Once in Grafana, you can use the UI to monitor logs, metrics and traces. Grafana also comes pre-packaged with sets of dashboards for monitoring the main components of your deployment.

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-grafana-dashboards.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=ee47243826737bab23944e01536dec71" alt="LangSmith Grafana Dashboards" data-og-width="1715" width="1715" data-og-height="1073" height="1073" data-path="langsmith/images/langsmith-grafana-dashboards.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-grafana-dashboards.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=11e0d71897053d012e929ca49533d6dd 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-grafana-dashboards.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=0e674be629c1c1e795e1b6e686ca28e2 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-grafana-dashboards.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=fb3bcdc4fa39e82fe9f3370834c7f7fa 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-grafana-dashboards.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=80c401f8794ac0671207e1a260aac25b 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-grafana-dashboards.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=3f5baa041ab9e5c39b7d8a62e2bfe1e5 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-grafana-dashboards.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=7620f91021593db1d87c92fc43d8fe2b 2500w" />

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/observability-stack.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
### 3. OpenTelemetry Operator

Use the following to install the OpenTelemetry Operator:
```

Example 2 (unknown):
```unknown
## Installation

The following instructions will bring up OTel collectors, the LGTM stack, Grafana and Prometheus exporters.

1. Create a local file called `langsmith_obs_config.yaml`
2. Copy over the values from this [file](https://github.com/langchain-ai/helm/blob/main/charts/langsmith-observability/examples/e2e-stack.yaml) into `langsmith_obs_config.yaml`, making sure to modify the values to match your LangSmith deployment.
3. Find the latest version of the chart by running `helm search repo langchain/langsmith-observability --versions`.
4. Grab the latest version number, and run `helm install langsmith-observability langchain/langsmith-observability --values langsmith_obs_config.yaml --version <version> -n <namespace> --wait --debug`

<Note>
  **You can selectively collect logs, metrics or traces by modifying the boolean values under `otelCollector` in your config file. You can also selectively bring up each respective piece of the backend (Loki, Mimir, Tempo).**
</Note>

You should see the following if the install went through:
```

Example 3 (unknown):
```unknown
And if you run `kubectl get pods -n langsmith-observability`, you should see:
```

Example 4 (unknown):
```unknown
## Post-Installation

### Enable Logs and Traces in LangSmith

Once you have installed the observability helm chart, you need to set the following values in your *LangSmith* helm configuration file to enable collection of logs and traces.
```

---

## Security policy

**URL:** llms-txt#security-policy

**Contents:**
- Best practices
- Reporting OSS vulnerabilities
  - In-scope targets
  - Out-of-scope targets
- Reporting LangSmith vulnerabilities
  - Other security concerns

Source: https://docs.langchain.com/oss/javascript/security-policy

LangChain has a large ecosystem of integrations with various external resources like local and remote file systems, APIs and databases. These integrations allow developers to create versatile applications that combine the power of LLMs with the ability to access, interact with and manipulate external resources.

When building such applications developers should remember to follow good security practices:

* [**Limit permissions**](https://en.wikipedia.org/wiki/Principle_of_least_privilege): Scope permissions specifically to the application's need. Granting broad or excessive permissions can introduce significant security vulnerabilities. To avoid such vulnerabilities, consider using read-only credentials, disallowing access to sensitive resources, using sandboxing techniques (such as running inside a container), specifying proxy configurations to control external requests, etc. as appropriate for your application.
* **Anticipate potential misuse**: Just as humans can err, so can Large Language Models (LLMs). Always assume that any system access or credentials may be used in any way allowed by the permissions they are assigned. For example, if a pair of database credentials allows deleting data, it's safest to assume that any LLM able to use those credentials may in fact delete data.
* [**Defense in depth**](https://en.wikipedia.org/wiki/Defense_in_depth_\(computing\)): No security technique is perfect. Fine-tuning and good chain design can reduce, but not eliminate, the odds that a Large Language Model (LLM) may make a mistake. It's best to combine multiple layered security approaches rather than relying on any single layer of defense to ensure security. For example: use both read-only permissions and sandboxing to ensure that LLMs are only able to access data that is explicitly meant for them to use.

Risks of not doing so include, but are not limited to:

* Data corruption or loss.
* Unauthorized access to confidential information.
* Compromised performance or availability of critical resources.

Example scenarios with mitigation strategies:

* A user may ask an agent with access to the file system to delete files that should not be deleted or read the content of files that contain sensitive information. To mitigate, limit the agent to only use a specific directory and only allow it to read or write files that are safe to read or write. Consider further sandboxing the agent by running it in a container.
* A user may ask an agent with write access to an external API to write malicious data to the API, or delete data from that API. To mitigate, give the agent read-only API keys, or limit it to only use endpoints that are already resistant to such misuse.
* A user may ask an agent with access to a database to drop a table or mutate the schema. To mitigate, scope the credentials to only the tables that the agent needs to access and consider issuing READ-ONLY credentials.

If you're building applications that access external resources like file systems, APIs
or databases, consider speaking with your company's security team to determine how to best
design and secure your applications.

## Reporting OSS vulnerabilities

LangChain is partnered with [huntr by Protect AI](https://huntr.com/) to provide
a bounty program for our open source projects.

Please report security vulnerabilities associated with the LangChain
open source projects at [huntr](https://huntr.com/bounties/disclose/?target=https%3A%2F%2Fgithub.com%2Flangchain-ai%2Flangchain\&validSearch=true).

Before reporting a vulnerability, please review:

1. [In-scope targets](#in-scope-targets) and [out-of-scope targets](#out-of-scope-targets).
2. The [langchain-ai/langchain](https://python.langchain.com/docs/contributing/repo_structure) monorepo structure.
3. The [best practices](#best-practices) above to
   understand what we consider to be a security vulnerability vs. developer
   responsibility.

The following packages and repositories are eligible for bug bounties:

* langchain-core
* langchain (see exceptions)
* langchain-community (see exceptions)
* langgraph
* langserve

### Out-of-scope targets

All out of scope targets defined by huntr as well as:

* **langchain-experimental**: This repository is for experimental code and is not
  eligible for bug bounties (see [package warning](https://pypi.org/project/langchain-experimental/)), bug reports to it will be marked as interesting or waste of
  time and published with no bounty attached.
* **tools**: Tools in either langchain or langchain-community are not eligible for bug
  bounties. This includes the following directories
  * libs/langchain/langchain/tools
  * libs/community/langchain\_community/tools
  * Please review the [best practices](#best-practices)
    for more details, but generally tools interact with the real world. Developers are
    expected to understand the security implications of their code and are responsible
    for the security of their tools.
* Code documented with security notices. This will be decided on a case by
  case basis, but likely will not be eligible for a bounty as the code is already
  documented with guidelines for developers that should be followed for making their
  application secure.
* Any LangSmith related repositories or APIs (see [Reporting LangSmith vulnerabilities](#reporting-langsmith-vulnerabilities)).

## Reporting LangSmith vulnerabilities

Please report security vulnerabilities associated with LangSmith by email to `security@langchain.dev`.

* LangSmith site: [https://smith.langchain.com](https://smith.langchain.com)
* SDK client: [https://github.com/langchain-ai/langsmith-sdk](https://github.com/langchain-ai/langsmith-sdk)

### Other security concerns

For any other security concerns, please contact us at `security@langchain.dev`.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/security-policy.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## See trace: https://smith.langchain.com/public/882f9ecf-5057-426a-ae98-0edf84fdcaf9/r

**URL:** llms-txt#see-trace:-https://smith.langchain.com/public/882f9ecf-5057-426a-ae98-0edf84fdcaf9/r

**Contents:**
- Ensure all traces are submitted before exiting
  - Using the LangSmith SDK
  - Using LangChain

MyClass(13).combine(29)
python Python theme={null}
  from langsmith import Client

@traceable(client=client)
  async def my_traced_func():
    # Your code here...
    pass

try:
    await my_traced_func()
  finally:
    await client.flush()
  typescript TypeScript theme={null}
  import { Client } from "langsmith";

const langsmithClient = new Client({});

const myTracedFunc = traceable(async () => {
    // Your code here...
  },{ client: langsmithClient });

try {
    await myTracedFunc();
  } finally {
    await langsmithClient.flush();
  }
  ```
</CodeGroup>

If you are using LangChain, please refer to our [LangChain tracing guide](/langsmith/trace-with-langchain#ensure-all-traces-are-submitted-before-exiting).

If you prefer a video tutorial, check out the [Tracing Basics video](https://academy.langchain.com/pages/intro-to-langsmith-preview) from the Introduction to LangSmith Course.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/annotate-code.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
## Ensure all traces are submitted before exiting

LangSmith's tracing is done in a background thread to avoid obstructing your production application. This means that your process may end before all traces are successfully posted to LangSmith. Here are some options for ensuring all traces are submitted before exiting your application.

### Using the LangSmith SDK

If you are using the LangSmith SDK standalone, you can use the `flush` method before exit:

<CodeGroup>
```

Example 2 (unknown):
```unknown

```

---

## Self-hosted LangSmith

**URL:** llms-txt#self-hosted-langsmith

**Contents:**
- Self-host LangSmith Observability and Evaluation
  - Services
  - Storage services
  - Setup methods
  - Setup guides
- Enable LangSmith Deployment
  - Workflow
- Standalone Server
  - Workflow
  - Supported compute platforms

Source: https://docs.langchain.com/langsmith/self-hosted

<Note>
  **Important**<br />
  Self-hosted LangSmith is an add-on to the Enterprise plan designed for our largest, most security-conscious customers. For more details, refer to [Pricing](https://www.langchain.com/pricing). [Contact our sales team](https://www.langchain.com/contact-sales) if you want to get a license key to trial LangSmith in your environment.
</Note>

LangSmith supports different self-hosted configurations depending on your scale, security, and infrastructure needs.

You can use LangSmith for [observability](/langsmith/observability) and [evaluation](/langsmith/evaluation) without agent deployment. Or, you can set up the **full self-hosted platform** for observability, evaluation, and [agent deployment](/langsmith/deployments). Alternatively, you can deploy agents directly without the [control plane](/langsmith/control-plane).

This page provides an overview of each self-hosted model:

<Columns cols={1}>
  <Card title="LangSmith Observability and Evaluation" icon="chart-line" href="#langsmith">
    Host an instance of LangSmith that includes observability, tracing, and evaluations in the UI and API. Best for teams who want self-hosted monitoring and evaluation without deploying agents.
  </Card>

<Card title="LangSmith Observability, Evaluation, and Deployment" icon="layer-group" href="#langsmith-deployment">
    Enables deploying graphs to Agent Server via the control plane. The control plane and data plane provide the full LangSmith platform for running and monitoring agents. This includes observability, evaluation, and deployment.
  </Card>

<Card title="Standalone server" icon="server" href="#standalone-server">
    Host an Agent Server directly without the control plane UI. A lightweight option for running one or a few agents as independent services, with full control over scaling and integration.
  </Card>
</Columns>

| Model                                      | Includes                                                                                                                                                                                                                | Best for                                                                                                                                                                               | Methods                                                                                                                                               |
| ------------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Observability & Evaluation**             | <ul><li>LangSmith (UI + API)</li><li>Backend services (queue, playground, ACE)</li><li>Datastores: PostgreSQL, Redis, ClickHouse, optional blob storage</li></ul>                                                       | <ul><li>Teams who need self-hosted observability, tracing, and evaluation</li><li>Running LangSmith without deploying agents/graphs</li></ul>                                          | <ul><li>Docker Compose (dev/test)</li><li>Kubernetes + Helm (production)</li></ul>                                                                    |
| **Observability, Evaluation & Deployment** | <ul><li>Everything from Observability and Evaluation</li><li>Control plane (deployments UI, revision management, Studio)</li><li>Data plane (Agent Server pods)</li><li>Kubernetes operator for orchestration</li></ul> | <ul><li>Enterprise teams needing a private LangChain Cloud</li><li>Centralized UI/API for managing multiple agents/graphs</li><li>Integrated observability and orchestration</li></ul> | <ul><li>Kubernetes with Helm (required)</li><li>Runs on EKS, GKE, AKS, or self-managed clusters</li></ul>                                             |
| **Standalone server**                      | <ul><li>Agent Server container(s)</li><li>Requires PostgreSQL + Redis (shared or dedicated)</li><li>Optional LangSmith integration for tracing</li></ul>                                                                | <ul><li>Lightweight deployments of one or a few agents</li><li>Integrating Agent Servers as microservices</li><li>Teams preferring to manage scaling & CI/CD themselves</li></ul>      | <ul><li>Docker / Docker Compose (dev/test)</li><li>Kubernetes + Helm (production)</li><li>Any container runtime or VM (ECS, EC2, ACI, etc.)</li></ul> |

<Note>
  For setup guides, refer to:

* [Enable LangSmith Deployment](/langsmith/deploy-self-hosted-full-platform)
  * [Deploy Standalone Server](/langsmith/deploy-standalone-server)

Supported compute platforms: [Kubernetes](https://kubernetes.io/) (for LangSmith Deployment), any compute platform (for Standalone Server)
</Note>

## Self-host LangSmith Observability and Evaluation

Host an instance of LangSmith that includes observability, tracing, and evaluations in the UI and API, but **without** the ability to deploy agents through the control plane.

* LangSmith frontend UI
* LangSmith backend API
* LangSmith Platform backend
* LangSmith Playground
* LangSmith queue
* LangSmith ACE (Arbitrary Code Execution) backend

**Storage services:**

* ClickHouse (traces and feedback data)
* PostgreSQL (operational data)
* Redis (queuing and caching)
* Blob storage (optional, but recommended for production)

<img className="block dark:hidden" src="https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-light.png?fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=0790cbdf4fe131c74d1e60bb120834e3" alt="LangSmith architecture showing services and datastores" data-og-width="2210" width="2210" data-og-height="1463" height="1463" data-path="langsmith/images/cloud-arch-light.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-light.png?w=280&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=c04d8a044d221559fe2f7b9121275638 280w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-light.png?w=560&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=a15351b254f11cc149ce237ba8853e91 560w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-light.png?w=840&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=d4a409e73830e588519cb1d0b2a17f3b 840w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-light.png?w=1100&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=6dbeda77b57083efb988e15af38f0a6e 1100w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-light.png?w=1650&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=24aadbe2e79db02d76fd5deaea6564e1 1650w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-light.png?w=2500&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=a126aa1f02d36de0a8e391f0e1059b8e 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-dark.png?fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=767f3bc3dc73ffe1a806f54e0aaa428b" alt="LangSmith architecture showing services and datastores" data-og-width="2210" width="2210" data-og-height="1463" height="1463" data-path="langsmith/images/cloud-arch-dark.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-dark.png?w=280&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=f7367df5b782c821882605418c50563f 280w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-dark.png?w=560&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=60759ef9e927ba0985e21e38acacae6d 560w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-dark.png?w=840&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=383ac38ba52733548d8d97ffabfe384e 840w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-dark.png?w=1100&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=b045b8e19a9926d4d10ec8ad2d2767c1 1100w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-dark.png?w=1650&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=23778aa891c1b42336b274ab1b2f8bec 1650w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-dark.png?w=2500&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=5a64734b4e9fb5dd4af690edf3fa6248 2500w" />

To access the LangSmith UI and send API requests, you will need to expose the [LangSmith frontend](#langsmith-frontend) service. Depending on your installation method, this can be a load balancer or a port exposed on the host machine.

| Service                                                                                                        | Description                                                                                                                                                                                                                                                                                                                                              |
| -------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| <a id="langsmith-frontend" /> **LangSmith frontend**                                                           | The frontend uses Nginx to serve the LangSmith UI and route API requests to the other servers. This serves as the entrypoint for the application and is the only component that must be exposed to users.                                                                                                                                                |
| <a id="langsmith-backend" /> **LangSmith backend**                                                             | The backend is the main entrypoint for CRUD API requests and handles the majority of the business logic for the application. This includes handling requests from the frontend and SDK, preparing traces for ingestion, and supporting the hub API.                                                                                                      |
| <a id="langsmith-queue" /> **LangSmith queue**                                                                 | The queue handles incoming traces and feedback to ensure that they are ingested and persisted into the traces and feedback datastore asynchronously, handling checks for data integrity and ensuring successful insert into the datastore, handling retries in situations such as database errors or the temporary inability to connect to the database. |
| <a id="langsmith-platform-backend" /> **LangSmith platform backend**                                           | The platform backend is another critical service that primarily handles authentication, run ingestion, and other high-volume tasks.                                                                                                                                                                                                                      |
| <a id="langsmith-playground" /> **LangSmith playground**                                                       | The playground is a service that handles forwarding requests to various LLM APIs to support the LangSmith Playground feature. This can also be used to connect to your own custom model servers.                                                                                                                                                         |
| <a id="langsmith-ace-arbitrary-code-execution-backend" /> **LangSmith ACE (Arbitrary Code Execution) backend** | The ACE backend is a service that handles executing arbitrary code in a secure environment. This is used to support running custom code within LangSmith.                                                                                                                                                                                                |

<Note>
  LangSmith will bundle all storage services by default. You can configure it to use external versions of all storage services. In a production setting, we **strongly recommend using external storage services**.
</Note>

| Service                                  | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
| ---------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| <a id="clickhouse" /> **ClickHouse**     | [ClickHouse](https://clickhouse.com/docs/en/intro) is a high-performance, column-oriented SQL database management system (DBMS) for online analytical processing (OLAP).<br /><br />LangSmith uses ClickHouse as the primary data store for traces and feedback (high-volume data).                                                                                                                                                                                      |
| <a id="postgresql" /> **PostgreSQL**     | [PostgreSQL](https://www.postgresql.org/about/) is a powerful, open source object-relational database system that uses and extends the SQL language combined with many features that safely store and scale the most complicated data workloads.<br /><br />LangSmith uses PostgreSQL as the primary data store for transactional workloads and operational data (almost everything besides traces and feedback).                                                        |
| <a id="redis" /> **Redis**               | [Redis](https://github.com/redis/redis) is a powerful in-memory key-value database that persists on disk. By holding data in memory, Redis offers high performance for operations like caching.<br /><br />LangSmith uses Redis to back queuing and caching operations.                                                                                                                                                                                                  |
| <a id="blob-storage" /> **Blob storage** | LangSmith supports several blob storage providers, including [AWS S3](https://aws.amazon.com/s3/), [Azure Blob Storage](https://azure.microsoft.com/en-us/services/storage/blobs/), and [Google Cloud Storage](https://cloud.google.com/storage).<br /><br />LangSmith uses blob storage to store large files, such as trace artifacts, feedback attachments, and other large data objects. Blob storage is optional, but highly recommended for production deployments. |

* **Docker Compose** (development/testing only)
* **Kubernetes + Helm** (recommended for production)

* [Install on Kubernetes](/langsmith/kubernetes) (production)
* [Install with Docker](/langsmith/docker) (development only)

## Enable LangSmith Deployment

**LangSmith Deployment** is an optional add-on that can be enabled on your [LangSmith](#langsmith) instance. It's ideal for enterprise teams who want a centralized, UI-driven platform to deploy and manage multiple agents and graphs, with all infrastructure, data, and orchestration fully under their control.

This includes everything from [LangSmith](#langsmith), plus:

| Component                                                                                                | Responsibilities                                                                                                                                    | Where it runs | Who manages it |
| -------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------- | ------------- | -------------- |
| <Tooltip tip="The LangSmith UI and APIs for managing deployments.">Control plane</Tooltip>               | <ul><li>UI for creating deployments & revisions</li><li>APIs for deployment management</li></ul>                                                    | Your cloud    | You            |
| <Tooltip tip="The runtime environment where your Agent Servers and agents execute.">Data plane</Tooltip> | <ul><li>Operator/listener to reconcile deployments</li><li>Agent Servers (agents/graphs)</li><li>Backing services (Postgres, Redis, etc.)</li></ul> | Your cloud    | You            |

You run both the control plane and the data plane entirely within your own infrastructure. You are responsible for provisioning and managing all components.

<Note>
  Learn more about the [control plane](/langsmith/control-plane) and [data plane](/langsmith/data-plane) architecture concepts.
</Note>

<img className="block dark:hidden" src="https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/full-platform-with-deployment-light.png?fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=78861d689cf446190d0c1e80ed1a65dd" alt="Full platform architecture with control plane and data plane" data-og-width="2138" width="2138" data-og-height="2104" height="2104" data-path="langsmith/images/full-platform-with-deployment-light.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/full-platform-with-deployment-light.png?w=280&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=28f2db3fb8d323baa1ed925fa39a0f82 280w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/full-platform-with-deployment-light.png?w=560&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=724b17ae55c6fc19c8ced2d8e081bcb3 560w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/full-platform-with-deployment-light.png?w=840&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=46d965e47f66ece91077b1dfdf56196b 840w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/full-platform-with-deployment-light.png?w=1100&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=d723ddff8b6942e8fcf15fe5523102b0 1100w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/full-platform-with-deployment-light.png?w=1650&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=06932f510e15f96376c67c790dcb31e3 1650w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/full-platform-with-deployment-light.png?w=2500&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=c482dbbe51709c394194bb3ea7842c09 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/full-platform-with-deployment-dark.png?fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=f5253bab42b0b351e1d13489fea4311b" alt="Full platform architecture with control plane and data plane" data-og-width="2138" width="2138" data-og-height="2104" height="2104" data-path="langsmith/images/full-platform-with-deployment-dark.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/full-platform-with-deployment-dark.png?w=280&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=ba56323cd1e66109847f8adea173063c 280w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/full-platform-with-deployment-dark.png?w=560&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=6e529069bd427fde7c940ab944f0a1ba 560w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/full-platform-with-deployment-dark.png?w=840&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=ee4ed77d09b804dc2de32af0ddedbfbe 840w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/full-platform-with-deployment-dark.png?w=1100&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=ce52832ec7c18f9dc374546651545c42 1100w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/full-platform-with-deployment-dark.png?w=1650&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=8c8781ac82576d68faa8fc11c4b47a83 1650w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/full-platform-with-deployment-dark.png?w=2500&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=a0b4f5a8a31a23136d03160744305700 2500w" />

If you want to self-host LangSmith for observability, evaluation, and agent deployment, follow these steps:

<Steps>
  <Step title="Install self-hosted LangSmith">
    You must already have a [self-hosted LangSmith instance](#langsmith) installed in your cloud with a Kubernetes cluster (required for control plane and data plane).
  </Step>

<Step title="Test your graph locally">
    Use `langgraph-cli` or [Studio](/langsmith/studio) to test your graph locally.
  </Step>

<Step title="Enable LangSmith Deployment">
    Follow the [setup guide](/langsmith/deploy-self-hosted-full-platform) to enable LangSmith Deployment on your LangSmith instance.
  </Step>
</Steps>

The **Standalone server** option is the most lightweight and flexible way to run LangSmith. Unlike the other models, you only manage a simplified <Tooltip tip="The runtime environment where your Agent Servers and agents execute.">data plane</Tooltip> made up of Agent Servers and their required backing services (PostgreSQL, Redis, etc.).

| Component         | Responsibilities                                              | Where it runs | Who manages it |
| ----------------- | ------------------------------------------------------------- | ------------- | -------------- |
| **Control plane** | n/a                                                           | n/a           | n/a            |
| **Data plane**    | <ul><li>Agent Servers</li><li>Postgres, Redis, etc.</li></ul> | Your cloud    | You            |

This option gives you full control over scaling, deployment, and CI/CD pipelines, while still allowing optional integration with LangSmith for tracing and evaluation.

<Warning>
  Do not run standalone servers in serverless environments. Scale-to-zero may cause task loss and scaling up will not work reliably.
</Warning>

<img className="block dark:hidden" src="https://mintcdn.com/langchain-5e9cc07a/Mwtbhvs2R50foe4Y/langsmith/images/standalone-server-light.png?fit=max&auto=format&n=Mwtbhvs2R50foe4Y&q=85&s=db67e2add4cf039b1ce2324fa1c1f244" alt="Standalone server architecture" data-og-width="752" width="752" data-og-height="821" height="821" data-path="langsmith/images/standalone-server-light.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Mwtbhvs2R50foe4Y/langsmith/images/standalone-server-light.png?w=280&fit=max&auto=format&n=Mwtbhvs2R50foe4Y&q=85&s=e28c7f91a45d2608e74febe474066b27 280w, https://mintcdn.com/langchain-5e9cc07a/Mwtbhvs2R50foe4Y/langsmith/images/standalone-server-light.png?w=560&fit=max&auto=format&n=Mwtbhvs2R50foe4Y&q=85&s=6956b33b01bd4fa69269d890d09fa571 560w, https://mintcdn.com/langchain-5e9cc07a/Mwtbhvs2R50foe4Y/langsmith/images/standalone-server-light.png?w=840&fit=max&auto=format&n=Mwtbhvs2R50foe4Y&q=85&s=9e4727b09b6c88780787c6d6ff7bd490 840w, https://mintcdn.com/langchain-5e9cc07a/Mwtbhvs2R50foe4Y/langsmith/images/standalone-server-light.png?w=1100&fit=max&auto=format&n=Mwtbhvs2R50foe4Y&q=85&s=e380af553cd4dd0ddec93a44aee6ce09 1100w, https://mintcdn.com/langchain-5e9cc07a/Mwtbhvs2R50foe4Y/langsmith/images/standalone-server-light.png?w=1650&fit=max&auto=format&n=Mwtbhvs2R50foe4Y&q=85&s=5a0da7b4a6b3da6b5ac6fe804fe219b4 1650w, https://mintcdn.com/langchain-5e9cc07a/Mwtbhvs2R50foe4Y/langsmith/images/standalone-server-light.png?w=2500&fit=max&auto=format&n=Mwtbhvs2R50foe4Y&q=85&s=11f1fbd4fdbb59dd77ef3ebde76422f3 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/langchain-5e9cc07a/Mwtbhvs2R50foe4Y/langsmith/images/standalone-server-dark.png?fit=max&auto=format&n=Mwtbhvs2R50foe4Y&q=85&s=57ede6682332db867f1900200f675a5f" alt="Standalone server architecture" data-og-width="752" width="752" data-og-height="821" height="821" data-path="langsmith/images/standalone-server-dark.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Mwtbhvs2R50foe4Y/langsmith/images/standalone-server-dark.png?w=280&fit=max&auto=format&n=Mwtbhvs2R50foe4Y&q=85&s=d38e2ee8deee27d3bbd2d465854bc850 280w, https://mintcdn.com/langchain-5e9cc07a/Mwtbhvs2R50foe4Y/langsmith/images/standalone-server-dark.png?w=560&fit=max&auto=format&n=Mwtbhvs2R50foe4Y&q=85&s=19f7f77f74f6c11c1b40c0169796cbe3 560w, https://mintcdn.com/langchain-5e9cc07a/Mwtbhvs2R50foe4Y/langsmith/images/standalone-server-dark.png?w=840&fit=max&auto=format&n=Mwtbhvs2R50foe4Y&q=85&s=68547f892a250fc426ce9f0dad79a80e 840w, https://mintcdn.com/langchain-5e9cc07a/Mwtbhvs2R50foe4Y/langsmith/images/standalone-server-dark.png?w=1100&fit=max&auto=format&n=Mwtbhvs2R50foe4Y&q=85&s=66b5b9d43d2d08cae16755d9f510d418 1100w, https://mintcdn.com/langchain-5e9cc07a/Mwtbhvs2R50foe4Y/langsmith/images/standalone-server-dark.png?w=1650&fit=max&auto=format&n=Mwtbhvs2R50foe4Y&q=85&s=1c045709ec08809225b084d8c850cdc2 1650w, https://mintcdn.com/langchain-5e9cc07a/Mwtbhvs2R50foe4Y/langsmith/images/standalone-server-dark.png?w=2500&fit=max&auto=format&n=Mwtbhvs2R50foe4Y&q=85&s=49c858d75788a600cdc558f78fa2dc41 2500w" />

1. Define and test your graph locally using the `langgraph-cli` or [Studio](/langsmith/studio)
2. Package your agent as a Docker image
3. Deploy the Agent Server to your compute platform of choice (Kubernetes, Docker, VM)
4. Optionally, configure LangSmith API keys and endpoints so the server reports traces and evaluations back to LangSmith (self-hosted or SaaS)

### Supported compute platforms

* **Kubernetes**: Use the LangSmith Helm chart to run Agent Servers in a Kubernetes cluster. This is the recommended option for production-grade deployments.

* **Docker**: Run in any Docker-supported compute platform (local dev machine, VM, ECS, etc.). This is best suited for development or small-scale workloads.

<Tip>
  To set up an [Agent Server](/langsmith/agent-server), refer to the [how-to guide](/langsmith/deploy-standalone-server) in the application deployment section.
</Tip>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/self-hosted.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Self-host LangSmith on Kubernetes

**URL:** llms-txt#self-host-langsmith-on-kubernetes

**Contents:**
- Prerequisites
  - Databases
  - Kubernetes cluster requirements
- Configure your Helm Charts:
- Deploying to Kubernetes:
- Validate your deployment:
- Using LangSmith

Source: https://docs.langchain.com/langsmith/kubernetes

<Info>
  Self-hosting LangSmith is an add-on to the Enterprise Plan designed for our largest, most security-conscious customers. See our [pricing page](https://www.langchain.com/pricing) for more detail, and [contact our sales team](https://www.langchain.com/contact-sales) if you want to get a license key to trial LangSmith in your environment.
</Info>

This page describes how to set up **LangSmith** (observability, tracing, and evaluation) in a Kubernetes cluster. You'll use Helm to install LangSmith and its dependencies.

After completing this page, you'll have:

* **LangSmith UI and APIs**: for [observability](/langsmith/observability), tracing, and [evaluation](/langsmith/evaluation).
* **Backend services**: (queue, playground, ACE).
* **Datastores**: (PostgreSQL, Redis, ClickHouse, optional blob storage).

For [agent deployment](/langsmith/deployments): To add deployment capabilities, complete this guide first, then follow [Enable LangSmith Deployment](/langsmith/deploy-self-hosted-full-platform).

LangChain has successfully tested LangSmith on the following Kubernetes distributions:

* Google Kubernetes Engine (GKE)
* Amazon Elastic Kubernetes Service (EKS)
* Azure Kubernetes Service (AKS)
* OpenShift (4.14+)
* Minikube and Kind (for development purposes)

<Note>
  LangChain has several Terraform modules the help in the provisioning of resources for LangSmith. You can find those in the LangChain [public Terraform repo](https://github.com/langchain-ai/terraform).

Supported cloud providers include:

* [AWS terraform modules](https://github.com/langchain-ai/terraform/tree/main/modules/aws)
  * [Azure terraform modules](https://github.com/langchain-ai/terraform/tree/main/modules/azure)

You can click on the links above to see the documentation for each module. These modules are designed to help you quickly set up the necessary infrastructure for LangSmith, including Kubernetes clusters, storage, and networking.
</Note>

Ensure you have the following tools/items ready. Some items are marked optional:

1. LangSmith License Key

1. You can get this from your LangChain representative. [Contact our sales team](https://www.langchain.com/contact-sales) for more information.

1. This is a secret key that you can generate. It should be a random string of characters.
   2. You can generate this using the following command:

3. JWT Secret (Optional but used for basic auth)

1. This is a secret key that you can generate. It should be a random string of characters.
   2. You can generate this using the following command:

LangSmith uses a PostgreSQL database, a Redis cache, and a ClickHouse database to store traces. By default, these services are installed inside your Kubernetes cluster. However, we highly recommend using external databases instead. For PostgreSQL and Redis, the best option is your cloud provider’s managed services.

For more information, refer to the following setup guides for external services:

* [PostgreSQL](/langsmith/self-host-external-postgres)
* [Redis](/langsmith/self-host-external-redis)
* [ClickHouse](/langsmith/self-host-external-clickhouse)

### Kubernetes cluster requirements

1. You will need a working Kubernetes cluster that you can access via `kubectl`. Your cluster should have the following minimum requirements:

1. Recommended: At least 16 vCPUs, 64GB Memory available

* You may need to tune resource requests/limits for all of our different services based off of organization size/usage. Our recommendations can be found [here](/langsmith/self-host-scale).
      * We recommend using a cluster autoscaler to handle scaling up/down of nodes based on resource usage.
      * We recommend setting up the metrics server so that autoscaling can be turned on.
      * If you are running Clickhouse in-cluster, you must have a node with at least 4 vCPUs and 16GB of memory **allocatable** as ClickHouse will request this amount of resources by default.

2. Valid Dynamic PV provisioner or PVs available on your cluster (required only if you are running databases in-cluster)

* To enable persistence, we will try to provision volumes for any database running in-cluster.
      * If using PVs in your cluster, we highly recommend setting up backups in a production environment.
      * **We strongly encourage using a storage class backed by SSDs for better performance. We recommend 7000 IOPS and 1000 MiB/s throughput.**
      * On EKS, you may need to ensure you have the `ebs-csi-driver` installed and configured for dynamic provisioning. Refer to the [EBS CSI Driver documentation](https://docs.aws.amazon.com/eks/latest/userguide/ebs-csi.html) for more information.

You can verify this by running:

The output should show at least one storage class with a provisioner that supports dynamic provisioning. For example:

<Note>
        We highly recommend using a storage class that supports volume expansion. This is because traces can potentially require a lot of disk space and your volumes may need to be resized over time.
      </Note>

Refer to the [Kubernetes documentation](https://kubernetes.io/do/langsmith/observability-concepts/storage/storage-classes/) for more information on storage classes.

1. To install `helm` refer to the [Helm documentation](https://helm.sh/docs/intro/install/)

3. Egress to `https://beacon.langchain.com` (if not running in offline mode)

1. LangSmith requires egress to `https://beacon.langchain.com` for license verification and usage reporting. This is required for LangSmith to function properly. You can find more information on egress requirements in the [Egress](/langsmith/self-host-egress) section.

## Configure your Helm Charts:

1. Create a new file called `langsmith_config.yaml` with the configuration options from the previous step.
   1. There are several configuration options that you can set in the `langsmith_config.yaml` file. You can find more information on specific configuration options in the [Configuration](/langsmith/self-hosted) section.
   2. If you are new to Kubernetes or Helm, we’d recommend starting with one of the example configurations in the examples directory of the Helm Chart repository here: [LangSmith helm chart examples](https://github.com/langchain-ai/helm/tree/main/charts/langsmith/examples).
   3. You can see a full list of configuration options in the `values.yaml` file in the Helm Chart repository here: [LangSmith Helm Chart](https://github.com/langchain-ai/helm/tree/main/charts/langsmith/values.yaml)

2. At a minimum, you will need to set the following configuration options (using basic auth):

You will also need to specify connection details for any external databases you are using.

## Deploying to Kubernetes:

1. Verify that you can connect to your Kubernetes cluster(note: We highly suggest installing into an empty namespace)

1. Run `kubectl get pods`

Output should look something like:

<Note>
     If you are using a namespace other than the default namespace, you will need to specify the namespace in the `helm` and `kubectl` commands by using the `-n <namespace>` flag.
   </Note>

2. Ensure you have the LangChain Helm repo added. (skip this step if you are using local charts)

3. Find the latest version of the chart. You can find the available versions in the [Helm Chart repository](https://github.com/langchain-ai/helm/releases).

* We generally recommend using the latest version.
   * You can also run `helm search repo langchain/langsmith --versions` to see the available versions. The output will look something like this:

4. Run `helm upgrade -i langsmith langchain/langsmith --values langsmith_config.yaml --version <version> -n <namespace> --wait --debug`

* Replace `<namespace>` with the namespace you want to deploy LangSmith to.
   * Replace `<version>` with the version of LangSmith you want to install from the previous step. Most users should install the latest version available.

Once the `helm install` command runs and finishes successfully, you should see output similar to this:

This may take a few minutes to complete as it will create several Kubernetes resources and run several jobs to initialize the database and other services.

5. Run `kubectl get pods` Output should now look something like this (note the exact pod names may vary based on the version and configuration you used):

## Validate your deployment:

1. Run `kubectl get services`

Output should look something like:

2. Curl the external ip of the `langsmith-frontend` service:

3. Visit the external ip for the `langsmith-frontend` service on your browser

The LangSmith UI should be visible/operational

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=5310f686e7b9eebaaee4fe2a152a8675" alt="" data-og-width="2886" width="2886" data-og-height="1698" height="1698" data-path="langsmith/images/langsmith-ui.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=5f155ce778ca848f89fefff237b69bcb 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=1d55d4068a9f53387c129b4688b0971e 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=feb20198d67249ece559e5fd0e6d8e98 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=3e5eba764d911e567d5aaa9e5702327b 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=d45af56632578a8d1b05e546dfc8d01d 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=16a49517a6c224930fdb81c9ccde5527 2500w" />

Now that LangSmith is running, you can start using it to trace your code. You can find more information on how to use self-hosted LangSmith in the [self-hosted usage guide](/langsmith/self-hosted).

Your LangSmith instance is now running but may not be fully setup yet.

If you used one of the basic configs, you will have a default admin user account created for you. You can log in with the email address and password you specified in the `langsmith_config.yaml` file.

As a next step, it is strongly recommended you work with your infrastructure administrators to:

* Setup DNS for your LangSmith instance to enable easier access
* Configure SSL to ensure in-transit encryption of traces submitted to LangSmith
* Configure LangSmith with [Single Sign-On](/langsmith/self-host-sso) to secure your LangSmith instance
* Connect LangSmith to external Postgres and Redis instances
* Set up [Blob Storage](/langsmith/self-host-blob-storage) for storing large files

Review our [configuration section](/langsmith/self-hosted) for more information on how to configure these options.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/kubernetes.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
3. JWT Secret (Optional but used for basic auth)

   1. This is a secret key that you can generate. It should be a random string of characters.
   2. You can generate this using the following command:
```

Example 2 (unknown):
```unknown
### Databases

LangSmith uses a PostgreSQL database, a Redis cache, and a ClickHouse database to store traces. By default, these services are installed inside your Kubernetes cluster. However, we highly recommend using external databases instead. For PostgreSQL and Redis, the best option is your cloud provider’s managed services.

For more information, refer to the following setup guides for external services:

* [PostgreSQL](/langsmith/self-host-external-postgres)
* [Redis](/langsmith/self-host-external-redis)
* [ClickHouse](/langsmith/self-host-external-clickhouse)

### Kubernetes cluster requirements

1. You will need a working Kubernetes cluster that you can access via `kubectl`. Your cluster should have the following minimum requirements:

   1. Recommended: At least 16 vCPUs, 64GB Memory available

      * You may need to tune resource requests/limits for all of our different services based off of organization size/usage. Our recommendations can be found [here](/langsmith/self-host-scale).
      * We recommend using a cluster autoscaler to handle scaling up/down of nodes based on resource usage.
      * We recommend setting up the metrics server so that autoscaling can be turned on.
      * If you are running Clickhouse in-cluster, you must have a node with at least 4 vCPUs and 16GB of memory **allocatable** as ClickHouse will request this amount of resources by default.

   2. Valid Dynamic PV provisioner or PVs available on your cluster (required only if you are running databases in-cluster)

      * To enable persistence, we will try to provision volumes for any database running in-cluster.
      * If using PVs in your cluster, we highly recommend setting up backups in a production environment.
      * **We strongly encourage using a storage class backed by SSDs for better performance. We recommend 7000 IOPS and 1000 MiB/s throughput.**
      * On EKS, you may need to ensure you have the `ebs-csi-driver` installed and configured for dynamic provisioning. Refer to the [EBS CSI Driver documentation](https://docs.aws.amazon.com/eks/latest/userguide/ebs-csi.html) for more information.

      You can verify this by running:
```

Example 3 (unknown):
```unknown
The output should show at least one storage class with a provisioner that supports dynamic provisioning. For example:
```

Example 4 (unknown):
```unknown
<Note>
        We highly recommend using a storage class that supports volume expansion. This is because traces can potentially require a lot of disk space and your volumes may need to be resized over time.
      </Note>

      Refer to the [Kubernetes documentation](https://kubernetes.io/do/langsmith/observability-concepts/storage/storage-classes/) for more information on storage classes.

2. Helm

   1. To install `helm` refer to the [Helm documentation](https://helm.sh/docs/intro/install/)

3. Egress to `https://beacon.langchain.com` (if not running in offline mode)

   1. LangSmith requires egress to `https://beacon.langchain.com` for license verification and usage reporting. This is required for LangSmith to function properly. You can find more information on egress requirements in the [Egress](/langsmith/self-host-egress) section.

## Configure your Helm Charts:

1. Create a new file called `langsmith_config.yaml` with the configuration options from the previous step.
   1. There are several configuration options that you can set in the `langsmith_config.yaml` file. You can find more information on specific configuration options in the [Configuration](/langsmith/self-hosted) section.
   2. If you are new to Kubernetes or Helm, we’d recommend starting with one of the example configurations in the examples directory of the Helm Chart repository here: [LangSmith helm chart examples](https://github.com/langchain-ai/helm/tree/main/charts/langsmith/examples).
   3. You can see a full list of configuration options in the `values.yaml` file in the Helm Chart repository here: [LangSmith Helm Chart](https://github.com/langchain-ai/helm/tree/main/charts/langsmith/values.yaml)

2. At a minimum, you will need to set the following configuration options (using basic auth):
```

---

## Self-host LangSmith with Docker

**URL:** llms-txt#self-host-langsmith-with-docker

**Contents:**
- Prerequisites
- Running via Docker Compose
  - 1. Fetch the LangSmith `docker-compose.yml` file
  - 2. Configure environment variables
  - 3. Start server
  - Validate your deployment:
  - Checking the logs
  - Stopping the server
- Using LangSmith

Source: https://docs.langchain.com/langsmith/docker

<Info>
  Self-hosting LangSmith is an add-on to the Enterprise Plan designed for our largest, most security-conscious customers. See our [pricing page](https://www.langchain.com/pricing) for more detail, and [contact our sales team](https://www.langchain.com/contact-sales) if you want to get a license key to trial LangSmith in your environment.
</Info>

This guide provides instructions for running the **LangSmith platform** locally using Docker for development and testing purposes.

<Warning>
  **For development/testing only**. Do not use Docker Compose for production. For production deployments, use [Kubernetes](/langsmith/kubernetes).
</Warning>

<Note>
  This page describes how to install the base [LangSmith platform](/langsmith/self-hosted#langsmith) for local testing. It does **not** include deployment management features. For more details, review the [self-hosted options](/langsmith/self-hosted).
</Note>

Note that Docker Compose is limited to local development environments only and does not extend support to container services such as AWS Elastic Container Service, Azure Container Instances, and Google Cloud Run.

1. Ensure Docker is installed and running on your system. You can verify this by running:

If you don't see any server information in the output, make sure Docker is installed correctly and launch the Docker daemon.

1. Recommended: At least 4 vCPUs, 16GB Memory available on your machine.
      * You may need to tune resource requests/limits for all of our different services based off of organization size/usage
   2. Disk Space: LangSmith can potentially require a lot of disk space. Ensure you have enough disk space available.

2. LangSmith License Key
   1. You can get this from your LangChain representative. [Contact our sales team](https://www.langchain.com/contact-sales) for more information.

1. This is a secret key that you can generate. It should be a random string of characters.
   2. You can generate this using the following command:

4. Egress to `https://beacon.langchain.com` (if not running in offline mode)
   1. LangSmith requires egress to `https://beacon.langchain.com` for license verification and usage reporting. This is required for LangSmith to function properly. You can find more information on egress requirements in the [Egress](/langsmith/self-host-egress) section.

5. Configuration
   1. There are several configuration options that you can set in the `.env` file. You can find more information on the available configuration options in the [Configuration](/langsmith/self-host-scale) section.

## Running via Docker Compose

The following explains how to run the LangSmith using Docker Compose. This is the most flexible way to run LangSmith without Kubernetes. The default configuration for Docker Compose is intended for local testing only and not for instances where any services are exposed to the public internet. **In production, we highly recommend using a secured Kubernetes environment.**

### 1. Fetch the LangSmith `docker-compose.yml` file

You can find the `docker-compose.yml` file and related files in the LangSmith SDK repository here: [*LangSmith Docker Compose File*](https://github.com/langchain-ai/helm/blob/main/charts/langsmith/docker-compose/docker-compose.yaml)

Copy the `docker-compose.yml` file and all files in that directory from the LangSmith SDK to your project directory.

* Ensure that you copy the `users.xml` file as well.

### 2. Configure environment variables

1. Copy the `.env.example` file from the LangSmith SDK to your project directory and rename it to `.env`.
2. Configure the appropriate values in the `.env` file. You can find the available configuration options in the [Configuration](/langsmith/self-hosted) section.

You can also set these environment variables in the `docker-compose.yml` file directly or export them in your terminal. We recommend setting them in the `.env` file.

Start the LangSmith application by executing the following command in your terminal:

You can also run the server in the background by running:

### Validate your deployment:

1. Curl the exposed port of the `cli-langchain-frontend-1` container:

2. Visit the exposed port of the `cli-langchain-frontend-1` container on your browser

The LangSmith UI should be visible/operational at `http://localhost:1980`

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=5310f686e7b9eebaaee4fe2a152a8675" alt=".langsmith_ui.png" data-og-width="2886" width="2886" data-og-height="1698" height="1698" data-path="langsmith/images/langsmith-ui.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=5f155ce778ca848f89fefff237b69bcb 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=1d55d4068a9f53387c129b4688b0971e 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=feb20198d67249ece559e5fd0e6d8e98 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=3e5eba764d911e567d5aaa9e5702327b 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=d45af56632578a8d1b05e546dfc8d01d 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=16a49517a6c224930fdb81c9ccde5527 2500w" />

### Checking the logs

If, at any point, you want to check if the server is running and see the logs, run

### Stopping the server

Now that LangSmith is running, you can start using it to trace your code. You can find more information on how to use self-hosted LangSmith in the [self-hosted usage guide](/langsmith/self-hosted).

Your LangSmith instance is now running but may not be fully setup yet.

If you used one of the basic configs, you may have deployed a no-auth configuration. In this state, there is no authentication or concept of user accounts nor API keys and traces can be submitted directly without an API key so long as the hostname is passed to the LangChain tracer/LangSmith SDK.

As a next step, it is strongly recommended you work with your infrastructure administrators to:

* Setup DNS for your LangSmith instance to enable easier access
* Configure SSL to ensure in-transit encryption of traces submitted to LangSmith
* Configure LangSmith for [oauth authentication](/langsmith/self-host-sso) or [basic authentication](/langsmith/self-host-basic-auth) to secure your LangSmith instance
* Secure access to your Docker environment to limit access to only the LangSmith frontend and API
* Connect LangSmith to secured Postgres and Redis instances

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/docker.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
If you don't see any server information in the output, make sure Docker is installed correctly and launch the Docker daemon.

   1. Recommended: At least 4 vCPUs, 16GB Memory available on your machine.
      * You may need to tune resource requests/limits for all of our different services based off of organization size/usage
   2. Disk Space: LangSmith can potentially require a lot of disk space. Ensure you have enough disk space available.

2. LangSmith License Key
   1. You can get this from your LangChain representative. [Contact our sales team](https://www.langchain.com/contact-sales) for more information.

3. Api Key Salt

   1. This is a secret key that you can generate. It should be a random string of characters.
   2. You can generate this using the following command:
```

Example 2 (unknown):
```unknown
4. Egress to `https://beacon.langchain.com` (if not running in offline mode)
   1. LangSmith requires egress to `https://beacon.langchain.com` for license verification and usage reporting. This is required for LangSmith to function properly. You can find more information on egress requirements in the [Egress](/langsmith/self-host-egress) section.

5. Configuration
   1. There are several configuration options that you can set in the `.env` file. You can find more information on the available configuration options in the [Configuration](/langsmith/self-host-scale) section.

## Running via Docker Compose

The following explains how to run the LangSmith using Docker Compose. This is the most flexible way to run LangSmith without Kubernetes. The default configuration for Docker Compose is intended for local testing only and not for instances where any services are exposed to the public internet. **In production, we highly recommend using a secured Kubernetes environment.**

### 1. Fetch the LangSmith `docker-compose.yml` file

You can find the `docker-compose.yml` file and related files in the LangSmith SDK repository here: [*LangSmith Docker Compose File*](https://github.com/langchain-ai/helm/blob/main/charts/langsmith/docker-compose/docker-compose.yaml)

Copy the `docker-compose.yml` file and all files in that directory from the LangSmith SDK to your project directory.

* Ensure that you copy the `users.xml` file as well.

### 2. Configure environment variables

1. Copy the `.env.example` file from the LangSmith SDK to your project directory and rename it to `.env`.
2. Configure the appropriate values in the `.env` file. You can find the available configuration options in the [Configuration](/langsmith/self-hosted) section.

You can also set these environment variables in the `docker-compose.yml` file directly or export them in your terminal. We recommend setting them in the `.env` file.

### 3. Start server

Start the LangSmith application by executing the following command in your terminal:
```

Example 3 (unknown):
```unknown
You can also run the server in the background by running:
```

Example 4 (unknown):
```unknown
### Validate your deployment:

1. Curl the exposed port of the `cli-langchain-frontend-1` container:
```

---

## Self-host standalone servers

**URL:** llms-txt#self-host-standalone-servers

**Contents:**
- Prerequisites
- Kubernetes
- Docker
- Docker Compose

Source: https://docs.langchain.com/langsmith/deploy-standalone-server

This guide shows you how to deploy **standalone <Tooltip tip="The server that runs your LangGraph applications.">Agent Servers</Tooltip>** without the LangSmith UI or control plane. This is the most lightweight self-hosting option for running one or a few agents as independent services.

<Warning>
  **Standalone servers are intended primarily for development purposes.**

This deployment option provides flexibility but requires you to manage your own infrastructure and configuration. Each environment can vary significantly and, as a result, troubleshooting configuration issues may be challenging.

For production workloads, we recommend [LangSmith Deployment](/langsmith/self-hosted#langsmith-deployment), which offers standardized deployment patterns and more comprehensive support options.
</Warning>

<Note>
  **This is the setup page for deploying Agent Servers directly without the LangSmith platform.**

Review the [self-hosted options](/langsmith/self-hosted) to understand:

* [Standalone Server](/langsmith/self-hosted#standalone-server): What this guide covers (no UI, just servers).
  * [LangSmith](/langsmith/self-hosted#langsmith): For the full LangSmith platform with UI.
  * [LangSmith Deployment](/langsmith/self-hosted#langsmith-deployment): For UI-based deployment management.

Before continuing, review the [standalone server overview](/langsmith/self-hosted#standalone-server).
</Note>

1. Use the [LangGraph CLI](/langsmith/cli) to [test your application locally](/langsmith/local-server).
2. Use the [LangGraph CLI](/langsmith/cli) to build a Docker image (i.e. `langgraph build`).
3. The following environment variables are needed for a data plane deployment.
4. `REDIS_URI`: Connection details to a Redis instance. Redis will be used as a pub-sub broker to enable streaming real time output from background runs. The value of `REDIS_URI` must be a valid [Redis connection URI](https://redis-py.readthedocs.io/en/stable/connections.html#redis.Redis.from_url).

<Note>
     **Shared Redis Instance**
     Multiple self-hosted deployments can share the same Redis instance. For example, for `Deployment A`, `REDIS_URI` can be set to `redis://<hostname_1>:<port>/1` and for `Deployment B`, `REDIS_URI` can be set to `redis://<hostname_1>:<port>/2`.

`1` and `2` are different database numbers within the same instance, but `<hostname_1>` is shared. **The same database number cannot be used for separate deployments**.
   </Note>
5. `DATABASE_URI`: Postgres connection details. Postgres will be used to store assistants, threads, runs, persist thread state and long term memory, and to manage the state of the background task queue with 'exactly once' semantics. The value of `DATABASE_URI` must be a valid [Postgres connection URI](https://www.postgresql.org/docs/current/libpq-connect.html#LIBPQ-CONNSTRING-URIS).

<Note>
     **Shared Postgres Instance**
     Multiple self-hosted deployments can share the same Postgres instance. For example, for `Deployment A`, `DATABASE_URI` can be set to `postgres://<user>:<password>@/<database_name_1>?host=<hostname_1>` and for `Deployment B`, `DATABASE_URI` can be set to `postgres://<user>:<password>@/<database_name_2>?host=<hostname_1>`.

`<database_name_1>` and `database_name_2` are different databases within the same instance, but `<hostname_1>` is shared. **The same database cannot be used for separate deployments**.
   </Note>
6. `LANGSMITH_API_KEY`: LangSmith API key.
7. `LANGGRAPH_CLOUD_LICENSE_KEY`: LangSmith license key. This will be used to authenticate ONCE at server start up.
8. `LANGSMITH_ENDPOINT`: To send traces to a [self-hosted LangSmith](/langsmith/self-hosted) instance, set `LANGSMITH_ENDPOINT` to the hostname of the self-hosted LangSmith instance.
9. Egress to `https://beacon.langchain.com` from your network. This is required for license verification and usage reporting if not running in air-gapped mode. See the [Egress documentation](/langsmith/self-host-egress) for more details.

Use this [Helm chart](https://github.com/langchain-ai/helm/blob/main/charts/langgraph-cloud/README.md) to deploy an Agent Server to a Kubernetes cluster.

Run the following `docker` command:

<Note>
  * You need to replace `my-image` with the name of the image you built in the prerequisite steps (from `langgraph build`)

and you should provide appropriate values for `REDIS_URI`, `DATABASE_URI`, and `LANGSMITH_API_KEY`.

* If your application requires additional environment variables, you can pass them in a similar way.
</Note>

Docker Compose YAML file:

You can run the command `docker compose up` with this Docker Compose file in the same folder.

This will launch an Agent Server on port `8123` (if you want to change this, you can change this by changing the ports in the `langgraph-api` volume). You can test if the application is healthy by running:

Assuming everything is running correctly, you should see a response like:

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/deploy-standalone-server.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
<Note>
  * You need to replace `my-image` with the name of the image you built in the prerequisite steps (from `langgraph build`)

  and you should provide appropriate values for `REDIS_URI`, `DATABASE_URI`, and `LANGSMITH_API_KEY`.

  * If your application requires additional environment variables, you can pass them in a similar way.
</Note>

## Docker Compose

Docker Compose YAML file:
```

Example 2 (unknown):
```unknown
You can run the command `docker compose up` with this Docker Compose file in the same folder.

This will launch an Agent Server on port `8123` (if you want to change this, you can change this by changing the ports in the `langgraph-api` volume). You can test if the application is healthy by running:
```

Example 3 (unknown):
```unknown
Assuming everything is running correctly, you should see a response like:
```

---

## Send your API Key in the request headers

**URL:** llms-txt#send-your-api-key-in-the-request-headers

headers = {
    "x-api-key": os.environ["LANGSMITH_API_KEY"],
    "x-tenant-id": os.environ["LANGSMITH_WORKSPACE_ID"]
}

def post_run(run_id, name, run_type, inputs, parent_id=None):
    """Function to post a new run to the API."""
    data = {
        "id": run_id.hex,
        "name": name,
        "run_type": run_type,
        "inputs": inputs,
        "start_time": datetime.utcnow().isoformat(),
        # "session_name": "project-name",  # the name of the project to trace to
        # "session_id": "project-id",  # the ID of the project to trace to. specify one of session_name or session_id
    }
    if parent_id:
        data["parent_run_id"] = parent_id.hex

requests.post(
        "https://api.smith.langchain.com/runs",  # Update appropriately for self-hosted installations or the EU region
        json=data,
        headers=headers
    )

def patch_run(run_id, outputs):
    """Function to patch a run with outputs."""
    requests.patch(
        f"https://api.smith.langchain.com/runs/{run_id}",
        json={
            "outputs": outputs,
            "end_time": datetime.now(timezone.utc).isoformat(),
        },
        headers=headers,
    )

---

## server.py

**URL:** llms-txt#server.py

import langsmith as ls
from fastapi import FastAPI, Request

@ls.traceable
async def my_application():
    ...

app = FastAPI()  # Or Flask, Django, or any other framework

@app.post("/my-route")
async def fake_route(request: Request):
    # request.headers:  {"langsmith-trace": "..."}
    # as well as optional metadata/tags in `baggage`
    with ls.tracing_context(parent=request.headers):
        return await my_application()
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
The example above uses the `tracing_context` context manager. You can also directly specify the parent run context in the `langsmith_extra` parameter of a method wrapped with `@traceable`.
```

---

## Service A: Create a span and propagate context to Service B

**URL:** llms-txt#service-a:-create-a-span-and-propagate-context-to-service-b

def service_a():
    with tracer.start_as_current_span("service_a_operation") as span:
        # Create a chain
        prompt = ChatPromptTemplate.from_template("Summarize: {text}")
        model = ChatOpenAI()
        chain = prompt | model

# Run the chain
        result = chain.invoke({"text": "OpenTelemetry is an observability framework"})

# Propagate context to Service B
        headers = {}
        inject(headers)  # Inject trace context into headers

# Call Service B with the trace context
        response = requests.post(
            "http://service-b.example.com/process",
            headers=headers,
            json={"summary": result.content}
        )
        return response.json()

---

## Service B: Extract the context and continue the trace

**URL:** llms-txt#service-b:-extract-the-context-and-continue-the-trace

from flask import Flask, request, jsonify

app = Flask(__name__)

@app.route("/process", methods=["POST"])
def service_b_endpoint():
    # Extract the trace context from the request headers
    context = extract(request.headers)
    with tracer.start_as_current_span("service_b_operation", context=context) as span:
        data = request.json
        summary = data.get("summary", "")

# Process the summary with another LLM chain
        prompt = ChatPromptTemplate.from_template("Analyze the sentiment of: {text}")
        model = ChatOpenAI()
        chain = prompt | model
        result = chain.invoke({"text": summary})

return jsonify({"analysis": result.content})

if __name__ == "__main__":
    app.run(port=5000)
```

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/trace-with-opentelemetry.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Sets the output format to use Open Telemetry Protocol

**URL:** llms-txt#sets-the-output-format-to-use-open-telemetry-protocol

export OTEL_LOGS_EXPORTER=otlp

---

## Setup claude_agent_sdk with langsmith tracing

**URL:** llms-txt#setup-claude_agent_sdk-with-langsmith-tracing

configure_claude_agent_sdk()

@tool(
    "get_weather",
    "Gets the current weather for a given city",
    {
        "city": str,
    },
)
async def get_weather(args: dict[str, Any]) -> dict[str, Any]:
    """Simulated weather lookup tool"""
    city = args["city"]

# Simulated weather data
    weather_data = {
        "San Francisco": "Foggy, 62°F",
        "New York": "Sunny, 75°F",
        "London": "Rainy, 55°F",
        "Tokyo": "Clear, 68°F",
    }

weather = weather_data.get(city, "Weather data not available")
    return {"content": [{"type": "text", "text": f"Weather in {city}: {weather}"}]}

async def main():
    # Create SDK MCP server with the weather tool
    weather_server = create_sdk_mcp_server(
        name="weather",
        version="1.0.0",
        tools=[get_weather],
    )

options = ClaudeAgentOptions(
        model="claude-sonnet-4-5-20250929",
        system_prompt="You are a friendly travel assistant who helps with weather information.",
        mcp_servers={"weather": weather_server},
        allowed_tools=["mcp__weather__get_weather"],
    )

async with ClaudeSDKClient(options=options) as client:
        await client.query("What's the weather like in San Francisco and Tokyo?")

async for message in client.receive_response():
            print(message)

if __name__ == "__main__":
    asyncio.run(main())
```

Once configured, all Claude Agent SDK operations will be automatically traced to LangSmith, including:

* Agent queries and responses
* Tool invocations and results
* Claude model interactions
* MCP server operations

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/trace-claude-agent-sdk.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Set a sampling rate for traces

**URL:** llms-txt#set-a-sampling-rate-for-traces

**Contents:**
- Set a global sampling rate
- Set different sampling rates per client

Source: https://docs.langchain.com/langsmith/sample-traces

When working with high-volume applications, you may not want to log every trace to LangSmith. Sampling rates allow you to control what percentage of traces are logged, helping you balance observability needs with cost considerations.

## Set a global sampling rate

<Note>
  This section is relevant for those using the LangSmith SDK or LangChain, not for those logging directly with the LangSmith API.
</Note>

By default, all traces are logged to LangSmith. To down-sample the number of traces logged to LangSmith, set the `LANGSMITH_TRACING_SAMPLING_RATE` environment variable to any float between `0` (no traces) and `1` (all traces). For instance, setting the following environment variable will log 75% of the traces.

This works for the `traceable` decorator and `RunTree` objects.

## Set different sampling rates per client

You can also set sampling rates on specific `Client` instances and use the `tracing_context` context manager:

```python  theme={null}
from langsmith import Client, tracing_context

**Examples:**

Example 1 (unknown):
```unknown
This works for the `traceable` decorator and `RunTree` objects.

## Set different sampling rates per client

You can also set sampling rates on specific `Client` instances and use the `tracing_context` context manager:
```

---

## Set environment variables for external services

**URL:** llms-txt#set-environment-variables-for-external-services

**Contents:**
- Troubleshooting
  - Wrong API endpoints

export POSTGRES_URI_CUSTOM="postgresql://user:pass@host:5432/db"
export REDIS_URI_CUSTOM="redis://host:6379/0"
```

See the [environment variables documentation](/langsmith/env-var#postgres-uri-custom) for more details.

### Wrong API endpoints

If you're experiencing connection issues, verify you're using the correct endpoint format for your LangSmith instance. There are two different APIs with different endpoints:

#### LangSmith API (Traces, Ingestion, etc.)

For LangSmith API operations (traces, evaluations, datasets):

| Region | Endpoint                             |
| ------ | ------------------------------------ |
| US     | `https://api.smith.langchain.com`    |
| EU     | `https://eu.api.smith.langchain.com` |

For self-hosted LangSmith instances, use `http(s)://<langsmith-url>/api` where `<langsmith-url>` is your self-hosted instance URL.

<Note>
  If you're setting the endpoint in the `LANGSMITH_ENDPOINT` environment variable, you need to add `/v1` at the end (e.g., `https://api.smith.langchain.com/v1` or `http(s)://<langsmith-url>/api/v1` if self-hosted).
</Note>

#### LangSmith Deployments API (Deployments)

For LangSmith Deployments operations (deployments, revisions):

| Region | Endpoint                            |
| ------ | ----------------------------------- |
| US     | `https://api.host.langchain.com`    |
| EU     | `https://eu.api.host.langchain.com` |

For self-hosted LangSmith instances, use `http(s)://<langsmith-url>/api-host` where `<langsmith-url>` is your self-hosted instance URL.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/cicd-pipeline-example.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Set environment variables for LangChain

**URL:** llms-txt#set-environment-variables-for-langchain

os.environ["LANGSMITH_OTEL_ENABLED"] = "true"
os.environ["LANGSMITH_TRACING"] = "true"

---

## Set stream_mode="custom" to receive the custom data in the stream

**URL:** llms-txt#set-stream_mode="custom"-to-receive-the-custom-data-in-the-stream

**Contents:**
- Disable streaming for specific chat models
  - Async with Python \< 3.11

for chunk in graph.stream(
    {"topic": "cats"},
    stream_mode="custom",  # [!code highlight]

):
    # The chunk will contain the custom data streamed from the llm
    print(chunk)
python  theme={null}
  import operator
  import json

from typing import TypedDict
  from typing_extensions import Annotated
  from langgraph.graph import StateGraph, START

from openai import AsyncOpenAI

openai_client = AsyncOpenAI()
  model_name = "gpt-4o-mini"

async def stream_tokens(model_name: str, messages: list[dict]):
      response = await openai_client.chat.completions.create(
          messages=messages, model=model_name, stream=True
      )
      role = None
      async for chunk in response:
          delta = chunk.choices[0].delta

if delta.role is not None:
              role = delta.role

if delta.content:
              yield {"role": role, "content": delta.content}

# this is our tool
  async def get_items(place: str) -> str:
      """Use this tool to list items one might find in a place you're asked about."""
      writer = get_stream_writer()
      response = ""
      async for msg_chunk in stream_tokens(
          model_name,
          [
              {
                  "role": "user",
                  "content": (
                      "Can you tell me what kind of items "
                      f"i might find in the following place: '{place}'. "
                      "List at least 3 such items separating them by a comma. "
                      "And include a brief description of each item."
                  ),
              }
          ],
      ):
          response += msg_chunk["content"]
          writer(msg_chunk)

class State(TypedDict):
      messages: Annotated[list[dict], operator.add]

# this is the tool-calling graph node
  async def call_tool(state: State):
      ai_message = state["messages"][-1]
      tool_call = ai_message["tool_calls"][-1]

function_name = tool_call["function"]["name"]
      if function_name != "get_items":
          raise ValueError(f"Tool {function_name} not supported")

function_arguments = tool_call["function"]["arguments"]
      arguments = json.loads(function_arguments)

function_response = await get_items(**arguments)
      tool_message = {
          "tool_call_id": tool_call["id"],
          "role": "tool",
          "name": function_name,
          "content": function_response,
      }
      return {"messages": [tool_message]}

graph = (
      StateGraph(State)
      .add_node(call_tool)
      .add_edge(START, "call_tool")
      .compile()
  )
  python  theme={null}
  inputs = {
      "messages": [
          {
              "content": None,
              "role": "assistant",
              "tool_calls": [
                  {
                      "id": "1",
                      "function": {
                          "arguments": '{"place":"bedroom"}',
                          "name": "get_items",
                      },
                      "type": "function",
                  }
              ],
          }
      ]
  }

async for chunk in graph.astream(
      inputs,
      stream_mode="custom",
  ):
      print(chunk["content"], end="|", flush=True)
  python  theme={null}
    from langchain.chat_models import init_chat_model

model = init_chat_model(
        "claude-sonnet-4-5-20250929",
        # Set disable_streaming=True to disable streaming for the chat model
        disable_streaming=True  # [!code highlight]

)
    python  theme={null}
    from langchain_openai import ChatOpenAI

# Set disable_streaming=True to disable streaming for the chat model
    model = ChatOpenAI(model="o1-preview", disable_streaming=True)
    python  theme={null}
  from typing import TypedDict
  from langgraph.graph import START, StateGraph
  from langchain.chat_models import init_chat_model

model = init_chat_model(model="gpt-4o-mini")

class State(TypedDict):
      topic: str
      joke: str

# Accept config as an argument in the async node function
  async def call_model(state, config):
      topic = state["topic"]
      print("Generating joke...")
      # Pass config to model.ainvoke() to ensure proper context propagation
      joke_response = await model.ainvoke(  # [!code highlight]
          [{"role": "user", "content": f"Write a joke about {topic}"}],
          config,
      )
      return {"joke": joke_response.content}

graph = (
      StateGraph(State)
      .add_node(call_model)
      .add_edge(START, "call_model")
      .compile()
  )

# Set stream_mode="messages" to stream LLM tokens
  async for chunk, metadata in graph.astream(
      {"topic": "ice cream"},
      stream_mode="messages",  # [!code highlight]
  ):
      if chunk.content:
          print(chunk.content, end="|", flush=True)
  python  theme={null}
  from typing import TypedDict
  from langgraph.types import StreamWriter

class State(TypedDict):
        topic: str
        joke: str

# Add writer as an argument in the function signature of the async node or tool
  # LangGraph will automatically pass the stream writer to the function
  async def generate_joke(state: State, writer: StreamWriter):  # [!code highlight]
        writer({"custom_key": "Streaming custom data while generating a joke"})
        return {"joke": f"This is a joke about {state['topic']}"}

graph = (
        StateGraph(State)
        .add_node(generate_joke)
        .add_edge(START, "generate_joke")
        .compile()
  )

# Set stream_mode="custom" to receive the custom data in the stream  # [!code highlight]
  async for chunk in graph.astream(
        {"topic": "ice cream"},
        stream_mode="custom",
  ):
        print(chunk)
  ```
</Accordion>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/streaming.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
<Accordion title="Extended example: streaming arbitrary chat model">
```

Example 2 (unknown):
```unknown
Let's invoke the graph with an [`AIMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessage) that includes a tool call:
```

Example 3 (unknown):
```unknown
</Accordion>

## Disable streaming for specific chat models

If your application mixes models that support streaming with those that do not, you may need to explicitly disable streaming for
models that do not support it.

Set `disable_streaming=True` when initializing the model.

<Tabs>
  <Tab title="init_chat_model">
```

Example 4 (unknown):
```unknown
</Tab>

  <Tab title="Chat model interface">
```

---

## Set the entrypoint as 'agent'

**URL:** llms-txt#set-the-entrypoint-as-'agent'

---

## Set the project name to whichever project you'd like to be testing against

**URL:** llms-txt#set-the-project-name-to-whichever-project-you'd-like-to-be-testing-against

project_name = "Tweet Writing Task"
os.environ["LANGSMITH_PROJECT"] = project_name
os.environ["LANGSMITH_TRACING"] = "true"

if not os.environ.get("LANGSMITH_API_KEY"):
    os.environ["LANGSMITH_API_KEY"] = getpass.getpass("YOUR API KEY")

---

## set the same access key credentials and region as you used for the destination

**URL:** llms-txt#set-the-same-access-key-credentials-and-region-as-you-used-for-the-destination

> AWS Access Key ID: <access_key_id>
> AWS Secret Access Key: <secret_access_key>
> Default region name [us-east-1]: <region>

---

## Set this to true in order to log the input user prompts

**URL:** llms-txt#set-this-to-true-in-order-to-log-the-input-user-prompts

export OTEL_LOG_USER_PROMPTS=1

---

## Set up Agent Auth (Beta)

**URL:** llms-txt#set-up-agent-auth-(beta)

**Contents:**
- Installation
- Quickstart
  - 1. Initialize the client
  - 2. Set up OAuth providers
  - 3. Authenticate from an agent

Source: https://docs.langchain.com/langsmith/agent-auth

Enable secure access from agents to any system using OAuth 2.0 credentials with Agent Auth.

<Note>Agent Auth is in **Beta** and under active development. To provide feedback or use this feature, reach out to the [LangChain team](https://forum.langchain.com/c/help/langsmith/).</Note>

Install the Agent Auth client library from PyPI:

### 1. Initialize the client

### 2. Set up OAuth providers

Before agents can authenticate, you need to configure an OAuth provider using the following process:

1. Select a unique identifier for your OAuth provider to use in LangChain's platform (e.g., "github-local-dev", "google-workspace-prod").

2. Go to your OAuth provider's developer console and create a new OAuth application.

3. Set LangChain's API as an available callback URL using this structure:
   
   For example, if your provider\_id is "github-local-dev", use:

4. Use `client.create_oauth_provider()` with the credentials from your OAuth app:

### 3. Authenticate from an agent

The client `authenticate()` API is used to get OAuth tokens from pre-configured providers. On the first call, it takes the caller through an OAuth 2.0 auth flow.

#### In LangGraph context

By default, tokens are scoped to the calling agent using the Assistant ID parameter.

```python  theme={null}
auth_result = await client.authenticate(
    provider="{provider_id}",
    scopes=["scopeA"],
    user_id="your_user_id" # Any unique identifier to scope this token to the human caller
)

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

## Quickstart

### 1. Initialize the client
```

Example 3 (unknown):
```unknown
### 2. Set up OAuth providers

Before agents can authenticate, you need to configure an OAuth provider using the following process:

1. Select a unique identifier for your OAuth provider to use in LangChain's platform (e.g., "github-local-dev", "google-workspace-prod").

2. Go to your OAuth provider's developer console and create a new OAuth application.

3. Set LangChain's API as an available callback URL using this structure:
```

Example 4 (unknown):
```unknown
For example, if your provider\_id is "github-local-dev", use:
```

---

## Set up automation rules

**URL:** llms-txt#set-up-automation-rules

**Contents:**
- View automation rules
- Create a rule
- View logs for your automations
- Video guide

Source: https://docs.langchain.com/langsmith/rules

While you can manually sift through and process production logs from your LLM application, it often becomes difficult as your application scales to more users.
LangSmith provides a powerful feature called **Automations** that allow you to trigger certain actions on your trace data.
At a high level, automations are defined by a **filter**, **sampling rate**, and **action**.

Automation rules can trigger actions such as: adding traces to a dataset, adding to an annotation queue, triggering a webhook (e.g. for remote evaluations) or extending data retention. Some examples of automations you can set up:

* Send all traces with negative feedback to an annotation queue for human review
* Send 10% of all traces to an annotation queue for human review to spot check for issues
* Upgrade all traces with errors for extended data retention

<Info>
  To configure online evaluations, visit the [online evaluations](/langsmith/online-evaluations) page.
</Info>

<Note>If an automation rule matches any run within a trace, the trace will be auto-upgraded to [extended data retention](/langsmith/administration-overview#data-retention-auto-upgrades). This upgrade will impact trace pricing, but ensures that traces meeting your automation criteria (typically those most valuable for analysis) are preserved for investigation. </Note>

## View automation rules

Head to the **Tracing Projects** tab and select a tracing project. To view existing automation rules for that tracing project, click on the **Automations** tab.

<img src="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-automation-rules.png?fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=9206c95784e0d572adddf7ad60e58717" alt="View automation rules" data-og-width="1349" width="1349" data-og-height="521" height="521" data-path="langsmith/images/view-automation-rules.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-automation-rules.png?w=280&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=88988b3e687419b0494b36e20d424965 280w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-automation-rules.png?w=560&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=8091094d8e938a31367fd456cdb16778 560w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-automation-rules.png?w=840&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=08bce1184a23c4345c41abc2ede29826 840w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-automation-rules.png?w=1100&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=faf56babdc5b9396a38e780600a31b4a 1100w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-automation-rules.png?w=1650&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=8996953f130ed137172b26af817a5c22 1650w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-automation-rules.png?w=2500&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=fea915cf43defcf09e6b7161a2c2af8a 2500w" />

<img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/aq-spot-check-rule.gif?s=0ea58303ab04ffa8ed16c49ab39701ef" alt="" data-og-width="1556" width="1556" data-og-height="1080" height="1080" data-path="langsmith/images/aq-spot-check-rule.gif" data-optimize="true" data-opv="3" />

#### 1. Navigate to rule creation

Head to the **Tracing Projects** tab and select a tracing project. Click on **+ New** in the top right corner of the tracing project page, then click on **New Automation**.

#### 2. Name your rule

#### 3. Create a filter

Automation rule filters work the same way as filters applied to traces in the project. For more information on filters, you can refer to [this guide](./filter-traces-in-application)

#### 4. Configure a sampling rate

Configure a sampling rate to control the percentage of filtered runs that trigger the automation action.

You can specify a sampling rate between 0 and 1 for automations. This will control the percent of the filtered runs that are sent to an automation action. For example, if you set the sampling rate to 0.5, then 50% of the traces that pass the filter will be sent to the action.

#### 5. (Optional) Apply rule to past runs

Apply rule to past runs by toggling the **Apply to past runs** and entering a "Backfill from" date. This is only possible upon rule creation. Note: the backfill is processed as a background job, so you will not see the results immediately. In order to track progress of the backfill, you can [view logs for your automations](./rules#view-logs-for-your-automations)

#### 6. Select an action to trigger when the rule is applied.

There are four actions you can take with an automation rule:

* **Add to dataset**: Add the inputs and outputs of the trace to a [dataset](/langsmith/evaluation-concepts#datasets).
* **Add to annotation queue**: Add the trace to an [annotation queue](/langsmith/evaluation-concepts#annotation-queues).
* **Trigger webhook**: Trigger a webhook with the trace data. For more information on webhooks, you can refer to [this guide](./webhooks).
* **Extend data retention**: Extends the data retention period on matching traces that use base retention [(see data retention docs for more details)](/langsmith/administration-overview#data-retention).
  Note that all other rules will also extend data retention on matching traces through the
  auto-upgrade mechanism described in the aforementioned data retention docs,
  but this rule takes no additional action.

## View logs for your automations

Logs allow you to gain confidence that your rules are working as expected. You can view logs for your automations by heading to the **Automations** tab within a tracing project and clicking the **Logs** button for the rule you created.

The logs tab allows you to:

* View all runs processed by a given rule for the time period selected
* If a particular rule execution has triggered an error, you can view the error message by hovering over the error icon
* You can monitor the progress of a backfill job by filtering to the rule's creation timestamp. This is because the backfill starts from when the rule was created.
* Inspect the run that the automation rule applied to using the **View run** button. For rules that add runs as examples to datasets, you can view the example produced.

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rule-logs.gif?s=2ec5562fea9e2079b687da149d1609b3" alt="Logs_Gif" data-og-width="1556" width="1556" data-og-height="1080" height="1080" data-path="langsmith/images/rule-logs.gif" data-optimize="true" data-opv="3" />

<iframe className="w-full aspect-video rounded-xl" src="https://www.youtube.com/embed/z69cBXTJFZ0?si=GBKQ9_muHR1zllLl" title="YouTube video player" frameBorder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowFullScreen />

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/rules.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Set up a workspace

**URL:** llms-txt#set-up-a-workspace

**Contents:**
- Set up an organization
  - Create an organization
  - Manage and navigate workspaces
  - Manage users
- Set up a workspace
  - Create a workspace
  - Manage users
  - Configure workspace settings
  - Delete a workspace
  - Delete a workspace via the UI

Source: https://docs.langchain.com/langsmith/set-up-a-workspace

This page describes setting up and managing your LangSmith [*organization*](/langsmith/administration-overview#organizations) and [*workspaces*](/langsmith/administration-overview#workspaces):

* [Set up an organization](#set-up-an-organization): Create and manage organizations for team collaboration, including user management and role assignments.
* [Set up a workspace](#set-up-a-workspace): Set up and configure workspaces to organize your LangSmith resources, manage workspace members, and configure settings for team collaboration.

<Check>
  You may find it helpful to refer to the [overview on LangSmith resource hierarchy](/langsmith/administration-overview) before you read this setup page.
</Check>

## Set up an organization

<Note>
  If you're interested in managing your organization and workspaces programmatically, see [this how-to guide](/langsmith/manage-organization-by-api).
</Note>

### Create an organization

When you log in for the first time, LangSmith will create a personal organization for you automatically. If you'd like to collaborate with others, you can create a separate organization and invite your team members to join.

To do this, open the Organizations drawer by clicking your profile icon in the bottom left and click **+ New**. Shared organizations require a credit card before they can be used. You will need to [set up billing](/langsmith/billing#set-up-billing-for-your-account) to proceed.

### Manage and navigate workspaces

Once you've subscribed to a plan that allows for multiple users per organization, you can [set up workspaces](/langsmith/administration-overview#workspaces) to collaborate more effectively and isolate LangSmith resources between different groups of users. To navigate between workspaces and access the resources within each workspace (trace projects, annotation queues, etc.), select the desired workspace from the picker in the top left:

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-workspace.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=91c38be270a4e9f7d613fca83192dc6b" alt="" data-og-width="2992" width="2992" data-og-height="478" height="478" data-path="langsmith/images/select-workspace.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-workspace.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=9c1ba64d54177f72feab7394b4a8ff28 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-workspace.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=772aa2168f043813208bd0a9af628b8b 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-workspace.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=ae8d1e560af5d320a772ef6edb23f934 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-workspace.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=cea6e37801913f953c0fd750ba3ff2a3 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-workspace.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=e40edbdce2c538e64c4833f4a10b44fc 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-workspace.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=b7598fd5dc01cb62f6fa22f9adfead8b 2500w" />

Manage membership in your shared organization in the **Members and roles** tabs on the [Settings page](https://smith.langchain.com/settings). Here you can:

* Invite new users to your organization, selecting workspace membership and (if RBAC is enabled) workspace role.
* Edit a user's organization role.
* Remove users from your organization.

<img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/organization-members-and-roles.png?fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=7f6b85051e5dcca2f074ba0ef4801ddd" alt="" data-og-width="3008" width="3008" data-og-height="890" height="890" data-path="langsmith/images/organization-members-and-roles.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/organization-members-and-roles.png?w=280&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=2285444015681aadc12dee88d8485294 280w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/organization-members-and-roles.png?w=560&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=5643ea639fa21f9df8faeb7bbcf4db4c 560w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/organization-members-and-roles.png?w=840&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=011efce2e1f2c89ef26e1c3c5c280d5b 840w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/organization-members-and-roles.png?w=1100&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=fe2dff2c95618365c5269c7fc0f6337f 1100w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/organization-members-and-roles.png?w=1650&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=cd20d709570a0e9e447e0a29b9f457c7 1650w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/organization-members-and-roles.png?w=2500&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=8dba05df82360444c35d94fe90a64dd4 2500w" />

Organizations on the Enterprise plan may set up custom workspace roles in the **Roles** tab. For more details, refer to the [access control setup guide](/langsmith/user-management).

#### Organization roles

Organization-scoped roles are used to determine access to organization settings. The role selected also impacts workspace membership:

* `Organization Admin` grants full access to manage all organization configuration, users, billing, and workspaces. Any `Organization Admin` has `Admin` access to all workspaces in an organization.

- `Organization User` may read organization information, but cannot execute any write actions at the organization level. You can add an `Organization User` to a subset of workspaces and assigned workspace roles as usual (if RBAC is enabled), which specify permissions at the workspace level.

<Info>
  The `Organization User` role is only available in organizations on plans with multiple workspaces. In organizations limited to a single workspace, all users are `Organization Admins`. Custom organization-scoped roles are not available.
</Info>

For a full list of permissions associated with each role, refer to the [Administration overview](/langsmith/administration-overview#organization-roles) page.

## Set up a workspace

When you log in for the first time, a default [workspace](/langsmith/administration-overview#workspaces) will be created for you in your personal organization. Workspaces are often used to separate resources between different teams or business units to establish clear trust boundaries between them. Within each workspace, Role-Based Access Control (RBAC) manages permissions and access levels, which ensures that users only have access to the resources and settings necessary for their role. Most LangSmith activity happens in the context of a workspace, each of which has its own settings and access controls.

To organize resources within a workspace, you can use [resource tags](/langsmith/set-up-resource-tags).

### Create a workspace

To create a new workspace, navigate to the [Settings page](https://smith.langchain.com/settings) **Workspaces** tab in your shared organization and click **Add Workspace**. Once you have created your workspace, you can manage its members and other configuration by selecting it on this page.

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-workspace.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=a26994889b28911c59daa8de557c7271" alt="" data-og-width="3014" width="3014" data-og-height="532" height="532" data-path="langsmith/images/create-workspace.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-workspace.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=e7542ce1dcc74278722aaa5b707eb7f8 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-workspace.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=c30f0578754fa71812905d1b964c2ebb 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-workspace.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=79f02189dc33d5f730defa4792d89f19 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-workspace.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=4138278a2a6f8b11c3df64a51d710b55 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-workspace.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=1a5964c468159d57a812efe66e8bd822 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-workspace.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=e5efd5a083c29f17b9bddad1f7423fe9 2500w" />

<Note>
  Different plans have different limits placed on the number of workspaces that can be used in an organization. For more information, refer to the [pricing page](https://www.langchain.com/pricing-langsmith).
</Note>

<Info>
  Only workspace `Admins` can manage workspace membership and, if RBAC is enabled, change a user's workspace role.
</Info>

For users that are already members of an organization, a workspace `Admin` may add them to a workspace in the **Workspace members** tab under [Workspaces settings page](https://smith.langchain.com/settings/workspaces). Users may also be invited directly to one or more workspaces when they are [invited to an organization](#manage-users).

### Configure workspace settings

Workspace configuration exists in the [Workspaces settings page](https://smith.langchain.com/settings/workspaces) tab. Select the workspace to configure and then the desired configuration sub-tab. The following example shows the **API keys**, and other configuration options including secrets, models, and shared URLs are available here as well.

<img src="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/workspace-settings.png?fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=0b95739c014bc31f2950d9d586303cbb" alt="" data-og-width="3012" width="3012" data-og-height="1226" height="1226" data-path="langsmith/images/workspace-settings.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/workspace-settings.png?w=280&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=ddd4f1738c7142be44e6966b0079cad6 280w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/workspace-settings.png?w=560&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=a8dcfe014fc2584946019acebc59fd3b 560w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/workspace-settings.png?w=840&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=659cffc42334b972d3a1f01f2926120b 840w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/workspace-settings.png?w=1100&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=7fdae2696aed94f5e7391d3e88c92d49 1100w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/workspace-settings.png?w=1650&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=1071e84f39202761f635e141b7828a82 1650w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/workspace-settings.png?w=2500&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=6edc496967555f2cfd3365bb846ce698 2500w" />

### Delete a workspace

<Warning>
  Deleting a workspace will permanently delete the workspace and all associated data. This action cannot be undone.
</Warning>

You can delete a workspace through the LangSmith UI or via [API](https://api.smith.langchain.com/redoc?#tag/workspaces/operation/delete_workspace_api_v1_workspaces__workspace_id__delete). You must be a workspace `Admin` in order to delete a workspace.

### Delete a workspace via the UI

1. Navigate to **Settings**.
2. Select the workspace you want to delete.
3. Click **Delete** in the top-right corner of the screen.

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/delete-workspace.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=33038784e813f06dae3c87e5d34a3dc1" alt="Delete a workspace" data-og-width="1106" width="1106" data-og-height="250" height="250" data-path="langsmith/images/delete-workspace.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/delete-workspace.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=829f2ad5874457f3023bf4441e408203 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/delete-workspace.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=c907fade390eff674deb3fafc038e885 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/delete-workspace.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=1b5c0c1dec248c82ef12cd61d4da9fed 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/delete-workspace.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=c9005738241ade3dc2516c6e9b395d39 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/delete-workspace.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=19a256c90e69d4db054d4a360a84cd40 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/delete-workspace.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=5cfe1a5279c11bdd0e33812b58418e92 2500w" />

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/set-up-a-workspace.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Set up custom authentication

**URL:** llms-txt#set-up-custom-authentication

**Contents:**
- 1. Create your app
- 2. Add authentication

Source: https://docs.langchain.com/langsmith/set-up-custom-auth

In this tutorial, we will build a chatbot that only lets specific users access it. We'll start with the LangGraph template and add token-based security step by step. By the end, you'll have a working chatbot that checks for valid tokens before allowing access.

This is part 1 of our authentication series:

1. Set up custom authentication (you are here) - Control who can access your bot
2. [Make conversations private](/langsmith/resource-auth) - Let users have private conversations
3. [Connect an authentication provider](/langsmith/add-auth-server) - Add real user accounts and validate using OAuth2 for production

This guide assumes basic familiarity with the following concepts:

* [**Authentication & Access Control**](/langsmith/auth)
* [**LangSmith**](/langsmith/home)

<Note>
  Custom auth is only available for LangSmith SaaS deployments or Enterprise Self-Hosted deployments.
</Note>

## 1. Create your app

Create a new chatbot using the LangGraph starter template:

The template gives us a placeholder LangGraph app. Try it out by installing the local dependencies and running the development server:

The server will start and open [Studio](/langsmith/studio) in your browser:

If you were to self-host this on the public internet, anyone could access it.

<img src="https://mintcdn.com/langchain-5e9cc07a/N1xJUsnxxRqnrjxV/langsmith/images/no-auth.png?fit=max&auto=format&n=N1xJUsnxxRqnrjxV&q=85&s=3ca2c9a8d65891ef71abfb7ad0aae7d3" alt="No authentication: the dev server is publicly reachable, anyone can access the bot if exposed to the internet." data-og-width="1974" width="1974" data-og-height="1412" height="1412" data-path="langsmith/images/no-auth.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/N1xJUsnxxRqnrjxV/langsmith/images/no-auth.png?w=280&fit=max&auto=format&n=N1xJUsnxxRqnrjxV&q=85&s=67bfe450ee04d2432e5e1b86cfa0af1c 280w, https://mintcdn.com/langchain-5e9cc07a/N1xJUsnxxRqnrjxV/langsmith/images/no-auth.png?w=560&fit=max&auto=format&n=N1xJUsnxxRqnrjxV&q=85&s=087f8083981ae85eeca794bdca3e0e05 560w, https://mintcdn.com/langchain-5e9cc07a/N1xJUsnxxRqnrjxV/langsmith/images/no-auth.png?w=840&fit=max&auto=format&n=N1xJUsnxxRqnrjxV&q=85&s=7967264c0243918be1a8561d4db89586 840w, https://mintcdn.com/langchain-5e9cc07a/N1xJUsnxxRqnrjxV/langsmith/images/no-auth.png?w=1100&fit=max&auto=format&n=N1xJUsnxxRqnrjxV&q=85&s=fd9d62383850b137834bfc7e35bc4533 1100w, https://mintcdn.com/langchain-5e9cc07a/N1xJUsnxxRqnrjxV/langsmith/images/no-auth.png?w=1650&fit=max&auto=format&n=N1xJUsnxxRqnrjxV&q=85&s=66100323459f51596c53b3d941e3f85d 1650w, https://mintcdn.com/langchain-5e9cc07a/N1xJUsnxxRqnrjxV/langsmith/images/no-auth.png?w=2500&fit=max&auto=format&n=N1xJUsnxxRqnrjxV&q=85&s=a7b9157f67b3094370cc5399d917a164 2500w" />

## 2. Add authentication

Now that you have a base LangGraph app, add authentication to it.

<Note>
  In this tutorial, you will start with a hard-coded token for example purposes. You will get to a "production-ready" authentication scheme in the third tutorial.
</Note>

The [`Auth`](https://langchain-ai.github.io/langgraph/cloud/reference/sdk/python-sdk_ref#langgraph_sdk.auth.Auth) object lets you register an authentication function that the LangSmith deployment will run on every request. This function receives each request and decides whether to accept or reject.

Create a new file `src/security/auth.py`. This is where your code will live to check if users are allowed to access your bot:

```python {highlight={10,15-16}} title="src/security/auth.py" theme={null}
from langgraph_sdk import Auth

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

The template gives us a placeholder LangGraph app. Try it out by installing the local dependencies and running the development server:

<CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown

```

---

## Set up feedback criteria

**URL:** llms-txt#set-up-feedback-criteria

**Contents:**
- Continuous feedback
- Categorical feedback

Source: https://docs.langchain.com/langsmith/set-up-feedback-criteria

<Tip>
  **Recommended Reading**

Before diving into this content, it might be helpful to read the following:

* [Conceptual guide on tracing and feedback](/langsmith/observability-concepts)
  * [Reference guide on feedback data format](/langsmith/feedback-data-format)
</Tip>

Feedback criteria are represented in the application as feedback tags. For human feedback, you can set up new feedback criteria as continuous feedback or categorical feedback.

To set up a new feedback criteria, follow [this link](https://smith.langchain.com/settings/workspaces/feedbacks) to view all existing tags for your workspace, then click **New Tag**.

## Continuous feedback

For continuous feedback, you can enter a feedback tag name, then select a minimum and maximum value. Every value, including floating-point numbers, within this range will be accepted as feedback scores.

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/cont-feedback.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=44798176648f0a65e873fddecc90d43d" alt="" data-og-width="350" width="350" data-og-height="529" height="529" data-path="langsmith/images/cont-feedback.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/cont-feedback.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=4181c432230e33e7b6a7839e64729efa 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/cont-feedback.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=d11740b1d6f782cb551b6c8e7af92b50 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/cont-feedback.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=c966a6835ae5e2aaf3a320cf9bb71c74 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/cont-feedback.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=d6ea2414a3f698cb66cd5f336f4bac51 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/cont-feedback.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=442f373f36d5e8aa7dcf0e9685eb29f5 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/cont-feedback.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=8e8d44fd982322dd6865a25af561bc24 2500w" />

## Categorical feedback

For categorical feedback, you can enter a feedback tag name, then add a list of categories, each category mapping to a score. When you provide feedback, you can select one of these categories as the feedback score.
Both the category label and the score will be logged as feedback in `value` and `score` fields, respectively.

<img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/cat-feedback.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=6ec5030c3ba55b1fb12d60bca91719f7" alt="" data-og-width="470" width="470" data-og-height="465" height="465" data-path="langsmith/images/cat-feedback.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/cat-feedback.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=a11c14d2e7361e9aebc7d5997944f4c8 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/cat-feedback.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=d5e1dcf94730da4f7664092c4582410a 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/cat-feedback.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=0c18cd85595c56d9cf795fc07902db38 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/cat-feedback.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=ba75e7685d5cbd1918789301333038e2 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/cat-feedback.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=311246a21107c2f66b130b720ec67121 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/cat-feedback.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=bc625d7b4686fb291f4bf795f8bd0f6e 2500w" />

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/set-up-feedback-criteria.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Set up hybrid LangSmith

**URL:** llms-txt#set-up-hybrid-langsmith

**Contents:**
- Kubernetes
  - Prerequisites
  - Setup
- Next steps

Source: https://docs.langchain.com/langsmith/deploy-hybrid

<Info>
  **Important**
  The Hybrid deployment option requires an [Enterprise](https://langchain.com/pricing) plan.
</Info>

The [**hybrid**](/langsmith/hybrid) model lets you run the [data plane](/langsmith/data-plane)—your Agent Server deployments and agent workloads—in your own cloud, while LangChain hosts and manages the [control plane](/langsmith/control-plane) (the LangSmith UI and orchestration). This setup gives you the flexibility of self-hosting your runtime environments with the convenience of a managed LangSmith instance.

The following steps describe how to connect your self-hosted data plane to the managed LangSmith control plane.

1. `KEDA` is installed on your cluster.
   
2. A valid `Ingress` controller is installed on your cluster. For more information about configuring ingress for your deployment, refer to [Create an ingress for installations](/langsmith/self-host-ingress).
3. You have slack space in your cluster for multiple deployments. `Cluster-Autoscaler` is recommended to automatically provision new nodes.
4. You will need to enable egress to two control plane URLs. The listener polls these endpoints for deployments:
   * [https://api.host.langchain.com](https://api.host.langchain.com)
   * [https://api.smith.langchain.com](https://api.smith.langchain.com)

1. Provide your LangSmith organization ID to us. Your LangSmith organization will be configured to deploy the data plane in your cloud.
2. Create a listener from the LangSmith UI. The `Listener` data model is configured for the actual ["listener" application](/langsmith/data-plane#”listener”-application).
   1. In the left-hand navigation, select `Deployments` > `Listeners`.
   2. In the top-right of the page, select `+ Create Listener`.
   3. Enter a unique `Compute ID` for the listener. The `Compute ID` is a user-defined identifier that should be unique across all listeners in the current LangSmith workspace. The `Compute ID` is displayed to end users when they are creating a new deployment. Ensure that the `Compute ID` provides context to the end user about where their Agent Server deployments will be deployed to. For example, a `Compute ID` can be set to `k8s-cluster-name-dev-01`. In this example, the name of the Kubernetes cluster is `k8s-cluster-name`, `dev` denotes that the cluster is reserved for "development" workloads, and `01` is a numerical suffix to reduce naming collisions.
   4. Enter one or more Kubernetes namespaces. Later, the "listener" application will be configured to deploy to each of these namespaces.
   5. In the top-right on the page, select `Submit`.
   6. After the listener is created, copy the listener ID. You will use it later when installing the actual "listener" application in the Kubernetes cluster (step 5).
   <Info>
     **Important**
     Creating a listener from the LangSmith UI does not install the "listener" application in the Kubernetes cluster.
   </Info>
3. A [Helm chart](https://github.com/langchain-ai/helm/tree/main/charts/langgraph-dataplane) is provided to install the necesssary components in your Kubernetes cluster.
   * `langgraph-listener`: This is a service that listens to LangChain's [control plane](/langsmith/control-plane) for changes to your deployments and creates/updates downstream CRDs. This is the ["listener" application](/langsmith/data-plane#”listener”-application).
   * `LangGraphPlatform CRD`: A CRD for LangSmith Deployment. This contains the spec for managing an instance of a LangSmith Deployment.
   * `langgraph-platform-operator`: This operator handles changes to your LangSmith CRDs.
4. Configure your `langgraph-dataplane-values.yaml` file.
   
   * `config.langsmithApiKey`: The `langgraph-listener` deployment authenticates with LangChain's LangGraph control plane API with the `langsmithApiKey`.
   * `config.langsmithWorkspaceId`: The `langgraph-listener` deployment is coupled to Agent Server deployments in the LangSmith workspace. In other words, the `langgraph-listener` deployment can only manage Agent Server deployments in the specified LangSmith workspace ID.
   * `config.langgraphListenerId`: In addition to being coupled with a LangSmith workspace, the `langgraph-listener` deployment is also coupled to a listener. When a new Agent Server deployment is created, it is automatically coupled to a `langgraphListenerId`. Specifying `langgraphListenerId` ensures that the `langgraph-listener` deployment can only manage Agent Server deployments that are coupled to `langgraphListenerId`.
   * `config.watchNamespaces`: A comma-separated list of Kubernetes namespaces that the `langgraph-listener` deployment will deploy to. This list should match the list of namespaces specified in step 2d.
   * `config.enableLGPDeploymentHealthCheck`: To disable the Agent Server health check, set this to `false`.
   * `ingress.hostname`: As part of the deployment workflow, the `langgraph-listener` deployment attempts to call the Agent Server health check endpoint (`GET /ok`) to verify that the application has started up correctly. A typical setup involves creating a shared DNS record or domain for Agent Server deployments. This is not managed by LangSmith. Once created, set `ingress.hostname` to the domain, which will be used to complete the health check.
   * `operator.enabled`: There can only be 1 instance of the `langgraph-platform-operator` deployed in a Kubernetes namespace. Set this to `false` if there is already an instance of `langgraph-platform-operator` deployed in the current Kubernetes namespace.
   * `operator.createCRDs`: Set this value to `false` if the Kubernetes cluster already has the `LangGraphPlatform CRD` installed. During installation, an error will occur if the CRD is already installed. This situation may occur if multiple listeners are deployed on the same Kubernetes cluster.
5. Deploy `langgraph-dataplane` Helm chart.
   
6. If successful, you will see three services start up in your namespace.

Your hybrid infrastructure is now ready to create deployments.

Once your infrastructure is set up, you're ready to deploy applications. See the deployment guides in the [Deployment tab](/langsmith/deployments) for instructions on building and deploying your applications.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/deploy-hybrid.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
2. A valid `Ingress` controller is installed on your cluster. For more information about configuring ingress for your deployment, refer to [Create an ingress for installations](/langsmith/self-host-ingress).
3. You have slack space in your cluster for multiple deployments. `Cluster-Autoscaler` is recommended to automatically provision new nodes.
4. You will need to enable egress to two control plane URLs. The listener polls these endpoints for deployments:
   * [https://api.host.langchain.com](https://api.host.langchain.com)
   * [https://api.smith.langchain.com](https://api.smith.langchain.com)

### Setup

1. Provide your LangSmith organization ID to us. Your LangSmith organization will be configured to deploy the data plane in your cloud.
2. Create a listener from the LangSmith UI. The `Listener` data model is configured for the actual ["listener" application](/langsmith/data-plane#”listener”-application).
   1. In the left-hand navigation, select `Deployments` > `Listeners`.
   2. In the top-right of the page, select `+ Create Listener`.
   3. Enter a unique `Compute ID` for the listener. The `Compute ID` is a user-defined identifier that should be unique across all listeners in the current LangSmith workspace. The `Compute ID` is displayed to end users when they are creating a new deployment. Ensure that the `Compute ID` provides context to the end user about where their Agent Server deployments will be deployed to. For example, a `Compute ID` can be set to `k8s-cluster-name-dev-01`. In this example, the name of the Kubernetes cluster is `k8s-cluster-name`, `dev` denotes that the cluster is reserved for "development" workloads, and `01` is a numerical suffix to reduce naming collisions.
   4. Enter one or more Kubernetes namespaces. Later, the "listener" application will be configured to deploy to each of these namespaces.
   5. In the top-right on the page, select `Submit`.
   6. After the listener is created, copy the listener ID. You will use it later when installing the actual "listener" application in the Kubernetes cluster (step 5).
   <Info>
     **Important**
     Creating a listener from the LangSmith UI does not install the "listener" application in the Kubernetes cluster.
   </Info>
3. A [Helm chart](https://github.com/langchain-ai/helm/tree/main/charts/langgraph-dataplane) is provided to install the necesssary components in your Kubernetes cluster.
   * `langgraph-listener`: This is a service that listens to LangChain's [control plane](/langsmith/control-plane) for changes to your deployments and creates/updates downstream CRDs. This is the ["listener" application](/langsmith/data-plane#”listener”-application).
   * `LangGraphPlatform CRD`: A CRD for LangSmith Deployment. This contains the spec for managing an instance of a LangSmith Deployment.
   * `langgraph-platform-operator`: This operator handles changes to your LangSmith CRDs.
4. Configure your `langgraph-dataplane-values.yaml` file.
```

Example 2 (unknown):
```unknown
* `config.langsmithApiKey`: The `langgraph-listener` deployment authenticates with LangChain's LangGraph control plane API with the `langsmithApiKey`.
   * `config.langsmithWorkspaceId`: The `langgraph-listener` deployment is coupled to Agent Server deployments in the LangSmith workspace. In other words, the `langgraph-listener` deployment can only manage Agent Server deployments in the specified LangSmith workspace ID.
   * `config.langgraphListenerId`: In addition to being coupled with a LangSmith workspace, the `langgraph-listener` deployment is also coupled to a listener. When a new Agent Server deployment is created, it is automatically coupled to a `langgraphListenerId`. Specifying `langgraphListenerId` ensures that the `langgraph-listener` deployment can only manage Agent Server deployments that are coupled to `langgraphListenerId`.
   * `config.watchNamespaces`: A comma-separated list of Kubernetes namespaces that the `langgraph-listener` deployment will deploy to. This list should match the list of namespaces specified in step 2d.
   * `config.enableLGPDeploymentHealthCheck`: To disable the Agent Server health check, set this to `false`.
   * `ingress.hostname`: As part of the deployment workflow, the `langgraph-listener` deployment attempts to call the Agent Server health check endpoint (`GET /ok`) to verify that the application has started up correctly. A typical setup involves creating a shared DNS record or domain for Agent Server deployments. This is not managed by LangSmith. Once created, set `ingress.hostname` to the domain, which will be used to complete the health check.
   * `operator.enabled`: There can only be 1 instance of the `langgraph-platform-operator` deployed in a Kubernetes namespace. Set this to `false` if there is already an instance of `langgraph-platform-operator` deployed in the current Kubernetes namespace.
   * `operator.createCRDs`: Set this value to `false` if the Kubernetes cluster already has the `LangGraphPlatform CRD` installed. During installation, an error will occur if the CRD is already installed. This situation may occur if multiple listeners are deployed on the same Kubernetes cluster.
5. Deploy `langgraph-dataplane` Helm chart.
```

Example 3 (unknown):
```unknown
6. If successful, you will see three services start up in your namespace.
```

---

## Set up LangSmith

**URL:** llms-txt#set-up-langsmith

**Contents:**
- Choose how to set up LangSmith
  - Comparison
  - Related

Source: https://docs.langchain.com/langsmith/platform-setup

This section covers how to host and manage LangSmith infrastructure. You can set up LangSmith for [observability](/langsmith/observability), [evaluation](/langsmith/evaluation), and [prompt engineering](/langsmith/prompt-engineering), or use the full platform experience with [LangSmith Deployment](/langsmith/deployments) to also deploy and manage your applications through the UI.

<Callout icon="building" color="#2563EB" iconType="regular">
  **Start here if you're setting up or maintaining LangSmith infrastructure.**

If you want to deploy an agent application, the [Deployment section](/langsmith/deployments) covers application structure and deployment configuration.
</Callout>

## Choose how to set up LangSmith

You can deploy LangSmith in one of three modes:

* [**Cloud**](/langsmith/cloud): fully managed by LangChain
* [**Hybrid**](/langsmith/hybrid): LangChain manages the <Tooltip tip="The LangSmith UI and APIs for managing deployments.">control plane</Tooltip>; you host the <Tooltip tip="The runtime environment where your Agent Servers and agents execute.">data plane</Tooltip>
* [**Self-hosted**](/langsmith/self-hosted): you manage the full stack within your infrastructure

<Columns cols={3}>
  <Card title="Cloud" icon="cloud" iconType="solid" href="/langsmith/cloud" cta="Get started">
    Fully managed observability, evaluation, prompt engineering, and application deployment. Deploy from GitHub with automated CI/CD.
  </Card>

<Card title="Hybrid" icon="cloud" href="/langsmith/hybrid" cta="Set up Hybrid">
    **(Enterprise)** Observability, evaluation, prompt engineering, and application deployment with your applications running in your infrastructure.
  </Card>

<Card title="Self-hosted" icon="server" iconType="solid" href="/langsmith/self-hosted" cta="Run self-hosted">
    **(Enterprise)** Full control with observability, evaluation, and prompt engineering. Enable the full platform experience with LangSmith Deployment or run standalone servers.
  </Card>
</Columns>

Refer to the following table for a comparison:

| Feature                                        | **Cloud**                           | **Hybrid**                                                        | **Self-Hosted**                           |
| ---------------------------------------------- | ----------------------------------- | ----------------------------------------------------------------- | ----------------------------------------- |
| **Infrastructure location**                    | LangChain's cloud                   | Split: Control plane in LangChain cloud, data plane in your cloud | Your cloud                                |
| **Who manages updates**                        | LangChain                           | LangChain (control plane), You (data plane)                       | You                                       |
| **Who manages CI/CD for your apps**            | LangChain                           | You                                                               | You                                       |
| **Can deploy applications?**                   | ✅ Yes                               | ✅ Yes                                                             | ✅ Yes (with LangSmith Deployment enabled) |
| **Observability data location**                | LangChain cloud                     | LangChain cloud                                                   | Your cloud                                |
| **[Pricing](https://www.langchain.com/plans)** | Plus tier                           | Enterprise                                                        | Enterprise                                |
| **Best for**                                   | Quick setup, managed infrastructure | Data residency requirements + managed control plane               | Full control, data isolation              |

<Tip>
  You can [run an Agent Server locally for free](/langsmith/local-server) for testing and development.
</Tip>

* [Plans](https://langchain.com/pricing)
* [Pricing](https://www.langchain.com/plans)
* [Observability](/langsmith/observability)

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/platform-setup.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Set up online evaluators

**URL:** llms-txt#set-up-online-evaluators

**Contents:**
- View online evaluators
- Configure online evaluators
  - Configure a LLM-as-a-judge online evaluator
  - Configure a custom code evaluator
  - Video guide
- Configure multi-turn online evaluators
  - Prerequisites
  - Configuration
  - Limits
  - Troubleshooting

Source: https://docs.langchain.com/langsmith/online-evaluations

<Tip>
  **Recommended Reading**

Before diving into this content, it might be helpful to read the following:

* Running [online evaluations](/langsmith/evaluation-concepts#online-evaluation)
</Tip>

Online evaluations provide real-time feedback on your production traces. This is useful to continuously monitor the performance of your application—to identify issues, measure improvements, and ensure consistent quality over time.

There are two types of online evaluations supported in LangSmith:

* **[LLM-as-a-judge](/langsmith/evaluation-concepts#llm-as-judge)**: Use an LLM to evaluate traces as a scalable substitute for human-like judgment (e.g., toxicity, hallucinations, correctness). Supports two different levels of granularity:
  * **Run level**: Evaluate a single run.
  * [**Thread level**](/langsmith/online-evaluations#configure-multi-turn-online-evaluators): Evaluate all traces in a thread.
* **Custom Code**: Write an evaluator in Python directly in LangSmith. Often used for validating structure or statistical properties of your data.

<Note>When an online evaluator runs on any run within a trace, the trace will be auto-upgraded to [extended data retention](/langsmith/administration-overview#data-retention-auto-upgrades). This upgrade will impact trace pricing, but ensures that traces meeting your evaluation criteria (typically those most valuable for analysis) are preserved for investigation. </Note>

## View online evaluators

Head to the **Tracing Projects** tab and select a tracing project. To view existing online evaluators for that project, click on the **Evaluators** tab.

<img src="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-evaluators.png?fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=471b55b0d23b6c54ea5044406f0c55f7" alt="View online evaluators" data-og-width="1350" width="1350" data-og-height="639" height="639" data-path="langsmith/images/view-evaluators.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-evaluators.png?w=280&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=141082993aba37d45550bfff9da502df 280w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-evaluators.png?w=560&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=211cc6c5359e00ab23f0cf55bd67fd93 560w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-evaluators.png?w=840&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=fcdae1f3bce28bfcdd91059e43f9e1be 840w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-evaluators.png?w=1100&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=93b239efbd10f6ab5013e91b08384df6 1100w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-evaluators.png?w=1650&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=b6e496bee86cfb221cccde72366f83bb 1650w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-evaluators.png?w=2500&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=79817ff090124e1ec7b5f25eb2ddd978 2500w" />

## Configure online evaluators

#### 1. Navigate to online evaluators

Head to the **Tracing Projects** tab and select a tracing project. Click on **+ New** in the top right corner of the tracing project page, then click on **New Evaluator**. Select the evaluator you want to configure.

#### 2. Name your evaluator

#### 3. Create a filter

For example, you may want to apply specific evaluators based on:

* Runs where a [user left feedback](/langsmith/attach-user-feedback) indicating the response was unsatisfactory.
* Runs that invoke a specific tool call. See [filtering for tool calls](/langsmith/filter-traces-in-application#example-filtering-for-tool-calls) for more information.
* Runs that match a particular piece of metadata (e.g. if you log traces with a `plan_type` and only want to run evaluations on traces from your enterprise customers). See [adding metadata to your traces](/langsmith/add-metadata-tags) for more information.

Filters on evaluators work the same way as when you're filtering traces in a project. For more information on filters, you can refer to [this guide](./filter-traces-in-application).

<Tip>
  It's often helpful to inspect runs as you're creating a filter for your evaluator. With the evaluator configuration panel open, you can inspect runs and apply filters to them. Any filters you apply to the runs table will automatically be reflected in filters on your evaluator.
</Tip>

#### 4. (Optional) Configure a sampling rate

Configure a sampling rate to control the percentage of filtered runs that trigger the automation action. For example, to control costs, you may want to set a filter to only apply the evaluator to 10% of traces. In order to do this, you would set the sampling rate to 0.1.

#### 5. (Optional) Apply rule to past runs

Apply rule to past runs by toggling the **Apply to past runs** and entering a "Backfill from" date. This is only possible upon rule creation. Note: the backfill is processed as a background job, so you will not see the results immediately.

In order to track progress of the backfill, you can view logs for your evaluator by heading to the **Evaluators** tab within a tracing project and clicking the Logs button for the evaluator you created. Online evaluator logs are similar to [automation rule logs](./rules#view-logs-for-your-automations).

* Add an evaluator name
* Optionally filter runs that you would like to apply your evaluator on or configure a sampling rate.
* Select **Apply Evaluator**

#### 6. Select evaluator type

* Configuring [LLM-as-a-judge evaluators](/langsmith/online-evaluations#configure-a-llm-as-a-judge-online-evaluator)
* Configuring [custom code evaluators](/langsmith/online-evaluations#configure-a-custom-code-evaluator)

### Configure a LLM-as-a-judge online evaluator

View this guide to configure an [LLM-as-a-judge evaluator](/langsmith/llm-as-judge?mode=ui#pre-built-evaluators-1).

### Configure a custom code evaluator

Select **custom code** evaluator.

#### Write your evaluation function

<Note>
  **Custom code evaluators restrictions.**

**Allowed Libraries**: You can import all standard library functions, as well as the following public packages:

**Network Access**: You cannot access the internet from a custom code evaluator.
</Note>

Custom code evaluators must be written inline. We recommend testing locally before setting up your custom code evaluator in LangSmith.

In the UI, you will see a panel that lets you write your code inline, with some starter code:

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/online-eval-custom-code.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=cf7b75691edb3afaa10652a79813e581" alt="" data-og-width="2910" width="2910" data-og-height="902" height="902" data-path="langsmith/images/online-eval-custom-code.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/online-eval-custom-code.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=acf6e6f3be5751c93a7287971fb18907 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/online-eval-custom-code.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=5df847b9d1f8171120853834d5d12f38 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/online-eval-custom-code.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=17d028d3e0709087a8be7e31343f0ab0 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/online-eval-custom-code.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=668fd653028ebe056fed1e3963d3dc8e 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/online-eval-custom-code.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=62465285a785c577d0b0537c5ad307ca 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/online-eval-custom-code.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=7c342ba749987066e8ae05129e6a4fb7 2500w" />

Custom code evaluators take in one argument:

* A `Run` ([reference](/langsmith/run-data-format)). This represents the sampled run to evaluate.

They return a single value:

* Feedback(s) Dictionary: A dictionary whose keys are the type of feedback you want to return, and values are the score you will give for that feedback key. For example, `{"correctness": 1, "silliness": 0}` would create two types of feedback on the run, one saying it is correct, and the other saying it is not silly.

In the below screenshot, you can see an example of a simple function that validates that each run in the experiment has a known json field:

#### Test and save your evaluation function

Before saving, you can test your evaluator function on a recent run by clicking **Test Code** to make sure that your code executes properly.

Once you **Save**, your online evaluator will run over newly sampled runs (or backfilled ones too if you chose the backfill option).

If you prefer a video tutorial, check out the [Online Evaluations video](https://academy.langchain.com/pages/intro-to-langsmith-preview) from the Introduction to LangSmith Course.

<iframe className="w-full aspect-video rounded-xl" src="https://www.youtube.com/embed/z69cBXTJFZ0?si=GBKQ9_muHR1zllLl" title="YouTube video player" frameBorder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowFullScreen />

## Configure multi-turn online evaluators

Multi-turn online evaluators allow you to evaluate entire conversations between a human and an agent — not just individual exchanges. They measure end-to-end interaction quality across all turns in a thread.

You can use multi-turn evaluations to measure:

1. Semantic Intent: What the user was trying to do.
2. Semantic Outcome: What actually happened, did the task succeed.
3. Trajectory: How the conversation unfolded, including trajectory of tool calls.

<Note> Running multi-turn online evals will auto-upgrade each trace within a thread to [extended data retention](/langsmith/administration-overview#data-retention-auto-upgrades). This upgrade will impact trace pricing, but ensures that traces meeting your evaluation criteria (typically those most valuable for analysis) are preserved for investigation. </Note>

* Your tracing project must be using [threads](/langsmith/threads).
* The top-level inputs and outputs of each trace in a thread must have a `messages` key that contains a list of messages. We support messages in [LangChain](/langsmith/log-llm-trace#messages-format), [OpenAI Chat Completions](https://platform.openai.com/docs/api-reference/chat/create), and [Anthropic Messages](https://docs.claude.com/en/api/messages) formats.
  * If the top-level inputs and outputs of each trace only contain the latest message in the conversation, LangSmith will automatically combine messages across turns into a thread.
  * If the top-level inputs and outputs of each trace contain the full conversation history, LangSmith will use that directly.

<Note>
  If your traces don't follow the format above, thread level evaluators won't work. You’ll need to update how you trace to LangSmith to ensure each trace’s top-level inputs and outputs contain a list of `messages`.

Please refer to the [troubleshooting](#troubleshooting) section for more information.
</Note>

1. Navigate to the **Tracing Projects** tab and select a tracing project.
2. Click **+ New** in the top right corner of the tracing project page >  **New Evaluator** > **Evaluate a multi-turn thread**.
3. **Name your evaluator**.
4. **Apply filters or a sampling rate**. <br />
   Use filters or sampling to control evaluator cost. For example, evaluate only threads under *N* turns or sample 10% of all threads.
5. **Configure an idle time**. <br />
   The first time you configure a thread level evaluator, you’ll define the idle time — the amount of time after the last trace in a thread before it’s considered complete and ready for evaluation. This value should reflect the expected length of user interactions in your app. It applies across all evaluators in the project.

<Tip>
  When first testing your evaluator, use a short idle time so you can see results quickly. Once validated, increase it to match the expected length of user interactions.
</Tip>

6. **Configure your model.**<br />
   Select the provider and model you want to use for your evaluator. Threads tend to get long, so you should use a model with a higher context window in order to avoid running into limits. For example, OpenAI's GPT-4.1 mini or Gemini 2.5 Flash are good options as they both have 1M+ token context windows.

7. **Configure your LLM-as-a-judge prompt.**<br />
   Define what you want to evaluate. This prompt will be used to evaluate the thread. You can also configure which parts of the `messages` list are passed to the evaluator to control the content it receives:
   * All messages: Send the full message list.
   * Human and AI pairs: Send only user and assistant messages (excluding system messages, tool calls, etc.).
   * First human and last AI: Send only the first user message and the last assistant reply.

8. **Set up your feedback configuration**.<br />
   Configure a name for the feedback key, the format for the feedback you want to collect and optionally enable reasoning on the feedback.

<Warning>
  We don't recommend using the same feedback key for a thread-level evaluator and a run-level evaluator as it can be hard to distinguish between the two.
</Warning>

8. **Save your evaluator.**

After saving, your evaluator will appear in the **Evaluators** tab. You can test it once the idle time has passed for any new threads created after saving.

These are the current limits for multi-turn online evaluators (subject to change). Please reach out if you are running into any of these limits.

* **Runs must be less than one week old**: When a thread becomes idle, only runs within the past 7 days are eligible for evaluation.
* **Maximum of 500 threads evaluated at once**: If you have more than 500 threads marked as idle in a five minute period, we will automatically sample beyond 500.
* **Maximum of 10 multi-turn online evaluators per workspace**

**Checking the status of your evaluator** <br />
You can check when your evaluator was last run by heading to the **Evaluators** tab within a tracing project and clicking the **Logs** button for the evaluator you created to view its run history.

**Inspect the data sent to the evaluator** <br />
Inspect the data sent to the evaluator by heading to the **Evaluators** tab within a tracing project, clicking on the evaluator you created and clicking the **Evaluator traces** tab.

In this tab, you can see the inputs passed into the LLM-as-a-judge evaluator. If your messages are not being passed in correctly, you will see blank values in the inputs. This can happen if your messages are not formatted in one of [the expected formats](/langsmith/online-evaluations#prerequisites).

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/online-evaluations.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
numpy (v2.2.2): "numpy"
  pandas (v1.5.2): "pandas"
  jsonschema (v4.21.1): "jsonschema"
  scipy (v1.14.1): "scipy"
  sklearn (v1.26.4): "scikit-learn"
```

Example 2 (unknown):
```unknown

```

---

## Set up OpenTelemetry trace provider

**URL:** llms-txt#set-up-opentelemetry-trace-provider

provider = TracerProvider()
otlp_exporter = OTLPSpanExporter(
    endpoint="https://api.smith.langchain.com/otel/v1/traces",
    headers={"x-api-key": os.getenv("LANGSMITH_API_KEY"), "Langsmith-Project": "my_project"}
)
processor = BatchSpanProcessor(otlp_exporter)
provider.add_span_processor(processor)
trace.set_tracer_provider(provider)
tracer = trace.get_tracer(__name__)

---

## Set up resource tags

**URL:** llms-txt#set-up-resource-tags

**Contents:**
- Create a tag
- Assign a tag to a resource
- Delete a tag
- Filter resources by tags

Source: https://docs.langchain.com/langsmith/set-up-resource-tags

<Check>
  Before diving into this content, it might be helpful to read the following:

* [Conceptual guide on organizations and workspaces](/langsmith/administration-overview)
</Check>

<Info>
  Resource tags are available for Plus and Enterprise plans.
</Info>

While workspaces help separate trust boundaries and access control, tags help you organize resources within a workspace. Tags are key-value pairs that you can attach to resources.

<Note>
  **Not to be confused with commit tags**: Resource tags are key-value pairs used to organize and filter workspace resources (projects, datasets, prompts, etc.). [Commit tags](/langsmith/manage-prompts#commit-tags) are labels that reference specific versions in a prompt's commit history. While both types of tags can use similar terminology (like `prod` or `staging`), resource tags help you *organize resources* across your workspace, while commit tags control *which version* of a prompt is used in your code.
</Note>

To create a tag, head to the workspace settings and click on the "Resource Tags" tab. Here, you'll be able to see the existing tag values, grouped by key. Two keys `Application` and `Environment` are created by default.

To create a new tag, click on the "New Tag" button. You'll be prompted to enter a key and a value for the tag. Note that you can use an existing key or create a new one.

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-tag.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=7482c51de04fbfa54731159c9f44c4e7" alt="" data-og-width="1460" width="1460" data-og-height="1268" height="1268" data-path="langsmith/images/create-tag.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-tag.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=5a5a406729553197c361b802663af22f 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-tag.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=5618e46f4895c858a234a62b2222b5d7 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-tag.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=0dc3a636385e4c0502d23c50013696d8 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-tag.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=a138f20d0755211084e7352a6aa98dbc 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-tag.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=0255e6c9c526c32ea01e46b80dde19c7 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-tag.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=e35a3c90d4bd9584e773e2bd1cb28294 2500w" />

## Assign a tag to a resource

Within the same side panel for creating a new tag, you can also create assign resources to tags. Search for corresponding resources in the "Assign Resources" section and select the resources you want to tag.

<Note>
  You can only tag workspace-scoped resources with resource tags. This includes Tracing Projects, Annotation Queues, Deployments, Experiments, Datasets, and Prompts.
</Note>

You can also assign tags to resources from the resource's detail page. Click on the Resource tags button to open up the tag panel and assign tags.

<img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/assign-tag.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=95e638f9d51af94017b4b9f077c319b4" alt="" data-og-width="1460" width="1460" data-og-height="607" height="607" data-path="langsmith/images/assign-tag.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/assign-tag.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=6941ebf3198b8b959e23110c9605061e 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/assign-tag.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=86c9dcba2b753beac4d229bf15c76aa2 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/assign-tag.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=5997def1f1c4294ffa4152f87edf2579 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/assign-tag.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=09645f720dc1d2ba3484194d9a0876ee 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/assign-tag.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=3bbee72af92ff7cbe1617ba6686603ce 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/assign-tag.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=d2833a4f8ef5aec89218ad22d619de7c 2500w" />

To un-assign a tag from a resource, click on the Trash icon next to the tag, both in the tag panel and the resource tag panel.

You can delete either a key or a value of a tag from the [workspace settings page](https://smith.langchain.com/settings/workspaces/resource_tags). To delete a key, click on the Trash icon next to the key. To delete a value, click on the Trash icon next to the value.

Note that if you delete a key, all values associated with that key will also be deleted. When you delete a value, you will lose all associations between that value and resources.

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/delete-tag.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=545b2cd5178bd4da31db56cde48f9f12" alt="" data-og-width="1175" width="1175" data-og-height="1030" height="1030" data-path="langsmith/images/delete-tag.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/delete-tag.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=059964ddd0ec5fffbe63bdf775dbd114 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/delete-tag.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=4b2fd607ef48d43555ec30f3689075d9 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/delete-tag.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=fe5f0a9b4c6695f9b1041e692bdf1799 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/delete-tag.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=6012f1f83a031d5d84f2921f28f3ae4c 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/delete-tag.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=485c5e41e66ef679ebed451e291ce3ca 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/delete-tag.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=c172ee7ff2edc13aaead3fa392de80ef 2500w" />

## Filter resources by tags

You can use resource tags to organize your experience navigating resources in the workspace.

To filter resources by tags in your workspace, open up the left-hand side panel and click on the tags icon. Here, you can select the tags you want to filter by.

In the homepage, you can see updated counts for resources based on the tags you've selected.

As you navigate through the different product surfaces, you will *only* see resources that match the tags you've selected. At any time, you can clear the tags to see all resources in the workspace or select different tags to filter by.

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-by-tags.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=b71f3118eb60146b576a02d2b7ddb8cd" alt="" data-og-width="1459" width="1459" data-og-height="1265" height="1265" data-path="langsmith/images/filter-by-tags.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-by-tags.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=30edd76e20e69234d15345fc94520d79 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-by-tags.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=154f682c20bf3a50cf3773c6f0e4550f 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-by-tags.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=21bb41e7cb0c4984ea98f07adcefcc09 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-by-tags.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=e544fd1d74706c9bc40f72a3a20edd2c 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-by-tags.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=bb90fb78c5e6b646f3a77e3175aef80f 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-by-tags.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=32da0bb71893aedb79ab0b00cc028f3e 2500w" />

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/set-up-resource-tags.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Set up SSO with OAuth2.0 and OIDC

**URL:** llms-txt#set-up-sso-with-oauth2.0-and-oidc

**Contents:**
- Overview
- With Client Secret (Recommended)
  - Prerequisites
  - Configuration
  - Session length controls
  - Override Sub Claim
  - Google Workspace IdP setup
  - Okta IdP setup
- Without Client Secret (PKCE) (Deprecated)
  - Requirements

Source: https://docs.langchain.com/langsmith/self-host-sso

LangSmith Self-Hosted provides SSO via OAuth2.0 and OIDC. This will delegate authentication to your Identity Provider (IdP) to manage access to LangSmith.

Our implementation supports almost anything that is OIDC compliant, with a few exceptions. Once configured, you will see a login screen like this:

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui-sso.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=bbe6447424ea3e97a486b67d21cd4f6b" alt="LangSmith UI with OAuth SSO" data-og-width="1596" width="1596" data-og-height="994" height="994" data-path="langsmith/images/langsmith-ui-sso.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui-sso.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=6046e421d56b069227060520ecf3c2a8 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui-sso.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=03fc4a6909af9f15dca14af7e3891cda 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui-sso.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=53693db675b2bdd6cefd7e9b8b605af6 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui-sso.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=4b2135a1a76bf2e34eb914b8b00f58dc 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui-sso.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=78141a4d748d77bc1cceede89e62087d 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui-sso.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=8532b1891c98046f171f1bded2523f3a 2500w" />

<Note>
  You may upgrade a [basic auth](/langsmith/self-host-basic-auth) installation to this mode, but not a [none auth](/langsmith/authentication-methods#none) installation. In order to upgrade, simply remove the basic auth configuration and add the required configuration parameters as shown below. Users may then login via OAuth *only*. **In order to maintain access post-upgrade, you must have access to login via OAuth using an email address that previously logged in via basic auth.**
</Note>

<Warning>
  LangSmith does not support moving from SSO to basic auth mode in self-hosted at the moment. We also do not support moving from OAuth Mode with client secret to OAuth mode without a client secret and vice versa. Finally, we do not support having both basic auth and OAuth at the same time. Ensure you disable the basic auth configuration when enabling OAuth.
</Warning>

## With Client Secret (Recommended)

By default, LangSmith Self-Hosted supports the `Authorization Code` flow with `Client Secret`. In this version of the flow, your client secret is stored security in LangSmith (not on the frontend) and used for authentication and establishing auth sessions.

* You must be self-hosted and on an Enterprise plan.
* Your IdP must support the `Authorization Code` flow with `Client Secret`.
* Your IdP must support using an external discovery/issuer URL. We will use this to fetch the necessary routes and keys for your IdP.
* You must provide the `OIDC`, `email`, and `profile` scopes to LangSmith. We use these to fetch the necessary user information and email for your users.

<Note>
  LangSmith SSO is only supported over `https`.
</Note>

* You will need to set the callback URL in your IdP to `https://<host>/api/v1/oauth/custom-oidc/callback`, where `host` is the domain or IP you have provisioned for your LangSmith instance. This is where your IdP will redirect the user after they have authenticated.
* You will need to provide the `oauthClientId`, `oauthClientSecret`, `hostname`, and `oauthIssuerUrl` in your `values.yaml` file. This is where you will configure your LangSmith instance.
* If you have **not** already configured Oauth with client secret or if you only have personal orgs, you must provide an email address to assign as the initial org admin for the newly provisioned SSO org. If you are upgrading from basic auth, your existing org will be reused instead.

### Session length controls

<Note>
  All of the environment variables in this section are for the `platform-backend` service and can be added using `platformBackend.deployment.extraEnv` in Helm.
</Note>

* By default, session length is controlled by the expiration of the identity token returned by the identity provider
* Most setups should use refresh tokens to enable session length extension beyond the identity token expiration up to `OAUTH_SESSION_MAX_SEC`, which may require including the `offline_access` scope by adding to `oauthScopes` (Helm) or `OAUTH_SCOPES` (Docker)
* `OAUTH_SESSION_MAX_SEC` (default 1 day) can be overridden to a maximum of one week (`604800`)
* For identity provider setups that don't support refresh tokens, setting `OAUTH_OVERRIDE_TOKEN_EXPIRY="true"` will take `OAUTH_SESSION_MAX_SEC` as the session length, ignoring the identity token expiration

### Override Sub Claim

In some scenarios, it may be necessary to override which claim is used as the `sub` claim from your identity provider.
For example, in SCIM, the resolved `sub` claim and SCIM `externalId` must match in order for login to succeed.
If there are restrictions on the source attribute of the `sub` claim and/or the SCIM `externalId`, set the `ISSUER_SUB_CLAIM_OVERRIDES` environment variable to select which OIDC JWT claim is used as the `sub`.

If an issuer URL **starts with** one of the URLs in this configuration, the `sub` claim is taken from the field name specified.
For example, with the following configuration, a token with the issuer `https://idp.yourdomain.com/application/uuid` would use the `customClaim` value as the `sub`:

If unset, the default value for this configuration uses the `oid` claim when Azure Entra ID is used as the identity provider:

### Google Workspace IdP setup

You can use Google Workspace as a single sign-on (SSO) provider using [OAuth2.0 and OIDC](https://developers.google.com/identity/openid-connect/openid-connect) without PKCE.

<Note>
  You must have administrator-level access to your organization's Google Cloud Platform (GCP) account to create a new project, or permissions to create and configure OAuth 2.0 credentials for an existing project. We recommend that you create a new project for managing access, since each GCP project has a single OAuth consent screen.
</Note>

1. Create a new GCP project, see the Google documentation topic [creating and managing projects](https://cloud.google.com/resource-manager/docs/creating-managing-projects)

2. After you have created the project, open the [Credentials](https://console.developers.google.com/apis/credentials) page in the Google API Console (making sure the project in the top left corner is correct)

3. Create new credentials: `Create Credentials → OAuth client ID`

4. Choose `Web application` as the `Application type` and enter a name for the application e.g. `LangSmith`

5. In `Authorized Javascript origins` put the domain of your LangSmith instance e.g. `https://langsmith.yourdomain.com`

6. In `Authorized redirect URIs` put the domain of your LangSmith instance followed by `/api/v1/oauth/custom-oidc/callback` e.g. `https://langsmith.yourdomain.com/api/v1/oauth/custom-oidc/callback`

7. Click `Create`, then download the JSON or copy and save the `Client ID` (ends with `.apps.googleusercontent.com`) and `Client secret` somewhere secure. **You will be able to access these later if needed**.

8. Select `OAuth consent screen` from the navigation menu on the left

1. Choose the Application type as `Internal`. **If you select `Public`, anyone with a Google account can sign in.**
   2. Enter a descriptive `Application name`. This name is shown to users on the consent screen when they sign in. For example, use `LangSmith` or `<organization_name> SSO for LangSmith`.
   3. Verify that the Scopes for Google APIs only lists email, profile, and openid scopes. Only these scopes are required for single sign-on. If you grant additional scopes it increases the risk of exposing sensitive data.

9. (Optional) control who within your organization has access to LangSmith: [https://admin.google.com/ac/owl/list?tab=configuredApps](https://admin.google.com/ac/owl/list?tab=configuredApps). See [Google's documentation](https://support.google.com/a/answer/7281227?hl=en\&fl=1\&sjid=9554153972856467090-NA) for additional details.

10. Configure LangSmith to use this OAuth application. For examples, here are the `config`values that would be used for Kubernetes configuration:

1. `oauthClientId`: `Client ID` (ends with `.apps.googleusercontent.com`)
    2. `oauthClientSecret`: `Client secret`
    3. `hostname`: the domain of your LangSmith instance e.g. `https://langsmith.yourdomain.com` (no trailing slash)
    4. `oauthIssuerUrl`: `https://accounts.google.com`
    5. `oauth.enabled`: `true`
    6. `authType`: `mixed`

#### Supported features

* IdP-initiated SSO
* SP-initiated SSO

#### Configuration steps

For additional information, see Okta's [documentation](https://help.okta.com/en-us/content/topics/apps/apps_app_integration_wizard_oidc.htm).
If you have any questions or issues, please reach out to [support@langchain.dev](mailto:support@langchain.dev).

<div id="via-okta-integration-network">
  <b>Via Okta Integration Network (recommended)</b>
</div>

<Note>
  This method of configuration is required in order to use SCIM with Okta.
</Note>

1. Sign in to [Okta](https://login.okta.com/).
2. In the upper-right corner, select Admin. The button is not visible from the Admin area.
3. Select `Browse App Integration Catalog`.
4. Find and select the LangSmith application.
5. On the application overview page, select Add Integration.
6. Fill in `ApiUrlBase`:
   * Your LangSmith API URL **without the protocol** (`https://`) formatted as `<langsmith_domain>/api/v1`, e.g., `langsmith.yourdomain.com/api/v1`.
   * If your installation is configured with a subdomain / path prefix, include that in the URL, e.g., `langsmith.yourdomain.com/prefix/api/v1`.
7. Leave `AuthHost` empty.
8. (Optional, if planning to use [SCIM](/langsmith/user-management#set-up-scim-for-your-organization) as well) Fill in `LangSmithUrl`: The `<langsmith_url>` portion from above, e.g., `langsmith.yourdomain.com`.
9. Under Application Visibility, keep the box unchecked.
10. Select Next.
11. Select `OpenID Connect`.
12. Fill in `Sign-On Options`:
    * `Application username format`: `Email`.
    * `Update application username on`: `Create and update`.
    * `Allow users to securely see their password`: leave **unchecked**.
13. Click **Save**.
14. Configure LangSmith to use this OAuth application (see [general configuration section](#configuration) for details about `initialOrgAdminEmail`):

<div id="via-okta-custom-app-integration">
  <b>Via Custom App Integration</b>
</div>

<Warning>
  SCIM is not compatible with this method of configuration. Refer to [**Via Okta Integration Network**](#via-okta-integration-network).
</Warning>

1. Log in to Okta as an administrator, and go to the **Okta Admin console**.
2. Under **Applications** > **Applications** click **Create App Integration**.
3. Select **OIDC - OpenID Connect** as the Sign-in method and **Web Application** as the Application type, then click **Next**.
4. Enter an `App integration name` (e.g., `LangSmith`).
5. Recommended: Check **Core grants > Refresh Token** (see [session length controls](#session-length-controls)).
6. In **Sign-in redirect URIs** put the domain of your LangSmith instance followed by `/api/v1/oauth/custom-oidc/callback`, e.g., `https://langsmith.yourdomain.com/api/v1/oauth/custom-oidc/callback`. If your installation is configured with a subdomain / path prefix, include that in the URL, e.g., `https://langsmith.yourdomain.com/prefix/api/v1/oauth/custom-oidc/callback`.
7. Remove the default URI under **Sign-out redirect URIs**.
8. Under **Trusted Origins > Base URIs** add your langsmith URL with the protocol, e.g., `https://langsmith.yourdomain.com`.
9. Select your desired option under **Assignments > Controlled access**:
   * Allow everyone in your organization to access.
   * Limit access to selected groups.
   * Skip group assignment for now.
10. Click **Save**.
11. Under **Sign On > OpenID Connect ID Token** set **Issuer** to **Okta URL**.
12. (Optional) Under **General > Login** set **Login initiated by** to `Either Okta or App` to enable IdP-initiated login.
13. (Recommended) Under **General > Login > Email verification experience** fill in the **Callback URI** with the LangSmith URL, e.g., `https://langsmith.yourdomain.com`.
14. Configure LangSmith to use this OAuth application (see [general configuration section](#configuration) for details about `initialOrgAdminEmail`):

#### SP-initiated SSO

Users can sign in using the **Login via SSO** button on the LangSmith homepage.

## Without Client Secret (PKCE) (Deprecated)

We recommend running with a `Client Secret` if possible (previously we didn't support this). However, if your IdP does not support this, you can use the `Authorization Code with PKCE` flow.

This flow does *not* require a `Client Secret`. For the alternative workflow, refer to [With client secret](#with-client-secret-recommended).

There are a couple of requirements for using OAuth SSO with LangSmith:

* Your IdP must support the `Authorization Code with PKCE` [flow](https://www.oauth.com/oauth2-servers/pkce) (Google does not support this flow for example, but see [above](#with-client-secret-recommended) for an alternative configuration that Google supports). This is often displayed in your OAuth Provider as configuring a "Single Page Application (SPA)"
* Your IdP must support using an external discovery/issuer URL. We will use this to fetch the necessary routes and keys for your IdP.
* You must provide the `OIDC`, `email`, and `profile` scopes to LangSmith. We use these to fetch the necessary user information and email for your users.
* You will need to set the callback URL in your IdP to `http://<host>/oauth-callback`, where host is the domain or IP you have provisioned for your LangSmith instance. This is where your IdP will redirect the user after they have authenticated.
* You will need to provide the `oauthClientId` and `oauthIssuerUrl` in your `values.yaml` file. This is where you will configure your LangSmith instance.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/self-host-sso.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

### Session length controls

<Note>
  All of the environment variables in this section are for the `platform-backend` service and can be added using `platformBackend.deployment.extraEnv` in Helm.
</Note>

* By default, session length is controlled by the expiration of the identity token returned by the identity provider
* Most setups should use refresh tokens to enable session length extension beyond the identity token expiration up to `OAUTH_SESSION_MAX_SEC`, which may require including the `offline_access` scope by adding to `oauthScopes` (Helm) or `OAUTH_SCOPES` (Docker)
* `OAUTH_SESSION_MAX_SEC` (default 1 day) can be overridden to a maximum of one week (`604800`)
* For identity provider setups that don't support refresh tokens, setting `OAUTH_OVERRIDE_TOKEN_EXPIRY="true"` will take `OAUTH_SESSION_MAX_SEC` as the session length, ignoring the identity token expiration

### Override Sub Claim

In some scenarios, it may be necessary to override which claim is used as the `sub` claim from your identity provider.
For example, in SCIM, the resolved `sub` claim and SCIM `externalId` must match in order for login to succeed.
If there are restrictions on the source attribute of the `sub` claim and/or the SCIM `externalId`, set the `ISSUER_SUB_CLAIM_OVERRIDES` environment variable to select which OIDC JWT claim is used as the `sub`.

If an issuer URL **starts with** one of the URLs in this configuration, the `sub` claim is taken from the field name specified.
For example, with the following configuration, a token with the issuer `https://idp.yourdomain.com/application/uuid` would use the `customClaim` value as the `sub`:
```

Example 3 (unknown):
```unknown
If unset, the default value for this configuration uses the `oid` claim when Azure Entra ID is used as the identity provider:
```

Example 4 (unknown):
```unknown
### Google Workspace IdP setup

You can use Google Workspace as a single sign-on (SSO) provider using [OAuth2.0 and OIDC](https://developers.google.com/identity/openid-connect/openid-connect) without PKCE.

<Note>
  You must have administrator-level access to your organization's Google Cloud Platform (GCP) account to create a new project, or permissions to create and configure OAuth 2.0 credentials for an existing project. We recommend that you create a new project for managing access, since each GCP project has a single OAuth consent screen.
</Note>

1. Create a new GCP project, see the Google documentation topic [creating and managing projects](https://cloud.google.com/resource-manager/docs/creating-managing-projects)

2. After you have created the project, open the [Credentials](https://console.developers.google.com/apis/credentials) page in the Google API Console (making sure the project in the top left corner is correct)

3. Create new credentials: `Create Credentials → OAuth client ID`

4. Choose `Web application` as the `Application type` and enter a name for the application e.g. `LangSmith`

5. In `Authorized Javascript origins` put the domain of your LangSmith instance e.g. `https://langsmith.yourdomain.com`

6. In `Authorized redirect URIs` put the domain of your LangSmith instance followed by `/api/v1/oauth/custom-oidc/callback` e.g. `https://langsmith.yourdomain.com/api/v1/oauth/custom-oidc/callback`

7. Click `Create`, then download the JSON or copy and save the `Client ID` (ends with `.apps.googleusercontent.com`) and `Client secret` somewhere secure. **You will be able to access these later if needed**.

8. Select `OAuth consent screen` from the navigation menu on the left

   1. Choose the Application type as `Internal`. **If you select `Public`, anyone with a Google account can sign in.**
   2. Enter a descriptive `Application name`. This name is shown to users on the consent screen when they sign in. For example, use `LangSmith` or `<organization_name> SSO for LangSmith`.
   3. Verify that the Scopes for Google APIs only lists email, profile, and openid scopes. Only these scopes are required for single sign-on. If you grant additional scopes it increases the risk of exposing sensitive data.

9. (Optional) control who within your organization has access to LangSmith: [https://admin.google.com/ac/owl/list?tab=configuredApps](https://admin.google.com/ac/owl/list?tab=configuredApps). See [Google's documentation](https://support.google.com/a/answer/7281227?hl=en\&fl=1\&sjid=9554153972856467090-NA) for additional details.

10. Configure LangSmith to use this OAuth application. For examples, here are the `config`values that would be used for Kubernetes configuration:

    1. `oauthClientId`: `Client ID` (ends with `.apps.googleusercontent.com`)
    2. `oauthClientSecret`: `Client secret`
    3. `hostname`: the domain of your LangSmith instance e.g. `https://langsmith.yourdomain.com` (no trailing slash)
    4. `oauthIssuerUrl`: `https://accounts.google.com`
    5. `oauth.enabled`: `true`
    6. `authType`: `mixed`

### Okta IdP setup

#### Supported features

* IdP-initiated SSO
* SP-initiated SSO

#### Configuration steps

For additional information, see Okta's [documentation](https://help.okta.com/en-us/content/topics/apps/apps_app_integration_wizard_oidc.htm).
If you have any questions or issues, please reach out to [support@langchain.dev](mailto:support@langchain.dev).

<div id="via-okta-integration-network">
  <b>Via Okta Integration Network (recommended)</b>
</div>

<Note>
  This method of configuration is required in order to use SCIM with Okta.
</Note>

1. Sign in to [Okta](https://login.okta.com/).
2. In the upper-right corner, select Admin. The button is not visible from the Admin area.
3. Select `Browse App Integration Catalog`.
4. Find and select the LangSmith application.
5. On the application overview page, select Add Integration.
6. Fill in `ApiUrlBase`:
   * Your LangSmith API URL **without the protocol** (`https://`) formatted as `<langsmith_domain>/api/v1`, e.g., `langsmith.yourdomain.com/api/v1`.
   * If your installation is configured with a subdomain / path prefix, include that in the URL, e.g., `langsmith.yourdomain.com/prefix/api/v1`.
7. Leave `AuthHost` empty.
8. (Optional, if planning to use [SCIM](/langsmith/user-management#set-up-scim-for-your-organization) as well) Fill in `LangSmithUrl`: The `<langsmith_url>` portion from above, e.g., `langsmith.yourdomain.com`.
9. Under Application Visibility, keep the box unchecked.
10. Select Next.
11. Select `OpenID Connect`.
12. Fill in `Sign-On Options`:
    * `Application username format`: `Email`.
    * `Update application username on`: `Create and update`.
    * `Allow users to securely see their password`: leave **unchecked**.
13. Click **Save**.
14. Configure LangSmith to use this OAuth application (see [general configuration section](#configuration) for details about `initialOrgAdminEmail`):

<CodeGroup>
```

---

## Share or unshare a trace publicly

**URL:** llms-txt#share-or-unshare-a-trace-publicly

Source: https://docs.langchain.com/langsmith/share-trace

<Warning>
  **Sharing a trace publicly will make it accessible to anyone with the link. Make sure you're not sharing sensitive information.**

If your self-hosted or hybrid LangSmith deployment is within a VPC, then the public link is accessible only to members authenticated within your VPC. For enhanced security, we recommend configuring your instance with a private URL accessible only to users with access to your network.
</Warning>

To share a trace publicly, simply click on the **Share** button in the upper right hand side of any trace view.
<img src="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/share-trace.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=f4d51afcb8b75809a08cf254b1797172" alt="" data-og-width="2011" width="2011" data-og-height="1005" height="1005" data-path="langsmith/images/share-trace.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/share-trace.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=2580f397804e880fa5772dd5541347b3 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/share-trace.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=d73de3d28cddf8585257cc5671218af4 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/share-trace.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=9495226170662b9eb0c270e2c9443210 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/share-trace.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=5f6f3b2a45a50a6610dd16f591651a82 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/share-trace.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=ef2e10b3ae15f87d85bf8b5bde7f9e02 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/share-trace.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=a2fb5e488e6b18113b6dd82457fc1720 2500w" />

This will open a dialog where you can copy the link to the trace.

Shared traces will be accessible to anyone with the link, even if they don't have a LangSmith account. They will be able to view the trace, but not edit it.

To "unshare" a trace, either:

1. Click on **Unshare** by clicking on **Public** in the upper right hand corner of any publicly shared trace, then **Unshare** in the dialog.
   <img src="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/unshare-trace.png?fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=30504d6c7fe0ee5d3c6bf9b52a9c3d77" alt="" data-og-width="750" width="750" data-og-height="223" height="223" data-path="langsmith/images/unshare-trace.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/unshare-trace.png?w=280&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=fd6850366fbdadfe8b60af3d675b7e7a 280w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/unshare-trace.png?w=560&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=f992e5a88562a93b88a0ce114452d426 560w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/unshare-trace.png?w=840&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=322987d1e066b41d6c5eef49ad95f4a7 840w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/unshare-trace.png?w=1100&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=0a9ff17d3ebb1e7ab9c3777bc9eea051 1100w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/unshare-trace.png?w=1650&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=be39ee08bd403a1c890687feaa3eb870 1650w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/unshare-trace.png?w=2500&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=99898693ec9d411c396104fc8dc9196d 2500w" />

2. Navigate to your organization's list of publicly shared traces, by clicking on **Settings** -> **Shared URLs**, then click on **Unshare** next to the trace you want to unshare.
   <img src="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/unshare-trace-list-share.png?fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=e139222bbde3e2b9530e92164e0e1efe" alt="" data-og-width="2294" width="2294" data-og-height="1113" height="1113" data-path="langsmith/images/unshare-trace-list-share.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/unshare-trace-list-share.png?w=280&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=fcd07bdf6a4968cefb9d13ab1c447e17 280w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/unshare-trace-list-share.png?w=560&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=5010be488c821b690b0c63b3eeb47e1c 560w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/unshare-trace-list-share.png?w=840&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=aba6af3a7889f20a7b7c00ca01633bde 840w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/unshare-trace-list-share.png?w=1100&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=efb8fd990042f594a07dde88d61a434c 1100w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/unshare-trace-list-share.png?w=1650&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=309427d94d2db88d43930b7de9b8ac65 1650w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/unshare-trace-list-share.png?w=2500&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=2b708410fd513598eb49e354aae33571 2500w" />

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/share-trace.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## shortlived: "604800"  # 7 days (default is 14 days)

**URL:** llms-txt#shortlived:-"604800"--#-7-days-(default-is-14-days)

frontend:
  deployment:
    replicas: 4 # OR enable autoscaling to this level (example below)

---

## Short-term memory

**URL:** llms-txt#short-term-memory

**Contents:**
- Overview
- Usage
  - In production
- Customizing agent memory
- Common patterns
  - Trim messages
  - Delete messages
  - Summarize messages
- Access memory
  - Tools

Source: https://docs.langchain.com/oss/javascript/langchain/short-term-memory

Memory is a system that remembers information about previous interactions. For AI agents, memory is crucial because it lets them remember previous interactions, learn from feedback, and adapt to user preferences. As agents tackle more complex tasks with numerous user interactions, this capability becomes essential for both efficiency and user satisfaction.

Short term memory lets your application remember previous interactions within a single thread or conversation.

<Note>
  A thread organizes multiple interactions in a session, similar to the way email groups messages in a single conversation.
</Note>

Conversation history is the most common form of short-term memory. Long conversations pose a challenge to today's LLMs; a full history may not fit inside an LLM's context window, resulting in an context loss or errors.

Even if your model supports the full context length, most LLMs still perform poorly over long contexts. They get "distracted" by stale or off-topic content, all while suffering from slower response times and higher costs.

Chat models accept context using [messages](/oss/javascript/langchain/messages), which include instructions (a system message) and inputs (human messages). In chat applications, messages alternate between human inputs and model responses, resulting in a list of messages that grows longer over time. Because context windows are limited, many applications can benefit from using techniques to remove or "forget" stale information.

To add short-term memory (thread-level persistence) to an agent, you need to specify a `checkpointer` when creating an agent.

<Info>
  LangChain's agent manages short-term memory as a part of your agent's state.

By storing these in the graph's state, the agent can access the full context for a given conversation while maintaining separation between different threads.

State is persisted to a database (or memory) using a checkpointer so the thread can be resumed at any time.

Short-term memory updates when the agent is invoked or a step (like a tool call) is completed, and the state is read at the start of each step.
</Info>

In production, use a checkpointer backed by a database:

## Customizing agent memory

By default, agents use @\[`AgentState`] to manage short term memory, specifically the conversation history via a `messages` key.

You can extend @\[`AgentState`] to add additional fields. Custom state schemas are passed to @\[`create_agent`] using the @\[`state_schema`] parameter.

With [short-term memory](#add-short-term-memory) enabled, long conversations can exceed the LLM's context window. Common solutions are:

<CardGroup cols={2}>
  <Card title="Trim messages" icon="scissors" href="#trim-messages" arrow>
    Remove first or last N messages (before calling LLM)
  </Card>

<Card title="Delete messages" icon="trash" href="#delete-messages" arrow>
    Delete messages from LangGraph state permanently
  </Card>

<Card title="Summarize messages" icon="layer-group" href="#summarize-messages" arrow>
    Summarize earlier messages in the history and replace them with a summary
  </Card>

<Card title="Custom strategies" icon="gears">
    Custom strategies (e.g., message filtering, etc.)
  </Card>
</CardGroup>

This allows the agent to keep track of the conversation without exceeding the LLM's context window.

Most LLMs have a maximum supported context window (denominated in tokens).

One way to decide when to truncate messages is to count the tokens in the message history and truncate whenever it approaches that limit. If you're using LangChain, you can use the trim messages utility and specify the number of tokens to keep from the list, as well as the `strategy` (e.g., keep the last `maxTokens`) to use for handling the boundary.

To trim message history in an agent, use `stateModifier` with the [`trimMessages`](https://js.langchain.com/docs/how_to/trim_messages/) function:

You can delete messages from the graph state to manage the message history.

This is useful when you want to remove specific messages or clear the entire message history.

To delete messages from the graph state, you can use the `RemoveMessage`. For `RemoveMessage` to work, you need to use a state key with [`messagesStateReducer`](https://langchain-ai.github.io/langgraphjs/reference/functions/langgraph.messagesStateReducer.html) [reducer](/oss/javascript/langgraph/graph-api#reducers), like `MessagesZodState`.

To remove specific messages:

<Warning>
  When deleting messages, **make sure** that the resulting message history is valid. Check the limitations of the LLM provider you're using. For example:

* Some providers expect message history to start with a `user` message
  * Most providers require `assistant` messages with tool calls to be followed by corresponding `tool` result messages.
</Warning>

### Summarize messages

The problem with trimming or removing messages, as shown above, is that you may lose information from culling of the message queue.
Because of this, some applications benefit from a more sophisticated approach of summarizing the message history using a chat model.

<img src="https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/summary.png?fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=c8ed3facdccd4ef5c7e52902c72ba938" alt="" data-og-width="609" width="609" data-og-height="242" height="242" data-path="oss/images/summary.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/summary.png?w=280&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=4208b9b0cc9f459f3dc4e5219918471b 280w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/summary.png?w=560&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=7acb77c081545f57042368f4e9d0c8cb 560w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/summary.png?w=840&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=2fcfdb0c481d2e1d361e76db763a41e5 840w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/summary.png?w=1100&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=4abdac693a562788aa0db8681bef8ea7 1100w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/summary.png?w=1650&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=40acfefa91dcb11b247a6e4a7705f22b 1650w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/summary.png?w=2500&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=8d765aaf7551e8b0fc2720de7d2ac2a8 2500w" />

To summarize message history in an agent, use the built-in [`summarizationMiddleware`](/oss/javascript/langchain/middleware#summarization):

See [`summarizationMiddleware`](/oss/javascript/langchain/middleware#summarization) for more configuration options.

You can access and modify the short-term memory (state) of an agent in several ways:

#### Read short-term memory in a tool

Access short term memory (state) in a tool using the `ToolRuntime` parameter.

The `tool_runtime` parameter is hidden from the tool signature (so the model doesn't see it), but the tool can access the state through it.

#### Write short-term memory from tools

To modify the agent's short-term memory (state) during execution, you can return state updates directly from the tools.

This is useful for persisting intermediate results or making information accessible to subsequent tools or prompts.

Access short term memory (state) in middleware to create dynamic prompts based on conversation history or custom state fields.

Access short term memory (state) in @\[`@before_model`] middleware to process messages before model calls.

Access short term memory (state) in @\[`@after_model`] middleware to process messages after model calls.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/short-term-memory.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
### In production

In production, use a checkpointer backed by a database:
```

Example 2 (unknown):
```unknown
## Customizing agent memory

By default, agents use @\[`AgentState`] to manage short term memory, specifically the conversation history via a `messages` key.

You can extend @\[`AgentState`] to add additional fields. Custom state schemas are passed to @\[`create_agent`] using the @\[`state_schema`] parameter.
```

Example 3 (unknown):
```unknown
## Common patterns

With [short-term memory](#add-short-term-memory) enabled, long conversations can exceed the LLM's context window. Common solutions are:

<CardGroup cols={2}>
  <Card title="Trim messages" icon="scissors" href="#trim-messages" arrow>
    Remove first or last N messages (before calling LLM)
  </Card>

  <Card title="Delete messages" icon="trash" href="#delete-messages" arrow>
    Delete messages from LangGraph state permanently
  </Card>

  <Card title="Summarize messages" icon="layer-group" href="#summarize-messages" arrow>
    Summarize earlier messages in the history and replace them with a summary
  </Card>

  <Card title="Custom strategies" icon="gears">
    Custom strategies (e.g., message filtering, etc.)
  </Card>
</CardGroup>

This allows the agent to keep track of the conversation without exceeding the LLM's context window.

### Trim messages

Most LLMs have a maximum supported context window (denominated in tokens).

One way to decide when to truncate messages is to count the tokens in the message history and truncate whenever it approaches that limit. If you're using LangChain, you can use the trim messages utility and specify the number of tokens to keep from the list, as well as the `strategy` (e.g., keep the last `maxTokens`) to use for handling the boundary.

To trim message history in an agent, use `stateModifier` with the [`trimMessages`](https://js.langchain.com/docs/how_to/trim_messages/) function:
```

Example 4 (unknown):
```unknown
### Delete messages

You can delete messages from the graph state to manage the message history.

This is useful when you want to remove specific messages or clear the entire message history.

To delete messages from the graph state, you can use the `RemoveMessage`. For `RemoveMessage` to work, you need to use a state key with [`messagesStateReducer`](https://langchain-ai.github.io/langgraphjs/reference/functions/langgraph.messagesStateReducer.html) [reducer](/oss/javascript/langgraph/graph-api#reducers), like `MessagesZodState`.

To remove specific messages:
```

---

## Show the workflow

**URL:** llms-txt#show-the-workflow

display(Image(orchestrator_worker.get_graph().draw_mermaid_png()))

---

## Since all of our subagents have compatible state,

**URL:** llms-txt#since-all-of-our-subagents-have-compatible-state,

---

## Since this is **more specific** than both the generic @auth.on handler and the @auth.on.threads handler,

**URL:** llms-txt#since-this-is-**more-specific**-than-both-the-generic-@auth.on-handler-and-the-@auth.on.threads-handler,

---

## Since this is **more specific** than the generic @auth.on handler, it will take precedence

**URL:** llms-txt#since-this-is-**more-specific**-than-the-generic-@auth.on-handler,-it-will-take-precedence

---

## songs by "prince" and our DB records the artist as "Prince", ideally when we query our

**URL:** llms-txt#songs-by-"prince"-and-our-db-records-the-artist-as-"prince",-ideally-when-we-query-our

---

## so the conversation can be paused and resumed (as is needed for human review).

**URL:** llms-txt#so-the-conversation-can-be-paused-and-resumed-(as-is-needed-for-human-review).

config = {"configurable": {"thread_id": "some_id"}} # [!code highlight]

---

## Spec: https://api.smith.langchain.com/redoc#tag/datasets/operation/create_comparative_experiment_api_v1_datasets_comparative_post

**URL:** llms-txt#spec:-https://api.smith.langchain.com/redoc#tag/datasets/operation/create_comparative_experiment_api_v1_datasets_comparative_post

resp = requests.post(
    "https://api.smith.langchain.com/api/v1/datasets/comparative",
    json={
        "experiment_ids": experiment_ids,
        "name": "Toxicity detection - API Example - Comparative - " + str(uuid4())[0:8],
        "description": "An optional description for the comparative experiment",
        "extra": {
            "metadata": {"foo": "bar"},  # Optional metadata
        },
        "reference_dataset_id": str(dataset_id),
    },
    headers={"x-api-key": os.environ["LANGSMITH_API_KEY"]}
)

comparative_experiment = resp.json()
comparative_experiment_id = comparative_experiment["id"]

---

## Spec: https://api.smith.langchain.com/redoc#tag/examples/operation/delete_example_api_v1_examples__example_id__delete

**URL:** llms-txt#spec:-https://api.smith.langchain.com/redoc#tag/examples/operation/delete_example_api_v1_examples__example_id__delete

dataset_id = dataset.id
params = { "dataset": dataset_id }

resp = requests.get(
    "https://api.smith.langchain.com/api/v1/examples",
    params=params,
    headers={"x-api-key": os.environ["LANGSMITH_API_KEY"]}
)

examples = resp.json()
python  theme={null}
os.environ["OPENAI_API_KEY"] = "sk-..."

def run_completion_on_example(example, model_name, experiment_id):
    """Run completions on a list of examples."""
    # We are using the OpenAI API here, but you can use any model you like

def _post_run(run_id, name, run_type, inputs, parent_id=None):
        """Function to post a new run to the API."""
        data = {
            "id": run_id.hex,
            "name": name,
            "run_type": run_type,
            "inputs": inputs,
            "start_time": datetime.utcnow().isoformat(),
            "reference_example_id": example["id"],
            "session_id": experiment_id,
        }
        if parent_id:
            data["parent_run_id"] = parent_id.hex
        resp = requests.post(
            "https://api.smith.langchain.com/api/v1/runs", # Update appropriately for self-hosted installations or the EU region
            json=data,
            headers=headers
        )
        resp.raise_for_status()

def _patch_run(run_id, outputs):
        """Function to patch a run with outputs."""
        resp = requests.patch(
            f"https://api.smith.langchain.com/api/v1/runs/{run_id}",
            json={
                "outputs": outputs,
                "end_time": datetime.utcnow().isoformat(),
            },
            headers=headers,
        )
        resp.raise_for_status()

# Send your API Key in the request headers
    headers = {"x-api-key": os.environ["LANGSMITH_API_KEY"]}

text = example["inputs"]["text"]

messages = [
        {
            "role": "system",
            "content": "Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't.",
        },
        {"role": "user", "content": text},
    ]

# Create parent run
    parent_run_id = uuid4()
    _post_run(parent_run_id, "LLM Pipeline", "chain", {"text": text})

# Create child run
    child_run_id = uuid4()
    _post_run(child_run_id, "OpenAI Call", "llm", {"messages": messages}, parent_run_id)

# Generate completion
    chat_completion = oa_client.chat.completions.create(model=model_name, messages=messages)
    output_text = chat_completion.choices[0].message.content

# End run
    _patch_run(child_run_id, {
    "messages": messages,
        "output": output_text,
        "model": model_name
    })

_patch_run(parent_run_id, {"label": output_text})
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
Next, we'll define a method that will create a run for a single example.
```

Example 2 (unknown):
```unknown
We are going to run completions on all examples using two models: gpt-3.5-turbo and gpt-4o-mini.
```

---

## Spec: https://api.smith.langchain.com/redoc#tag/run/operation/query_runs_api_v1_runs_query_post

**URL:** llms-txt#spec:-https://api.smith.langchain.com/redoc#tag/run/operation/query_runs_api_v1_runs_query_post

runs = requests.post(
    f"https://api.smith.langchain.com/api/v1/runs/query",
    headers={"x-api-key": os.environ["LANGSMITH_API_KEY"]},
    json={
        "session": experiment_ids,
        "is_root": True, # Only fetch root runs (spans) which contain the end outputs
        "select": ["id", "reference_example_id", "outputs"],
    }
).json()
runs = runs["runs"]
for run in runs:
    example_id = run["reference_example_id"]
    example_id_to_runs_map[example_id].append(run)

for example_id, runs in example_id_to_runs_map.items():
    print(f"Example ID: {example_id}")
    # Preferentially rank the outputs, in this case we will always prefer the first output
    # In reality, you can use an LLM to rank the outputs
    feedback_group_id = uuid4()

# Post a feedback score for each run, with the first run being the preferred one
    # Spec: https://api.smith.langchain.com/redoc#tag/feedback/operation/create_feedback_api_v1_feedback_post
    # We'll use the feedback group ID to associate the feedback scores with the same group
    for i, run in enumerate(runs):
        print(f"Run ID: {run['id']}")
        feedback = {
            "score": 1 if i == 0 else 0,
            "run_id": str(run["id"]),
            "key": "ranked_preference",
            "feedback_group_id": str(feedback_group_id),
            "comparative_experiment_id": comparative_experiment_id,
        }
        resp = requests.post(
            "https://api.smith.langchain.com/api/v1/feedback",
            json=feedback,
            headers={"x-api-key": os.environ["LANGSMITH_API_KEY"]}
        )
        resp.raise_for_status()
```

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/run-evals-api-only.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Spec: https://api.smith.langchain.com/redoc#tag/tracer-sessions/operation/create_tracer_session_api_v1_sessions_post

**URL:** llms-txt#spec:-https://api.smith.langchain.com/redoc#tag/tracer-sessions/operation/create_tracer_session_api_v1_sessions_post

**Contents:**
- Run a pairwise experiment

model_names = ("gpt-3.5-turbo", "gpt-4o-mini")
experiment_ids = []
for model_name in model_names:
    resp = requests.post(
        "https://api.smith.langchain.com/api/v1/sessions",
        json={
            "start_time": datetime.utcnow().isoformat(),
            "reference_dataset_id": str(dataset_id),
            "description": "An optional description for the experiment",
            "name": f"Toxicity detection - API Example - {model_name} - {str(uuid4())[0:8]}",  # A name for the experiment
            "extra": {
                "metadata": {"foo": "bar"},  # Optional metadata
            },
        },
        headers={"x-api-key": os.environ["LANGSMITH_API_KEY"]}
    )

experiment = resp.json()
    experiment_ids.append(experiment["id"])

# Run completions on all examples
    for example in examples:
        run_completion_on_example(example, model_name, experiment["id"])

# Issue a patch request to "end" the experiment by updating the end_time
    requests.patch(
        f"https://api.smith.langchain.com/api/v1/sessions/{experiment['id']}",
        json={"end_time": datetime.utcnow().isoformat()},
        headers={"x-api-key": os.environ["LANGSMITH_API_KEY"]}
    )
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
## Run a pairwise experiment

Next, we'll demonstrate how to run a pairwise experiment. In a pairwise experiment, you compare two examples against each other.

For more information, check out [this guide](/langsmith/evaluate-pairwise).
```

---

## ./src/agent/webapp.py

**URL:** llms-txt#./src/agent/webapp.py

**Contents:**
- Configure `langgraph.json`
- Start server
- Deploying
- Next steps

from fastapi import FastAPI

@app.get("/hello")
def read_root():
    return {"Hello": "World"}

json  theme={null}
{
  "dependencies": ["."],
  "graphs": {
    "agent": "./src/agent/graph.py:graph"
  },
  "env": ".env",
  "http": {
    "app": "./src/agent/webapp.py:app"
  }
  // Other configuration options like auth, store, etc.
}
bash  theme={null}
langgraph dev --no-browser
```

If you navigate to `localhost:2024/hello` in your browser (`2024` is the default development port), you should see the `/hello` endpoint returning `{"Hello": "World"}`.

<Note>
  **Shadowing default endpoints**
  The routes you create in the app are given priority over the system defaults, meaning you can shadow and redefine the behavior of any default endpoint.
</Note>

You can deploy this app as-is to LangSmith or to your self-hosted platform.

Now that you've added a custom route to your deployment, you can use this same technique to further customize how your server behaves, such as defining custom [custom middleware](/langsmith/custom-middleware) and [custom lifespan events](/langsmith/custom-lifespan).

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/custom-routes.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
## Configure `langgraph.json`

Add the following to your `langgraph.json` configuration file. Make sure the path points to the FastAPI application instance `app` in the `webapp.py` file you created above.
```

Example 2 (unknown):
```unknown
## Start server

Test the server out locally:
```

---

## Start local development server with Studio

**URL:** llms-txt#start-local-development-server-with-studio

**Contents:**
  - Method 1: LangSmith Deployment UI
  - Method 2: Control Plane API

langgraph dev
bash  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
This will:

* Spin up a local server with Studio.
* Allow you to visualize and interact with your graph.
* Validate that your agent works correctly before deployment.

<Note>
  If your agent runs locally without any errors, it means that deployment to LangSmith will likely succeed. This local testing helps catch configuration issues, dependency problems, and agent logic errors before attempting deployment.
</Note>

See the [LangGraph CLI documentation](/langsmith/cli#dev) for more details.

### Method 1: LangSmith Deployment UI

Deploy your agent using the LangSmith deployment interface:

1. Go to your [LangSmith dashboard](https://smith.langchain.com).
2. Navigate to the **Deployments** section.
3. Click the **+ New Deployment** button in the top right.
4. Select your GitHub repository containing your LangGraph agent from the dropdown menu.

**Supported deployments:**

* <Icon icon="cloud" /> **Cloud LangSmith**: Direct GitHub integration with dropdown menu
* <Icon icon="server" /> **Self-Hosted/Hybrid LangSmith**: Specify your image URI in the Image Path field (e.g., `docker.io/username/my-agent:latest`)

<Info>
  **Benefits:**

  * Simple UI-based deployment
  * Direct integration with your GitHub repository (cloud)
  * No manual Docker image management required (cloud)
</Info>

### Method 2: Control Plane API

Deploy using the Control Plane API with different approaches for each deployment type:

**For Cloud LangSmith:**

* Use the Control Plane API to create deployments by pointing to your GitHub repository
* No Docker image building required for cloud deployments

**For Self-Hosted/Hybrid LangSmith:**
```

---

## Stateless runs

**URL:** llms-txt#stateless-runs

**Contents:**
- Setup
- Stateless streaming
- Waiting for stateless results

Source: https://docs.langchain.com/langsmith/stateless-runs

Most of the time, you provide a `thread_id` to your client when you run your graph in order to keep track of prior runs through the persistent state implemented in LangSmith Deployment. However, if you don't need to persist the runs you don't need to use the built-in persistent state and can create stateless runs.

First, let's setup our client:

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

## Stateless streaming

We can stream the results of a stateless run in an almost identical fashion to how we stream from a run with the state attribute, but instead of passing a value to the `thread_id` parameter, we pass `None`:

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

## Waiting for stateless results

In addition to streaming, you can also wait for a stateless result by using the `.wait` function like follows:

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/stateless-runs.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
</Tab>

  <Tab title="Javascript">
```

Example 2 (unknown):
```unknown
</Tab>

  <Tab title="CURL">
```

Example 3 (unknown):
```unknown
</Tab>
</Tabs>

## Stateless streaming

We can stream the results of a stateless run in an almost identical fashion to how we stream from a run with the state attribute, but instead of passing a value to the `thread_id` parameter, we pass `None`:

<Tabs>
  <Tab title="Python">
```

Example 4 (unknown):
```unknown
</Tab>

  <Tab title="Javascript">
```

---

## state we defined for the refund agent can also be passed to our lookup agent.

**URL:** llms-txt#state-we-defined-for-the-refund-agent-can-also-be-passed-to-our-lookup-agent.

qa_graph = create_agent(qa_llm, tools=[lookup_track, lookup_artist, lookup_album])

display(Image(qa_graph.get_graph(xray=True).draw_mermaid_png()))
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/qa-graph.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=fa838edc78b2b29e8c29807d8c3dd7fd" alt="QA Graph" data-og-width="214" width="214" data-og-height="249" height="249" data-path="langsmith/images/qa-graph.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/qa-graph.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=920e82f376d6bbbcfe02c07ac7a45b80 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/qa-graph.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=938d3bd8c19abfe27ea5efd1c996494c 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/qa-graph.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=e46ece85318d4c376cd6bb632bf41ab4 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/qa-graph.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=3e3c715ef37db24fd0cbf8eb4ca19190 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/qa-graph.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=e3270477acbc50eb9e4e9736a5ec6afc 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/qa-graph.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=667b26bb91f33aaacbeb0a2ea749825a 2500w" />

#### Parent agent

Now let's define a parent agent that combines our two task-specific agents. The only job of the parent agent is to route to one of the sub-agents by classifying the user's current intent, and to compile the output into a followup message.
```

---

## Store persists embeddings to the local filesystem

**URL:** llms-txt#store-persists-embeddings-to-the-local-filesystem

---

## Store without embedding (still retrievable, but not searchable)

**URL:** llms-txt#store-without-embedding-(still-retrievable,-but-not-searchable)

**Contents:**
  - Using in LangGraph

store.put(
    namespace_for_memory,
    str(uuid.uuid4()),
    {"system_info": "Last updated: 2024-01-01"},
    index=False
)
python  theme={null}
from langgraph.checkpoint.memory import InMemorySaver

**Examples:**

Example 1 (unknown):
```unknown
### Using in LangGraph

With this all in place, we use the `in_memory_store` in LangGraph. The `in_memory_store` works hand-in-hand with the checkpointer: the checkpointer saves state to threads, as discussed above, and the `in_memory_store` allows us to store arbitrary information for access *across* threads. We compile the graph with both the checkpointer and the `in_memory_store` as follows.
```

---

## Store with specific fields to embed

**URL:** llms-txt#store-with-specific-fields-to-embed

store.put(
    namespace_for_memory,
    str(uuid.uuid4()),
    {
        "food_preference": "I love Italian cuisine",
        "context": "Discussing dinner plans"
    },
    index=["food_preference"]  # Only embed "food_preferences" field
)

---

## Streaming

**URL:** llms-txt#streaming

**Contents:**
- Supported stream modes
- Basic usage example
- Stream multiple modes
- Stream graph state
- Stream subgraph outputs
  - Debugging
- LLM tokens
- Stream custom data
- Use with any LLM
- Disable streaming for specific chat models

Source: https://docs.langchain.com/oss/javascript/langgraph/streaming

LangGraph implements a streaming system to surface real-time updates. Streaming is crucial for enhancing the responsiveness of applications built on LLMs. By displaying output progressively, even before a complete response is ready, streaming significantly improves user experience (UX), particularly when dealing with the latency of LLMs.

What's possible with LangGraph streaming:

* <Icon icon="share-nodes" size={16} /> [**Stream graph state**](#stream-graph-state) — get state updates / values with `updates` and `values` modes.
* <Icon icon="square-poll-horizontal" size={16} /> [**Stream subgraph outputs**](#stream-subgraph-outputs) — include outputs from both the parent graph and any nested subgraphs.
* <Icon icon="square-binary" size={16} /> [**Stream LLM tokens**](#messages) — capture token streams from anywhere: inside nodes, subgraphs, or tools.
* <Icon icon="table" size={16} /> [**Stream custom data**](#stream-custom-data) — send custom updates or progress signals directly from tool functions.
* <Icon icon="layer-plus" size={16} /> [**Use multiple streaming modes**](#stream-multiple-modes) — choose from `values` (full state), `updates` (state deltas), `messages` (LLM tokens + metadata), `custom` (arbitrary user data), or `debug` (detailed traces).

## Supported stream modes

Pass one or more of the following stream modes as a list to the [`stream`](https://langchain-ai.github.io/langgraphjs/reference/classes/langgraph.CompiledStateGraph.html#stream) method:

| Mode       | Description                                                                                                                                                                         |
| ---------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `values`   | Streams the full value of the state after each step of the graph.                                                                                                                   |
| `updates`  | Streams the updates to the state after each step of the graph. If multiple updates are made in the same step (e.g., multiple nodes are run), those updates are streamed separately. |
| `custom`   | Streams custom data from inside your graph nodes.                                                                                                                                   |
| `messages` | Streams 2-tuples (LLM token, metadata) from any graph nodes where an LLM is invoked.                                                                                                |
| `debug`    | Streams as much information as possible throughout the execution of the graph.                                                                                                      |

## Basic usage example

LangGraph graphs expose the [`stream`](https://langchain-ai.github.io/langgraphjs/reference/classes/langgraph.Pregel.html#stream) method to yield streamed outputs as iterators.

<Accordion title="Extended example: streaming updates">

## Stream multiple modes

You can pass an array as the `streamMode` parameter to stream multiple modes at once.

The streamed outputs will be tuples of `[mode, chunk]` where `mode` is the name of the stream mode and `chunk` is the data streamed by that mode.

## Stream graph state

Use the stream modes `updates` and `values` to stream the state of the graph as it executes.

* `updates` streams the **updates** to the state after each step of the graph.
* `values` streams the **full value** of the state after each step of the graph.

<Tabs>
  <Tab title="updates">
    Use this to stream only the **state updates** returned by the nodes after each step. The streamed outputs include the name of the node as well as the update.

<Tab title="values">
    Use this to stream the **full state** of the graph after each step.

## Stream subgraph outputs

To include outputs from [subgraphs](/oss/javascript/langgraph/use-subgraphs) in the streamed outputs, you can set `subgraphs: true` in the `.stream()` method of the parent graph. This will stream outputs from both the parent graph and any subgraphs.

The outputs will be streamed as tuples `[namespace, data]`, where `namespace` is a tuple with the path to the node where a subgraph is invoked, e.g. `["parent_node:<task_id>", "child_node:<task_id>"]`.

<Accordion title="Extended example: streaming from subgraphs">

**Note** that we are receiving not just the node updates, but we also the namespaces which tell us what graph (or subgraph) we are streaming from.
</Accordion>

Use the `debug` streaming mode to stream as much information as possible throughout the execution of the graph. The streamed outputs include the name of the node as well as the full state.

Use the `messages` streaming mode to stream Large Language Model (LLM) outputs **token by token** from any part of your graph, including nodes, tools, subgraphs, or tasks.

The streamed output from [`messages` mode](#supported-stream-modes) is a tuple `[message_chunk, metadata]` where:

* `message_chunk`: the token or message segment from the LLM.
* `metadata`: a dictionary containing details about the graph node and LLM invocation.

> If your LLM is not available as a LangChain integration, you can stream its outputs using `custom` mode instead. See [use with any LLM](#use-with-any-llm) for details.

#### Filter by LLM invocation

You can associate `tags` with LLM invocations to filter the streamed tokens by LLM invocation.

<Accordion title="Extended example: filtering by tags">
  
</Accordion>

To stream tokens only from specific nodes, use `stream_mode="messages"` and filter the outputs by the `langgraph_node` field in the streamed metadata:

<Accordion title="Extended example: streaming LLM tokens from specific nodes">
  
</Accordion>

## Stream custom data

To send **custom user-defined data** from inside a LangGraph node or tool, follow these steps:

1. Use the `writer` parameter from the `LangGraphRunnableConfig` to emit custom data.
2. Set `streamMode: "custom"` when calling `.stream()` to get the custom data in the stream. You can combine multiple modes (e.g., `["updates", "custom"]`), but at least one must be `"custom"`.

<Tabs>
  <Tab title="node">
    
  </Tab>

<Tab title="tool">
    
  </Tab>
</Tabs>

You can use `streamMode: "custom"` to stream data from **any LLM API** — even if that API does **not** implement the LangChain chat model interface.

This lets you integrate raw LLM clients or external services that provide their own streaming interfaces, making LangGraph highly flexible for custom setups.

<Accordion title="Extended example: streaming arbitrary chat model">

Let's invoke the graph with an [`AIMessage`](https://v03.api.js.langchain.com/classes/_langchain_core.messages_ai_message.AIMessage.html) that includes a tool call:

## Disable streaming for specific chat models

If your application mixes models that support streaming with those that do not, you may need to explicitly disable streaming for
models that do not support it.

Set `streaming: false` when initializing the model.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/streaming.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
<Accordion title="Extended example: streaming updates">
```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
</Accordion>

## Stream multiple modes

You can pass an array as the `streamMode` parameter to stream multiple modes at once.

The streamed outputs will be tuples of `[mode, chunk]` where `mode` is the name of the stream mode and `chunk` is the data streamed by that mode.
```

Example 4 (unknown):
```unknown
## Stream graph state

Use the stream modes `updates` and `values` to stream the state of the graph as it executes.

* `updates` streams the **updates** to the state after each step of the graph.
* `values` streams the **full value** of the state after each step of the graph.
```

---

## Streaming API

**URL:** llms-txt#streaming-api

**Contents:**
- Basic usage
  - Supported stream modes
  - Stream multiple modes
- Stream graph state
  - Stream Mode: `updates`
  - Stream Mode: `values`
- Subgraphs
- Debugging
- LLM tokens
  - Filter LLM tokens

Source: https://docs.langchain.com/langsmith/streaming

[LangGraph SDK](/langsmith/langgraph-python-sdk) allows you to [stream outputs](/oss/python/langgraph/streaming/) from the [LangSmith Deployment API](/langsmith/server-api-ref).

<Note>
  LangGraph SDK and Agent Server are a part of [LangSmith](/langsmith/home).
</Note>

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="JavaScript">
    
  </Tab>

<Tab title="cURL">
    Create a thread:

Create a streaming run:

<Accordion title="Extended example: streaming updates">
  This is an example graph you can run in the Agent Server.
  See [LangSmith quickstart](/langsmith/deployment-quickstart) for more details.

Once you have a running Agent Server, you can interact with it using
  [LangGraph SDK](/langsmith/langgraph-python-sdk)

<Tabs>
    <Tab title="Python">

1. The `client.runs.stream()` method returns an iterator that yields streamed outputs.
         2\. Set `stream_mode="updates"` to stream only the updates to the graph state after each node. Other stream modes are also available. See [supported stream modes](#supported-stream-modes) for details.
    </Tab>

<Tab title="JavaScript">

1. The `client.runs.stream()` method returns an iterator that yields streamed outputs.
      2. Set `streamMode: "updates"` to stream only the updates to the graph state after each node. Other stream modes are also available. See [supported stream modes](#supported-stream-modes) for details.
    </Tab>

<Tab title="cURL">
      Create a thread:

Create a streaming run:

### Supported stream modes

| Mode                             | Description                                                                                                                                                                         | LangGraph Library Method                                                                                      |
| -------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------- |
| [`values`](#stream-graph-state)  | Stream the full graph state after each [super-step](/langsmith/graph-rebuild#graphs).                                                                                               | `.stream()` / `.astream()` with [`stream_mode="values"`](/oss/python/langgraph/streaming#stream-graph-state)  |
| [`updates`](#stream-graph-state) | Streams the updates to the state after each step of the graph. If multiple updates are made in the same step (e.g., multiple nodes are run), those updates are streamed separately. | `.stream()` / `.astream()` with [`stream_mode="updates"`](/oss/python/langgraph/streaming#stream-graph-state) |
| [`messages-tuple`](#messages)    | Streams LLM tokens and metadata for the graph node where the LLM is invoked (useful for chat apps).                                                                                 | `.stream()` / `.astream()` with [`stream_mode="messages"`](/oss/python/langgraph/streaming#messages)          |
| [`debug`](#debug)                | Streams as much information as possible throughout the execution of the graph.                                                                                                      | `.stream()` / `.astream()` with [`stream_mode="debug"`](/oss/python/langgraph/streaming#stream-graph-state)   |
| [`custom`](#stream-custom-data)  | Streams custom data from inside your graph                                                                                                                                          | `.stream()` / `.astream()` with [`stream_mode="custom"`](/oss/python/langgraph/streaming#stream-custom-data)  |
| [`events`](#stream-events)       | Stream all events (including the state of the graph); mainly useful when migrating large LCEL apps.                                                                                 | `.astream_events()`                                                                                           |

### Stream multiple modes

You can pass a list as the `stream_mode` parameter to stream multiple modes at once.

The streamed outputs will be tuples of `(mode, chunk)` where `mode` is the name of the stream mode and `chunk` is the data streamed by that mode.

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="JavaScript">
    
  </Tab>

<Tab title="cURL">
    
  </Tab>
</Tabs>

## Stream graph state

Use the stream modes `updates` and `values` to stream the state of the graph as it executes.

* `updates` streams the **updates** to the state after each step of the graph.
* `values` streams the **full value** of the state after each step of the graph.

<Accordion title="Example graph">
  
</Accordion>

<Note>
  **Stateful runs**
  Examples below assume that you want to **persist the outputs** of a streaming run in the [checkpointer](/oss/python/langgraph/persistence) DB and have created a thread. To create a thread:

<Tabs>
    <Tab title="Python">
      
    </Tab>

<Tab title="JavaScript">
      
    </Tab>

<Tab title="cURL">
      
    </Tab>
  </Tabs>

If you don't need to persist the outputs of a run, you can pass `None` instead of `thread_id` when streaming.
</Note>

### Stream Mode: `updates`

Use this to stream only the **state updates** returned by the nodes after each step. The streamed outputs include the name of the node as well as the update.

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="JavaScript">
    
  </Tab>

<Tab title="cURL">
    
  </Tab>
</Tabs>

### Stream Mode: `values`

Use this to stream the **full state** of the graph after each step.

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="JavaScript">
    
  </Tab>

<Tab title="cURL">
    
  </Tab>
</Tabs>

To include outputs from [subgraphs](/oss/python/langgraph/use-subgraphs) in the streamed outputs, you can set `subgraphs=True` in the `.stream()` method of the parent graph. This will stream outputs from both the parent graph and any subgraphs.

1. Set `stream_subgraphs=True` to stream outputs from subgraphs.

<Accordion title="Extended example: streaming from subgraphs">
  This is an example graph you can run in the Agent Server.
  See [LangSmith quickstart](/langsmith/deployment-quickstart) for more details.

Once you have a running Agent Server, you can interact with it using
  [LangGraph SDK](/langsmith/langgraph-python-sdk)

<Tabs>
    <Tab title="Python">

1. Set `stream_subgraphs=True` to stream outputs from subgraphs.
    </Tab>

<Tab title="JavaScript">

1. Set `streamSubgraphs: true` to stream outputs from subgraphs.
    </Tab>

<Tab title="cURL">
      Create a thread:

Create a streaming run:

**Note** that we are receiving not just the node updates, but we also the namespaces which tell us what graph (or subgraph) we are streaming from.
</Accordion>

Use the `debug` streaming mode to stream as much information as possible throughout the execution of the graph. The streamed outputs include the name of the node as well as the full state.

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="JavaScript">
    
  </Tab>

<Tab title="cURL">
    
  </Tab>
</Tabs>

Use the `messages-tuple` streaming mode to stream Large Language Model (LLM) outputs **token by token** from any part of your graph, including nodes, tools, subgraphs, or tasks.

The streamed output from [`messages-tuple` mode](#supported-stream-modes) is a tuple `(message_chunk, metadata)` where:

* `message_chunk`: the token or message segment from the LLM.
* `metadata`: a dictionary containing details about the graph node and LLM invocation.

<Accordion title="Example graph">

1. Note that the message events are emitted even when the LLM is run using `invoke` rather than `stream`.
</Accordion>

<Tabs>
  <Tab title="Python">

1. The "messages-tuple" stream mode returns an iterator of tuples `(message_chunk, metadata)` where `message_chunk` is the token streamed by the LLM and `metadata` is a dictionary with information about the graph node where the LLM was called and other information.
  </Tab>

<Tab title="JavaScript">

1. The "messages-tuple" stream mode returns an iterator of tuples `(message_chunk, metadata)` where `message_chunk` is the token streamed by the LLM and `metadata` is a dictionary with information about the graph node where the LLM was called and other information.
  </Tab>

<Tab title="cURL">
    
  </Tab>
</Tabs>

### Filter LLM tokens

* To filter the streamed tokens by LLM invocation, you can [associate `tags` with LLM invocations](/oss/python/langgraph/streaming#filter-by-llm-invocation).
* To stream tokens only from specific nodes, use `stream_mode="messages"` and [filter the outputs by the `langgraph_node` field](/oss/python/langgraph/streaming#filter-by-node) in the streamed metadata.

## Stream custom data

To send **custom user-defined data**:

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="JavaScript">
    
  </Tab>

<Tab title="cURL">
    
  </Tab>
</Tabs>

To stream all events, including the state of the graph:

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="JavaScript">
    
  </Tab>

<Tab title="cURL">
    
  </Tab>
</Tabs>

If you don't want to **persist the outputs** of a streaming run in the [checkpointer](/oss/python/langgraph/persistence) DB, you can create a stateless run without creating a thread:

<Tabs>
  <Tab title="Python">

1. We are passing `None` instead of a `thread_id` UUID.
  </Tab>

<Tab title="JavaScript">

1. We are passing `None` instead of a `thread_id` UUID.
  </Tab>

<Tab title="cURL">
    
  </Tab>
</Tabs>

LangSmith allows you to join an active [background run](/langsmith/background-run) and stream outputs from it. To do so, you can use [LangGraph SDK's](/langsmith/langgraph-python-sdk) `client.runs.join_stream` method:

<Tabs>
  <Tab title="Python">

1. This is the `run_id` of an existing run you want to join.
  </Tab>

<Tab title="JavaScript">

1. This is the `run_id` of an existing run you want to join.
  </Tab>

<Tab title="cURL">
    
  </Tab>
</Tabs>

<Warning>
  **Outputs not buffered**
  When you use `.join_stream`, output is not buffered, so any output produced before joining will not be received.
</Warning>

For API usage and implementation, refer to the [API reference](https://langchain-ai.github.io/langgraph/cloud/reference/api/api_ref.html#tag/thread-runs/POST/threads/\{thread_id}/runs/stream).

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/streaming.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
</Tab>

  <Tab title="JavaScript">
```

Example 2 (unknown):
```unknown
</Tab>

  <Tab title="cURL">
    Create a thread:
```

Example 3 (unknown):
```unknown
Create a streaming run:
```

Example 4 (unknown):
```unknown
</Tab>
</Tabs>

<Accordion title="Extended example: streaming updates">
  This is an example graph you can run in the Agent Server.
  See [LangSmith quickstart](/langsmith/deployment-quickstart) for more details.
```

---

## Strict enforcement with "error" behavior

**URL:** llms-txt#strict-enforcement-with-"error"-behavior

web_scraper_limiter = ToolCallLimitMiddleware(
    tool_name="scrape_webpage",
    run_limit=2,
    exit_behavior="error",
)

---

## String content

**URL:** llms-txt#string-content

human_message = HumanMessage("Hello, how are you?")

---

## Structured output

**URL:** llms-txt#structured-output

**Contents:**
- Response Format
- Provider strategy
- Tool calling strategy
  - Custom tool message content

Source: https://docs.langchain.com/oss/javascript/langchain/structured-output

Structured output allows agents to return data in a specific, predictable format. Instead of parsing natural language responses, you get typed structured data.

LangChain's prebuilt ReAct agent `createAgent` handles structured output automatically. The user sets their desired structured output schema, and when the model generates the structured data, it's captured, validated, and returned in the `structuredResponse` key of the agent's state.

Controls how the agent returns structured data. You can provide either a Zod object or JSON schema. By default, the agent uses a tool calling strategy, in which the output is created by an additional tool call. Certain models support native structured output, in which case the agent will use that strategy instead.

You can control the behavior by wrapping `ResponseFormat` in a `toolStrategy` or `providerStrategy` function call:

The structured response is returned in the `structuredResponse` key of the agent's final state.

Some model providers support structured output natively through their APIs (currently only OpenAI and Grok). This is the most reliable method when available.

To use this strategy, configure a `ProviderStrategy`:

<ParamField path="schema" required>
  The schema defining the structured output format. Supports:

* **Zod Schema**: A zod schema
  * **JSON Schema**: A JSON schema object
</ParamField>

LangChain automatically uses `ProviderStrategy` when you pass a schema type directly to `createAgent.responseFormat` and the model supports native structured output:

Provider-native structured output provides high reliability and strict validation because the model provider enforces the schema. Use it when available.

<Note>
  If the provider natively supports structured output for your model choice, it is functionally equivalent to write `responseFormat: contactInfoSchema` instead of `responseFormat: toolStrategy(contactInfoSchema)`. In either case, if structured output is not supported, the agent will fall back to a tool calling strategy.
</Note>

## Tool calling strategy

For models that don't support native structured output, LangChain uses tool calling to achieve the same result. This works with all models that support tool calling, which is most modern models.

To use this strategy, configure a `ToolStrategy`:

<ParamField path="schema" required>
  The schema defining the structured output format. Supports:

* **Zod Schema**: A zod schema
  * **JSON Schema**: A JSON schema object
</ParamField>

<ParamField path="options.toolMessageContent">
  Custom content for the tool message returned when structured output is generated.
  If not provided, defaults to a message showing the structured response data.
</ParamField>

<ParamField path="options.handleError">
  Options parameter containing an optional `handleError` parameter for customizing the error handling strategy.

* **`true`**: Catch all errors with default error template (default)
  * **`False`**: No retry, let exceptions propagate
  * **`(error: ToolStrategyError) => string | Promise<string>`**: retry with the provided message or throw the error
</ParamField>

### Custom tool message content

The `toolMessageContent` parameter allows you to customize the message that appears in the conversation history when structured output is generated:

Without `toolMessageContent`, we'd see:

**Examples:**

Example 1 (unknown):
```unknown
## Response Format

Controls how the agent returns structured data. You can provide either a Zod object or JSON schema. By default, the agent uses a tool calling strategy, in which the output is created by an additional tool call. Certain models support native structured output, in which case the agent will use that strategy instead.

You can control the behavior by wrapping `ResponseFormat` in a `toolStrategy` or `providerStrategy` function call:
```

Example 2 (unknown):
```unknown
The structured response is returned in the `structuredResponse` key of the agent's final state.

## Provider strategy

Some model providers support structured output natively through their APIs (currently only OpenAI and Grok). This is the most reliable method when available.

To use this strategy, configure a `ProviderStrategy`:
```

Example 3 (unknown):
```unknown
<ParamField path="schema" required>
  The schema defining the structured output format. Supports:

  * **Zod Schema**: A zod schema
  * **JSON Schema**: A JSON schema object
</ParamField>

LangChain automatically uses `ProviderStrategy` when you pass a schema type directly to `createAgent.responseFormat` and the model supports native structured output:

<CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Studio

**URL:** llms-txt#studio

**Contents:**
- Prerequisites
- Setup local Agent server
  - 1. Install the LangGraph CLI

Source: https://docs.langchain.com/oss/javascript/langgraph/studio

This guide will walk you through how to use **Studio** to visualize, interact, and debug your agent locally.

Studio is our free-to-use, powerful agent IDE that integrates with [LangSmith](/langsmith/home) to enable tracing, evaluation, and prompt engineering. See exactly how your agent thinks, trace every decision, and ship smarter, more reliable agents.

<Frame>
  <iframe className="w-full aspect-video rounded-xl" src="https://www.youtube.com/embed/Mi1gSlHwZLM?si=zA47TNuTC5aH0ahd" title="Studio" frameBorder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowFullScreen />
</Frame>

Before you begin, ensure you have the following:

* An API key for [LangSmith](https://smith.langchain.com/settings) (free to sign up)

## Setup local Agent server

### 1. Install the LangGraph CLI

```shell  theme={null}

---

## Studio troubleshooting

**URL:** llms-txt#studio-troubleshooting

**Contents:**
- Safari Connection Issues
  - Solution 1: Use Cloudflare Tunnel
  - Solution 2: Use Chromium browser
- Brave Connection Issues
  - Solution 1: Disable Brave Shields
  - Solution 2: Use Cloudflare Tunnel
- Graph Edge Issues
  - Solution 1: Path map
  - Solution 2: Router type definition
- Experiment troubleshooting in Studio

Source: https://docs.langchain.com/langsmith/troubleshooting-studio

## Safari Connection Issues

Safari blocks plain-HTTP traffic on localhost. When running Studio with `langgraph dev`, you may see "Failed to load assistants" errors.

### Solution 1: Use Cloudflare Tunnel

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="JS">
    
  </Tab>
</Tabs>

The command outputs a URL in this format:

Use this URL in Safari to load Studio. Here, the `baseUrl` parameter specifies your agent server endpoint.

### Solution 2: Use Chromium browser

Chrome and other Chromium browsers allow HTTP on localhost. Use `langgraph dev` without additional configuration.

## Brave Connection Issues

Brave blocks plain-HTTP traffic on localhost when Brave Shields are enabled. When running Studio with `langgraph dev`, you may see "Failed to load assistants" errors.

### Solution 1: Disable Brave Shields

Disable Brave Shields for LangSmith using the Brave icon in the URL bar.

### Solution 2: Use Cloudflare Tunnel

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="JS">
    
  </Tab>
</Tabs>

The command outputs a URL in this format:

Use this URL in Brave to load Studio. Here, the `baseUrl` parameter specifies your agent server endpoint.

Undefined conditional edges may show unexpected connections in your graph. This is
because without proper definition, Studio assumes the conditional edge could access all other nodes. To address this, explicitly define the routing paths using one of these methods:

### Solution 1: Path map

Define a mapping between router outputs and target nodes:

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>
</Tabs>

### Solution 2: Router type definition

Specify possible routing destinations using Python's `Literal` type:

## Experiment troubleshooting in Studio

### **Run experiment** button is disabled

* **Deployed application**: If your application is deployed on LangSmith, you may need to create a new revision to enable this feature.
* **Local development server**: If you are running your application locally, make sure you have upgraded to the latest version of the `langgraph-cli` (`pip install -U langgraph-cli`). Additionally, ensure you have tracing enabled by setting the `LANGSMITH_API_KEY` in your project's `.env` file.

### Evaluator results are missing

When you run an experiment, any attached evaluators are scheduled for execution in a queue. If you don't see results immediately, it likely means they are still pending.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/troubleshooting-studio.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
</Tab>

  <Tab title="JS">
```

Example 2 (unknown):
```unknown
</Tab>
</Tabs>

The command outputs a URL in this format:
```

Example 3 (unknown):
```unknown
Use this URL in Safari to load Studio. Here, the `baseUrl` parameter specifies your agent server endpoint.

### Solution 2: Use Chromium browser

Chrome and other Chromium browsers allow HTTP on localhost. Use `langgraph dev` without additional configuration.

## Brave Connection Issues

Brave blocks plain-HTTP traffic on localhost when Brave Shields are enabled. When running Studio with `langgraph dev`, you may see "Failed to load assistants" errors.

### Solution 1: Disable Brave Shields

Disable Brave Shields for LangSmith using the Brave icon in the URL bar.

### Solution 2: Use Cloudflare Tunnel

<Tabs>
  <Tab title="Python">
```

Example 4 (unknown):
```unknown
</Tab>

  <Tab title="JS">
```

---

## Subagents

**URL:** llms-txt#subagents

**Contents:**
- Why use subagents?
- Configuration
  - SubAgent (Dictionary-based)
  - CompiledSubAgent
- Using SubAgent
- Using CompiledSubAgent
- The general-purpose subagent
  - When to use it
- Best practices
  - Write clear descriptions

Source: https://docs.langchain.com/oss/javascript/deepagents/subagents

Learn how to use subagents to delegate work and keep context clean

Deep agents can create subagents to delegate work. You can specify custom subagents in the `subagents` parameter. Subagents are useful for [context quarantine](https://www.dbreunig.com/2025/06/26/how-to-fix-your-context.html#context-quarantine) (keeping the main agent's context clean) and for providing specialized instructions.

## Why use subagents?

Subagents solve the **context bloat problem**. When agents use tools with large outputs (web search, file reads, database queries), the context window fills up quickly with intermediate results. Subagents isolate this detailed work—the main agent receives only the final result, not the dozens of tool calls that produced it.

**When to use subagents:**

* ✅ Multi-step tasks that would clutter the main agent's context
* ✅ Specialized domains that need custom instructions or tools
* ✅ Tasks requiring different model capabilities
* ✅ When you want to keep the main agent focused on high-level coordination

**When NOT to use subagents:**

* ❌ Simple, single-step tasks
* ❌ When you need to maintain intermediate context
* ❌ When the overhead outweighs benefits

`subagents` should be a list of dictionaries or `CompiledSubAgent` objects. There are two types:

### SubAgent (Dictionary-based)

For most use cases, define subagents as dictionaries:

* **name** (`str`): Unique identifier for the subagent. The main agent uses this name when calling the `task()` tool.
* **description** (`str`): What this subagent does. Be specific and action-oriented. The main agent uses this to decide when to delegate.
* **system\_prompt** (`str`): Instructions for the subagent. Include tool usage guidance and output format requirements.
* **tools** (`List[Callable]`): Tools the subagent can use. Keep this minimal and include only what's needed.

* **model** (`str | BaseChatModel`): Override the main agent's model. Use the format `"provider:model-name"` (for example, `"openai:gpt-4o"`).
* **middleware** (`List[Middleware]`): Additional middleware for custom behavior, logging, or rate limiting.
* **interrupt\_on** (`Dict[str, bool]`): Configure human-in-the-loop for specific tools. Requires a checkpointer.

For complex workflows, use a pre-built LangGraph graph:

* **name** (`str`): Unique identifier
* **description** (`str`): What this subagent does
* **runnable** (`Runnable`): A compiled LangGraph graph (must call `.compile()` first)

## Using CompiledSubAgent

For more complex use cases, you can provide your own pre-built LangGraph graph as a subagent:

## The general-purpose subagent

In addition to any user-defined subagents, deep agents have access to a `general-purpose` subagent at all times. This subagent:

* Has the same system prompt as the main agent
* Has access to all the same tools
* Uses the same model (unless overridden)

The general-purpose subagent is ideal for context isolation without specialized behavior. The main agent can delegate a complex multi-step task to this subagent and get a concise result back without bloat from intermediate tool calls.

<Card title="Example">
  Instead of the main agent making 10 web searches and filling its context with results, it delegates to the general-purpose subagent: `task(name="general-purpose", task="Research quantum computing trends")`. The subagent performs all the searches internally and returns only a summary.
</Card>

### Write clear descriptions

The main agent uses descriptions to decide which subagent to call. Be specific:

✅ **Good:** `"Analyzes financial data and generates investment insights with confidence scores"`

❌ **Bad:** `"Does finance stuff"`

### Keep system prompts detailed

Include specific guidance on how to use tools and format outputs:

### Minimize tool sets

Only give subagents the tools they need. This improves focus and security:

### Choose models by task

Different models excel at different tasks:

### Return concise results

Instruct subagents to return summaries, not raw data:

### Multiple specialized subagents

Create specialized subagents for different domains:

1. Main agent creates high-level plan
2. Delegates data collection to data-collector
3. Passes results to data-analyzer
4. Sends insights to report-writer
5. Compiles final output

Each subagent works with clean context focused only on its task.

### Subagent not being called

**Problem**: Main agent tries to do work itself instead of delegating.

1. **Make descriptions more specific:**

2. **Instruct main agent to delegate:**

### Context still getting bloated

**Problem**: Context fills up despite using subagents.

1. **Instruct subagent to return concise results:**

2. **Use filesystem for large data:**

### Wrong subagent being selected

**Problem**: Main agent calls inappropriate subagent for the task.

**Solution**: Differentiate subagents clearly in descriptions:

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/deepagents/subagents.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
## Using CompiledSubAgent

For more complex use cases, you can provide your own pre-built LangGraph graph as a subagent:
```

Example 2 (unknown):
```unknown
## The general-purpose subagent

In addition to any user-defined subagents, deep agents have access to a `general-purpose` subagent at all times. This subagent:

* Has the same system prompt as the main agent
* Has access to all the same tools
* Uses the same model (unless overridden)

### When to use it

The general-purpose subagent is ideal for context isolation without specialized behavior. The main agent can delegate a complex multi-step task to this subagent and get a concise result back without bloat from intermediate tool calls.

<Card title="Example">
  Instead of the main agent making 10 web searches and filling its context with results, it delegates to the general-purpose subagent: `task(name="general-purpose", task="Research quantum computing trends")`. The subagent performs all the searches internally and returns only a summary.
</Card>

## Best practices

### Write clear descriptions

The main agent uses descriptions to decide which subagent to call. Be specific:

✅ **Good:** `"Analyzes financial data and generates investment insights with confidence scores"`

❌ **Bad:** `"Does finance stuff"`

### Keep system prompts detailed

Include specific guidance on how to use tools and format outputs:
```

Example 3 (unknown):
```unknown
### Minimize tool sets

Only give subagents the tools they need. This improves focus and security:
```

Example 4 (unknown):
```unknown
### Choose models by task

Different models excel at different tasks:
```

---

## Subgraphs

**URL:** llms-txt#subgraphs

**Contents:**
- Setup
- Invoke a graph from a node
- Add a graph as a node
- Add persistence
- View subgraph state
- Stream subgraph outputs

Source: https://docs.langchain.com/oss/javascript/langgraph/use-subgraphs

This guide explains the mechanics of using subgraphs. A subgraph is a [graph](/oss/javascript/langgraph/graph-api#graphs) that is used as a [node](/oss/javascript/langgraph/graph-api#nodes) in another graph.

Subgraphs are useful for:

* Building [multi-agent systems](/oss/javascript/langchain/multi-agent)
* Re-using a set of nodes in multiple graphs
* Distributing development: when you want different teams to work on different parts of the graph independently, you can define each part as a subgraph, and as long as the subgraph interface (the input and output schemas) is respected, the parent graph can be built without knowing any details of the subgraph

When adding subgraphs, you need to define how the parent graph and the subgraph communicate:

* [Invoke a graph from a node](#invoke-a-graph-from-a-node) — subgraphs are called from inside a node in the parent graph
* [Add a graph as a node](#add-a-graph-as-a-node) — a subgraph is added directly as a node in the parent and **shares [state keys](/oss/javascript/langgraph/graph-api#state)** with the parent

<Tip>
  **Set up LangSmith for LangGraph development**
  Sign up for [LangSmith](https://smith.langchain.com) to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph — read more about how to get started [here](https://docs.smith.langchain.com).
</Tip>

## Invoke a graph from a node

A simple way to implement a subgraph is to invoke a graph from inside the node of another graph. In this case subgraphs can have **completely different schemas** from the parent graph (no shared keys). For example, you might want to keep a private message history for each of the agents in a [multi-agent](/oss/javascript/langchain/multi-agent) system.

If that's the case for your application, you need to define a node **function that invokes the subgraph**. This function needs to transform the input (parent) state to the subgraph state before invoking the subgraph, and transform the results back to the parent state before returning the state update from the node.

<Accordion title="Full example: different state schemas">

1. Transform the state to the subgraph state
  2. Transform response back to the parent state

<Accordion title="Full example: different state schemas (two levels of subgraphs)">
  This is an example with two levels of subgraphs: parent -> child -> grandchild.

1. We're transforming the state from the child state channels (`myChildKey`) to the grandchild state channels (`myGrandchildKey`)
  2. We're transforming the state from the grandchild state channels (`myGrandchildKey`) back to the child state channels (`myChildKey`)
  3. We're passing a function here instead of just compiled graph (`grandchildGraph`)
  4. We're transforming the state from the parent state channels (`myKey`) to the child state channels (`myChildKey`)
  5. We're transforming the state from the child state channels (`myChildKey`) back to the parent state channels (`myKey`)
  6. We're passing a function here instead of just a compiled graph (`childGraph`)

## Add a graph as a node

When the parent graph and subgraph can communicate over a shared state key (channel) in the [schema](/oss/javascript/langgraph/graph-api#state), you can add a graph as a [node](/oss/javascript/langgraph/graph-api#nodes) in another graph. For example, in [multi-agent](/oss/javascript/langchain/multi-agent) systems, the agents often communicate over a shared [messages](/oss/javascript/langgraph/graph-api#why-use-messages) key.

<img src="https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/subgraph.png?fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=c280df5c968cd4237b0b5d03823d8946" alt="SQL agent graph" style={{ height: "450px" }} data-og-width="1177" width="1177" data-og-height="818" height="818" data-path="oss/images/subgraph.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/subgraph.png?w=280&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=e3d08dae8fb81e15b4d8069a48999eac 280w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/subgraph.png?w=560&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=8d8942031ba051119e0cb772ef697e0b 560w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/subgraph.png?w=840&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=0d5285bd104c542fe660bc09fed53e5e 840w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/subgraph.png?w=1100&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=32bc8ffa0eda13a0f3bb163631774a60 1100w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/subgraph.png?w=1650&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=6a511f3b9dc44383614803d32390875a 1650w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/subgraph.png?w=2500&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=169d55e154e5ea0146a57373235f768e 2500w" />

If your subgraph shares state keys with the parent graph, you can follow these steps to add it to your graph:

1. Define the subgraph workflow (`subgraphBuilder` in the example below) and compile it
2. Pass compiled subgraph to the `.addNode` method when defining the parent graph workflow

<Accordion title="Full example: shared state schemas">

1. This key is shared with the parent graph state
  2. This key is private to the `SubgraphState` and is not visible to the parent graph

You only need to **provide the checkpointer when compiling the parent graph**. LangGraph will automatically propagate the checkpointer to the child subgraphs.

If you want the subgraph to **have its own memory**, you can compile it with the appropriate checkpointer option. This is useful in [multi-agent](/oss/javascript/langchain/multi-agent) systems, if you want agents to keep track of their internal message histories:

## View subgraph state

When you enable [persistence](/oss/javascript/langgraph/persistence), you can [inspect the graph state](/oss/javascript/langgraph/persistence#checkpoints) (checkpoint) via the appropriate method. To view the subgraph state, you can use the subgraphs option.

You can inspect the graph state via `graph.getState(config)`. To view the subgraph state, you can use `graph.getState(config, { subgraphs: true })`.

<Warning>
  **Available **only** when interrupted**
  Subgraph state can only be viewed **when the subgraph is interrupted**. Once you resume the graph, you won't be able to access the subgraph state.
</Warning>

<Accordion title="View interrupted subgraph state">
  
</Accordion>

## Stream subgraph outputs

To include outputs from subgraphs in the streamed outputs, you can set the subgraphs option in the stream method of the parent graph. This will stream outputs from both the parent graph and any subgraphs.

1. Set `subgraphs: true` to stream outputs from subgraphs.

<Accordion title="Stream from subgraphs">

1. Set `subgraphs: true` to stream outputs from subgraphs.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/use-subgraphs.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
<Tip>
  **Set up LangSmith for LangGraph development**
  Sign up for [LangSmith](https://smith.langchain.com) to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph — read more about how to get started [here](https://docs.smith.langchain.com).
</Tip>

## Invoke a graph from a node

A simple way to implement a subgraph is to invoke a graph from inside the node of another graph. In this case subgraphs can have **completely different schemas** from the parent graph (no shared keys). For example, you might want to keep a private message history for each of the agents in a [multi-agent](/oss/javascript/langchain/multi-agent) system.

If that's the case for your application, you need to define a node **function that invokes the subgraph**. This function needs to transform the input (parent) state to the subgraph state before invoking the subgraph, and transform the results back to the parent state before returning the state update from the node.
```

Example 2 (unknown):
```unknown
<Accordion title="Full example: different state schemas">
```

Example 3 (unknown):
```unknown
1. Transform the state to the subgraph state
  2. Transform response back to the parent state
```

Example 4 (unknown):
```unknown
</Accordion>

<Accordion title="Full example: different state schemas (two levels of subgraphs)">
  This is an example with two levels of subgraphs: parent -> child -> grandchild.
```

---

## Subgraph

**URL:** llms-txt#subgraph

def subgraph_node_1(state: State):
    return {"foo": state["foo"] + "bar"}

subgraph_builder = StateGraph(State)
subgraph_builder.add_node(subgraph_node_1)
subgraph_builder.add_edge(START, "subgraph_node_1")
subgraph = subgraph_builder.compile()

---

## Subsequent calls use the cache

**URL:** llms-txt#subsequent-calls-use-the-cache

**Contents:**
- All embedding models

tic = time.time()
print(cached_embedder.embed_query("Hello, world!"))
print(f"Second call took: {time.time() - tic:.2f} seconds")
```

In production, you would typically use a more robust persistent store, such as a database or cloud storage. Please see [stores integrations](/oss/python/integrations/stores/) for options.

## All embedding models

<Columns cols={3}>
  <Card title="Aleph Alpha" icon="link" href="/oss/python/integrations/text_embedding/aleph_alpha" arrow="true" cta="View guide" />

<Card title="Anyscale" icon="link" href="/oss/python/integrations/text_embedding/anyscale" arrow="true" cta="View guide" />

<Card title="Ascend" icon="link" href="/oss/python/integrations/text_embedding/ascend" arrow="true" cta="View guide" />

<Card title="AI/ML API" icon="link" href="/oss/python/integrations/text_embedding/aimlapi" arrow="true" cta="View guide" />

<Card title="AwaDB" icon="link" href="/oss/python/integrations/text_embedding/awadb" arrow="true" cta="View guide" />

<Card title="AzureOpenAI" icon="link" href="/oss/python/integrations/text_embedding/azure_openai" arrow="true" cta="View guide" />

<Card title="Baichuan Text Embeddings" icon="link" href="/oss/python/integrations/text_embedding/baichuan" arrow="true" cta="View guide" />

<Card title="Baidu Qianfan" icon="link" href="/oss/python/integrations/text_embedding/baidu_qianfan_endpoint" arrow="true" cta="View guide" />

<Card title="Baseten" icon="link" href="/oss/python/integrations/text_embedding/baseten" arrow="true" cta="View guide" />

<Card title="Bedrock" icon="link" href="/oss/python/integrations/text_embedding/bedrock" arrow="true" cta="View guide" />

<Card title="BGE on Hugging Face" icon="link" href="/oss/python/integrations/text_embedding/bge_huggingface" arrow="true" cta="View guide" />

<Card title="Bookend AI" icon="link" href="/oss/python/integrations/text_embedding/bookend" arrow="true" cta="View guide" />

<Card title="Clarifai" icon="link" href="/oss/python/integrations/text_embedding/clarifai" arrow="true" cta="View guide" />

<Card title="Cloudflare Workers AI" icon="link" href="/oss/python/integrations/text_embedding/cloudflare_workersai" arrow="true" cta="View guide" />

<Card title="Clova Embeddings" icon="link" href="/oss/python/integrations/text_embedding/clova" arrow="true" cta="View guide" />

<Card title="Cohere" icon="link" href="/oss/python/integrations/text_embedding/cohere" arrow="true" cta="View guide" />

<Card title="DashScope" icon="link" href="/oss/python/integrations/text_embedding/dashscope" arrow="true" cta="View guide" />

<Card title="Databricks" icon="link" href="/oss/python/integrations/text_embedding/databricks" arrow="true" cta="View guide" />

<Card title="DeepInfra" icon="link" href="/oss/python/integrations/text_embedding/deepinfra" arrow="true" cta="View guide" />

<Card title="EDEN AI" icon="link" href="/oss/python/integrations/text_embedding/edenai" arrow="true" cta="View guide" />

<Card title="Elasticsearch" icon="link" href="/oss/python/integrations/text_embedding/elasticsearch" arrow="true" cta="View guide" />

<Card title="Embaas" icon="link" href="/oss/python/integrations/text_embedding/embaas" arrow="true" cta="View guide" />

<Card title="Fake Embeddings" icon="link" href="/oss/python/integrations/text_embedding/fake" arrow="true" cta="View guide" />

<Card title="FastEmbed by Qdrant" icon="link" href="/oss/python/integrations/text_embedding/fastembed" arrow="true" cta="View guide" />

<Card title="Fireworks" icon="link" href="/oss/python/integrations/text_embedding/fireworks" arrow="true" cta="View guide" />

<Card title="Google Gemini" icon="link" href="/oss/python/integrations/text_embedding/google_generative_ai" arrow="true" cta="View guide" />

<Card title="Google Vertex AI" icon="link" href="/oss/python/integrations/text_embedding/google_vertex_ai_palm" arrow="true" cta="View guide" />

<Card title="GPT4All" icon="link" href="/oss/python/integrations/text_embedding/gpt4all" arrow="true" cta="View guide" />

<Card title="Gradient" icon="link" href="/oss/python/integrations/text_embedding/gradient" arrow="true" cta="View guide" />

<Card title="GreenNode" icon="link" href="/oss/python/integrations/text_embedding/greennode" arrow="true" cta="View guide" />

<Card title="Hugging Face" icon="link" href="/oss/python/integrations/text_embedding/huggingfacehub" arrow="true" cta="View guide" />

<Card title="IBM watsonx.ai" icon="link" href="/oss/python/integrations/text_embedding/ibm_watsonx" arrow="true" cta="View guide" />

<Card title="Infinity" icon="link" href="/oss/python/integrations/text_embedding/infinity" arrow="true" cta="View guide" />

<Card title="Instruct Embeddings" icon="link" href="/oss/python/integrations/text_embedding/instruct_embeddings" arrow="true" cta="View guide" />

<Card title="IPEX-LLM CPU" icon="link" href="/oss/python/integrations/text_embedding/ipex_llm" arrow="true" cta="View guide" />

<Card title="IPEX-LLM GPU" icon="link" href="/oss/python/integrations/text_embedding/ipex_llm_gpu" arrow="true" cta="View guide" />

<Card title="Intel Extension for Transformers" icon="link" href="/oss/python/integrations/text_embedding/itrex" arrow="true" cta="View guide" />

<Card title="Jina" icon="link" href="/oss/python/integrations/text_embedding/jina" arrow="true" cta="View guide" />

<Card title="John Snow Labs" icon="link" href="/oss/python/integrations/text_embedding/johnsnowlabs_embedding" arrow="true" cta="View guide" />

<Card title="LASER" icon="link" href="/oss/python/integrations/text_embedding/laser" arrow="true" cta="View guide" />

<Card title="Lindorm" icon="link" href="/oss/python/integrations/text_embedding/lindorm" arrow="true" cta="View guide" />

<Card title="Llama.cpp" icon="link" href="/oss/python/integrations/text_embedding/llamacpp" arrow="true" cta="View guide" />

<Card title="LLMRails" icon="link" href="/oss/python/integrations/text_embedding/llm_rails" arrow="true" cta="View guide" />

<Card title="LocalAI" icon="link" href="/oss/python/integrations/text_embedding/localai" arrow="true" cta="View guide" />

<Card title="MiniMax" icon="link" href="/oss/python/integrations/text_embedding/minimax" arrow="true" cta="View guide" />

<Card title="MistralAI" icon="link" href="/oss/python/integrations/text_embedding/mistralai" arrow="true" cta="View guide" />

<Card title="Model2Vec" icon="link" href="/oss/python/integrations/text_embedding/model2vec" arrow="true" cta="View guide" />

<Card title="ModelScope" icon="link" href="/oss/python/integrations/text_embedding/modelscope_embedding" arrow="true" cta="View guide" />

<Card title="MosaicML" icon="link" href="/oss/python/integrations/text_embedding/mosaicml" arrow="true" cta="View guide" />

<Card title="Naver" icon="link" href="/oss/python/integrations/text_embedding/naver" arrow="true" cta="View guide" />

<Card title="Nebius" icon="link" href="/oss/python/integrations/text_embedding/nebius" arrow="true" cta="View guide" />

<Card title="Netmind" icon="link" href="/oss/python/integrations/text_embedding/netmind" arrow="true" cta="View guide" />

<Card title="NLP Cloud" icon="link" href="/oss/python/integrations/text_embedding/nlp_cloud" arrow="true" cta="View guide" />

<Card title="Nomic" icon="link" href="/oss/python/integrations/text_embedding/nomic" arrow="true" cta="View guide" />

<Card title="NVIDIA NIMs" icon="link" href="/oss/python/integrations/text_embedding/nvidia_ai_endpoints" arrow="true" cta="View guide" />

<Card title="Oracle Cloud Infrastructure" icon="link" href="/oss/python/integrations/text_embedding/oci_generative_ai" arrow="true" cta="View guide" />

<Card title="Ollama" icon="link" href="/oss/python/integrations/text_embedding/ollama" arrow="true" cta="View guide" />

<Card title="OpenClip" icon="link" href="/oss/python/integrations/text_embedding/open_clip" arrow="true" cta="View guide" />

<Card title="OpenAI" icon="link" href="/oss/python/integrations/text_embedding/openai" arrow="true" cta="View guide" />

<Card title="OpenVINO" icon="link" href="/oss/python/integrations/text_embedding/openvino" arrow="true" cta="View guide" />

<Card title="Optimum Intel" icon="link" href="/oss/python/integrations/text_embedding/optimum_intel" arrow="true" cta="View guide" />

<Card title="Oracle AI Vector Search" icon="link" href="/oss/python/integrations/text_embedding/oracleai" arrow="true" cta="View guide" />

<Card title="OVHcloud" icon="link" href="/oss/python/integrations/text_embedding/ovhcloud" arrow="true" cta="View guide" />

<Card title="Pinecone Embeddings" icon="link" href="/oss/python/integrations/text_embedding/pinecone" arrow="true" cta="View guide" />

<Card title="PredictionGuard" icon="link" href="/oss/python/integrations/text_embedding/predictionguard" arrow="true" cta="View guide" />

<Card title="PremAI" icon="link" href="/oss/python/integrations/text_embedding/premai" arrow="true" cta="View guide" />

<Card title="SageMaker" icon="link" href="/oss/python/integrations/text_embedding/sagemaker-endpoint" arrow="true" cta="View guide" />

<Card title="SambaNova" icon="link" href="/oss/python/integrations/text_embedding/sambanova" arrow="true" cta="View guide" />

<Card title="Self Hosted" icon="link" href="/oss/python/integrations/text_embedding/self-hosted" arrow="true" cta="View guide" />

<Card title="Sentence Transformers" icon="link" href="/oss/python/integrations/text_embedding/sentence_transformers" arrow="true" cta="View guide" />

<Card title="Solar" icon="link" href="/oss/python/integrations/text_embedding/solar" arrow="true" cta="View guide" />

<Card title="SpaCy" icon="link" href="/oss/python/integrations/text_embedding/spacy_embedding" arrow="true" cta="View guide" />

<Card title="SparkLLM" icon="link" href="/oss/python/integrations/text_embedding/sparkllm" arrow="true" cta="View guide" />

<Card title="TensorFlow Hub" icon="link" href="/oss/python/integrations/text_embedding/tensorflowhub" arrow="true" cta="View guide" />

<Card title="Text Embeddings Inference" icon="link" href="/oss/python/integrations/text_embedding/text_embeddings_inference" arrow="true" cta="View guide" />

<Card title="TextEmbed" icon="link" href="/oss/python/integrations/text_embedding/textembed" arrow="true" cta="View guide" />

<Card title="Titan Takeoff" icon="link" href="/oss/python/integrations/text_embedding/titan_takeoff" arrow="true" cta="View guide" />

<Card title="Together AI" icon="link" href="/oss/python/integrations/text_embedding/together" arrow="true" cta="View guide" />

<Card title="Upstage" icon="link" href="/oss/python/integrations/text_embedding/upstage" arrow="true" cta="View guide" />

<Card title="Volc Engine" icon="link" href="/oss/python/integrations/text_embedding/volcengine" arrow="true" cta="View guide" />

<Card title="Voyage AI" icon="link" href="/oss/python/integrations/text_embedding/voyageai" arrow="true" cta="View guide" />

<Card title="Xinference" icon="link" href="/oss/python/integrations/text_embedding/xinference" arrow="true" cta="View guide" />

<Card title="YandexGPT" icon="link" href="/oss/python/integrations/text_embedding/yandex" arrow="true" cta="View guide" />

<Card title="ZhipuAI" icon="link" href="/oss/python/integrations/text_embedding/zhipuai" arrow="true" cta="View guide" />
</Columns>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/text_embedding/index.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Supported tools

**URL:** llms-txt#supported-tools

Source: https://docs.langchain.com/langsmith/agent-builder-tools

Use these built-in tools to give your agents access to email, calendars, chat, project management, search, social, and general web utilities.

<Callout icon="circle-info" color="#DCFCE7" iconType="regular">
  Google, Slack, Linear, and LinkedIn use OAuth. Exa, Tavily, and Twitter/X use workspace secrets.
</Callout>

<CardGroup cols={2}>
  <Card title="Gmail" icon="google">
    Read and send email

<ul>
      <li>Read emails (optionally include body, filter with search)</li>
      <li>Send email or reply to an existing message</li>
      <li>Create draft emails</li>
      <li>Mark messages as read</li>
      <li>Get a conversation thread</li>
      <li>Apply or create labels</li>
      <li>List mailbox labels</li>
    </ul>
  </Card>

<Card title="Slack" icon="slack">
    Send and read messages

<ul>
      <li>Send a direct message to a user</li>
      <li>Post a message to a channel</li>
      <li>Reply in a thread</li>
      <li>Read channel history</li>
      <li>Read thread messages</li>
    </ul>
  </Card>

<div style={{ position: 'relative' }}>
    <Card title="Search" icon="magnifying-glass">
      <ul>
        <li>Exa web search (optionally fetch page contents)</li>
        <li>Exa LinkedIn profile search</li>
        <li>Tavily web search</li>
      </ul>
    </Card>

<div style={{ position: 'absolute', top: 16, right: 16 }}>
      <Tooltip tip="Exa: EXA_API_KEY; Tavily: TAVILY_API_KEY">
        <Icon icon="key" size={16} />
      </Tooltip>
    </div>
  </div>

<Card title="LinkedIn" icon="linkedin">
    Post to profile

<ul>
      <li>Publish a post with optional image or link</li>
    </ul>
  </Card>

<Card title="Google Calendar" icon="google">
    Manage events

<ul>
      <li>List events for a date</li>
      <li>Get event details</li>
      <li>Create new events</li>
    </ul>
  </Card>

<Card title="Linear" icon="list-check">
    Manage issues and teams

<ul>
      <li>List teams and team members</li>
      <li>List issues with filters</li>
      <li>Get issue details</li>
      <li>Create, update, or delete issues</li>
    </ul>
  </Card>

<div style={{ position: 'relative' }}>
    <Card title="Twitter/X" icon="twitter">
      <ul>
        <li>Read a tweet by ID</li>
        <li>Read recent posts from a list</li>
      </ul>
    </Card>

<div style={{ position: 'absolute', top: 16, right: 16 }}>
      <Tooltip tip="Required keys: TWITTER_API_KEY, TWITTER_API_KEY_SECRET">
        <Icon icon="key" size={16} />
      </Tooltip>
    </div>
  </div>

<Card title="Web utilities" icon="globe">
    <ul>
      <li>Read webpage text content</li>
      <li>Extract image URLs and metadata</li>
      <li>Notify user (for confirmations/updates)</li>
    </ul>
  </Card>
</CardGroup>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/agent-builder-tools.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## System prompt to steer the agent to be an expert researcher

**URL:** llms-txt#system-prompt-to-steer-the-agent-to-be-an-expert-researcher

**Contents:**
- `internet_search`
  - Step 5: Run the agent

research_instructions = """You are an expert researcher. Your job is to conduct thorough research and then write a polished report.

You have access to an internet search tool as your primary means of gathering information.

Use this to run an internet search for a given query. You can specify the max number of results to return, the topic, and whether raw content should be included.
"""

agent = create_deep_agent(
    tools=[internet_search],
    system_prompt=research_instructions
)
python  theme={null}
result = agent.invoke({"messages": [{"role": "user", "content": "What is langgraph?"}]})

**Examples:**

Example 1 (unknown):
```unknown
### Step 5: Run the agent
```

---

## Target function

**URL:** llms-txt#target-function

async def run_graph(inputs: dict) -> dict:
    """Run graph and track the trajectory it takes along with the final response."""
    result = await graph.ainvoke({"messages": [
        { "role": "user", "content": inputs['question']},
    ]}, config={"env": "test"})
    return {"response": result["followup"]}

---

## Target function for running the relevant step

**URL:** llms-txt#target-function-for-running-the-relevant-step

async def run_intent_classifier(inputs: dict) -> dict:
    # Note that we can access and run the intent_classifier node of our graph directly.
    command = await graph.nodes['intent_classifier'].ainvoke(inputs)
    return {"route": command.goto}

---

## Test a ReAct agent with Pytest/Vitest and LangSmith

**URL:** llms-txt#test-a-react-agent-with-pytest/vitest-and-langsmith

**Contents:**
- Setup
  - Installation
  - Environment variables
- Create your app
  - Define tools
  - Define agent
- Write tests
  - Test 1: Handle off-topic questions
  - Test 2: Simple tool calling
  - Test 3: Complex tool calling

Source: https://docs.langchain.com/langsmith/test-react-agent-pytest

This tutorial will show you how to use LangSmith's integrations with popular testing tools (Pytest, Vitest, and Jest) to evaluate your LLM application. We will create a ReAct agent that answers questions about publicly traded stocks and write a comprehensive test suite for it.

This tutorial uses [LangGraph](https://langchain-ai.github.io/langgraph/tutorials/introduction/) for agent orchestration, [OpenAI's GPT-4o](https://platform.openai.com/docs/models#gpt-4o), [Tavily](https://tavily.com/) for search, [E2B's](https://e2b.dev/) code interpreter, and [Polygon](https://polygon.io/stocks) to retrieve stock data but it can be adapted for other frameworks, models and tools with minor modifications. Tavily, E2B and Polygon are free to sign up for.

First, install the packages required for making the agent:

Next, install the testing framework:

### Environment variables

Set the following environment variables:

To define our React agent, we will use LangGraph/LangGraph.js for the orchestation and LangChain for the LLM and tools.

First we are going to define the tools we are going to use in our agent. There are going to be 3 tools:

* A search tool using Tavily
* A code interpreter tool using E2B
* A stock information tool using Polygon

Now that we have defined all of our tools, we can use [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent) to create our agent.

Now that we have defined our agent, let's write a few tests to ensure basic functionality. In this tutorial we are going to test whether the agent's tool calling abilities are working, whether the agent knows to ignore irrelevant questions, and whether it is able to answer complex questions that involve using all of the tools.

We need to first set up a test file and add the imports needed at the top of the file.

### Test 1: Handle off-topic questions

The first test will be a simple check that the agent does not use tools on irrelevant queries.

### Test 2: Simple tool calling

For tool calling, we are going to verify that the agent calls the correct tool with the correct parameters.

### Test 3: Complex tool calling

Some tool calls are easier to test than others. With the ticker lookup, we can assert that the correct ticker is searched. With the coding tool, the inputs and outputs of the tool are much less constrained, and there are lots of ways to get to the right answer. In this case, it's simpler to test that the tool is used correctly by running the full agent and asserting that it both calls the coding tool and that it ends up with the right answer.

### Test 4: LLM-as-a-judge

We are going to ensure that the agent's answer is grounded in the search results by running an LLM-as-a-judge evaluation. In order to trace the LLM as a judge call separately from our agent, we will use the LangSmith provided `trace_feedback` context manager in Python and `wrapEvaluator` function in JS/TS.

Once you have setup your config files (if you are using Vitest or Jest), you can run your tests using the following commands:

<Accordion title="Config files for Vitest/Jest">
  <CodeGroup>

</CodeGroup>
</Accordion>

Remember to also add the config files for [Vitest](#config-files-for-vitestjest) and [Jest](#config-files-for-vitestjest) to your project.

<Accordion title="Agent code">
  <CodeGroup>

</CodeGroup>
</Accordion>

<Accordion title="Test code">
  <CodeGroup>

</CodeGroup>
</Accordion>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/test-react-agent-pytest.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

Next, install the testing framework:

<CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown

```

---

## Test

**URL:** llms-txt#test

**Contents:**
- Prerequisites
- Getting started
- Testing individual nodes and edges
- Partial execution

Source: https://docs.langchain.com/oss/javascript/langgraph/test

After you've prototyped your LangGraph agent, a natural next step is to add tests. This guide covers some useful patterns you can use when writing unit tests.

Note that this guide is LangGraph-specific and covers scenarios around graphs with custom structures - if you are just getting started, check out [this section](/oss/javascript/langchain/test/) that uses LangChain's built-in @\[`create_agent`] instead.

First, make sure you have [`vitest`](https://vitest.dev/) installed:

Because many LangGraph agents depend on state, a useful pattern is to create your graph before each test where you use it, then compile it within tests with a new checkpointer instance.

The below example shows how this works with a simple, linear graph that progresses through `node1` and `node2`. Each node updates the single state key `my_key`:

## Testing individual nodes and edges

Compiled LangGraph agents expose references to each individual node as `graph.nodes`. You can take advantage of this to test individual nodes within your agent. Note that this will bypass any checkpointers passed when compiling the graph:

For agents made up of larger graphs, you may wish to test partial execution paths within your agent rather than the entire flow end-to-end. In some cases, it may make semantic sense to [restructure these sections as subgraphs](/oss/javascript/langgraph/use-subgraphs), which you can invoke in isolation as normal.

However, if you do not wish to make changes to your agent graph's overall structure, you can use LangGraph's persistence mechanisms to simulate a state where your agent is paused right before the beginning of the desired section, and will pause again at the end of the desired section. The steps are as follows:

1. Compile your agent with a checkpointer (the in-memory checkpointer [`MemorySaver`](https://reference.langchain.com/javascript/classes/_langchain_langgraph-checkpoint.MemorySaver.html) will suffice for testing).
2. Call your agent's [`update_state`](/oss/javascript/langgraph/use-time-travel) method with an [`asNode`](/oss/javascript/langgraph/persistence#as-node) parameter set to the name of the node *before* the one you want to start your test.
3. Invoke your agent with the same `thread_id` you used to update the state and an `interruptBefore` parameter set to the name of the node you want to stop at.

Here's an example that executes only the second and third nodes in a linear graph:

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/test.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
## Getting started

Because many LangGraph agents depend on state, a useful pattern is to create your graph before each test where you use it, then compile it within tests with a new checkpointer instance.

The below example shows how this works with a simple, linear graph that progresses through `node1` and `node2`. Each node updates the single state key `my_key`:
```

Example 2 (unknown):
```unknown
## Testing individual nodes and edges

Compiled LangGraph agents expose references to each individual node as `graph.nodes`. You can take advantage of this to test individual nodes within your agent. Note that this will bypass any checkpointers passed when compiling the graph:
```

Example 3 (unknown):
```unknown
## Partial execution

For agents made up of larger graphs, you may wish to test partial execution paths within your agent rather than the entire flow end-to-end. In some cases, it may make semantic sense to [restructure these sections as subgraphs](/oss/javascript/langgraph/use-subgraphs), which you can invoke in isolation as normal.

However, if you do not wish to make changes to your agent graph's overall structure, you can use LangGraph's persistence mechanisms to simulate a state where your agent is paused right before the beginning of the desired section, and will pause again at the end of the desired section. The steps are as follows:

1. Compile your agent with a checkpointer (the in-memory checkpointer [`MemorySaver`](https://reference.langchain.com/javascript/classes/_langchain_langgraph-checkpoint.MemorySaver.html) will suffice for testing).
2. Call your agent's [`update_state`](/oss/javascript/langgraph/use-time-travel) method with an [`asNode`](/oss/javascript/langgraph/persistence#as-node) parameter set to the name of the node *before* the one you want to start your test.
3. Invoke your agent with the same `thread_id` you used to update the state and an `interruptBefore` parameter set to the name of the node you want to stop at.

Here's an example that executes only the second and third nodes in a linear graph:
```

---

## Test multi-turn conversations

**URL:** llms-txt#test-multi-turn-conversations

**Contents:**
- From an existing run
- From a dataset
- Manually
- Next steps

Source: https://docs.langchain.com/langsmith/multiple-messages

This how-to guide walks you through the various ways you can set up the playground for multi-turn conversations, which will allow you to test different tool configurations and system prompts against longer threads of messages.

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiturn-diagram.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=8d468f069fe0ee6eac2e95c8942990aa" alt="" data-og-width="963" width="963" data-og-height="552" height="552" data-path="langsmith/images/multiturn-diagram.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiturn-diagram.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=ee61b9e81315c0b78ca5de6edb93f303 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiturn-diagram.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=ae0c7a1b4e68b46ef29469a2dc04610e 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiturn-diagram.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=9ce886cc977da99e442bf0ffeb82b683 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiturn-diagram.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=b4365db9abb545cf507c7d3333404ffa 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiturn-diagram.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=fd3ff5634b263980b0f9b9d97756d2b8 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiturn-diagram.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=c28271d3bc0988362e988a64c1f1b396 2500w" />

## From an existing run

First, ensure you have properly [traced](/langsmith/observability) a multi-turn conversation, and then navigate to your tracing project. Once you get to your tracing project simply open the run, select the LLM call, and open it in the playground as follows:

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiturn-from-run.gif?s=b4918bc6c6fac9c71859d962495db053" alt="" data-og-width="1632" width="1632" data-og-height="1080" height="1080" data-path="langsmith/images/multiturn-from-run.gif" data-optimize="true" data-opv="3" />

You can then edit the system prompt, tweak the tools and/or output schema and observe how the output of the multi-turn conversation changes.

Before starting, make sure you have [set up your dataset](/langsmith/manage-datasets-in-application). Since you want to evaluate multi-turn conversations, make sure there is a key in your inputs that contains a list of messages.

Once you have created your dataset, head to the playground and [load your dataset](/langsmith/manage-datasets-in-application#from-the-prompt-playground) to evaluate.

Then, add a messages list variable to your prompt, making sure to name it the same as the key in your inputs that contains the list of messages:

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiturn-from-dataset.gif?s=42e2f11a348f50a7d2a0c8b6630c57e9" alt="" data-og-width="1632" width="1632" data-og-height="1080" height="1080" data-path="langsmith/images/multiturn-from-dataset.gif" data-optimize="true" data-opv="3" />

When you run your prompt, the messages from each example will be added as a list in place of the 'Messages List' variable.

There are two ways to manually create multi-turn conversations. The first way is by simply appending messages to the prompt:

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiturn-manual.gif?s=1278a29854a66ee3dec92cc6f5059da0" alt="" data-og-width="1632" width="1632" data-og-height="1080" height="1080" data-path="langsmith/images/multiturn-manual.gif" data-optimize="true" data-opv="3" />

This is helpful for quick iteration, but is rigid since the multi-turn conversation is hardcoded. Instead, if you want your prompt to work with any multi-turn conversation you can add a 'Messages List' variable and add your multi-turn conversation there:

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiturn-manual-list.gif?s=12458af5558482bdfe40855c3117c02b" alt="" data-og-width="1632" width="1632" data-og-height="1080" height="1080" data-path="langsmith/images/multiturn-manual-list.gif" data-optimize="true" data-opv="3" />

This allows you to just tweak the system prompt or the tools, while allowing any multi-turn conversation to take the place of the `Messages List` variable, allowing you to reuse this prompt across various runs.

Now that you know how to set up the playground for multi-turn interactions, you can either manually inspect and judge the outputs, or you can [add evaluators](/langsmith/code-evaluator) to classify results.

You can also read [these how-to guides](/langsmith/create-a-prompt) to learn more about how to use the playground to run evaluations.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/multiple-messages.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Test the graph with a valid input

**URL:** llms-txt#test-the-graph-with-a-valid-input

**Contents:**
- Add runtime configuration

graph.invoke({"a": "hello"})
python  theme={null}
try:
    graph.invoke({"a": 123})  # Should be a string
except Exception as e:
    print("An exception was raised because `a` is an integer rather than a string.")
    print(e)

An exception was raised because `a` is an integer rather than a string.
1 validation error for OverallState
a
  Input should be a valid string [type=string_type, input_value=123, input_type=int]
    For further information visit https://errors.pydantic.dev/2.9/v/string_type
python  theme={null}
  from langgraph.graph import StateGraph, START, END
  from pydantic import BaseModel

class NestedModel(BaseModel):
      value: str

class ComplexState(BaseModel):
      text: str
      count: int
      nested: NestedModel

def process_node(state: ComplexState):
      # Node receives a validated Pydantic object
      print(f"Input state type: {type(state)}")
      print(f"Nested type: {type(state.nested)}")
      # Return a dictionary update
      return {"text": state.text + " processed", "count": state.count + 1}

# Build the graph
  builder = StateGraph(ComplexState)
  builder.add_node("process", process_node)
  builder.add_edge(START, "process")
  builder.add_edge("process", END)
  graph = builder.compile()

# Create a Pydantic instance for input
  input_state = ComplexState(text="hello", count=0, nested=NestedModel(value="test"))
  print(f"Input object type: {type(input_state)}")

# Invoke graph with a Pydantic instance
  result = graph.invoke(input_state)
  print(f"Output type: {type(result)}")
  print(f"Output content: {result}")

# Convert back to Pydantic model if needed
  output_model = ComplexState(**result)
  print(f"Converted back to Pydantic: {type(output_model)}")
  python  theme={null}
  from langgraph.graph import StateGraph, START, END
  from pydantic import BaseModel

class CoercionExample(BaseModel):
      # Pydantic will coerce string numbers to integers
      number: int
      # Pydantic will parse string booleans to bool
      flag: bool

def inspect_node(state: CoercionExample):
      print(f"number: {state.number} (type: {type(state.number)})")
      print(f"flag: {state.flag} (type: {type(state.flag)})")
      return {}

builder = StateGraph(CoercionExample)
  builder.add_node("inspect", inspect_node)
  builder.add_edge(START, "inspect")
  builder.add_edge("inspect", END)
  graph = builder.compile()

# Demonstrate coercion with string inputs that will be converted
  result = graph.invoke({"number": "42", "flag": "true"})

# This would fail with a validation error
  try:
      graph.invoke({"number": "not-a-number", "flag": "true"})
  except Exception as e:
      print(f"\nExpected validation error: {e}")
  python  theme={null}
  from langgraph.graph import StateGraph, START, END
  from pydantic import BaseModel
  from langchain.messages import HumanMessage, AIMessage, AnyMessage
  from typing import List

class ChatState(BaseModel):
      messages: List[AnyMessage]
      context: str

def add_message(state: ChatState):
      return {"messages": state.messages + [AIMessage(content="Hello there!")]}

builder = StateGraph(ChatState)
  builder.add_node("add_message", add_message)
  builder.add_edge(START, "add_message")
  builder.add_edge("add_message", END)
  graph = builder.compile()

# Create input with a message
  initial_state = ChatState(
      messages=[HumanMessage(content="Hi")], context="Customer support chat"
  )

result = graph.invoke(initial_state)
  print(f"Output: {result}")

# Convert back to Pydantic model to see message types
  output_model = ChatState(**result)
  for i, msg in enumerate(output_model.messages):
      print(f"Message {i}: {type(msg).__name__} - {msg.content}")
  python  theme={null}
from langgraph.graph import END, StateGraph, START
from langgraph.runtime import Runtime
from typing_extensions import TypedDict

**Examples:**

Example 1 (unknown):
```unknown
Invoke the graph with an **invalid** input
```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
See below for additional features of Pydantic model state:

<Accordion title="Serialization Behavior">
  When using Pydantic models as state schemas, it's important to understand how serialization works, especially when:

  * Passing Pydantic objects as inputs
  * Receiving outputs from the graph
  * Working with nested Pydantic models

  Let's see these behaviors in action.
```

Example 4 (unknown):
```unknown
</Accordion>

<Accordion title="Runtime Type Coercion">
  Pydantic performs runtime type coercion for certain data types. This can be helpful but also lead to unexpected behavior if you're not aware of it.
```

---

## test write permissions

**URL:** llms-txt#test-write-permissions

**Contents:**
  - Monitoring Runs
  - Common Errors

touch ./test.txt
aws s3 --endpoint-url=<endpoint_url> cp ./test.txt s3://<bucket-name>/tmp/test.txt
```

You can monitor your runs using the [List Runs API](#list-runs-for-an-export). If this is a known error, this will be added to the `errors` field of the run.

Here are some common errors:

| Error                              | Description                                                                                                                                                                                                                                                                                              |
| ---------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Access denied                      | The blob store credentials or bucket are not valid. This error occurs when the provided access key and secret key combination doesn't have the necessary permissions to access the specified bucket or perform the required operations.                                                                  |
| Bucket is not valid                | The specified blob store bucket is not valid. This error is thrown when the bucket doesn't exist or there is not enough access to perform writes on the bucket.                                                                                                                                          |
| Key ID you provided does not exist | The blob store credentials provided are not valid. This error occurs when the access key ID used for authentication is not a valid key.                                                                                                                                                                  |
| Invalid endpoint                   | The endpoint\_url provided is invalid. This error is raised when the specified endpoint is an invalid endpoint. Only S3 compatible endpoints are supported, for example `https://storage.googleapis.com` for GCS, `https://play.min.io` for minio, etc. If using AWS, you should omit the endpoint\_url. |

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/data-export.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Text block

**URL:** llms-txt#text-block

text_block = {
    "type": "text",
    "text": "Hello world",
}

---

## Text splitters

**URL:** llms-txt#text-splitters

**Contents:**
- Text structure-based
- Length-based
- Document structure-based

Source: https://docs.langchain.com/oss/javascript/integrations/splitters/index

**Text splitters** break large docs into smaller chunks that will be retrievable individually and fit within model context window limit.

There are several strategies for splitting documents, each with its own advantages.

<Tip>
  For most use cases, start with the [RecursiveCharacterTextSplitter](/oss/javascript/integrations/splitters/recursive_text_splitter). It provides a solid balance between keeping context intact and managing chunk size. This default strategy works well out of the box, and you should only consider adjusting it if you need to fine-tune performance for your specific application.
</Tip>

## Text structure-based

Text is naturally organized into hierarchical units such as paragraphs, sentences, and words. We can leverage this inherent structure to inform our splitting strategy, creating split that maintain natural language flow, maintain semantic coherence within split, and adapts to varying levels of text granularity. LangChain's `RecursiveCharacterTextSplitter` implements this concept:

* The [RecursiveCharacterTextSplitter](/oss/javascript/integrations/splitters/recursive_text_splitter) attempts to keep larger units (e.g., paragraphs) intact.
* If a unit exceeds the chunk size, it moves to the next level (e.g., sentences).
* This process continues down to the word level if necessary.

**Available text splitters**:

* [Recursively split text](/oss/javascript/integrations/splitters/recursive_text_splitter)

An intuitive strategy is to split documents based on their length. This simple yet effective approach ensures that each chunk doesn't exceed a specified size limit. Key benefits of length-based splitting:

* Straightforward implementation
* Consistent chunk sizes
* Easily adaptable to different model requirements

Types of length-based splitting:

* Token-based: Splits text based on the number of tokens, which is useful when working with language models.
* Character-based: Splits text based on the number of characters, which can be more consistent across different types of text.

Example implementation using LangChain's CharacterTextSplitter with token-based splitting:

**Available text splitters**:

* [Split by tokens](/oss/javascript/integrations/splitters/split_by_token)
* [Split by characters](/oss/javascript/integrations/splitters/character_text_splitter)

## Document structure-based

Some documents have an inherent structure, such as HTML, Markdown, or JSON files. In these cases, it's beneficial to split the document based on its structure, as it often naturally groups semantically related text. Key benefits of structure-based splitting:

* Preserves the logical organization of the document
* Maintains context within each chunk
* Can be more effective for downstream tasks like retrieval or summarization

**Available text splitters**:

* [Split code](/oss/javascript/integrations/splitters/code_splitter)

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/integrations/splitters/index.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
**Available text splitters**:

* [Recursively split text](/oss/javascript/integrations/splitters/recursive_text_splitter)

## Length-based

An intuitive strategy is to split documents based on their length. This simple yet effective approach ensures that each chunk doesn't exceed a specified size limit. Key benefits of length-based splitting:

* Straightforward implementation
* Consistent chunk sizes
* Easily adaptable to different model requirements

Types of length-based splitting:

* Token-based: Splits text based on the number of tokens, which is useful when working with language models.
* Character-based: Splits text based on the number of characters, which can be more consistent across different types of text.

Example implementation using LangChain's CharacterTextSplitter with token-based splitting:
```

---

## Then, update the runs with their end times and any outputs

**URL:** llms-txt#then,-update-the-runs-with-their-end-times-and-any-outputs

child_run_update = {
    **child_run,
    "end_time": datetime.now(timezone.utc).isoformat(),
    "outputs": {"answer": "Paris is the capital of France."},
}

parent_run_update = {
    **parent_run,
    "end_time": datetime.now(timezone.utc).isoformat(),
    "outputs": {"summary": "Discussion about France, including its capital."},
}

patches = [parent_run_update, child_run_update]
batch_ingest_runs(api_url, api_key, patches=patches)

---

## The

**URL:** llms-txt#the

---

## The above chain will be traced as a child run of the traceable function

**URL:** llms-txt#the-above-chain-will-be-traced-as-a-child-run-of-the-traceable-function

**Contents:**
- Interoperability between LangChain.JS and LangSmith SDK
  - Tracing LangChain objects inside `traceable` (JS only)
  - Tracing LangChain child runs via `traceable` / RunTree API (JS only)

@traceable(
    tags=["openai", "chat"],
    metadata={"foo": "bar"}
)
def invoke_runnnable(question, context):
    result = chain.invoke({"question": question, "context": context})
    return "The response is: " + result

invoke_runnnable("Can you summarize this morning's meetings?", "During this morning's meeting, we solved all world conflict.")
typescript  theme={null}
import { ChatOpenAI } from "@langchain/openai";
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { StringOutputParser } from "@langchain/core/output_parsers";
import { getLangchainCallbacks } from "langsmith/langchain";

const prompt = ChatPromptTemplate.fromMessages([
  [
    "system",
    "You are a helpful assistant. Please respond to the user's request only based on the given context.",
  ],
  ["user", "Question: {question}\nContext: {context}"],
]);

const model = new ChatOpenAI({ modelName: "gpt-4o-mini" });
const outputParser = new StringOutputParser();
const chain = prompt.pipe(model).pipe(outputParser);

const main = traceable(
  async (input: { question: string; context: string }) => {
    const callbacks = await getLangchainCallbacks();
    const response = await chain.invoke(input, { callbacks });
    return response;
  },
  { name: "main" }
);
typescript  theme={null}
import { traceable } from "langsmith/traceable";
import { RunnableLambda } from "@langchain/core/runnables";
import { RunnableConfig } from "@langchain/core/runnables";

const tracedChild = traceable((input: string) => `Child Run: ${input}`, {
  name: "Child Run",
});

const parrot = new RunnableLambda({
  func: async (input: { text: string }, config?: RunnableConfig) => {
    return await tracedChild(input.text);
  },
});
typescript Traceable theme={null}
  import { traceable } from "langsmith/traceable";
  import { RunnableLambda } from "@langchain/core/runnables";
  import { RunnableConfig } from "@langchain/core/runnables";

const tracedChild = traceable((input: string) => `Child Run: ${input}`, {
    name: "Child Run",
  });

const parrot = new RunnableLambda({
    func: async (input: { text: string }, config?: RunnableConfig) => {
      // Pass the config to existing traceable function
      await tracedChild(config, input.text);
      return input.text;
    },
  });
  typescript Run Tree theme={null}
  import { RunTree } from "langsmith/run_trees";
  import { RunnableLambda } from "@langchain/core/runnables";
  import { RunnableConfig } from "@langchain/core/runnables";

const parrot = new RunnableLambda({
    func: async (input: { text: string }, config?: RunnableConfig) => {
      // create the RunTree from the RunnableConfig of the RunnableLambda
      const childRunTree = RunTree.fromRunnableConfig(config, {
        name: "Child Run",
      });

childRunTree.inputs = { input: input.text };
      await childRunTree.postRun();

childRunTree.outputs = { output: `Child Run: ${input.text}` };
      await childRunTree.patchRun();

return input.text;
    },
  });
  ```
</CodeGroup>

If you prefer a video tutorial, check out the [Alternative Ways to Trace video](https://academy.langchain.com/pages/intro-to-langsmith-preview) from the Introduction to LangSmith Course.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/trace-with-langchain.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
This will produce the following trace tree: <img src="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-tree-python-interop.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=52c64fd784522c4b2d75886ae76f8c18" alt="" data-og-width="1334" width="1334" data-og-height="734" height="734" data-path="langsmith/images/trace-tree-python-interop.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-tree-python-interop.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=21a424e2326767bb66a6b5a207390bec 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-tree-python-interop.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=72f1854193fd30317d1b69d8de433d73 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-tree-python-interop.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=34fa74aff75c11172b350d38319bf276 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-tree-python-interop.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=4ebfb4252764af54033e62ad088f60b1 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-tree-python-interop.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=2d783eecf72ba0680e78a0f122bd4411 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-tree-python-interop.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=9d977793e5281e5d255d962224bd70df 2500w" />

## Interoperability between LangChain.JS and LangSmith SDK

### Tracing LangChain objects inside `traceable` (JS only)

Starting with `langchain@0.2.x`, LangChain objects are traced automatically when used inside `@traceable` functions, inheriting the client, tags, metadata and project name of the traceable function.

For older versions of LangChain below `0.2.x`, you will need to manually pass an instance `LangChainTracer` created from the tracing context found in `@traceable`.
```

Example 2 (unknown):
```unknown
### Tracing LangChain child runs via `traceable` / RunTree API (JS only)

<Note>
  We're working on improving the interoperability between `traceable` and LangChain. The following limitations are present when using combining LangChain with `traceable`:

  1. Mutating RunTree obtained from `getCurrentRunTree()` of the RunnableLambda context will result in a no-op.
  2. It's discouraged to traverse the RunTree obtained from RunnableLambda via `getCurrentRunTree()` as it may not contain all the RunTree nodes.
  3. Different child runs may have the same `execution_order` and `child_execution_order` value. Thus in extreme circumstances, some runs may end up in a different order, depending on the `start_time`.
</Note>

In some uses cases, you might want to run `traceable` functions as part of the RunnableSequence or trace child runs of LangChain run imperatively via the `RunTree` API. Starting with LangSmith 0.1.39 and @langchain/core 0.2.18, you can directly invoke `traceable`-wrapped functions within RunnableLambda.
```

Example 3 (unknown):
```unknown
<img src="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-tree-manual-tracing.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=7b117d3aa9b419fe2a314ec6d9cc7c16" alt="Trace Tree" data-og-width="2564" width="2564" data-og-height="1530" height="1530" data-path="langsmith/images/trace-tree-manual-tracing.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-tree-manual-tracing.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=517f8a525908d5241c0d635726bf2da7 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-tree-manual-tracing.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=2772298bff6569c12537d8b31cc90e78 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-tree-manual-tracing.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=4b02aa33df7ef1d18a33bb36c3e2edfe 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-tree-manual-tracing.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=e9d4021c62cf3bad95e01c8aa2895d44 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-tree-manual-tracing.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=02ef49a98151ad9fc63c742241542d94 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-tree-manual-tracing.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=765dabfe056ce5f28b1b09ca7eb735d7 2500w" />

Alternatively, you can convert LangChain's [`RunnableConfig`](https://reference.langchain.com/python/langchain_core/runnables/#langchain_core.runnables.RunnableConfig) to a equivalent RunTree object by using `RunTree.fromRunnableConfig` or pass the [`RunnableConfig`](https://reference.langchain.com/python/langchain_core/runnables/#langchain_core.runnables.RunnableConfig) as the first argument of `traceable`-wrapped function.

<CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## The agent can now track additional state beyond messages

**URL:** llms-txt#the-agent-can-now-track-additional-state-beyond-messages

**Contents:**
  - Streaming
  - Middleware

result = agent.invoke({
    "messages": [{"role": "user", "content": "I prefer technical explanations"}],
    "user_preferences": {"style": "technical", "verbosity": "detailed"},
})
python  theme={null}
for chunk in agent.stream({
    "messages": [{"role": "user", "content": "Search for AI news and summarize the findings"}]
}, stream_mode="values"):
    # Each chunk contains the full state at that point
    latest_message = chunk["messages"][-1]
    if latest_message.content:
        print(f"Agent: {latest_message.content}")
    elif latest_message.tool_calls:
        print(f"Calling tools: {[tc['name'] for tc in latest_message.tool_calls]}")
```

<Tip>
  For more details on streaming, see [Streaming](/oss/python/langchain/streaming).
</Tip>

[Middleware](/oss/python/langchain/middleware) provides powerful extensibility for customizing agent behavior at different stages of execution. You can use middleware to:

* Process state before the model is called (e.g., message trimming, context injection)
* Modify or validate the model's response (e.g., guardrails, content filtering)
* Handle tool execution errors with custom logic
* Implement dynamic model selection based on state or context
* Add custom logging, monitoring, or analytics

Middleware integrates seamlessly into the agent's execution graph, allowing you to intercept and modify data flow at key points without changing the core agent logic.

<Tip>
  For comprehensive middleware documentation including decorators like [`@before_model`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.before_model), [`@after_model`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.after_model), and [`@wrap_tool_call`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.wrap_tool_call), see [Middleware](/oss/python/langchain/middleware).
</Tip>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/agents.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
<Note>
  As of `langchain 1.0`, custom state schemas **must** be `TypedDict` types. Pydantic models and dataclasses are no longer supported. See the [v1 migration guide](/oss/python/migrate/langchain-v1#state-type-restrictions) for more details.
</Note>

<Tip>
  To learn more about memory, see [Memory](/oss/python/concepts/memory). For information on implementing long-term memory that persists across sessions, see [Long-term memory](/oss/python/langchain/long-term-memory).
</Tip>

### Streaming

We've seen how the agent can be called with `invoke` to get a final response. If the agent executes multiple steps, this may take a while. To show intermediate progress, we can stream back messages as they occur.
```

---

## The agent will use write_todos to plan and track:

**URL:** llms-txt#the-agent-will-use-write_todos-to-plan-and-track:

---

## The `authenticate` decorator tells LangGraph to call this function as middleware

**URL:** llms-txt#the-`authenticate`-decorator-tells-langgraph-to-call-this-function-as-middleware

---

## The "Auth" object is a container that LangGraph will use to mark our authentication function

**URL:** llms-txt#the-"auth"-object-is-a-container-that-langgraph-will-use-to-mark-our-authentication-function

---

## The default RetryPolicy is optimized for retrying specific network errors.

**URL:** llms-txt#the-default-retrypolicy-is-optimized-for-retrying-specific-network-errors.

**Contents:**
- Caching Tasks
- Resuming after an error

retry_policy = RetryPolicy(retry_on=ValueError)

@task(retry_policy=retry_policy)
def get_info():
    global attempts
    attempts += 1

if attempts < 2:
        raise ValueError('Failure')
    return "OK"

checkpointer = InMemorySaver()

@entrypoint(checkpointer=checkpointer)
def main(inputs, writer):
    return get_info().result()

config = {
    "configurable": {
        "thread_id": "1"
    }
}

main.invoke({'any_input': 'foobar'}, config=config)
pycon  theme={null}
'OK'
python  theme={null}
import time
from langgraph.cache.memory import InMemoryCache
from langgraph.func import entrypoint, task
from langgraph.types import CachePolicy

@task(cache_policy=CachePolicy(ttl=120))    # [!code highlight]
def slow_add(x: int) -> int:
    time.sleep(1)
    return x * 2

@entrypoint(cache=InMemoryCache())
def main(inputs: dict) -> dict[str, int]:
    result1 = slow_add(inputs["x"]).result()
    result2 = slow_add(inputs["x"]).result()
    return {"result1": result1, "result2": result2}

for chunk in main.stream({"x": 5}, stream_mode="updates"):
    print(chunk)

#> {'slow_add': 10}
#> {'slow_add': 10, '__metadata__': {'cached': True}}
#> {'main': {'result1': 10, 'result2': 10}}
python  theme={null}
import time
from langgraph.checkpoint.memory import InMemorySaver
from langgraph.func import entrypoint, task
from langgraph.types import StreamWriter

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
## Caching Tasks
```

Example 3 (unknown):
```unknown
1. `ttl` is specified in seconds. The cache will be invalidated after this time.

## Resuming after an error
```

---

## the desired output.

**URL:** llms-txt#the-desired-output.

class UserIntent(TypedDict):
    """The user's current intent in the conversation"""

intent: Literal["refund", "question_answering"]

---

## The instrucitons are passed as a system message to the agent

**URL:** llms-txt#the-instrucitons-are-passed-as-a-system-message-to-the-agent

instructions = """You are a tweet writing assistant. Given a topic, do some research and write a relevant and engaging tweet about it.
- Use at least 3 emojis in each tweet
- The tweet should be no longer than 280 characters
- Always use the search tool to gather recent information on the tweet topic
- Write the tweet only based on the search content. Do not rely on your internal knowledge
- When relevant, link to your sources
- Make your tweet as engaging as possible"""

---

## The interrupt contains the full HITL request with action_requests and review_configs

**URL:** llms-txt#the-interrupt-contains-the-full-hitl-request-with-action_requests-and-review_configs

print(result['__interrupt__'])

---

## The "messages" stream mode returns an iterator of tuples (message_chunk, metadata)

**URL:** llms-txt#the-"messages"-stream-mode-returns-an-iterator-of-tuples-(message_chunk,-metadata)

---

## The "messages" stream mode returns a tuple of (message_chunk, metadata)

**URL:** llms-txt#the-"messages"-stream-mode-returns-a-tuple-of-(message_chunk,-metadata)

---

## The metadata contains information about the LLM invocation, including the tags

**URL:** llms-txt#the-metadata-contains-information-about-the-llm-invocation,-including-the-tags

async for msg, metadata in graph.astream(
    {"topic": "cats"},
    stream_mode="messages",  # [!code highlight]
):
    # Filter the streamed tokens by the tags field in the metadata to only include
    # the tokens from the LLM invocation with the "joke" tag
    if metadata["tags"] == ["joke"]:
        print(msg.content, end="|", flush=True)
python  theme={null}
  from typing import TypedDict

from langchain.chat_models import init_chat_model
  from langgraph.graph import START, StateGraph

# The joke_model is tagged with "joke"
  joke_model = init_chat_model(model="gpt-4o-mini", tags=["joke"])
  # The poem_model is tagged with "poem"
  poem_model = init_chat_model(model="gpt-4o-mini", tags=["poem"])

class State(TypedDict):
        topic: str
        joke: str
        poem: str

async def call_model(state, config):
        topic = state["topic"]
        print("Writing joke...")
        # Note: Passing the config through explicitly is required for python < 3.11
        # Since context var support wasn't added before then: https://docs.python.org/3/library/asyncio-task.html#creating-tasks
        # The config is passed through explicitly to ensure the context vars are propagated correctly
        # This is required for Python < 3.11 when using async code. Please see the async section for more details
        joke_response = await joke_model.ainvoke(
              [{"role": "user", "content": f"Write a joke about {topic}"}],
              config,
        )
        print("\n\nWriting poem...")
        poem_response = await poem_model.ainvoke(
              [{"role": "user", "content": f"Write a short poem about {topic}"}],
              config,
        )
        return {"joke": joke_response.content, "poem": poem_response.content}

graph = (
        StateGraph(State)
        .add_node(call_model)
        .add_edge(START, "call_model")
        .compile()
  )

# The stream_mode is set to "messages" to stream LLM tokens
  # The metadata contains information about the LLM invocation, including the tags
  async for msg, metadata in graph.astream(
        {"topic": "cats"},
        stream_mode="messages",
  ):
      if metadata["tags"] == ["joke"]:
          print(msg.content, end="|", flush=True)
  python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
<Accordion title="Extended example: filtering by tags">
```

Example 2 (unknown):
```unknown
</Accordion>

#### Filter by node

To stream tokens only from specific nodes, use `stream_mode="messages"` and filter the outputs by the `langgraph_node` field in the streamed metadata:
```

---

## The overall state of the graph (this is the public state shared across nodes)

**URL:** llms-txt#the-overall-state-of-the-graph-(this-is-the-public-state-shared-across-nodes)

class OverallState(BaseModel):
    a: str

def node(state: OverallState):
    return {"a": "goodbye"}

---

## The prebuilt ReACT agent only expects State to have a 'messages' key, so the

**URL:** llms-txt#the-prebuilt-react-agent-only-expects-state-to-have-a-'messages'-key,-so-the

---

## The private data is only shared between node_1 and node_2

**URL:** llms-txt#the-private-data-is-only-shared-between-node_1-and-node_2

def node_1(state: OverallState) -> Node1Output:
    output = {"private_data": "set by node_1"}
    print(f"Entered node `node_1`:\n\tInput: {state}.\n\tReturned: {output}")
    return output

---

## The rest of your code

**URL:** llms-txt#the-rest-of-your-code

**Contents:**
- API reference

import langsmith
langsmith_client = langsmith.Client(
    api_key='<api_key>',
    api_url='http(s)://<host>/api/v1',
)
```

To access the API reference, navigate to `http://<host>/api/docs` in your browser.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/self-host-usage.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## The resume payload becomes the return value of interrupt() inside the node

**URL:** llms-txt#the-resume-payload-becomes-the-return-value-of-interrupt()-inside-the-node

**Contents:**
- Common patterns
  - Approve or reject

graph.invoke(Command(resume=True), config=config)
python  theme={null}
from typing import Literal
from langgraph.types import interrupt, Command

def approval_node(state: State) -> Command[Literal["proceed", "cancel"]]:
    # Pause execution; payload shows up under result["__interrupt__"]
    is_approved = interrupt({
        "question": "Do you want to proceed with this action?",
        "details": state["action_details"]
    })

# Route based on the response
    if is_approved:
        return Command(goto="proceed")  # Runs after the resume payload is provided
    else:
        return Command(goto="cancel")
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
**Key points about resuming:**

* You must use the **same thread ID** when resuming that was used when the interrupt occurred
* The value passed to `Command(resume=...)` becomes the return value of the [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) call
* The node restarts from the beginning of the node where the [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) was called when resumed, so any code before the [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) runs again
* You can pass any JSON-serializable value as the resume value

## Common patterns

The key thing that interrupts unlock is the ability to pause execution and wait for external input. This is useful for a variety of use cases, including:

* <Icon icon="check-circle" /> [Approval workflows](#approve-or-reject): Pause before executing critical actions (API calls, database changes, financial transactions)
* <Icon icon="pencil" /> [Review and edit](#review-and-edit-state): Let humans review and modify LLM outputs or tool calls before continuing
* <Icon icon="wrench" /> [Interrupting tool calls](#interrupts-in-tools): Pause before executing tool calls to review and edit the tool call before execution
* <Icon icon="shield-check" /> [Validating human input](#validating-human-input): Pause before proceeding to the next step to validate human input

### Approve or reject

One of the most common uses of interrupts is to pause before a critical action and ask for approval. For example, you might want to ask a human to approve an API call, a database change, or any other important decision.
```

Example 2 (unknown):
```unknown
When you resume the graph, pass `true` to approve or `false` to reject:
```

---

## The Secret Life of Socks in the Dryer

**URL:** llms-txt#the-secret-life-of-socks-in-the-dryer

**Contents:**
  - 2. Identify a checkpoint
  - 3. Update the state
  - 4. Resume execution from the checkpoint

I finally discovered where all my missing socks go after the dryer. Turns out they're not missing at all—they've just eloped with someone else's socks from the laundromat to start new lives together.

My blue argyle is now living in Bermuda with a red polka dot, posting vacation photos on Sockstagram and sending me lint as alimony.
typescript  theme={null}
// The states are returned in reverse chronological order.
const states = [];
for await (const state of graph.getStateHistory(config)) {
  states.push(state);
}

for (const state of states) {
  console.log(state.next);
  console.log(state.config.configurable?.checkpoint_id);
  console.log();
}

[]
1f02ac4a-ec9f-6524-8002-8f7b0bbeed0e

['writeJoke']
1f02ac4a-ce2a-6494-8001-cb2e2d651227

['generateTopic']
1f02ac4a-a4e0-630d-8000-b73c254ba748

['__start__']
1f02ac4a-a4dd-665e-bfff-e6c8c44315d9
typescript  theme={null}
// This is the state before last (states are listed in chronological order)
const selectedState = states[1];
console.log(selectedState.next);
console.log(selectedState.values);

['writeJoke']
{'topic': 'How about "The Secret Life of Socks in the Dryer"? You know, exploring the mysterious phenomenon of how socks go into the laundry as pairs but come out as singles. Where do they go? Are they starting new lives elsewhere? Is there a sock paradise we don\\'t know about? There\\'s a lot of comedic potential in the everyday mystery that unites us all!'}
typescript  theme={null}
const newConfig = await graph.updateState(selectedState.config, {
  topic: "chickens",
});
console.log(newConfig);

{'configurable': {'thread_id': 'c62e2e03-c27b-4cb6-8cea-ea9bfedae006', 'checkpoint_ns': '', 'checkpoint_id': '1f02ac4a-ecee-600b-8002-a1d21df32e4c'}}
typescript  theme={null}
await graph.invoke(null, newConfig);
typescript  theme={null}
{
  'topic': 'chickens',
  'joke': 'Why did the chicken join a band?\n\nBecause it had excellent drumsticks!'
}
```

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/use-time-travel.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
### 2. Identify a checkpoint
```

Example 2 (unknown):
```unknown
**Output:**
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
**Output:**
```

---

## The sky

**URL:** llms-txt#the-sky

---

## The sky is

**URL:** llms-txt#the-sky-is

---

## The sky is typically

**URL:** llms-txt#the-sky-is-typically

---

## The sky is typically blue

**URL:** llms-txt#the-sky-is-typically-blue

---

## The states are returned in reverse chronological order.

**URL:** llms-txt#the-states-are-returned-in-reverse-chronological-order.

states = list(graph.get_state_history(config))

for state in states:
    print(state.next)
    print(state.config["configurable"]["checkpoint_id"])
    print()

()
1f02ac4a-ec9f-6524-8002-8f7b0bbeed0e

('write_joke',)
1f02ac4a-ce2a-6494-8001-cb2e2d651227

('generate_topic',)
1f02ac4a-a4e0-630d-8000-b73c254ba748

('__start__',)
1f02ac4a-a4dd-665e-bfff-e6c8c44315d9
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
**Output:**
```

Example 2 (unknown):
```unknown

```

---

## The stream_mode is set to "messages" to stream LLM tokens

**URL:** llms-txt#the-stream_mode-is-set-to-"messages"-to-stream-llm-tokens

---

## The system prompt will be set dynamically based on context

**URL:** llms-txt#the-system-prompt-will-be-set-dynamically-based-on-context

**Contents:**
- Invocation
- Advanced concepts
  - Structured output

result = agent.invoke(
    {"messages": [{"role": "user", "content": "Explain machine learning"}]},
    context={"user_role": "expert"}
)
python  theme={null}
result = agent.invoke(
    {"messages": [{"role": "user", "content": "What's the weather in San Francisco?"}]}
)
python wrap theme={null}
from pydantic import BaseModel
from langchain.agents import create_agent
from langchain.agents.structured_output import ToolStrategy

class ContactInfo(BaseModel):
    name: str
    email: str
    phone: str

agent = create_agent(
    model="gpt-4o-mini",
    tools=[search_tool],
    response_format=ToolStrategy(ContactInfo)
)

result = agent.invoke({
    "messages": [{"role": "user", "content": "Extract contact info from: John Doe, john@example.com, (555) 123-4567"}]
})

result["structured_response"]

**Examples:**

Example 1 (unknown):
```unknown
<Tip>
  For more details on message types and formatting, see [Messages](/oss/python/langchain/messages). For comprehensive middleware documentation, see [Middleware](/oss/python/langchain/middleware).
</Tip>

## Invocation

You can invoke an agent by passing an update to its [`State`](/oss/python/langgraph/graph-api#state). All agents include a [sequence of messages](/oss/python/langgraph/use-graph-api#messagesstate) in their state; to invoke the agent, pass a new message:
```

Example 2 (unknown):
```unknown
For streaming steps and / or tokens from the agent, refer to the [streaming](/oss/python/langchain/streaming) guide.

Otherwise, the agent follows the LangGraph [Graph API](/oss/python/langgraph/use-graph-api) and supports all associated methods.

## Advanced concepts

### Structured output

In some situations, you may want the agent to return an output in a specific format. LangChain provides strategies for structured output via the [`response_format`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.ModelRequest\(response_format\)) parameter.

#### ToolStrategy

`ToolStrategy` uses artificial tool calling to generate structured output. This works with any model that supports tool calling:
```

---

## The trace produced will have its metadata present, but the inputs and outputs will be anonymized

**URL:** llms-txt#the-trace-produced-will-have-its-metadata-present,-but-the-inputs-and-outputs-will-be-anonymized

response_with_anonymization = openai_client.chat.completions.create(
  model="gpt-4o-mini",
  messages=[
      {"role": "system", "content": "You are a helpful assistant."},
      {"role": "user", "content": "My name is Slim Shady, call me at 313-666-7440 or email me at real.slim.shady@gmail.com"},
  ],
  langsmith_extra={"client": langsmith_client},
)

---

## The trace produced will not have anonymized inputs and outputs

**URL:** llms-txt#the-trace-produced-will-not-have-anonymized-inputs-and-outputs

response_without_anonymization = openai_client.chat.completions.create(
  model="gpt-4o-mini",
  messages=[
      {"role": "system", "content": "You are a helpful assistant."},
      {"role": "user", "content": "My name is Slim Shady, call me at 313-666-7440 or email me at real.slim.shady@gmail.com"},
  ],
)
```

The anonymized run will look like this in LangSmith: <img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/aws-comprehend-anonymized.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=cea182d95ef02e614a6f1bbd7e3a2657" alt="Anonymized run" data-og-width="3180" width="3180" data-og-height="1616" height="1616" data-path="langsmith/images/aws-comprehend-anonymized.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/aws-comprehend-anonymized.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=d3c5a665e2ee726ad6dacf89ade8daea 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/aws-comprehend-anonymized.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=77bccbc4ba3bcde3bd771866e44ce535 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/aws-comprehend-anonymized.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=527a6563672cb66d28bf7ae3272c0c5e 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/aws-comprehend-anonymized.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=108afc60434f26addae7525049850aac 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/aws-comprehend-anonymized.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=e0216a9846c6e7ff041bcccdb23ce98a 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/aws-comprehend-anonymized.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=cd4119b7e81420f54db86cad58db5426 2500w" />

The non-anonymized run will look like this in LangSmith: <img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/aws-comprehend-not-anonymized.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=ec61e5c8d78268b5b34b6b9c184871cc" alt="Non-anonymized run" data-og-width="3180" width="3180" data-og-height="1648" height="1648" data-path="langsmith/images/aws-comprehend-not-anonymized.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/aws-comprehend-not-anonymized.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=34ad34770c55fcea58caee9dfa7f856e 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/aws-comprehend-not-anonymized.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=089c86d993f3dc8a5998bfc2adc8a75d 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/aws-comprehend-not-anonymized.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=7fcd25c38a94d366762cf74629dd07c7 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/aws-comprehend-not-anonymized.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=b82a670a0e25bc656475cccea5d1a50d 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/aws-comprehend-not-anonymized.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=c422ed47282a844783713e5dc291a7b0 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/aws-comprehend-not-anonymized.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=a5a62a5bd72b2cf49c9495d14d3059fa 2500w" />

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/mask-inputs-outputs.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Thinking in LangGraph

**URL:** llms-txt#thinking-in-langgraph

**Contents:**
- Start with the process you want to automate
- Step 1: Map out your workflow as discrete steps
- Step 2: Identify what each step needs to do
  - LLM Steps
  - Data Steps
  - Action Steps
  - User Input Steps
- Step 3: Design your state
  - What belongs in state?
  - Keep state raw, format prompts on-demand

Source: https://docs.langchain.com/oss/javascript/langgraph/thinking-in-langgraph

Learn how to think about building agents with LangGraph by breaking down a customer support email agent into discrete steps

LangGraph can change how you think about the agents you build. When you build an agent with LangGraph, you will first break it apart into discrete steps called **nodes**. Then, you will describe the different decisions and transitions for each of your nodes. Finally, you will connect your nodes together through a shared **state** that each node can read from and write to. In this tutorial, we'll guide you through the thought process of building a customer support email agent with LangGraph.

## Start with the process you want to automate

Imagine that you need to build an AI agent that handles customer support emails. Your product team has given you these requirements:

* Read incoming customer emails
* Classify them by urgency and topic
* Search relevant documentation to answer questions
* Draft appropriate responses
* Escalate complex issues to human agents
* Schedule follow-ups when needed

Example scenarios to handle:

1. Simple product question: "How do I reset my password?"
2. Bug report: "The export feature crashes when I select PDF format"
3. Urgent billing issue: "I was charged twice for my subscription!"
4. Feature request: "Can you add dark mode to the mobile app?"
5. Complex technical issue: "Our API integration fails intermittently with 504 errors"

To implement an agent in LangGraph, you will usually follow the same five steps.

## Step 1: Map out your workflow as discrete steps

Start by identifying the distinct steps in your process. Each step will become a **node** (a function that does one specific thing). Then sketch how these steps connect to each other.

The arrows show possible paths, but the actual decision of which path to take happens inside each node.

Now that you've identified the components in your workflow, let's understand what each node needs to do:

* Read Email: Extract and parse the email content
* Classify Intent: Use an LLM to categorize urgency and topic, then route to appropriate action
* Doc Search: Query your knowledge base for relevant information
* Bug Track: Create or update issue in tracking system
* Draft Reply: Generate an appropriate response
* Human Review: Escalate to human agent for approval or handling
* Send Reply: Dispatch the email response

<Tip>
  Notice that some nodes make decisions about where to go next (Classify Intent, Draft Reply, Human Review), while others always proceed to the same next step (Read Email always goes to Classify Intent, Doc Search always goes to Draft Reply).
</Tip>

## Step 2: Identify what each step needs to do

For each node in your graph, determine what type of operation it represents and what context it needs to work properly.

<CardGroup cols={2}>
  <Card title="LLM Steps" icon="brain" href="#llm-steps">
    Use when you need to understand, analyze, generate text, or make reasoning decisions
  </Card>

<Card title="Data Steps" icon="database" href="#data-steps">
    Use when you need to retrieve information from external sources
  </Card>

<Card title="Action Steps" icon="bolt" href="#action-steps">
    Use when you need to perform external actions
  </Card>

<Card title="User Input Steps" icon="user" href="#user-input-steps">
    Use when you need human intervention
  </Card>
</CardGroup>

When a step needs to understand, analyze, generate text, or make reasoning decisions:

<AccordionGroup>
  <Accordion title="Classify Intent Node">
    * Static context (prompt): Classification categories, urgency definitions, response format
    * Dynamic context (from state): Email content, sender information
    * Desired outcome: Structured classification that determines routing
  </Accordion>

<Accordion title="Draft Reply Node">
    * Static context (prompt): Tone guidelines, company policies, response templates
    * Dynamic context (from state): Classification results, search results, customer history
    * Desired outcome: Professional email response ready for review
  </Accordion>
</AccordionGroup>

When a step needs to retrieve information from external sources:

<AccordionGroup>
  <Accordion title="Document Search Node">
    * Parameters: Query built from intent and topic
    * Retry strategy: Yes, with exponential backoff for transient failures
    * Caching: Could cache common queries to reduce API calls
  </Accordion>

<Accordion title="Customer History Lookup">
    * Parameters: Customer email or ID from state
    * Retry strategy: Yes, but with fallback to basic info if unavailable
    * Caching: Yes, with time-to-live to balance freshness and performance
  </Accordion>
</AccordionGroup>

When a step needs to perform an external action:

<AccordionGroup>
  <Accordion title="Send Reply Node">
    * When to execute: After approval (human or automated)
    * Retry strategy: Yes, with exponential backoff for network issues
    * Should not cache: Each send is a unique action
  </Accordion>

<Accordion title="Bug Track Node">
    * When to execute: Always when intent is "bug"
    * Retry strategy: Yes, critical to not lose bug reports
    * Returns: Ticket ID to include in response
  </Accordion>
</AccordionGroup>

When a step needs human intervention:

<AccordionGroup>
  <Accordion title="Human Review Node">
    * Context for decision: Original email, draft response, urgency, classification
    * Expected input format: Approval boolean plus optional edited response
    * When triggered: High urgency, complex issues, or quality concerns
  </Accordion>
</AccordionGroup>

## Step 3: Design your state

State is the shared [memory](/oss/javascript/concepts/memory) accessible to all nodes in your agent. Think of it as the notebook your agent uses to keep track of everything it learns and decides as it works through the process.

### What belongs in state?

Ask yourself these questions about each piece of data:

<CardGroup cols={2}>
  <Card title="Include in State" icon="check">
    Does it need to persist across steps? If yes, it goes in state.
  </Card>

<Card title="Don't Store" icon="code">
    Can you derive it from other data? If yes, compute it when needed instead of storing it in state.
  </Card>
</CardGroup>

For our email agent, we need to track:

* The original email and sender info (can't reconstruct these)
* Classification results (needed by multiple downstream nodes)
* Search results and customer data (expensive to re-fetch)
* The draft response (needs to persist through review)
* Execution metadata (for debugging and recovery)

### Keep state raw, format prompts on-demand

<Tip>
  A key principle: your state should store raw data, not formatted text. Format prompts inside nodes when you need them.
</Tip>

This separation means:

* Different nodes can format the same data differently for their needs
* You can change prompt templates without modifying your state schema
* Debugging is clearer - you see exactly what data each node received
* Your agent can evolve without breaking existing state

Let's define our state:

Notice that the state contains only raw data - no prompt templates, no formatted strings, no instructions. The classification output is stored as a single dictionary, straight from the LLM.

## Step 4: Build your nodes

Now we implement each step as a function. A node in LangGraph is just a JavaScript function that takes the current state and returns updates to it.

### Handle errors appropriately

Different errors need different handling strategies:

| Error Type                                                      | Who Fixes It       | Strategy                           | When to Use                                      |
| --------------------------------------------------------------- | ------------------ | ---------------------------------- | ------------------------------------------------ |
| Transient errors (network issues, rate limits)                  | System (automatic) | Retry policy                       | Temporary failures that usually resolve on retry |
| LLM-recoverable errors (tool failures, parsing issues)          | LLM                | Store error in state and loop back | LLM can see the error and adjust its approach    |
| User-fixable errors (missing information, unclear instructions) | Human              | Pause with `interrupt()`           | Need user input to proceed                       |
| Unexpected errors                                               | Developer          | Let them bubble up                 | Unknown issues that need debugging               |

<Tabs>
  <Tab title="Transient errors" icon="rotate">
    Add a retry policy to automatically retry network issues and rate limits:

<Tab title="LLM-recoverable" icon="brain">
    Store the error in state and loop back so the LLM can see what went wrong and try again:

<Tab title="User-fixable" icon="user">
    Pause and collect information from the user when needed (like account IDs, order numbers, or clarifications):

<Tab title="Unexpected" icon="triangle-exclamation">
    Let them bubble up for debugging. Don't catch what you can't handle:

### Implementing our email agent nodes

We'll implement each node as a simple function. Remember: nodes take state, do work, and return updates.

<AccordionGroup>
  <Accordion title="Read and classify nodes" icon="brain">
    
  </Accordion>

<Accordion title="Search and tracking nodes" icon="database">
    
  </Accordion>

<Accordion title="Response nodes" icon="pen-to-square">
    
  </Accordion>
</AccordionGroup>

## Step 5: Wire it together

Now we connect our nodes into a working graph. Since our nodes handle their own routing decisions, we only need a few essential edges.

To enable [human-in-the-loop](/oss/javascript/langgraph/interrupts) with `interrupt()`, we need to compile with a [checkpointer](/oss/javascript/langgraph/persistence) to save state between runs:

<Accordion title="Graph compilation code" icon="diagram-project" defaultOpen={true}>
  
</Accordion>

The graph structure is minimal because routing happens inside nodes through `Command` objects. Each node declares where it can go, making the flow explicit and traceable.

### Try out your agent

Let's run our agent with an urgent billing issue that needs human review:

<Accordion title="Testing the agent" icon="flask">
  
</Accordion>

The graph pauses when it hits `interrupt()`, saves everything to the checkpointer, and waits. It can resume days later, picking up exactly where it left off. The `thread_id` ensures all state for this conversation is preserved together.

## Summary and next steps

Building this email agent has shown us the LangGraph way of thinking:

<CardGroup cols={2}>
  <Card title="Break into discrete steps" icon="sitemap" href="#step-1-map-out-your-workflow-as-discrete-steps">
    Each node does one thing well. This decomposition enables streaming progress updates, durable execution that can pause and resume, and clear debugging since you can inspect state between steps.
  </Card>

<Card title="State is shared memory" icon="database" href="#step-3-design-your-state">
    Store raw data, not formatted text. This lets different nodes use the same information in different ways.
  </Card>

<Card title="Nodes are functions" icon="code" href="#step-4-build-your-nodes">
    They take state, do work, and return updates. When they need to make routing decisions, they specify both the state updates and the next destination.
  </Card>

<Card title="Errors are part of the flow" icon="triangle-exclamation" href="#handle-errors-appropriately">
    Transient failures get retries, LLM-recoverable errors loop back with context, user-fixable problems pause for input, and unexpected errors bubble up for debugging.
  </Card>

<Card title="Human input is first-class" icon="user" href="/oss/javascript/langgraph/interrupts">
    The `interrupt()` function pauses execution indefinitely, saves all state, and resumes exactly where it left off when you provide input. When combined with other operations in a node, it must come first.
  </Card>

<Card title="Graph structure emerges naturally" icon="diagram-project" href="#step-5-wire-it-together">
    You define the essential connections, and your nodes handle their own routing logic. This keeps control flow explicit and traceable - you can always understand what your agent will do next by looking at the current node.
  </Card>
</CardGroup>

### Advanced considerations

<Accordion title="Node granularity trade-offs" icon="sliders">
  <Info>
    This section explores the trade-offs in node granularity design. Most applications can skip this and use the patterns shown above.
  </Info>

You might wonder: why not combine `Read Email` and `Classify Intent` into one node?

Or why separate Doc Search from Draft Reply?

The answer involves trade-offs between resilience and observability.

**The resilience consideration:** LangGraph's [durable execution](/oss/javascript/langgraph/durable-execution) creates checkpoints at node boundaries. When a workflow resumes after an interruption or failure, it starts from the beginning of the node where execution stopped. Smaller nodes mean more frequent checkpoints, which means less work to repeat if something goes wrong. If you combine multiple operations into one large node, a failure near the end means re-executing everything from the start of that node.

Why we chose this breakdown for the email agent:

* **Isolation of external services:** Doc Search and Bug Track are separate nodes because they call external APIs. If the search service is slow or fails, we want to isolate that from the LLM calls. We can add retry policies to these specific nodes without affecting others.

* **Intermediate visibility:** Having `Classify Intent` as its own node lets us inspect what the LLM decided before taking action. This is valuable for debugging and monitoring—you can see exactly when and why the agent routes to human review.

* **Different failure modes:** LLM calls, database lookups, and email sending have different retry strategies. Separate nodes let you configure these independently.

* **Reusability and testing:** Smaller nodes are easier to test in isolation and reuse in other workflows.

A different valid approach: You could combine `Read Email` and `Classify Intent` into a single node. You'd lose the ability to inspect the raw email before classification and would repeat both operations on any failure in that node. For most applications, the observability and debugging benefits of separate nodes are worth the trade-off.

Application-level concerns: The caching discussion in Step 2 (whether to cache search results) is an application-level decision, not a LangGraph framework feature. You implement caching within your node functions based on your specific requirements—LangGraph doesn't prescribe this.

Performance considerations: More nodes doesn't mean slower execution. LangGraph writes checkpoints in the background by default ([async durability mode](/oss/javascript/langgraph/durable-execution#durability-modes)), so your graph continues running without waiting for checkpoints to complete. This means you get frequent checkpoints with minimal performance impact. You can adjust this behavior if needed—use `"exit"` mode to checkpoint only at completion, or `"sync"` mode to block execution until each checkpoint is written.
</Accordion>

### Where to go from here

This was an introduction to thinking about building agents with LangGraph. You can extend this foundation with:

<CardGroup cols={2}>
  <Card title="Human-in-the-loop patterns" icon="user-check" href="/oss/javascript/langgraph/interrupts">
    Learn how to add tool approval before execution, batch approval, and other patterns
  </Card>

<Card title="Subgraphs" icon="diagram-nested" href="/oss/javascript/langgraph/use-subgraphs">
    Create subgraphs for complex multi-step operations
  </Card>

<Card title="Streaming" icon="tower-broadcast" href="/oss/javascript/langgraph/streaming">
    Add streaming to show real-time progress to users
  </Card>

<Card title="Observability" icon="chart-line" href="/oss/javascript/langgraph/observability">
    Add observability with LangSmith for debugging and monitoring
  </Card>

<Card title="Tool Integration" icon="wrench" href="/oss/javascript/langchain/tools">
    Integrate more tools for web search, database queries, and API calls
  </Card>

<Card title="Retry Logic" icon="rotate" href="/oss/javascript/langgraph/use-graph-api#add-retry-policies">
    Implement retry logic with exponential backoff for failed operations
  </Card>
</CardGroup>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/thinking-in-langgraph.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
The arrows show possible paths, but the actual decision of which path to take happens inside each node.

Now that you've identified the components in your workflow, let's understand what each node needs to do:

* Read Email: Extract and parse the email content
* Classify Intent: Use an LLM to categorize urgency and topic, then route to appropriate action
* Doc Search: Query your knowledge base for relevant information
* Bug Track: Create or update issue in tracking system
* Draft Reply: Generate an appropriate response
* Human Review: Escalate to human agent for approval or handling
* Send Reply: Dispatch the email response

<Tip>
  Notice that some nodes make decisions about where to go next (Classify Intent, Draft Reply, Human Review), while others always proceed to the same next step (Read Email always goes to Classify Intent, Doc Search always goes to Draft Reply).
</Tip>

## Step 2: Identify what each step needs to do

For each node in your graph, determine what type of operation it represents and what context it needs to work properly.

<CardGroup cols={2}>
  <Card title="LLM Steps" icon="brain" href="#llm-steps">
    Use when you need to understand, analyze, generate text, or make reasoning decisions
  </Card>

  <Card title="Data Steps" icon="database" href="#data-steps">
    Use when you need to retrieve information from external sources
  </Card>

  <Card title="Action Steps" icon="bolt" href="#action-steps">
    Use when you need to perform external actions
  </Card>

  <Card title="User Input Steps" icon="user" href="#user-input-steps">
    Use when you need human intervention
  </Card>
</CardGroup>

### LLM Steps

When a step needs to understand, analyze, generate text, or make reasoning decisions:

<AccordionGroup>
  <Accordion title="Classify Intent Node">
    * Static context (prompt): Classification categories, urgency definitions, response format
    * Dynamic context (from state): Email content, sender information
    * Desired outcome: Structured classification that determines routing
  </Accordion>

  <Accordion title="Draft Reply Node">
    * Static context (prompt): Tone guidelines, company policies, response templates
    * Dynamic context (from state): Classification results, search results, customer history
    * Desired outcome: Professional email response ready for review
  </Accordion>
</AccordionGroup>

### Data Steps

When a step needs to retrieve information from external sources:

<AccordionGroup>
  <Accordion title="Document Search Node">
    * Parameters: Query built from intent and topic
    * Retry strategy: Yes, with exponential backoff for transient failures
    * Caching: Could cache common queries to reduce API calls
  </Accordion>

  <Accordion title="Customer History Lookup">
    * Parameters: Customer email or ID from state
    * Retry strategy: Yes, but with fallback to basic info if unavailable
    * Caching: Yes, with time-to-live to balance freshness and performance
  </Accordion>
</AccordionGroup>

### Action Steps

When a step needs to perform an external action:

<AccordionGroup>
  <Accordion title="Send Reply Node">
    * When to execute: After approval (human or automated)
    * Retry strategy: Yes, with exponential backoff for network issues
    * Should not cache: Each send is a unique action
  </Accordion>

  <Accordion title="Bug Track Node">
    * When to execute: Always when intent is "bug"
    * Retry strategy: Yes, critical to not lose bug reports
    * Returns: Ticket ID to include in response
  </Accordion>
</AccordionGroup>

### User Input Steps

When a step needs human intervention:

<AccordionGroup>
  <Accordion title="Human Review Node">
    * Context for decision: Original email, draft response, urgency, classification
    * Expected input format: Approval boolean plus optional edited response
    * When triggered: High urgency, complex issues, or quality concerns
  </Accordion>
</AccordionGroup>

## Step 3: Design your state

State is the shared [memory](/oss/javascript/concepts/memory) accessible to all nodes in your agent. Think of it as the notebook your agent uses to keep track of everything it learns and decides as it works through the process.

### What belongs in state?

Ask yourself these questions about each piece of data:

<CardGroup cols={2}>
  <Card title="Include in State" icon="check">
    Does it need to persist across steps? If yes, it goes in state.
  </Card>

  <Card title="Don't Store" icon="code">
    Can you derive it from other data? If yes, compute it when needed instead of storing it in state.
  </Card>
</CardGroup>

For our email agent, we need to track:

* The original email and sender info (can't reconstruct these)
* Classification results (needed by multiple downstream nodes)
* Search results and customer data (expensive to re-fetch)
* The draft response (needs to persist through review)
* Execution metadata (for debugging and recovery)

### Keep state raw, format prompts on-demand

<Tip>
  A key principle: your state should store raw data, not formatted text. Format prompts inside nodes when you need them.
</Tip>

This separation means:

* Different nodes can format the same data differently for their needs
* You can change prompt templates without modifying your state schema
* Debugging is clearer - you see exactly what data each node received
* Your agent can evolve without breaking existing state

Let's define our state:
```

Example 2 (unknown):
```unknown
Notice that the state contains only raw data - no prompt templates, no formatted strings, no instructions. The classification output is stored as a single dictionary, straight from the LLM.

## Step 4: Build your nodes

Now we implement each step as a function. A node in LangGraph is just a JavaScript function that takes the current state and returns updates to it.

### Handle errors appropriately

Different errors need different handling strategies:

| Error Type                                                      | Who Fixes It       | Strategy                           | When to Use                                      |
| --------------------------------------------------------------- | ------------------ | ---------------------------------- | ------------------------------------------------ |
| Transient errors (network issues, rate limits)                  | System (automatic) | Retry policy                       | Temporary failures that usually resolve on retry |
| LLM-recoverable errors (tool failures, parsing issues)          | LLM                | Store error in state and loop back | LLM can see the error and adjust its approach    |
| User-fixable errors (missing information, unclear instructions) | Human              | Pause with `interrupt()`           | Need user input to proceed                       |
| Unexpected errors                                               | Developer          | Let them bubble up                 | Unknown issues that need debugging               |

<Tabs>
  <Tab title="Transient errors" icon="rotate">
    Add a retry policy to automatically retry network issues and rate limits:
```

Example 3 (unknown):
```unknown
</Tab>

  <Tab title="LLM-recoverable" icon="brain">
    Store the error in state and loop back so the LLM can see what went wrong and try again:
```

Example 4 (unknown):
```unknown
</Tab>

  <Tab title="User-fixable" icon="user">
    Pause and collect information from the user when needed (like account IDs, order numbers, or clarifications):
```

---

## This can be a user input to your app

**URL:** llms-txt#this-can-be-a-user-input-to-your-app

question = "Can you summarize this morning's meetings?"

---

## (This can be done after putting memories into the store)

**URL:** llms-txt#(this-can-be-done-after-putting-memories-into-the-store)

memories = store.search(
    namespace_for_memory,
    query="What does the user like to eat?",
    limit=3  # Return top 3 matches
)
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
You can control which parts of your memories get embedded by configuring the `fields` parameter or by specifying the `index` parameter when storing memories:
```

---

## This can be retrieved in a retrieval step

**URL:** llms-txt#this-can-be-retrieved-in-a-retrieval-step

context = "During this morning's meeting, we solved all world conflict."

messages = [
    {"role": "system", "content": "You are a helpful assistant. Please respond to the user's request only based on the given context."},
    {"role": "user", "content": f"Question: {question}\nContext: {context}"}
]

---

## This compiles it into a LangChain Runnable,

**URL:** llms-txt#this-compiles-it-into-a-langchain-runnable,

---

## This converts the runs to a dataset + experiment

**URL:** llms-txt#this-converts-the-runs-to-a-dataset-+-experiment

**Contents:**
- Benchmark against new system
  - Define evaluators
  - Evaluate baseline

convert_runs_to_test(
    prod_runs,
    # Name of the resulting dataset
    dataset_name=dataset_name,
    # Whether to include the run outputs as reference/ground truth
    include_outputs=False,
    # Whether to include the full traces in the resulting experiment
    # (default is to just include the root run)
    load_child_runs=True,
    # Name of the experiment so we can apply evalautors to it after
    test_project_name=baseline_experiment_name
)
python  theme={null}
import emoji
from pydantic import BaseModel, Field
from langchain_core.messages import convert_to_openai_messages

class Grade(BaseModel):
    """Grade whether a response is supported by some context."""
    grounded: bool = Field(..., description="Is the majority of the response supported by the retrieved context?")

grounded_instructions = f"""You have given somebody some contextual information and asked them to write a statement grounded in that context.

Grade whether their response is fully supported by the context you have provided. \
If any meaningful part of their statement is not backed up directly by the context you provided, then their response is not grounded. \
Otherwise it is grounded."""
grounded_model = init_chat_model(model="gpt-4o").with_structured_output(Grade)

def lt_280_chars(outputs: dict) -> bool:
    messages = convert_to_openai_messages(outputs["messages"])
    return len(messages[-1]['content']) <= 280

def gte_3_emojis(outputs: dict) -> bool:
    messages = convert_to_openai_messages(outputs["messages"])
    return len(emoji.emoji_list(messages[-1]['content'])) >= 3

async def is_grounded(outputs: dict) -> bool:
    context = ""
    messages = convert_to_openai_messages(outputs["messages"])
    for message in messages:
        if message["role"] == "tool":
            # Tool message outputs are the results returned from the Tavily/DuckDuckGo tool
            context += "\n\n" + message["content"]
    tweet = messages[-1]["content"]
    user = f"""CONTEXT PROVIDED:
    {context}

RESPONSE GIVEN:
    {tweet}"""
    grade = await grounded_model.ainvoke([
        {"role": "system", "content": grounded_instructions},
        {"role": "user", "content": user}
    ])
    return grade.grounded
python  theme={null}
baseline_results = await client.aevaluate(
    baseline_experiment_name,
    evaluators=[lt_280_chars, gte_3_emojis, is_grounded],
)

**Examples:**

Example 1 (unknown):
```unknown
Once this step is complete, you should see a new dataset in your LangSmith project called "Tweet Writing Task-backtesting TODAYS DATE", with a single experiment like so:

<img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/baseline-experiment.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=73b60a75d6b33f2830f5ed68464c586b" alt="" data-og-width="3456" width="3456" data-og-height="1852" height="1852" data-path="langsmith/images/baseline-experiment.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/baseline-experiment.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=e459a884bbec6e3741617830b9e70848 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/baseline-experiment.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=6116dd6057709be29d84f0cbad32e7a1 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/baseline-experiment.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=a86ab5cffeac0ceace81adbc59dba649 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/baseline-experiment.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=e0058d68f4061df5fe4249c9e3954c38 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/baseline-experiment.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=270a7eb33f39fd612d1732aadaa0b373 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/baseline-experiment.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=fe9dc037876fcf1c4eb22317d7bb3f45 2500w" />

## Benchmark against new system

Now we can start the process of benchmarking our production runs against a new system.

### Define evaluators

First let's define the evaluators we will use to compare the two systems. Note that we have no reference outputs, so we'll need to come up with evaluation metrics that only require the actual outputs.
```

Example 2 (unknown):
```unknown
### Evaluate baseline

Now, let's run our evaluators against the baseline experiment.
```

---

## This example uses OpenAI, but you can use any LLM provider of choice

**URL:** llms-txt#this-example-uses-openai,-but-you-can-use-any-llm-provider-of-choice

**Contents:**
  - 3. Log a trace

export OPENAI_API_KEY=<your-openai-api-key>
python Python theme={null}
  import json
  import openai
  import operator
  from langsmith import traceable
  from langsmith.wrappers import wrap_openai
  from typing import Annotated, Literal, TypedDict
  from langgraph.graph import StateGraph

class State(TypedDict):
      messages: Annotated[list, operator.add]

tool_schema = {
      "type": "function",
      "function": {
          "name": "search",
          "description": "Call to surf the web.",
          "parameters": {
              "type": "object",
              "properties": {"query": {"type": "string"}},
              "required": ["query"],
          },
      },
  }

# Decorating the tool function will automatically trace it with the correct context
  @traceable(run_type="tool", name="Search Tool")
  def search(query: str):
      """Call to surf the web."""
      if "sf" in query.lower() or "san francisco" in query.lower():
          return "It's 60 degrees and foggy."
      return "It's 90 degrees and sunny."

def call_tools(state):
      function_name_to_function = {"search": search}
      messages = state["messages"]
      tool_call = messages[-1]["tool_calls"][0]
      function_name = tool_call["function"]["name"]
      function_arguments = tool_call["function"]["arguments"]
      arguments = json.loads(function_arguments)
      function_response = function_name_to_function[function_name](**arguments)
      tool_message = {
          "tool_call_id": tool_call["id"],
          "role": "tool",
          "name": function_name,
          "content": function_response,
      }
      return {"messages": [tool_message]}

wrapped_client = wrap_openai(openai.Client())

def should_continue(state: State) -> Literal["tools", "__end__"]:
      messages = state["messages"]
      last_message = messages[-1]
      if last_message["tool_calls"]:
          return "tools"
      return "__end__"

def call_model(state: State):
      messages = state["messages"]
      # Calling the wrapped client will automatically infer the correct tracing context
      response = wrapped_client.chat.completions.create(
          messages=messages, model="gpt-4o-mini", tools=[tool_schema]
      )
      raw_tool_calls = response.choices[0].message.tool_calls
      tool_calls = [tool_call.to_dict() for tool_call in raw_tool_calls] if raw_tool_calls else []
      response_message = {
          "role": "assistant",
          "content": response.choices[0].message.content,
          "tool_calls": tool_calls,
      }
      return {"messages": [response_message]}

workflow = StateGraph(State)
  workflow.add_node("agent", call_model)
  workflow.add_node("tools", call_tools)
  workflow.add_edge("__start__", "agent")
  workflow.add_conditional_edges(
      "agent",
      should_continue,
  )
  workflow.add_edge("tools", 'agent')

app = workflow.compile()

final_state = app.invoke(
      {"messages": [{"role": "user", "content": "what is the weather in sf"}]}
  )

final_state["messages"][-1]["content"]
  typescript TypeScript theme={null}
  **Note:** The below example requires `langsmith>=0.1.39` and `@langchain/langgraph>=0.0.31`

import OpenAI from "openai";
  import { StateGraph } from "@langchain/langgraph";
  import { wrapOpenAI } from "langsmith/wrappers/openai";
  import { traceable } from "langsmith/traceable";

type GraphState = {
    messages: OpenAI.ChatCompletionMessageParam[];
  };

const wrappedClient = wrapOpenAI(new OpenAI({}));

const toolSchema: OpenAI.ChatCompletionTool = {
    type: "function",
    function: {
      name: "search",
      description: "Use this tool to query the web.",
      parameters: {
        type: "object",
        properties: {
          query: {
            type: "string",
          },
        },
        required: ["query"],
      }
    }
  };

// Wrapping the tool function will automatically trace it with the correct context
  const search = traceable(async ({ query }: { query: string }) => {
    if (
      query.toLowerCase().includes("sf") ||
      query.toLowerCase().includes("san francisco")
    ) {
      return "It's 60 degrees and foggy.";
    }
    return "It's 90 degrees and sunny.";
  }, { run_type: "tool", name: "Search Tool" });

const callTools = async ({ messages }: GraphState) => {
    const mostRecentMessage = messages[messages.length - 1];
    const toolCalls = (mostRecentMessage as OpenAI.ChatCompletionAssistantMessageParam).tool_calls;
    if (toolCalls === undefined || toolCalls.length === 0) {
      throw new Error("No tool calls passed to node.");
    }
    const toolNameMap = {
      search,
    };
    const functionName = toolCalls[0].function.name;
    const functionArguments = JSON.parse(toolCalls[0].function.arguments);
    const response = await toolNameMap[functionName](functionArguments);
    const toolMessage = {
      tool_call_id: toolCalls[0].id,
      role: "tool",
      name: functionName,
      content: response,
    }
    return { messages: [toolMessage] };
  };

const callModel = async ({ messages }: GraphState) => {
    // Calling the wrapped client will automatically infer the correct tracing context
    const response = await wrappedClient.chat.completions.create({
      messages,
      model: "gpt-4o-mini",
      tools: [toolSchema],
    });
    const responseMessage = {
      role: "assistant",
      content: response.choices[0].message.content,
      tool_calls: response.choices[0].message.tool_calls ?? [],
    };
    return { messages: [responseMessage] };
  };

const shouldContinue = ({ messages }: GraphState) => {
    const lastMessage =
      messages[messages.length - 1] as OpenAI.ChatCompletionAssistantMessageParam;
    if (
      lastMessage?.tool_calls !== undefined &&
      lastMessage?.tool_calls.length > 0
    ) {
      return "tools";
    }
    return "__end__";
  }

const workflow = new StateGraph<GraphState>({
    channels: {
      messages: {
        reducer: (a: any, b: any) => a.concat(b),
      }
    }
  });

const graph = workflow
    .addNode("model", callModel)
    .addNode("tools", callTools)
    .addEdge("__start__", "model")
    .addConditionalEdges("model", shouldContinue, {
      tools: "tools",
      __end__: "__end__",
    })
    .addEdge("tools", "model")
    .compile();

await graph.invoke({
    messages: [{ role: "user", content: "what is the weather in sf" }]
  });
  ```
</CodeGroup>

An example trace from running the above code [looks like this](https://smith.langchain.com/public/353f27da-c221-4b67-b9ec-ede3777f3271/r):

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-without-langchain-trace.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=abe0ae173d182563c343f6596e0ce4e2" alt="Trace tree for a LangGraph run without LangChain" data-og-width="3296" width="3296" data-og-height="1774" height="1774" data-path="langsmith/images/langgraph-without-langchain-trace.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-without-langchain-trace.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=794e9ce04677bbf721880ebb07ada7c6 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-without-langchain-trace.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=d6208cbec91ba187ba8f75f6cc916b3f 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-without-langchain-trace.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=418509190558d6a87363d3ba146b7722 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-without-langchain-trace.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=a3c3e2e11cbdac8c32e80e7a895b1eb0 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-without-langchain-trace.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=ea893862eb3f9bc5910649cf0ccd2abe 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-without-langchain-trace.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=2b244ca0ec1296552991428d1efffde6 2500w" />

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/trace-with-langgraph.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
<Info>
  If you are using LangChain.js with LangSmith and are not in a serverless environment, we also recommend setting the following explicitly to reduce latency:

  `export LANGCHAIN_CALLBACKS_BACKGROUND=true`

  If you are in a serverless environment, we recommend setting the reverse to allow tracing to finish before your function ends:

  `export LANGCHAIN_CALLBACKS_BACKGROUND=false`

  See [this LangChain.js guide](https://js.langchain.com/docs/how_to/callbacks_serverless) for more information.
</Info>

### 3. Log a trace

Once you've set up your environment, [wrap or decorate the custom functions/SDKs](/langsmith/annotate-code#use-traceable--traceable) you want to trace. LangSmith will then infer the proper tracing config:

<CodeGroup>
```

Example 2 (unknown):
```unknown

```

---

## This invocation will take ~1 second due to the slow_task execution

**URL:** llms-txt#this-invocation-will-take-~1-second-due-to-the-slow_task-execution

**Contents:**
- Human-in-the-loop
  - Basic human-in-the-loop workflow

try:
    # First invocation will raise an exception due to the `get_info` task failing
    main.invoke({'any_input': 'foobar'}, config=config)
except ValueError:
    pass  # Handle the failure gracefully
python  theme={null}
main.invoke(None, config=config)
pycon  theme={null}
'Ran slow task.'
python  theme={null}
from langgraph.func import entrypoint, task
from langgraph.types import Command, interrupt

@task
def step_1(input_query):
    """Append bar."""
    return f"{input_query} bar"

@task
def human_feedback(input_query):
    """Append user input."""
    feedback = interrupt(f"Please provide feedback: {input_query}")
    return f"{input_query} {feedback}"

@task
def step_3(input_query):
    """Append qux."""
    return f"{input_query} qux"
python  theme={null}
from langgraph.checkpoint.memory import InMemorySaver

checkpointer = InMemorySaver()

@entrypoint(checkpointer=checkpointer)
def graph(input_query):
    result_1 = step_1(input_query).result()
    result_2 = human_feedback(result_1).result()
    result_3 = step_3(result_2).result()

return result_3
python  theme={null}
config = {"configurable": {"thread_id": "1"}}

for event in graph.stream("foo", config):
    print(event)
    print("\n")
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
When we resume execution, we won't need to re-run the `slow_task` as its result is already saved in the checkpoint.
```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
## Human-in-the-loop

The functional API supports [human-in-the-loop](/oss/python/langgraph/interrupts) workflows using the [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) function and the `Command` primitive.

### Basic human-in-the-loop workflow

We will create three [tasks](/oss/python/langgraph/functional-api#task):

1. Append `"bar"`.
2. Pause for human input. When resuming, append human input.
3. Append `"qux"`.
```

Example 4 (unknown):
```unknown
We can now compose these tasks in an [entrypoint](/oss/python/langgraph/functional-api#entrypoint):
```

---

## This isn't for production use, but is useful for local

**URL:** llms-txt#this-isn't-for-production-use,-but-is-useful-for-local

store = LocalFileStore("./cache/") # [!code highlight]

cached_embedder = CacheBackedEmbeddings.from_bytes_store(
    underlying_embeddings,
    store,
    namespace=underlying_embeddings.model
)

---

## This is loaded from the `.env` file you created above

**URL:** llms-txt#this-is-loaded-from-the-`.env`-file-you-created-above

SUPABASE_URL = os.environ["SUPABASE_URL"]
SUPABASE_SERVICE_KEY = os.environ["SUPABASE_SERVICE_KEY"]

@auth.authenticate
async def get_current_user(authorization: str | None):
    """Validate JWT tokens and extract user information."""
    assert authorization
    scheme, token = authorization.split()
    assert scheme.lower() == "bearer"

try:
        # Verify token with auth provider
        async with httpx.AsyncClient() as client:
            response = await client.get(
                f"{SUPABASE_URL}/auth/v1/user",
                headers={
                    "Authorization": authorization,
                    "apiKey": SUPABASE_SERVICE_KEY,
                },
            )
            assert response.status_code == 200
            user = response.json()
            return {
                "identity": user["id"],  # Unique user identifier
                "email": user["email"],
                "is_authenticated": True,
            }
    except Exception as e:
        raise Auth.exceptions.HTTPException(status_code=401, detail=str(e))

---

## This is our toy user database. Do not do this in production

**URL:** llms-txt#this-is-our-toy-user-database.-do-not-do-this-in-production

VALID_TOKENS = {
    "user1-token": {"id": "user1", "name": "Alice"},
    "user2-token": {"id": "user2", "name": "Bob"},
}

---

## this is supported

**URL:** llms-txt#this-is-supported

{"messages": [HumanMessage(content="message")]}

---

## this is the graph making function that will decide which graph to

**URL:** llms-txt#this-is-the-graph-making-function-that-will-decide-which-graph-to

---

## This is the state before last (states are listed in chronological order)

**URL:** llms-txt#this-is-the-state-before-last-(states-are-listed-in-chronological-order)

**Contents:**
  - 3. Update the state
  - 4. Resume execution from the checkpoint

selected_state = states[1]
print(selected_state.next)
print(selected_state.values)

('write_joke',)
{'topic': 'How about "The Secret Life of Socks in the Dryer"? You know, exploring the mysterious phenomenon of how socks go into the laundry as pairs but come out as singles. Where do they go? Are they starting new lives elsewhere? Is there a sock paradise we don\\'t know about? There\\'s a lot of comedic potential in the everyday mystery that unites us all!'}
python  theme={null}
new_config = graph.update_state(selected_state.config, values={"topic": "chickens"})
print(new_config)

{'configurable': {'thread_id': 'c62e2e03-c27b-4cb6-8cea-ea9bfedae006', 'checkpoint_ns': '', 'checkpoint_id': '1f02ac4a-ecee-600b-8002-a1d21df32e4c'}}
python  theme={null}
graph.invoke(None, new_config)
python  theme={null}
{'topic': 'chickens',
 'joke': 'Why did the chicken join a band?\n\nBecause it had excellent drumsticks!'}
```

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/use-time-travel.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
**Output:**
```

Example 2 (unknown):
```unknown
<a id="optional" />

### 3. Update the state

[`update_state`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.CompiledStateGraph.update_state) will create a new checkpoint. The new checkpoint will be associated with the same thread, but a new checkpoint ID.
```

Example 3 (unknown):
```unknown
**Output:**
```

Example 4 (unknown):
```unknown
### 4. Resume execution from the checkpoint
```

---

## This is your PUBLIC anon key (which is safe to use client-side)

**URL:** llms-txt#this-is-your-public-anon-key-(which-is-safe-to-use-client-side)

---

## This means that after 'tools' is called, 'agent' node is called next.

**URL:** llms-txt#this-means-that-after-'tools'-is-called,-'agent'-node-is-called-next.

workflow.add_edge("tools", 'agent')

---

## This means that this node is the first one called

**URL:** llms-txt#this-means-that-this-node-is-the-first-one-called

workflow.add_edge(START, "agent")

---

## This takes precedenceover the generic @auth.on handler and the @auth.on.threads handler

**URL:** llms-txt#this-takes-precedenceover-the-generic-@auth.on-handler-and-the-@auth.on.threads-handler

@auth.on.threads.create_run
async def on_run_create(
    ctx: Auth.types.AuthContext,
    value: Auth.types.threads.create_run.value
):
    metadata = value.setdefault("metadata", {})
    metadata["owner"] = ctx.user.identity
    # Inherit thread's access control
    return {"owner": ctx.user.identity}

---

## This variable is just used for demonstration purposes to simulate a network failure.

**URL:** llms-txt#this-variable-is-just-used-for-demonstration-purposes-to-simulate-a-network-failure.

---

## This will become important when we're running our evaluations.

**URL:** llms-txt#this-will-become-important-when-we're-running-our-evaluations.

def refund(state: State, config: RunnableConfig) -> dict:
    # Whether to mock the deletion. True if the configurable var 'env' is set to 'test'.
    mock = config.get("configurable", {}).get("env", "prod") == "test"
    refunded = _refund(
        invoice_id=state["invoice_id"], invoice_line_ids=state["invoice_line_ids"], mock=mock
    )
    response = f"You have been refunded a total of: ${refunded:.2f}. Is there anything else I can help with?"
    return {
        "messages": [{"role": "assistant", "content": response}],
        "followup": response,
    }

---

## This WILL be traced

**URL:** llms-txt#this-will-be-traced

with ls.tracing_context(enabled=True):
    agent.invoke({"messages": [{"role": "user", "content": "Send a test email to alice@example.com"}]})

---

## This will NOT be traced (if LANGSMITH_TRACING is not set)

**URL:** llms-txt#this-will-not-be-traced-(if-langsmith_tracing-is-not-set)

**Contents:**
- Log to a project
- Add metadata to traces
- Use anonymizers to prevent logging of sensitive data in traces

agent.invoke({"messages": [{"role": "user", "content": "Send another email"}]})
bash  theme={null}
  export LANGSMITH_PROJECT=my-agent-project
  python  theme={null}
  import langsmith as ls

with ls.tracing_context(project_name="email-agent-test", enabled=True):
      response = agent.invoke({
          "messages": [{"role": "user", "content": "Send a welcome email"}]
      })
  python  theme={null}
response = agent.invoke(
    {"messages": [{"role": "user", "content": "Send a welcome email"}]},
    config={
        "tags": ["production", "email-assistant", "v1.0"],
        "metadata": {
            "user_id": "user_123",
            "session_id": "session_456",
            "environment": "production"
        }
    }
)
python  theme={null}
with ls.tracing_context(
    project_name="email-agent-test",
    enabled=True,
    tags=["production", "email-assistant", "v1.0"],
    metadata={"user_id": "user_123", "session_id": "session_456", "environment": "production"}):
    response = agent.invoke(
        {"messages": [{"role": "user", "content": "Send a welcome email"}]}
    )
typescript TypeScript theme={null}
import { StateGraph } from "@langchain/langgraph";
import { LangChainTracer } from "@langchain/core/tracers/tracer_langchain";
import { StateAnnotation } from "./state.js";
import { createAnonymizer } from "langsmith/anonymizer"
import { Client } from "langsmith"

const anonymizer = createAnonymizer([
    # Matches SSNs
    { pattern: /\b\d{3}-?\d{2}-?\d{4}\b/, replace: "<ssn>" }
])

const langsmithClient = new Client({ anonymizer })
const tracer = new LangChainTracer({
  client: langsmithClient,
});

export const graph = new StateGraph(StateAnnotation)
  .compile()
  .withConfig({
    callbacks: [tracer],
});
```

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/observability.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
## Log to a project

<Accordion title="Statically">
  You can set a custom project name for your entire application by setting the `LANGSMITH_PROJECT` environment variable:
```

Example 2 (unknown):
```unknown
</Accordion>

<Accordion title="Dynamically">
  You can set the project name programmatically for specific operations:
```

Example 3 (unknown):
```unknown
</Accordion>

## Add metadata to traces

You can annotate your traces with custom metadata and tags:
```

Example 4 (unknown):
```unknown
`tracing_context` also accepts tags and metadata for fine-grained control:
```

---

## Thread 1: Write to long-term memory

**URL:** llms-txt#thread-1:-write-to-long-term-memory

config1 = {"configurable": {"thread_id": str(uuid.uuid4())}}
agent.invoke({
    "messages": [{"role": "user", "content": "Save my preferences to /memories/preferences.txt"}]
}, config=config1)

---

## Thread 2: Read from long-term memory (different conversation!)

**URL:** llms-txt#thread-2:-read-from-long-term-memory-(different-conversation!)

config2 = {"configurable": {"thread_id": str(uuid.uuid4())}}
agent.invoke({
    "messages": [{"role": "user", "content": "What are my preferences?"}]
}, config=config2)

---

## Thread creation. This will match only on thread create actions

**URL:** llms-txt#thread-creation.-this-will-match-only-on-thread-create-actions

---

## thread_id is the persistent pointer (stores a stable ID in production)

**URL:** llms-txt#thread_id-is-the-persistent-pointer-(stores-a-stable-id-in-production)

config = {"configurable": {"thread_id": "thread-1"}}
result = graph.invoke({"input": "data"}, config=config)

---

## Thread limit only (no per-run limit)

**URL:** llms-txt#thread-limit-only-(no-per-run-limit)

database_limiter = ToolCallLimitMiddleware(
    tool_name="query_database",
    thread_limit=10,
)

---

## Time travel using the server API

**URL:** llms-txt#time-travel-using-the-server-api

**Contents:**
- Use time travel in a workflow
  - 1. Run the graph
  - 2. Identify a checkpoint
  - 3. Update the state
  - 4. Resume execution from the checkpoint
- Learn more

Source: https://docs.langchain.com/langsmith/human-in-the-loop-time-travel

LangGraph provides the [**time travel**](/oss/python/langgraph/use-time-travel) functionality to resume execution from a prior checkpoint, either replaying the same state or modifying it to explore alternatives. In all cases, resuming past execution produces a new fork in the history.

To time travel using the LangSmith Deployment API (via the LangGraph SDK):

1. **Run the graph** with initial inputs using [LangGraph SDK](/langsmith/langgraph-python-sdk)'s [client.runs.wait](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.client.RunsClient.wait) or [client.runs.stream](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.client.RunsClient.stream) APIs.
2. **Identify a checkpoint in an existing thread**: Use [client.threads.get\_history](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.client.ThreadsClient.get_history) method to retrieve the execution history for a specific `thread_id` and locate the desired `checkpoint_id`.
   Alternatively, set a [breakpoint](/oss/python/langgraph/interrupts) before the node(s) where you want execution to pause. You can then find the most recent checkpoint recorded up to that breakpoint.
3. **(Optional) modify the graph state**: Use the [client.threads.update\_state](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.client.ThreadsClient.update_state) method to modify the graph’s state at the checkpoint and resume execution from alternative state.
4. **Resume execution from the checkpoint**: Use the [client.runs.wait](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.client.RunsClient.wait) or [client.runs.stream](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.client.RunsClient.stream) APIs with an input of `None` and the appropriate `thread_id` and `checkpoint_id`.

## Use time travel in a workflow

<Accordion title="Example graph">
  
</Accordion>

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="JavaScript">
    
  </Tab>

<Tab title="cURL">
    Create a thread:

### 2. Identify a checkpoint

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="JavaScript">
    
  </Tab>

<Tab title="cURL">
    
  </Tab>
</Tabs>

### 3. Update the state

[`update_state`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.CompiledStateGraph.update_state) will create a new checkpoint. The new checkpoint will be associated with the same thread, but a new checkpoint ID.

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="JavaScript">
    
  </Tab>

<Tab title="cURL">
    
  </Tab>
</Tabs>

### 4. Resume execution from the checkpoint

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="JavaScript">
    
  </Tab>

<Tab title="cURL">
    
  </Tab>
</Tabs>

* [**LangGraph time travel guide**](/oss/python/langgraph/use-time-travel): learn more about using time travel in LangGraph.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/human-in-the-loop-time-travel.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
</Accordion>

### 1. Run the graph

<Tabs>
  <Tab title="Python">
```

Example 2 (unknown):
```unknown
</Tab>

  <Tab title="JavaScript">
```

Example 3 (unknown):
```unknown
</Tab>

  <Tab title="cURL">
    Create a thread:
```

Example 4 (unknown):
```unknown
Run the graph:
```

---

## TodoListMiddleware is included by default in create_deep_agent

**URL:** llms-txt#todolistmiddleware-is-included-by-default-in-create_deep_agent

---

## Tools

**URL:** llms-txt#tools

from langchain.tools import tool

---

## Tools and Toolkits

**URL:** llms-txt#tools-and-toolkits

**Contents:**
- Integration Platforms
- All Tools and Toolkits

Source: https://docs.langchain.com/oss/javascript/integrations/tools/index

[Tools](/oss/javascript/langchain/tools) are utilities designed to be called by a model: their inputs are designed to be generated by models, and their outputs are designed to be passed back to models.

A [toolkit](/oss/javascript/langchain/tools#toolkits) is a collection of tools meant to be used together.

## Integration Platforms

The following platforms provide access to multiple tools and services through a unified interface:

| Tool/Toolkit                                            | Number of Integrations | Pricing             | Key Features                                               |
| ------------------------------------------------------- | ---------------------- | ------------------- | ---------------------------------------------------------- |
| [Composio](/oss/javascript/integrations/tools/composio) | 500+                   | Free tier available | OAuth handling, event-driven workflows, multi-user support |

## All Tools and Toolkits

<Columns cols={3}>
  <Card title="Azure Container Apps Dynamic Sessions" icon="link" href="/oss/javascript/integrations/tools/azure_dynamic_sessions" arrow="true" cta="View guide" />

<Card title="Connery Action Tool" icon="link" href="/oss/javascript/integrations/tools/connery" arrow="true" cta="View guide" />

<Card title="Composio" icon="link" href="/oss/javascript/integrations/tools/composio" arrow="true" cta="View guide" />

<Card title="Dall-E Tool" icon="link" href="/oss/javascript/integrations/tools/dalle" arrow="true" cta="View guide" />

<Card title="Decodo Tools" icon="link" href="/oss/javascript/integrations/tools/decodo" arrow="true" cta="View guide" />

<Card title="Discord Tool" icon="link" href="/oss/javascript/integrations/tools/discord_tool" arrow="true" cta="View guide" />

<Card title="DuckDuckGoSearch" icon="link" href="/oss/javascript/integrations/tools/duckduckgo_search" arrow="true" cta="View guide" />

<Card title="ExaSearchResults" icon="link" href="/oss/javascript/integrations/tools/exa_search" arrow="true" cta="View guide" />

<Card title="FalkorDB" icon="link" href="/oss/javascript/integrations/tools/falkordb" arrow="true" cta="View guide" />

<Card title="Gmail Tool" icon="link" href="/oss/javascript/integrations/tools/gmail" arrow="true" cta="View guide" />

<Card title="GOAT" icon="link" href="/oss/javascript/integrations/tools/goat" arrow="true" cta="View guide" />

<Card title="Google Calendar Tool" icon="link" href="/oss/javascript/integrations/tools/google_calendar" arrow="true" cta="View guide" />

<Card title="Google Places Tool" icon="link" href="/oss/javascript/integrations/tools/google_places" arrow="true" cta="View guide" />

<Card title="Google Routes Tool" icon="link" href="/oss/javascript/integrations/tools/google_routes" arrow="true" cta="View guide" />

<Card title="Google Scholar" icon="link" href="/oss/javascript/integrations/tools/google_scholar" arrow="true" cta="View guide" />

<Card title="Google Trends Tool" icon="link" href="/oss/javascript/integrations/tools/google_trends" arrow="true" cta="View guide" />

<Card title="JigsawStack Tool" icon="link" href="/oss/javascript/integrations/tools/jigsawstack" arrow="true" cta="View guide" />

<Card title="Agent with AWS Lambda" icon="link" href="/oss/javascript/integrations/tools/lambda_agent" arrow="true" cta="View guide" />

<Card title="Python interpreter tool" icon="link" href="/oss/javascript/integrations/tools/pyinterpreter" arrow="true" cta="View guide" />

<Card title="SearchApi tool" icon="link" href="/oss/javascript/integrations/tools/searchapi" arrow="true" cta="View guide" />

<Card title="Searxng Search tool" icon="link" href="/oss/javascript/integrations/tools/searxng" arrow="true" cta="View guide" />

<Card title="SerpAPI" icon="link" href="/oss/javascript/integrations/tools/serpapi" arrow="true" cta="View guide" />

<Card title="StackExchange Tool" icon="link" href="/oss/javascript/integrations/tools/stackexchange" arrow="true" cta="View guide" />

<Card title="Stagehand AI Web Automation Toolkit" icon="link" href="/oss/javascript/integrations/tools/stagehand" arrow="true" cta="View guide" />

<Card title="Tavily Crawl" icon="link" href="/oss/javascript/integrations/tools/tavily_crawl" arrow="true" cta="View guide" />

<Card title="Tavily Extract" icon="link" href="/oss/javascript/integrations/tools/tavily_extract" arrow="true" cta="View guide" />

<Card title="Tavily Map" icon="link" href="/oss/javascript/integrations/tools/tavily_map" arrow="true" cta="View guide" />

<Card title="Tavily Search" icon="link" href="/oss/javascript/integrations/tools/tavily_search" arrow="true" cta="View guide" />

<Card title="Web Browser Tool" icon="link" href="/oss/javascript/integrations/tools/webbrowser" arrow="true" cta="View guide" />

<Card title="Wikipedia tool" icon="link" href="/oss/javascript/integrations/tools/wikipedia" arrow="true" cta="View guide" />

<Card title="WolframAlpha Tool" icon="link" href="/oss/javascript/integrations/tools/wolframalpha" arrow="true" cta="View guide" />
</Columns>

<Info>
  If you'd like to write your own tool, see [this how-to](/oss/javascript/langchain/tools#customize-tool-properties). If you'd like to contribute an integration, see [Contributing integrations](/oss/javascript/contributing#add-a-new-integration).
</Info>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/javascript/integrations/tools/index.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## tools = [DuckDuckGoSearchRun(rate_limiter=rate_limiter)]

**URL:** llms-txt#tools-=-[duckduckgosearchrun(rate_limiter=rate_limiter)]

**Contents:**
  - Simulate production data
- Convert Production Traces to Experiment
  - Select runs to backtest on

tools = [TavilySearchResults(max_results=5, rate_limiter=rate_limiter)]

agent = create_agent(gpt_3_5_turbo, tools=tools, system_prompt=instructions)
python  theme={null}
fake_production_inputs = [
    "Alan turing's early childhood",
    "Economic impacts of the European Union",
    "Underrated philosophers",
    "History of the Roxie theater in San Francisco",
    "ELI5: gravitational waves",
    "The arguments for and against a parliamentary system",
    "Pivotal moments in music history",
    "Big ideas in programming languages",
    "Big questions in biology",
    "The relationship between math and reality",
    "What makes someone funny",
]

agent.batch(
    [{"messages": [{"role": "user", "content": content}]} for content in fake_production_inputs],
)
python  theme={null}
from datetime import datetime, timedelta, timezone
from uuid import uuid4
from langsmith import Client
from langsmith.beta import convert_runs_to_test

**Examples:**

Example 1 (unknown):
```unknown
### Simulate production data

Now lets simulate some production data:
```

Example 2 (unknown):
```unknown
## Convert Production Traces to Experiment

The first step is to generate a dataset based on the production *inputs*. Then copy over all the traces to serve as a baseline experiment.

### Select runs to backtest on

You can select the runs to backtest on using the `filter` argument of `list_runs`. The `filter` argument uses the LangSmith [trace query syntax](/langsmith/trace-query-syntax) to select runs.
```

---

## Tool-specific limit with default "continue" behavior

**URL:** llms-txt#tool-specific-limit-with-default-"continue"-behavior

search_limiter = ToolCallLimitMiddleware(
    tool_name="search",
    thread_limit=5,
    run_limit=3,
)

---

## Tool that allows agent to update user information (useful for chat applications)

**URL:** llms-txt#tool-that-allows-agent-to-update-user-information-(useful-for-chat-applications)

@tool
def save_user_info(user_info: UserInfo, runtime: ToolRuntime[Context]) -> str:
    """Save user info."""
    # Access the store - same as that provided to `create_agent`
    store = runtime.store # [!code highlight]
    user_id = runtime.context.user_id # [!code highlight]
    # Store data in the store (namespace, key, data)
    store.put(("users",), user_id, user_info) # [!code highlight]
    return "Successfully saved user info."

agent = create_agent(
    model="claude-sonnet-4-5-20250929",
    tools=[save_user_info],
    store=store, # [!code highlight]
    context_schema=Context
)

---

## To approve

**URL:** llms-txt#to-approve

graph.invoke(Command(resume=True), config=config)

---

## To ensure this, we'll create vectorstore indexes for all of the artists, tracks and albums

**URL:** llms-txt#to-ensure-this,-we'll-create-vectorstore-indexes-for-all-of-the-artists,-tracks-and-albums

---

## To reject

**URL:** llms-txt#to-reject

**Contents:**
  - Review and edit state
  - Interrupts in tools
  - Validating human input
- Rules of interrupts
  - Do not wrap `interrupt` calls in try/except
  - Do not reorder `interrupt` calls within a node
  - Do not return complex values in `interrupt` calls
  - Side effects called before `interrupt` must be idempotent
- Using with subgraphs called as functions
- Debugging with interrupts

graph.invoke(Command(resume=False), config=config)
python  theme={null}
  import sqlite3
  from typing import Literal, Optional, TypedDict

from langgraph.checkpoint.memory import MemorySaver
  from langgraph.graph import StateGraph, START, END
  from langgraph.types import Command, interrupt

class ApprovalState(TypedDict):
      action_details: str
      status: Optional[Literal["pending", "approved", "rejected"]]

def approval_node(state: ApprovalState) -> Command[Literal["proceed", "cancel"]]:
      # Expose details so the caller can render them in a UI
      decision = interrupt({
          "question": "Approve this action?",
          "details": state["action_details"],
      })

# Route to the appropriate node after resume
      return Command(goto="proceed" if decision else "cancel")

def proceed_node(state: ApprovalState):
      return {"status": "approved"}

def cancel_node(state: ApprovalState):
      return {"status": "rejected"}

builder = StateGraph(ApprovalState)
  builder.add_node("approval", approval_node)
  builder.add_node("proceed", proceed_node)
  builder.add_node("cancel", cancel_node)
  builder.add_edge(START, "approval")
  builder.add_edge("approval", "proceed")
  builder.add_edge("approval", "cancel")
  builder.add_edge("proceed", END)
  builder.add_edge("cancel", END)

# Use a more durable checkpointer in production
  checkpointer = MemorySaver()
  graph = builder.compile(checkpointer=checkpointer)

config = {"configurable": {"thread_id": "approval-123"}}
  initial = graph.invoke(
      {"action_details": "Transfer $500", "status": "pending"},
      config=config,
  )
  print(initial["__interrupt__"])  # -> [Interrupt(value={'question': ..., 'details': ...})]

# Resume with the decision; True routes to proceed, False to cancel
  resumed = graph.invoke(Command(resume=True), config=config)
  print(resumed["status"])  # -> "approved"
  python  theme={null}
from langgraph.types import interrupt

def review_node(state: State):
    # Pause and show the current content for review (surfaces in result["__interrupt__"])
    edited_content = interrupt({
        "instruction": "Review and edit this content",
        "content": state["generated_text"]
    })

# Update the state with the edited version
    return {"generated_text": edited_content}
python  theme={null}
graph.invoke(
    Command(resume="The edited and improved text"),  # Value becomes the return from interrupt()
    config=config
)
python  theme={null}
  import sqlite3
  from typing import TypedDict

from langgraph.checkpoint.memory import MemorySaver
  from langgraph.graph import StateGraph, START, END
  from langgraph.types import Command, interrupt

class ReviewState(TypedDict):
      generated_text: str

def review_node(state: ReviewState):
      # Ask a reviewer to edit the generated content
      updated = interrupt({
          "instruction": "Review and edit this content",
          "content": state["generated_text"],
      })
      return {"generated_text": updated}

builder = StateGraph(ReviewState)
  builder.add_node("review", review_node)
  builder.add_edge(START, "review")
  builder.add_edge("review", END)

checkpointer = MemorySaver()
  graph = builder.compile(checkpointer=checkpointer)

config = {"configurable": {"thread_id": "review-42"}}
  initial = graph.invoke({"generated_text": "Initial draft"}, config=config)
  print(initial["__interrupt__"])  # -> [Interrupt(value={'instruction': ..., 'content': ...})]

# Resume with the edited text from the reviewer
  final_state = graph.invoke(
      Command(resume="Improved draft after review"),
      config=config,
  )
  print(final_state["generated_text"])  # -> "Improved draft after review"
  python  theme={null}
from langchain.tools import tool
from langgraph.types import interrupt

@tool
def send_email(to: str, subject: str, body: str):
    """Send an email to a recipient."""

# Pause before sending; payload surfaces in result["__interrupt__"]
    response = interrupt({
        "action": "send_email",
        "to": to,
        "subject": subject,
        "body": body,
        "message": "Approve sending this email?"
    })

if response.get("action") == "approve":
        # Resume value can override inputs before executing
        final_to = response.get("to", to)
        final_subject = response.get("subject", subject)
        final_body = response.get("body", body)
        return f"Email sent to {final_to} with subject '{final_subject}'"
    return "Email cancelled by user"
python  theme={null}
  import sqlite3
  from typing import TypedDict

from langchain.tools import tool
  from langchain_anthropic import ChatAnthropic
  from langgraph.checkpoint.sqlite import SqliteSaver
  from langgraph.graph import StateGraph, START, END
  from langgraph.types import Command, interrupt

class AgentState(TypedDict):
      messages: list[dict]

@tool
  def send_email(to: str, subject: str, body: str):
      """Send an email to a recipient."""

# Pause before sending; payload surfaces in result["__interrupt__"]
      response = interrupt({
          "action": "send_email",
          "to": to,
          "subject": subject,
          "body": body,
          "message": "Approve sending this email?",
      })

if response.get("action") == "approve":
          final_to = response.get("to", to)
          final_subject = response.get("subject", subject)
          final_body = response.get("body", body)

# Actually send the email (your implementation here)
          print(f"[send_email] to={final_to} subject={final_subject} body={final_body}")
          return f"Email sent to {final_to}"

return "Email cancelled by user"

model = ChatAnthropic(model="claude-sonnet-4-5-20250929").bind_tools([send_email])

def agent_node(state: AgentState):
      # LLM may decide to call the tool; interrupt pauses before sending
      result = model.invoke(state["messages"])
      return {"messages": state["messages"] + [result]}

builder = StateGraph(AgentState)
  builder.add_node("agent", agent_node)
  builder.add_edge(START, "agent")
  builder.add_edge("agent", END)

checkpointer = SqliteSaver(sqlite3.connect("tool-approval.db"))
  graph = builder.compile(checkpointer=checkpointer)

config = {"configurable": {"thread_id": "email-workflow"}}
  initial = graph.invoke(
      {
          "messages": [
              {"role": "user", "content": "Send an email to alice@example.com about the meeting"}
          ]
      },
      config=config,
  )
  print(initial["__interrupt__"])  # -> [Interrupt(value={'action': 'send_email', ...})]

# Resume with approval and optionally edited arguments
  resumed = graph.invoke(
      Command(resume={"action": "approve", "subject": "Updated subject"}),
      config=config,
  )
  print(resumed["messages"][-1])  # -> Tool result returned by send_email
  python  theme={null}
from langgraph.types import interrupt

def get_age_node(state: State):
    prompt = "What is your age?"

while True:
        answer = interrupt(prompt)  # payload surfaces in result["__interrupt__"]

# Validate the input
        if isinstance(answer, int) and answer > 0:
            # Valid input - continue
            break
        else:
            # Invalid input - ask again with a more specific prompt
            prompt = f"'{answer}' is not a valid age. Please enter a positive number."

return {"age": answer}
python  theme={null}
  import sqlite3
  from typing import TypedDict

from langgraph.checkpoint.sqlite import SqliteSaver
  from langgraph.graph import StateGraph, START, END
  from langgraph.types import Command, interrupt

class FormState(TypedDict):
      age: int | None

def get_age_node(state: FormState):
      prompt = "What is your age?"

while True:
          answer = interrupt(prompt)  # payload surfaces in result["__interrupt__"]

if isinstance(answer, int) and answer > 0:
              return {"age": answer}

prompt = f"'{answer}' is not a valid age. Please enter a positive number."

builder = StateGraph(FormState)
  builder.add_node("collect_age", get_age_node)
  builder.add_edge(START, "collect_age")
  builder.add_edge("collect_age", END)

checkpointer = SqliteSaver(sqlite3.connect("forms.db"))
  graph = builder.compile(checkpointer=checkpointer)

config = {"configurable": {"thread_id": "form-1"}}
  first = graph.invoke({"age": None}, config=config)
  print(first["__interrupt__"])  # -> [Interrupt(value='What is your age?', ...)]

# Provide invalid data; the node re-prompts
  retry = graph.invoke(Command(resume="thirty"), config=config)
  print(retry["__interrupt__"])  # -> [Interrupt(value="'thirty' is not a valid age...", ...)]

# Provide valid data; loop exits and state updates
  final = graph.invoke(Command(resume=30), config=config)
  print(final["age"])  # -> 30
  python Separating logic theme={null}
  def node_a(state: State):
      # ✅ Good: interrupting first, then handling
      # error conditions separately
      interrupt("What's your name?")
      try:
          fetch_data()  # This can fail
      except Exception as e:
          print(e)
      return state
  python Explicit exception handling theme={null}
  def node_a(state: State):
      # ✅ Good: catching specific exception types
      # will not catch the interrupt exception
      try:
          name = interrupt("What's your name?")
          fetch_data()  # This can fail
      except NetworkException as e:
          print(e)
      return state
  python  theme={null}
def node_a(state: State):
    # ❌ Bad: wrapping interrupt in bare try/except
    # will catch the interrupt exception
    try:
        interrupt("What's your name?")
    except Exception as e:
        print(e)
    return state
python  theme={null}
def node_a(state: State):
    # ✅ Good: interrupt calls happen in the same order every time
    name = interrupt("What's your name?")
    age = interrupt("What's your age?")
    city = interrupt("What's your city?")

return {
        "name": name,
        "age": age,
        "city": city
    }
python Skipping interrupts theme={null}
  def node_a(state: State):
      # ❌ Bad: conditionally skipping interrupts changes the order
      name = interrupt("What's your name?")

# On first run, this might skip the interrupt
      # On resume, it might not skip it - causing index mismatch
      if state.get("needs_age"):
          age = interrupt("What's your age?")

city = interrupt("What's your city?")

return {"name": name, "city": city}
  python Looping interrupts theme={null}
  def node_a(state: State):
      # ❌ Bad: looping based on non-deterministic data
      # The number of interrupts changes between executions
      results = []
      for item in state.get("dynamic_list", []):  # List might change between runs
          result = interrupt(f"Approve {item}?")
          results.append(result)

return {"results": results}
  python Simple values theme={null}
  def node_a(state: State):
      # ✅ Good: passing simple types that are serializable
      name = interrupt("What's your name?")
      count = interrupt(42)
      approved = interrupt(True)

return {"name": name, "count": count, "approved": approved}
  python Structured data theme={null}
  def node_a(state: State):
      # ✅ Good: passing dictionaries with simple values
      response = interrupt({
          "question": "Enter user details",
          "fields": ["name", "email", "age"],
          "current_values": state.get("user", {})
      })

return {"user": response}
  python Functions theme={null}
  def validate_input(value):
      return len(value) > 0

def node_a(state: State):
      # ❌ Bad: passing a function to interrupt
      # The function cannot be serialized
      response = interrupt({
          "question": "What's your name?",
          "validator": validate_input  # This will fail
      })
      return {"name": response}
  python Class instances theme={null}
  class DataProcessor:
      def __init__(self, config):
          self.config = config

def node_a(state: State):
      processor = DataProcessor({"mode": "strict"})

# ❌ Bad: passing a class instance to interrupt
      # The instance cannot be serialized
      response = interrupt({
          "question": "Enter data to process",
          "processor": processor  # This will fail
      })
      return {"result": response}
  python Idempotent operations theme={null}
  def node_a(state: State):
      # ✅ Good: using upsert operation which is idempotent
      # Running this multiple times will have the same result
      db.upsert_user(
          user_id=state["user_id"],
          status="pending_approval"
      )

approved = interrupt("Approve this change?")

return {"approved": approved}
  python Side effects after interrupt theme={null}
  def node_a(state: State):
      # ✅ Good: placing side effect after the interrupt
      # This ensures it only runs once after approval is received
      approved = interrupt("Approve this change?")

if approved:
          db.create_audit_log(
              user_id=state["user_id"],
              action="approved"
          )

return {"approved": approved}
  python Separating into different nodes theme={null}
  def approval_node(state: State):
      # ✅ Good: only handling the interrupt in this node
      approved = interrupt("Approve this change?")

return {"approved": approved}

def notification_node(state: State):
      # ✅ Good: side effect happens in a separate node
      # This runs after approval, so it only executes once
      if (state.approved):
          send_notification(
              user_id=state["user_id"],
              status="approved"
          )

return state
  python Creating records theme={null}
  def node_a(state: State):
      # ❌ Bad: creating a new record before interrupt
      # This will create duplicate records on each resume
      audit_id = db.create_audit_log({
          "user_id": state["user_id"],
          "action": "pending_approval",
          "timestamp": datetime.now()
      })

approved = interrupt("Approve this change?")

return {"approved": approved, "audit_id": audit_id}
  python Appending to lists theme={null}
  def node_a(state: State):
      # ❌ Bad: appending to a list before interrupt
      # This will add duplicate entries on each resume
      db.append_to_history(state["user_id"], "approval_requested")

approved = interrupt("Approve this change?")

return {"approved": approved}
  python  theme={null}
def node_in_parent_graph(state: State):
    some_code()  # <-- This will re-execute when resumed
    # Invoke a subgraph as a function.
    # The subgraph contains an `interrupt` call.
    subgraph_result = subgraph.invoke(some_input)

async function node_in_subgraph(state: State) {
    someOtherCode(); # <-- This will also re-execute when resumed
    result = interrupt("What's your name?")
    ...
}
python  theme={null}
    graph = builder.compile(
        interrupt_before=["node_a"],  # [!code highlight]
        interrupt_after=["node_b", "node_c"],  # [!code highlight]
        checkpointer=checkpointer,
    )

# Pass a thread ID to the graph
    config = {
        "configurable": {
            "thread_id": "some_thread"
        }
    }

# Run the graph until the breakpoint
    graph.invoke(inputs, config=config)  # [!code highlight]

# Resume the graph
    graph.invoke(None, config=config)  # [!code highlight]
    python  theme={null}
    config = {
        "configurable": {
            "thread_id": "some_thread"
        }
    }

# Run the graph until the breakpoint
    graph.invoke(
        inputs,
        interrupt_before=["node_a"],  # [!code highlight]
        interrupt_after=["node_b", "node_c"],  # [!code highlight]
        config=config,
    )

# Resume the graph
    graph.invoke(None, config=config)  # [!code highlight]
    ```

1. `graph.invoke` is called with the `interrupt_before` and `interrupt_after` parameters. This is a run-time configuration and can be changed for every invocation.
    2. `interrupt_before` specifies the nodes where execution should pause before the node is executed.
    3. `interrupt_after` specifies the nodes where execution should pause after the node is executed.
    4. The graph is run until the first breakpoint is hit.
    5. The graph is resumed by passing in `None` for the input. This will run the graph until the next breakpoint is hit.
  </Tab>
</Tabs>

### Using LangGraph Studio

You can use [LangGraph Studio](/langsmith/studio) to set static interrupts in your graph in the UI before running the graph. You can also use the UI to inspect the graph state at any point in the execution.

<img src="https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/static-interrupt.png?fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=5aa4e7cea2ab147cef5b4e210dd6c4a1" alt="image" data-og-width="1252" width="1252" data-og-height="1040" height="1040" data-path="oss/images/static-interrupt.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/static-interrupt.png?w=280&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=52d02b507d0a6a879f7fb88d9c6767d0 280w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/static-interrupt.png?w=560&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=e363cd4980edff9bab422f4f1c0ee3c8 560w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/static-interrupt.png?w=840&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=49d26a3641953c23ef3fbc51e828c305 840w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/static-interrupt.png?w=1100&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=2dba15683b3baa1a61bc3bcada35ae1e 1100w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/static-interrupt.png?w=1650&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=9f9a2c0f2631c0e69cd248f6319933fe 1650w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/static-interrupt.png?w=2500&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=5a46b765b436ab5d0dc2f41c01ffad80 2500w" />

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/interrupts.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
<Accordion title="Full example">
```

Example 2 (unknown):
```unknown
</Accordion>

### Review and edit state

Sometimes you want to let a human review and edit part of the graph state before continuing. This is useful for correcting LLMs, adding missing information, or making adjustments.
```

Example 3 (unknown):
```unknown
When resuming, provide the edited content:
```

Example 4 (unknown):
```unknown
<Accordion title="Full example">
```

---

## Traces: [OTel Example](/langsmith/langsmith-collector#traces)

**URL:** llms-txt#traces:-[otel-example](/langsmith/langsmith-collector#traces)

The LangSmith Backend, Platform Backend, Playground and LangSmith Queue deployments have been instrumented to emit [Otel](https://opentelemetry.io/do/langsmith/observability-concepts/signals/traces/) traces. Tracing is toggled off by default, and can be enabled for all LangSmith services with the following in your `langsmith_config.yaml` (or equivalent) file:

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/export-backend.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Trace a RAG application tutorial

**URL:** llms-txt#trace-a-rag-application-tutorial

**Contents:**
- Prototyping
  - Set up your environment
  - Trace your LLM calls
  - Trace the whole chain
- Beta Testing
  - Collecting Feedback
  - Logging Metadata
- Production
  - Monitoring
  - A/B Testing

Source: https://docs.langchain.com/langsmith/observability-llm-tutorial

In this tutorial, we'll build a simple RAG application using the OpenAI SDK. We'll add observability to the application at each stage of development, from prototyping to production.

Having observability set up from the start can help you iterate **much** more quickly than you would otherwise be able to. It allows you to have great visibility into your application as you are rapidly iterating on the prompt, or changing the data and models you are using. In this section we'll walk through how to set up observability so you can have maximal observability as you are prototyping.

### Set up your environment

First, create an API key by navigating to the [settings page](https://smith.langchain.com/settings).

Next, install the LangSmith SDK:

Finally, set up the appropriate environment variables. This will log traces to the `default` project (though you can easily change that).

<Note>
  You may see these variables referenced as `LANGCHAIN_*` in other places. These are all equivalent, however the best practice is to use `LANGSMITH_TRACING`, `LANGSMITH_API_KEY`, `LANGSMITH_PROJECT`.

The `LANGSMITH_PROJECT` flag is only supported in JS SDK versions >= 0.2.16, use `LANGCHAIN_PROJECT` instead if you are using an older version.
</Note>

### Trace your LLM calls

The first thing you might want to trace is all your OpenAI calls. After all, this is where the LLM is actually being called, so it is the most important part! We've tried to make this as easy as possible with LangSmith by introducing a dead-simple OpenAI wrapper. All you have to do is modify your code to look something like:

Notice how we import `from langsmith.wrappers import wrap_openai` and use it to wrap the OpenAI client (`openai_client = wrap_openai(OpenAI())`).

What happens if you call it in the following way?

This will produce a trace of just the OpenAI call - it should look something like [this](https://smith.langchain.com/public/e7b7d256-10fe-4d49-a8d5-36ca8e5af0d2/r)

<img src="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tracing-tutorial-openai.png?fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=8b3ad3b0d00851bce313311efa4e8bbb" alt="" data-og-width="1027" width="1027" data-og-height="615" height="615" data-path="langsmith/images/tracing-tutorial-openai.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tracing-tutorial-openai.png?w=280&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=c4ee9e306124a884702a7c0f5685e279 280w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tracing-tutorial-openai.png?w=560&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=995ebe3b7342ea797887a052f962919d 560w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tracing-tutorial-openai.png?w=840&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=24e6e8b84336197d23ed294d4d37c842 840w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tracing-tutorial-openai.png?w=1100&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=822b72e7b0ea10a96cc05eeeefad7b51 1100w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tracing-tutorial-openai.png?w=1650&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=985061817d26e8f76b50d2638b34ecb7 1650w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tracing-tutorial-openai.png?w=2500&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=bf5bb01fa39af440f985666c39809cc5 2500w" />

### Trace the whole chain

Great - we've traced the LLM call. But it's often very informative to trace more than that. LangSmith is **built** for tracing the entire LLM pipeline - so let's do that! We can do this by modifying the code to now look something like this:

Notice how we import `from langsmith import traceable` and use it decorate the overall function (`@traceable`).

What happens if you call it in the following way?

This will produce a trace of the entire RAG pipeline - it should look something like [this](https://smith.langchain.com/public/8cafba6a-1a6d-4a73-8565-483186f31c29/r)

<img src="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-chain.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=00afea1ffa117b90159d30a53aac5a7f" alt="" data-og-width="1016" width="1016" data-og-height="635" height="635" data-path="langsmith/images/tracing-tutorial-chain.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-chain.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=01eb0588af8534c636796b1ffc673a14 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-chain.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=38625d3b2d93cd41b344bc2610272ff4 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-chain.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=f713b97cfd693fec7ea51d85d06c1358 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-chain.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=039262907891da16b2a13d69ce3650ac 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-chain.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=97e50a697c1ac249ca91a38c42beaaa9 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-chain.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=871e76f1794d7c3d0d7fc3bb875fd613 2500w" />

The next stage of LLM application development is beta testing your application. This is when you release it to a few initial users. Having good observability set up here is crucial as often you don't know exactly how users will actually use your application, so this allows you get insights into how they do so. This also means that you probably want to make some changes to your tracing set up to better allow for that. This extends the observability you set up in the previous section

### Collecting Feedback

A huge part of having good observability during beta testing is collecting feedback. What feedback you collect is often application specific - but at the very least a simple thumbs up/down is a good start. After logging that feedback, you need to be able to easily associate it with the run that caused that. Luckily LangSmith makes it easy to do that.

First, you need to log the feedback from your app. An easy way to do this is to keep track of a run ID for each run, and then use that to log feedback. Keeping track of the run ID would look something like:

Associating feedback with that run would look something like:

Once the feedback is logged, you can then see it associated with each run by clicking into the `Metadata` tab when inspecting the run. It should look something like [this](https://smith.langchain.com/public/8cafba6a-1a6d-4a73-8565-483186f31c29/r)

<img src="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-feedback.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=cb81a556fe40895ebb29d4428d4c62d9" alt="" data-og-width="1025" width="1025" data-og-height="345" height="345" data-path="langsmith/images/tracing-tutorial-feedback.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-feedback.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=165c08ee4c4f96f9f3ebb6e8183dc539 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-feedback.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=f50e70bda816d314ac233430fe5703be 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-feedback.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=579ca04b25401fd98dbd1109e55f4a8c 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-feedback.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=9763f2b2f5cbb347f796e2c1951102cf 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-feedback.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=8c84bd91d25dbb0514385aabf8b2dbc2 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-feedback.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=b9f44b64e395eb392e9a6a0348189e84 2500w" />

You can also query for all runs with positive (or negative) feedback by using the filtering logic in the runs table. You can do this by creating a filter like the following:

<img src="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-filtering.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=57ebc19f2e5443c21353064c082971bc" alt="" data-og-width="940" width="940" data-og-height="496" height="496" data-path="langsmith/images/tracing-tutorial-filtering.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-filtering.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=630d2ed8d85794026cbf07fcc186f791 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-filtering.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=9bc6f8a5464f3c0bbdcf172ecb3e1f67 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-filtering.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=9daf8b6d1ecbc23112dba100a77bc0a6 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-filtering.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=fa18baf671267ec4a1bfce3cfdc1c789 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-filtering.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=b3184ab4bfe8361d1d52b4e63c054e58 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-filtering.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=2e1aeade5945fe20527a58c1966b5a10 2500w" />

It is also a good idea to start logging metadata. This allows you to start keep track of different attributes of your app. This is important in allowing you to know what version or variant of your app was used to produce a given result.

For this example, we will log the LLM used. Oftentimes you may be experimenting with different LLMs, so having that information as metadata can be useful for filtering. In order to do that, we can add it as such:

Notice we added `@traceable(metadata={"llm": "gpt-4o-mini"})` to the `rag` function.

Keeping track of metadata in this way assumes that it is known ahead of time. This is fine for LLM types, but less desirable for other types of information - like a User ID. In order to log information that, we can pass it in at run time with the run ID.

Now that we've logged these two pieces of metadata, we should be able to see them both show up in the UI [here](https://smith.langchain.com/public/37adf7e5-97aa-42d0-9850-99c0199bddf6/r).

<img src="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-metadata.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=db49738eba3e0ce26514df3c9b72f87c" alt="" data-og-width="1016" width="1016" data-og-height="337" height="337" data-path="langsmith/images/tracing-tutorial-metadata.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-metadata.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=c9f46f9e36cc47a6b10cc37870e17ffc 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-metadata.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=dba6e396adec67b74df789cfd9cd2491 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-metadata.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=e31ff00027f6c5741935cec40d997697 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-metadata.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=cf82f0eca41b0fb04ae181e38ae47ece 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-metadata.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=fd57501d821496d68d5921e3cb8ce457 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-metadata.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=568f6fb60e84333f677775817dbc79e8 2500w" />

We can filter for these pieces of information by constructing a filter like the following:

<img src="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-metadata-filtering.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=3662dfc2d8fb2c274f622e3f67e14b34" alt="" data-og-width="932" width="932" data-og-height="436" height="436" data-path="langsmith/images/tracing-tutorial-metadata-filtering.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-metadata-filtering.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=561f6c48361533d9ddcfc005136ff6c3 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-metadata-filtering.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=1ea92820bc977b16368e89d923a631f5 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-metadata-filtering.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=137d1faa53ec75e8aaecf3aa2da32378 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-metadata-filtering.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=f9c7902430dbae7b10543bd9b747a5fc 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-metadata-filtering.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=1494db66f66fceba483829a249a44a31 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-metadata-filtering.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=06c2f159f5ce8bf601640a8484be187f 2500w" />

Great - you've used this newfound observability to iterate quickly and gain confidence that your app is performing well. Time to ship it to production! What new observability do you need to add?

First of all, let's note that the same observability you've already added will keep on providing value in production. You will continue to be able to drill down into particular runs.

In production you likely have a LOT more traffic. So you don't really want to be stuck looking at datapoints one at a time. Luckily, LangSmith has a set of tools to help with observability in production.

If you click on the `Monitor` tab in a project, you will see a series of monitoring charts. Here we track lots of LLM specific statistics - number of traces, feedback, time-to-first-token, etc. You can view these over time across a few different time bins.

<img src="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tracing-tutorial-monitor.png?fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=74f49882d9e6323e2ed467b525b81b9a" alt="" data-og-width="946" width="946" data-og-height="746" height="746" data-path="langsmith/images/tracing-tutorial-monitor.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tracing-tutorial-monitor.png?w=280&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=d99a10225425733cc18b1da00c04ff27 280w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tracing-tutorial-monitor.png?w=560&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=88a2737937bfa46e5d44f665a757fb68 560w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tracing-tutorial-monitor.png?w=840&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=5825a3a92fc134fe8fb8f8f4a7b46e55 840w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tracing-tutorial-monitor.png?w=1100&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=848482d02d40236a1911a9623c39a2f0 1100w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tracing-tutorial-monitor.png?w=1650&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=278ba4c130e01f1042d49069093a7285 1650w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tracing-tutorial-monitor.png?w=2500&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=dcef7cc8a6be73d02b2030f2bb9dc783 2500w" />

<Note>
  Group-by functionality for A/B testing requires at least 2 different values to exist for a given metadata key.
</Note>

You can also use this tab to perform a version of A/B Testing. In the previous tutorial we starting tracking a few different metadata attributes - one of which was `llm`. We can group the monitoring charts by ANY metadata attribute, and instantly get grouped charts over time. This allows us to experiment with different LLMs (or prompts, or other) and track their performance over time.

In order to do this, we just need to click on the `Metadata` button at the top. This will give us a drop down of options to choose from to group by:

<img src="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tracing-tutorial-monitor-metadata.png?fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=dc91e44dac01fcb3966c7b57b1f41d66" alt="" data-og-width="957" width="957" data-og-height="534" height="534" data-path="langsmith/images/tracing-tutorial-monitor-metadata.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tracing-tutorial-monitor-metadata.png?w=280&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=8ddc7be16e3b0f6e09e83af55fbe917f 280w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tracing-tutorial-monitor-metadata.png?w=560&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=cbc8c4287707ff477695f12d9220c55f 560w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tracing-tutorial-monitor-metadata.png?w=840&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=70345f5ed014329e07046681734f412f 840w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tracing-tutorial-monitor-metadata.png?w=1100&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=73662cff14eebb9498e295834e6b8c86 1100w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tracing-tutorial-monitor-metadata.png?w=1650&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=7a5e5124ffbc658a5a97d372f41b96c0 1650w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tracing-tutorial-monitor-metadata.png?w=2500&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=24dcb9a46ea8684c2fedd0b3f136d8a0 2500w" />

Once we select this, we will start to see charts grouped by this attribute:

<img src="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tracing-tutorial-monitor-grouped.png?fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=a1bf8fb453d7721d85bca20fbd7cb431" alt="" data-og-width="973" width="973" data-og-height="621" height="621" data-path="langsmith/images/tracing-tutorial-monitor-grouped.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tracing-tutorial-monitor-grouped.png?w=280&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=896d99b5a7456e10d92aa58c8d3bb6d8 280w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tracing-tutorial-monitor-grouped.png?w=560&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=6cc318a5f30ed0e8e11accf1d6f7428d 560w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tracing-tutorial-monitor-grouped.png?w=840&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=892af43a1ce178ec34ed4124a7a0f5d4 840w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tracing-tutorial-monitor-grouped.png?w=1100&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=7be53f1392b471c45b7d14febe0df6f9 1100w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tracing-tutorial-monitor-grouped.png?w=1650&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=61d68a5703c7e69fe4c96df6a861ab37 1650w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tracing-tutorial-monitor-grouped.png?w=2500&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=e0a3af90e154e85c45f3de1cb0a908d5 2500w" />

One of the awesome abilities that LangSmith provides is the ability to easily drilldown into datapoints that you identify as problematic while looking at monitoring charts. In order to do this, you can simply hover over a datapoint in the monitoring chart. When you do this, you will be able to click the datapoint. This will lead you back to the runs table with a filtered view:

<img src="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-monitor-drilldown.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=1ca02e0473f1fdfff102f2ccba371828" alt="" data-og-width="952" width="952" data-og-height="708" height="708" data-path="langsmith/images/tracing-tutorial-monitor-drilldown.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-monitor-drilldown.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=ddf0256ff1e85656a8339e16a652480d 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-monitor-drilldown.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=0fe248ccc799c0cef9e661c861e81605 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-monitor-drilldown.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=4c3f95dd9e31e1884f569bbf736b852c 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-monitor-drilldown.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=8ce416eec3bdc7b4289ba9fbedf6959a 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-monitor-drilldown.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=a2f93d8d48a5cf33f2f4b664c72644a7 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-monitor-drilldown.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=506f485087e2e7ab91e7a53c44fd8205 2500w" />

In this tutorial you saw how to set up your LLM application with best-in-class observability. No matter what stage your application is in, you will still benefit from observability.

If you have more in-depth questions about observability, check out the [how-to section](/langsmith/observability-concepts) for guides on topics like testing, prompt management, and more.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/observability-llm-tutorial.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

## Prototyping

Having observability set up from the start can help you iterate **much** more quickly than you would otherwise be able to. It allows you to have great visibility into your application as you are rapidly iterating on the prompt, or changing the data and models you are using. In this section we'll walk through how to set up observability so you can have maximal observability as you are prototyping.

### Set up your environment

First, create an API key by navigating to the [settings page](https://smith.langchain.com/settings).

Next, install the LangSmith SDK:

<CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>

Finally, set up the appropriate environment variables. This will log traces to the `default` project (though you can easily change that).
```

---

## Trace Claude Agent SDK

**URL:** llms-txt#trace-claude-agent-sdk

**Contents:**
- Installation
- Quickstart

Source: https://docs.langchain.com/langsmith/trace-claude-agent-sdk

The [Claude Agent SDK](https://docs.claude.com/en/api/agent-sdk/overview) is an SDK for building agentic applications with Claude. LangSmith provides native integration with the Claude Agent SDK to automatically trace your agent executions, tool calls, and interactions with Claude models.

Install the LangSmith integration for Claude Agent SDK

To enable LangSmith tracing for your Claude Agent SDK application, call `configure_claude_agent_sdk()` at the start of your application:

```python  theme={null}
import asyncio
from claude_agent_sdk import (
    ClaudeAgentOptions,
    ClaudeSDKClient,
    tool,
    create_sdk_mcp_server,
)
from typing import Any

from langsmith.integrations.claude_agent_sdk import configure_claude_agent_sdk

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

## Quickstart

To enable LangSmith tracing for your Claude Agent SDK application, call `configure_claude_agent_sdk()` at the start of your application:
```

---

## Trace Claude Code

**URL:** llms-txt#trace-claude-code

**Contents:**
- Quick Start

Source: https://docs.langchain.com/langsmith/trace-claude-code

[Claude Code](https://docs.claude.com/en/docs/claude-code/overview) is one of the most impressive and useful AI coding tools to date. Claude code emits events for monitoring and observability. LangSmith can collect and display these events to give you a full detailed log on what Claude Code does under the hood.

You can integrate LangSmith tracing with Claude Code by setting the following environment variables in the environment in which you run Claude Code.

```bash  theme={null}

---

## Trace generator functions

**URL:** llms-txt#trace-generator-functions

**Contents:**
- Aggregate Results[](#aggregate-results "Direct link to Aggregate Results")

Source: https://docs.langchain.com/langsmith/trace-generator-functions

In most LLM applications, you will want to stream outputs to minimize the time to the first token seen by the user.

LangSmith's tracing functionality natively supports streamed outputs via `generator` functions. Below is an example.

## Aggregate Results[](#aggregate-results "Direct link to Aggregate Results")

By default, the `outputs` of the traced function are aggregated into a single array in LangSmith. If you want to customize how it is stored (for instance, concatenating the outputs into a single string), you can use the `aggregate` option (`reduce_fn` in python). This is especially useful for aggregating streamed LLM outputs.

<Note>
  Aggregating outputs **only** impacts the traced representation of the outputs. It doesn not alter the values returned by your function.
</Note>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/trace-generator-functions.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

## Aggregate Results[](#aggregate-results "Direct link to Aggregate Results")

By default, the `outputs` of the traced function are aggregated into a single array in LangSmith. If you want to customize how it is stored (for instance, concatenating the outputs into a single string), you can use the `aggregate` option (`reduce_fn` in python). This is especially useful for aggregating streamed LLM outputs.

<Note>
  Aggregating outputs **only** impacts the traced representation of the outputs. It doesn not alter the values returned by your function.
</Note>

<CodeGroup>
```

Example 3 (unknown):
```unknown

```

---

## Trace JS functions in serverless environments

**URL:** llms-txt#trace-js-functions-in-serverless-environments

**Contents:**
- Rate limits at high concurrency[](#rate-limits-at-high-concurrency "Direct link to Rate limits at high concurrency")

Source: https://docs.langchain.com/langsmith/serverless-environments

<Note>
  This section is relevant for those using the LangSmith JS SDK version 0.2.0 and higher. If you are tracing using LangChain.js or LangGraph.js in serverless environments, see [this guide](https://js.langchain.com/docs/how_to/callbacks_serverless).
</Note>

When tracing JavaScript functions, LangSmith will trace runs in the background by default to avoid adding latency. In serverless environments where the execution context may be terminated abruptly, it's important to ensure that all tracing data is properly flushed before the function completes.

To make sure this occurs, you can either:

* Set an environment variable named `LANGSMITH_TRACING_BACKGROUND` to `"false"`. This will cause your traced functions to wait for tracing to complete before returning.
  * Note that this is named differently from the [environment variable](https://js.langchain.com/docs/how_to/callbacks_serverless) in LangChain.js because LangSmith can be used without LangChain.
* Pass a custom client into your traced runs and `await` the `client.awaitPendingTraceBatches();` method.

Here's an example of using `awaitPendingTraceBatches` alongside the [`traceable`](/langsmith/annotate-code) method:

## Rate limits at high concurrency[](#rate-limits-at-high-concurrency "Direct link to Rate limits at high concurrency")

By default, the LangSmith client will batch operations as your traced run executions, sending a new batch every few milliseconds.

This works well in most situations, but if your traced function is long-running and you have very high concurrency, you may also hit rate limits related to overall request count.

If you are seeing rate limit errors related to this, you can try setting `manualFlushMode: true` in your client like this:

And then manually calling `client.flush()` like this before your serverless function closes:

Note that this will prevent runs from appearing in the LangSmith UI until you call `.flush()`.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/serverless-environments.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
## Rate limits at high concurrency[](#rate-limits-at-high-concurrency "Direct link to Rate limits at high concurrency")

By default, the LangSmith client will batch operations as your traced run executions, sending a new batch every few milliseconds.

This works well in most situations, but if your traced function is long-running and you have very high concurrency, you may also hit rate limits related to overall request count.

If you are seeing rate limit errors related to this, you can try setting `manualFlushMode: true` in your client like this:
```

Example 2 (unknown):
```unknown
And then manually calling `client.flush()` like this before your serverless function closes:
```

---

## Trace query syntax

**URL:** llms-txt#trace-query-syntax

**Contents:**
- Filter arguments
- Filter query language

Source: https://docs.langchain.com/langsmith/trace-query-syntax

Using the method in the SDK or endpoint in the API, you can filter runs to analyze and export.

| Keys                          | Description                                                                                                                                                                                                                    |
| ----------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| `project_id` / `project_name` | The project(s) to fetch runs from - can be a single project or a list of projects.                                                                                                                                             |
| `trace_id`                    | Fetch runs that are part of a specific trace.                                                                                                                                                                                  |
| `run_type`                    | The type of run to get, such as `llm`, `chain`, `tool`, `retriever`, etc.                                                                                                                                                      |
| `dataset_name` / `dataset_id` | Fetch runs that are associated with an example row in the specified dataset. This is useful for comparing prompts or models over a given dataset.                                                                              |
| `reference_example_id`        | Fetch runs that are associated with a specific example row. This is useful for comparing prompts or models on a given input.                                                                                                   |
| `parent_run_id`               | Fetch runs that are children of a given run. This is useful for fetching runs grouped together using the context manager or for fetching an agent trajectory.                                                                  |
| `error`                       | Fetch runs that errored or did not error.                                                                                                                                                                                      |
| `run_ids`                     | Fetch runs with a given list of run ids. Note: **This will ignore all other filtering arguments.**                                                                                                                             |
| `filter`                      | Fetch runs that match a given structured filter statement. See the guide below for more information.                                                                                                                           |
| `trace_filter`                | Filter to apply to the ROOT run in the trace tree. This is meant to be used in conjunction with the regular `filter` parameter to let you filter runs by attributes of the root run within a trace.                            |
| `tree_filter`                 | Filter to apply to OTHER runs in the trace tree, including sibling and child runs. This is meant to be used in conjunction with the regular `filter` parameter to let you filter runs by attributes of any run within a trace. |
| `is_root`                     | Only return root runs.                                                                                                                                                                                                         |
| `select`                      | Select the fields to return in the response. By default, all fields are returned.                                                                                                                                              |
| `query` (*experimental*)      | Natural language query, which translates your query into a filter statement.                                                                                                                                                   |

## Filter query language

LangSmith supports powerful filtering capabilities with a filter query language to permit complex filtering operations when fetching runs.

The filtering grammar is based on common comparators on fields in the run object. Supported comparators include:

* `gte` (greater than or equal to)
* `gt` (greater than)
* `lte` (less than or equal to)
* `lt` (less than)
* `eq` (equal to)
* `neq` (not equal to)
* `has` (check if run contains a tag or metadata json blob)
* `search` (search for a substring in a string field)

Additionally, you can combine multiple comparisons through the `and` operator.

These can be applied on fields of the run object, such as its `id`, `name`, `run_type`, `start_time` / `end_time`, `latency`, `total_tokens`, `error`, `execution_order`, `tags`, and any associated feedback through `feedback_key` and `feedback_score`.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/trace-query-syntax.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Trace without setting environment variables

**URL:** llms-txt#trace-without-setting-environment-variables

Source: https://docs.langchain.com/langsmith/trace-without-env-vars

As mentioned in other guides, the following environment variables allow you to configure tracing enabled, the api endpoint, the api key, and the tracing project:

* `LANGSMITH_TRACING`
* `LANGSMITH_API_KEY`
* `LANGSMITH_ENDPOINT`
* `LANGSMITH_PROJECT`

If you need to trace runs with a custom configuration, are working in an environment that doesn’t support typical environment variables (such as Cloudflare Workers), or would simply prefer not to rely on environment variables, LangSmith allows you to configure tracing programmatically.

<Warning>
  Due to a number of asks for finer-grained control of tracing using the `trace` context manager, **we changed the behavior** of `with trace` to honor the `LANGSMITH_TRACING` environment variable in version **0.1.95** of the Python SDK. You can find more details in the [release notes](https://github.com/langchain-ai/langsmith-sdk/releases/tag/v0.1.95). The recommended way to disable/enable tracing without setting environment variables is to use the `with tracing_context` context manager, as shown in the example below.
</Warning>

* Python: The recommended way to do this in Python is to use the `tracing_context` context manager. This works for both code annotated with `traceable` and code within the `trace` context manager.
* TypeScript: You can pass in both the client and the `tracingEnabled` flag to the `traceable` decorator.

If you prefer a video tutorial, check out the [Alternative Ways to Trace video](https://academy.langchain.com/pages/intro-to-langsmith-preview) from the Introduction to LangSmith Course.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/trace-without-env-vars.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

---

## Trace with Anthropic

**URL:** llms-txt#trace-with-anthropic

Source: https://docs.langchain.com/langsmith/trace-anthropic

The `wrap_anthropic` methods in Python allows you to wrap your Anthropic client in order to automatically log traces -- no decorator or function wrapping required! Using the wrapper ensures that messages, including tool calls and multimodal content blocks will be rendered nicely in LangSmith. The wrapper works seamlessly with the `@traceable` decorator or `traceable` function and you can use both in the same application.

<Note>
  The `LANGSMITH_TRACING` environment variable must be set to `'true'` in order for traces to be logged to LangSmith, even when using `wrap_anthropic`. This allows you to toggle tracing on and off without changing your code.

Additionally, you will need to set the `LANGSMITH_API_KEY` environment variable to your API key (see [Setup](/) for more information).

If your LangSmith API key is linked to multiple workspaces, set the `LANGSMITH_WORKSPACE_ID` environment variable to specify which workspace to use.

By default, the traces will be logged to a project named `default`. To log traces to a different project, see [this section](/langsmith/log-traces-to-project).
</Note>

```python  theme={null}
import anthropic
from langsmith import traceable
from langsmith.wrappers import wrap_anthropic

client = wrap_anthropic(anthropic.Anthropic())

---

## Trace with API

**URL:** llms-txt#trace-with-api

**Contents:**
- Basic tracing

Source: https://docs.langchain.com/langsmith/trace-with-api

Learn how to trace your LLM applications using the LangSmith API directly.

It is **highly** recommended to use our Python or TypeScript SDKs to send traces to LangSmith. We have designed these SDKs with optimizations like batching and backgrounding to ensure that your application's performance is not impacted by sending traces to LangSmith. However, if you are unable to use our SDKs, you can use the LangSmith REST API to send traces. Performance may be impacted if you send traces synchronously in your application. This guide will show you how to trace a request using the LangSmith REST API. Please view our API documentation  for a full list of endpoints and request/response schemas.

The simplest way to log runs is via the POST and PATCH `/runs` endpoint. These routes expect minimal contextual information about the tree structure to

<Note>
  When using the LangSmith REST API, you will need to provide your API key in the request headers as `"x-api-key"`.

If your API key is linked to multiple workspaces, you will need to specify the workspace being used in the header with `"x-tenant-id"`.

In the simple example, you do not need to set the `dotted_order` opr `trace_id` fields in the request body. These fields will be automatically generated by the system. Though this is simpler, it is slower and has a lower rate limit in LangSmith.
</Note>

The following example shows how you might leverage our API directly in Python. The same principles apply to other languages.

```python  theme={null}
import openai
import os
import requests
from datetime import datetime, timezone
from uuid import uuid4

---

## Trace with AutoGen

**URL:** llms-txt#trace-with-autogen

**Contents:**
- Installation
- Setup
  - 1. Configure environment variables
  - 2. Configure OpenTelemetry integration

Source: https://docs.langchain.com/langsmith/trace-with-autogen

LangSmith can capture traces generated by [AutoGen](https://microsoft.github.io/autogen/stable/) using OpenInference's AutoGen instrumentation. This guide shows you how to automatically capture traces from your AutoGen multi-agent conversations and send them to LangSmith for monitoring and analysis.

Install the required packages using your preferred package manager:

<Info>
  Requires LangSmith Python SDK version `langsmith>=0.4.26` for optimal OpenTelemetry support.
</Info>

### 1. Configure environment variables

Set your API keys and project name:

<CodeGroup>
  
</CodeGroup>

### 2. Configure OpenTelemetry integration

In your AutoGen application, import and configure the LangSmith OpenTelemetry integration along with the AutoGen and OpenAI instrumentors:

```python  theme={null}
from langsmith.integrations.otel import configure
from openinference.instrumentation.autogen import AutogenInstrumentor
from openinference.instrumentation.openai import OpenAIInstrumentor

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

<Info>
  Requires LangSmith Python SDK version `langsmith>=0.4.26` for optimal OpenTelemetry support.
</Info>

## Setup

### 1. Configure environment variables

Set your API keys and project name:

<CodeGroup>
```

Example 3 (unknown):
```unknown
</CodeGroup>

### 2. Configure OpenTelemetry integration

In your AutoGen application, import and configure the LangSmith OpenTelemetry integration along with the AutoGen and OpenAI instrumentors:
```

---

## Trace with CrewAI

**URL:** llms-txt#trace-with-crewai

**Contents:**
- Installation
- Setup
  - 1. Configure environment variables
  - 2. Configure OpenTelemetry integration

Source: https://docs.langchain.com/langsmith/trace-with-crewai

LangSmith can capture traces generated by [CrewAI](https://github.com/crewAIInc/crewAI) using OpenInference's CrewAI instrumentation. This guide shows you how to automatically capture traces from your CrewAI multi-agent workflows and send them to LangSmith for monitoring and analysis.

Install the required packages using your preferred package manager:

### 1. Configure environment variables

Set your API keys and project name:

<CodeGroup>
  
</CodeGroup>

### 2. Configure OpenTelemetry integration

In your CrewAI application, import and configure the LangSmith OpenTelemetry integration along with the CrewAI and OpenAI instrumentors:

```python  theme={null}
from langsmith.integrations.otel import OtelSpanProcessor
from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from openinference.instrumentation.crewai import CrewAIInstrumentor
from openinference.instrumentation.openai import OpenAIInstrumentor

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

## Setup

### 1. Configure environment variables

Set your API keys and project name:

<CodeGroup>
```

Example 3 (unknown):
```unknown
</CodeGroup>

### 2. Configure OpenTelemetry integration

In your CrewAI application, import and configure the LangSmith OpenTelemetry integration along with the CrewAI and OpenAI instrumentors:
```

---

## Trace with Google ADK

**URL:** llms-txt#trace-with-google-adk

**Contents:**
- Installation
- Setup
  - 1. Configure environment variables
  - 2. Configure OpenTelemetry integration

Source: https://docs.langchain.com/langsmith/trace-with-google-adk

LangSmith supports tracing Google Agent Development Kit (ADK) applications through the OpenTelemetry integration. This guide shows you how to automatically capture traces from your [Google ADK](https://github.com/google/adk-python) agents and send them to LangSmith for monitoring and analysis.

Install the required packages using your preferred package manager:

<Info>
  Requires LangSmith Python SDK version `langsmith>=0.4.26` for optimal OpenTelemetry support.
</Info>

### 1. Configure environment variables

Set your LangSmith API key and project name:

<CodeGroup>
  
</CodeGroup>

### 2. Configure OpenTelemetry integration

In your Google ADK application, import and configure the LangSmith OpenTelemetry integration. This will automatically instrument Google ADK spans for OpenTelemetry.

```python  theme={null}
from langsmith.integrations.otel import configure

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

<Info>
  Requires LangSmith Python SDK version `langsmith>=0.4.26` for optimal OpenTelemetry support.
</Info>

## Setup

### 1. Configure environment variables

Set your LangSmith API key and project name:

<CodeGroup>
```

Example 3 (unknown):
```unknown
</CodeGroup>

### 2. Configure OpenTelemetry integration

In your Google ADK application, import and configure the LangSmith OpenTelemetry integration. This will automatically instrument Google ADK spans for OpenTelemetry.
```

---

## Trace with Instructor

**URL:** llms-txt#trace-with-instructor

Source: https://docs.langchain.com/langsmith/trace-with-instructor

LangSmith provides a convenient integration with [Instructor](https://python.useinstructor.com/), a popular open-source library for generating structured outputs with LLMs.

In order to use, you first need to set your LangSmith API key.

```shell  theme={null}
export LANGSMITH_API_KEY=<your-api-key>

---

## Trace with LangChain (Python and JS/TS)

**URL:** llms-txt#trace-with-langchain-(python-and-js/ts)

**Contents:**
- Installation
- Quick start
  - 1. Configure your environment

Source: https://docs.langchain.com/langsmith/trace-with-langchain

LangSmith integrates seamlessly with LangChain (Python and JavaScript), the popular open-source framework for building LLM applications.

Install the core library and the OpenAI integration for Python and JS (we use the OpenAI integration for the code snippets below).

For a full list of packages available, see the [LangChain docs](/oss/python/integrations/providers/overview).

### 1. Configure your environment

```bash wrap theme={null}
export LANGSMITH_TRACING=true
export LANGSMITH_API_KEY=<your-api-key>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>

## Quick start

### 1. Configure your environment
```

---

## Trace with LangGraph

**URL:** llms-txt#trace-with-langgraph

**Contents:**
- With LangChain
  - 1. Installation
  - 2. Configure your environment

Source: https://docs.langchain.com/langsmith/trace-with-langgraph

LangSmith smoothly integrates with LangGraph (Python and JS) to help you trace agents, whether you're using LangChain modules or other SDKs.

If you are using LangChain modules within LangGraph, you only need to set a few environment variables to enable tracing.

This guide will walk through a basic example. For more detailed information on configuration, see the [Trace With LangChain](/langsmith/trace-with-langchain) guide.

Install the LangGraph library and the OpenAI integration for Python and JS (we use the OpenAI integration for the code snippets below).

For a full list of packages available, see the [LangChain Python docs](https://python.langchain.com/docs/integrations/platforms/) and [LangChain JS docs](https://js.langchain.com/docs/integrations/platforms/).

### 2. Configure your environment

```bash wrap theme={null}
export LANGSMITH_TRACING=true
export LANGSMITH_API_KEY=<your-api-key>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>

### 2. Configure your environment
```

---

## Trace with OpenAI Agents SDK

**URL:** llms-txt#trace-with-openai-agents-sdk

**Contents:**
- Installation
- Quick Start

Source: https://docs.langchain.com/langsmith/trace-with-openai-agents-sdk

The OpenAI Agents SDK allows you to build agentic applications powered by OpenAI's models.

Learn how to trace your LLM applications using the OpenAI Agents SDK with LangSmith.

<Info>
  Requires Python SDK version `langsmith>=0.3.15`.
</Info>

Install LangSmith with OpenAI Agents support:

This will install both the LangSmith library and the OpenAI Agents SDK.

You can integrate LangSmith tracing with the OpenAI Agents SDK by using the `OpenAIAgentsTracingProcessor` class.

The agent's execution flow, including all spans and their details, will be logged to LangSmith.

<img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/agent-trace.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=7544fc0deb9c6279a9848da17d70bf8b" alt="OpenAI Agents SDK Trace in LangSmith" data-og-width="2984" width="2984" data-og-height="1782" height="1782" data-path="langsmith/images/agent-trace.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/agent-trace.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=18b3ec39553d20f562c61e68120b5ed7 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/agent-trace.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=e278d9a842f33c876cf1bb6937edaa9d 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/agent-trace.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=cf1fd1047a4f61bfe3cb0917d64cb403 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/agent-trace.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=6d98b5286390e19b818c82fa3dcdd3e8 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/agent-trace.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=a20187910934b3921b5cac30df0922cb 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/agent-trace.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=3434b20ed1dfef6fc3751a50bb49b062 2500w" />

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/trace-with-openai-agents-sdk.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

This will install both the LangSmith library and the OpenAI Agents SDK.

## Quick Start

You can integrate LangSmith tracing with the OpenAI Agents SDK by using the `OpenAIAgentsTracingProcessor` class.
```

---

## Trace with OpenAI

**URL:** llms-txt#trace-with-openai

Source: https://docs.langchain.com/langsmith/trace-openai

The `wrap_openai`/`wrapOpenAI` methods in Python/TypeScript allow you to wrap your OpenAI client in order to automatically log traces -- no decorator or function wrapping required! Using the wrapper ensures that messages, including tool calls and multimodal content blocks will be rendered nicely in LangSmith. Also note that the wrapper works seamlessly with the `@traceable` decorator or `traceable` function and you can use both in the same application.

<Note>
  The `LANGSMITH_TRACING` environment variable must be set to `'true'` in order for traces to be logged to LangSmith, even when using `wrap_openai` or `wrapOpenAI`. This allows you to toggle tracing on and off without changing your code.

Additionally, you will need to set the `LANGSMITH_API_KEY` environment variable to your API key (see [Setup](/) for more information).

If your LangSmith API key is linked to multiple workspaces, set the `LANGSMITH_WORKSPACE_ID` environment variable to specify which workspace to use.

By default, the traces will be logged to a project named `default`. To log traces to a different project, see [this section](/langsmith/log-traces-to-project).
</Note>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/trace-openai.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

---

## Trace with OpenTelemetry

**URL:** llms-txt#trace-with-opentelemetry

**Contents:**
- Trace a LangChain application
- Trace a non-LangChain application
- Send traces to an alternate provider
  - Use environment variables for global configuration
  - Configure alternate OTLP endpoints

Source: https://docs.langchain.com/langsmith/trace-with-opentelemetry

LangSmith supports OpenTelemetry-based tracing, allowing you to send traces from any OpenTelemetry-compatible application. This guide covers both automatic instrumentation for LangChain applications and manual instrumentation for other frameworks.

Learn how to trace your LLM applications using OpenTelemetry with LangSmith.

<Note>
  Update the LangSmith URL appropriately for self-hosted installations or organizations in the EU region in the requests below. For the EU region, use `eu.api.smith.langchain.com`.
</Note>

## Trace a LangChain application

If you're using LangChain or LangGraph, use the built-in integration to trace your application:

1. Install the LangSmith package with OpenTelemetry support:

<CodeGroup>
     
   </CodeGroup>

<Info>
     Requires Python SDK version `langsmith>=0.3.18`. We recommend `langsmith>=0.4.25` to benefit from important OpenTelemetry fixes.
   </Info>

2. In your LangChain/LangGraph App, enable the OpenTelemetry integration by setting the `LANGSMITH_OTEL_ENABLED` environment variable:

<CodeGroup>
     
   </CodeGroup>

3. Create a LangChain application with tracing. For example:

4. View the traces in your LangSmith dashboard ([example](https://smith.langchain.com/public/a762af6c-b67d-4f22-90a0-728df16baeba/r)) once your application runs.

## Trace a non-LangChain application

For non-LangChain applications or custom instrumentation, you can trace your application in LangSmith with a standard OpenTelemetry client. (We recommend **langsmith ≥ 0.4.25**.)

1. Install the OpenTelemetry SDK, OpenTelemetry exporter packages, as well as the OpenAI package:

<CodeGroup>
     
   </CodeGroup>

2. Setup environment variables for the endpoint, substitute your specific values:

<CodeGroup>
     
   </CodeGroup>

<Note>
     Depending on how your otel exporter is configured, you may need to append `/v1/traces` to the endpoint if you are only sending traces.
   </Note>

<Note>
     If you're self-hosting LangSmith, replace the base endpoint with your LangSmith api endpoint and append `/api/v1`. For example: `OTEL_EXPORTER_OTLP_ENDPOINT=https://ai-company.com/api/v1/otel`
   </Note>

Optional: Specify a custom project name other than "default":

<CodeGroup>
     
   </CodeGroup>

This code sets up an OTEL tracer and exporter that will send traces to LangSmith. It then calls OpenAI and sends the required OpenTelemetry attributes.

4. View the trace in your LangSmith dashboard ([example](https://smith.langchain.com/public/4f2890b1-f105-44aa-a6cf-c777dcc27a37/r)).

## Send traces to an alternate provider

While LangSmith is the default destination for OpenTelemetry traces, you can also configure OpenTelemetry to send traces to other observability platforms.

<Info>
  Available in LangSmith Python SDK **≥ 0.4.1**. We recommend **≥ 0.4.25** for fixes that improve OTEL export and hybrid fan-out stability.
</Info>

### Use environment variables for global configuration

By default, the LangSmith OpenTelemetry exporter will send data to the LangSmith API OTEL endpoint, but this can be customized by setting standard OTEL environment variables:

LangSmith uses the HTTP trace exporter by default. If you'd like to use your own tracing provider, you can either:

1. Set the OTEL environment variables as shown above, or
2. Set a global trace provider before initializing LangChain components, which LangSmith will detect and use instead of creating its own.

### Configure alternate OTLP endpoints

To send traces to a different provider, configure the OTLP exporter with your provider's endpoint:

```python  theme={null}
import os
from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

**Examples:**

Example 1 (unknown):
```unknown
</CodeGroup>

   <Info>
     Requires Python SDK version `langsmith>=0.3.18`. We recommend `langsmith>=0.4.25` to benefit from important OpenTelemetry fixes.
   </Info>

2. In your LangChain/LangGraph App, enable the OpenTelemetry integration by setting the `LANGSMITH_OTEL_ENABLED` environment variable:

   <CodeGroup>
```

Example 2 (unknown):
```unknown
</CodeGroup>

3. Create a LangChain application with tracing. For example:
```

Example 3 (unknown):
```unknown
4. View the traces in your LangSmith dashboard ([example](https://smith.langchain.com/public/a762af6c-b67d-4f22-90a0-728df16baeba/r)) once your application runs.

## Trace a non-LangChain application

For non-LangChain applications or custom instrumentation, you can trace your application in LangSmith with a standard OpenTelemetry client. (We recommend **langsmith ≥ 0.4.25**.)

1. Install the OpenTelemetry SDK, OpenTelemetry exporter packages, as well as the OpenAI package:

   <CodeGroup>
```

Example 4 (unknown):
```unknown
</CodeGroup>

2. Setup environment variables for the endpoint, substitute your specific values:

   <CodeGroup>
```

---

## Trace with Semantic Kernel

**URL:** llms-txt#trace-with-semantic-kernel

**Contents:**
- Installation
- Setup
  - 1. Configure environment variables
  - 2. Configure OpenTelemetry integration

Source: https://docs.langchain.com/langsmith/trace-with-semantic-kernel

LangSmith can capture traces generated by [Semantic Kernel](https://learn.microsoft.com/en-us/semantic-kernel/overview/) using OpenInference's OpenAI instrumentation. This guide shows you how to automatically capture traces from your Semantic Kernel applications and send them to LangSmith for monitoring and analysis.

Install the required packages using your preferred package manager:

<Info>
  Requires LangSmith Python SDK version `langsmith>=0.4.26` for optimal OpenTelemetry support.
</Info>

### 1. Configure environment variables

Set your API keys and project name:

<CodeGroup>
  
</CodeGroup>

### 2. Configure OpenTelemetry integration

In your Semantic Kernel application, import and configure the LangSmith OpenTelemetry integration along with the OpenAI instrumentor:

```python  theme={null}
from langsmith.integrations.otel import configure
from openinference.instrumentation.openai import OpenAIInstrumentor

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

<Info>
  Requires LangSmith Python SDK version `langsmith>=0.4.26` for optimal OpenTelemetry support.
</Info>

## Setup

### 1. Configure environment variables

Set your API keys and project name:

<CodeGroup>
```

Example 3 (unknown):
```unknown
</CodeGroup>

### 2. Configure OpenTelemetry integration

In your Semantic Kernel application, import and configure the LangSmith OpenTelemetry integration along with the OpenAI instrumentor:
```

---

## Trace with the Vercel AI SDK (JS/TS only)

**URL:** llms-txt#trace-with-the-vercel-ai-sdk-(js/ts-only)

**Contents:**
- Installation
- Environment configuration
- Basic setup
  - With `traceable`
- Tracing in serverless environments
- Passing LangSmith config
- Redacting data

Source: https://docs.langchain.com/langsmith/trace-with-vercel-ai-sdk

You can use LangSmith to trace runs from the Vercel AI SDK. This guide will walk through an example.

<Note>
  This wrapper requires AI SDK v5 and `langsmith>=0.3.63`. If you are using an older version of the AI SDK or `langsmith`, see the OpenTelemetry (OTEL)
  based approach [on this page](/langsmith/legacy-trace-with-vercel-ai-sdk).
</Note>

Install the Vercel AI SDK. This guide uses Vercel's OpenAI integration for the code snippets below, but you can use any of their other options as well.

## Environment configuration

<CodeGroup>
  
</CodeGroup>

Import and wrap AI SDK methods, then use them as you normally would:

You should see a trace in your LangSmith dashboard [like this one](https://smith.langchain.com/public/4f0e689e-c801-44d3-8857-93b47ab100cc/r).

You can also trace runs with tool calls:

Which results in a trace like [this one](https://smith.langchain.com/public/6075fa2c-d255-4885-a66a-4fc798afaa9f/r).

You can use other AI SDK methods exactly as you usually would.

You can wrap `traceable` calls around AI SDK calls or within AI SDK tool calls. This is useful if you
want to group runs together in LangSmith:

The resulting trace will look [like this](https://smith.langchain.com/public/ff25bc26-9389-4798-8b91-2bdcc95d4a8e/r).

## Tracing in serverless environments

When tracing in serverless environments, you must wait for all runs to flush before your environment
shuts down. To do this, you can pass a LangSmith [`Client`](https://docs.smith.langchain.com/reference/js/classes/client.Client) instance when wrapping the AI SDK method,
then call `await client.awaitPendingTraceBatches()`.
Make sure to also pass it into any `traceable` wrappers you create as well:

If you are using `Next.js`, there is a convenient [`after`](https://nextjs.org/docs/app/api-reference/functions/after) hook
where you can put this logic:

See [this page](/langsmith/serverless-environments) for more detail, including information
around managing rate limits in serverless environments.

## Passing LangSmith config

You can pass LangSmith-specific config to your wrapper both when initially wrapping your
AI SDK methods and while running them via `providerOptions.langsmith`.
This includes metadata (which you can later use to filter runs in LangSmith), top-level run name,
tags, custom client instances, and more.

Config passed while wrapping will apply to all future calls you make with the wrapped method:

While passing config at runtime via `providerOptions.langsmith` will apply only to that run.
We suggest importing and wrapping your config in `createLangSmithProviderOptions` to ensure
proper typing:

You can customize what inputs and outputs the AI SDK sends to LangSmith by specifying custom input/output
processing functions. This is useful if you are dealing with sensitive data that you would like to
avoid sending to LangSmith.

Because output formats vary depending on which AI SDK method you are using, we suggest defining and passing config
individually into wrapped methods. You will also need to provide separate functions for child LLM runs within
AI SDK calls, since calling `generateText` at top level calls the LLM internally and can do so multiple times.

We also suggest passing a generic parameter into `createLangSmithProviderOptions` to get proper types for inputs and outputs.
Here's an example for `generateText`:

The actual return value will contain the original, non-redacted result but the trace in LangSmith
will be redacted. [Here's an example](https://smith.langchain.com/public/b4c69c8e-285b-4c0c-8492-e571e2cf562f/r).

For redacting tool input/output, wrap your `execute` method in a `traceable` like this:

The `traceable` return type is complex, which makes the cast necessary. You may also omit the AI SDK `tool` wrapper function
if you wish to avoid the cast.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/trace-with-vercel-ai-sdk.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
</CodeGroup>

## Environment configuration

<CodeGroup>
```

Example 4 (unknown):
```unknown
</CodeGroup>

## Basic setup

Import and wrap AI SDK methods, then use them as you normally would:
```

---

## Tracing quickstart

**URL:** llms-txt#tracing-quickstart

**Contents:**
- Prerequisites
- 1. Create a directory and install dependencies
- 2. Set up environment variables
- 3. Define your application
- 4. Trace LLM calls
- 5. Trace an entire application
- Next steps
- Video guide

Source: https://docs.langchain.com/langsmith/observability-quickstart

[*Observability*](/langsmith/observability-concepts) is a critical requirement for applications built with large language models (LLMs). LLMs are non-deterministic, which means that the same prompt can produce different responses. This behavior makes debugging and monitoring more challenging than with traditional software.

LangSmith addresses this by providing end-to-end visibility into how your application handles a request. Each request generates a [*trace*](/langsmith/observability-concepts#traces), which captures the full record of what happened. Within a trace are individual [*runs*](/langsmith/observability-concepts#runs), the specific operations your application performed, such as an LLM call or a retrieval step. Tracing runs allows you to inspect, debug, and validate your application’s behavior.

In this quickstart, you will set up a minimal [*Retrieval Augmented Generation (RAG)*](https://www.mckinsey.com/featured-insights/mckinsey-explainers/what-is-retrieval-augmented-generation-rag) application and add tracing with LangSmith. You will:

1. Configure your environment.
2. Create an application that retrieves context and calls an LLM.
3. Enable tracing to capture both the retrieval step and the LLM call.
4. View the resulting traces in the LangSmith UI.

<Tip>
  If you prefer to watch a video on getting started with tracing, refer to the quickstart [Video guide](#video-guide).
</Tip>

Before you begin, make sure you have:

* **A LangSmith account**: Sign up or log in at [smith.langchain.com](https://smith.langchain.com).
* **A LangSmith API key**: Follow the [Create an API key](/langsmith/create-account-api-key#create-an-api-key) guide.
* **An OpenAI API key**: Generate this from the [OpenAI dashboard](https://platform.openai.com/account/api-keys).

The example app in this quickstart will use OpenAI as the LLM provider. You can adapt the example for your app's LLM provider.

<Tip>
  If you're building an application with [LangChain](https://python.langchain.com/docs/introduction/) or [LangGraph](https://langchain-ai.github.io/langgraph/), you can enable LangSmith tracing with a single environment variable. Get started by reading the guides for tracing with [LangChain](/langsmith/trace-with-langchain) or tracing with [LangGraph](/langsmith/trace-with-langgraph).
</Tip>

## 1. Create a directory and install dependencies

In your terminal, create a directory for your project and install the dependencies in your environment:

## 2. Set up environment variables

Set the following environment variables:

* `LANGSMITH_TRACING`
* `LANGSMITH_API_KEY`
* `OPENAI_API_KEY` (or your LLM provider's API key)
* (optional) `LANGSMITH_WORKSPACE_ID`: If your LangSmith API key is linked to multiple workspaces, set this variable to specify which workspace to use.

If you're using Anthropic, use the [Anthropic wrapper](/langsmith/annotate-code#wrap-the-anthropic-client-python-only) to trace your calls. For other providers, use [the traceable wrapper](/langsmith/annotate-code#use-%40traceable-%2F-traceable).

## 3. Define your application

You can use the example app code outlined in this step to instrument a RAG application. Or, you can use your own application code that includes an LLM call.

This is a minimal RAG app that uses the OpenAI SDK directly without any LangSmith tracing added yet. It has three main parts:

* **Retriever function**: Simulates document retrieval that always returns the same string.
* **OpenAI client**: Instantiates a plain OpenAI client to send a chat completion request.
* **RAG function**: Combines the retrieved documents with the user’s question to form a system prompt, calls the `chat.completions.create()` endpoint with `gpt-4o-mini`, and returns the assistant’s response.

Add the following code into your app file (e.g., `app.py` or `app.ts`):

## 4. Trace LLM calls

To start, you’ll trace all your OpenAI calls. LangSmith provides wrappers:

* Python: [`wrap_openai`](https://docs.smith.langchain.com/reference/python/wrappers/langsmith.wrappers._openai.wrap_openai)
* TypeScript: [`wrapOpenAI`](https://docs.smith.langchain.com/reference/js/functions/wrappers_openai.wrapOpenAI)

This snippet wraps the OpenAI client so that every subsequent model call is logged automatically as a traced child run in LangSmith.

1. Include the highlighted lines in your app file:

2. Call your application:

You'll receive the following output:

3. In the [LangSmith UI](https://smith.langchain.com), navigate to the **default** Tracing Project for your workspace (or the workspace you specified in [Step 2](#2-set-up-environment-variables)). You'll see the OpenAI call you just instrumented.

<div style={{ textAlign: 'center' }}>
  <img className="block dark:hidden" src="https://mintcdn.com/langchain-5e9cc07a/C5sS0isXOt0-nMfw/langsmith/images/trace-quickstart-llm-call.png?fit=max&auto=format&n=C5sS0isXOt0-nMfw&q=85&s=ba8074e55cc17ec7bbf0f6987ce15b8d" alt="LangSmith UI showing an LLM call trace called ChatOpenAI with a system and human input followed by an AI Output." data-og-width="750" width="750" data-og-height="573" height="573" data-path="langsmith/images/trace-quickstart-llm-call.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/C5sS0isXOt0-nMfw/langsmith/images/trace-quickstart-llm-call.png?w=280&fit=max&auto=format&n=C5sS0isXOt0-nMfw&q=85&s=b94da918edfd11078bc637fdfc7fcc44 280w, https://mintcdn.com/langchain-5e9cc07a/C5sS0isXOt0-nMfw/langsmith/images/trace-quickstart-llm-call.png?w=560&fit=max&auto=format&n=C5sS0isXOt0-nMfw&q=85&s=7f5f480bee06c54f0e5ad7ce122f722c 560w, https://mintcdn.com/langchain-5e9cc07a/C5sS0isXOt0-nMfw/langsmith/images/trace-quickstart-llm-call.png?w=840&fit=max&auto=format&n=C5sS0isXOt0-nMfw&q=85&s=5e4e621619664b26cbe2d54719667ded 840w, https://mintcdn.com/langchain-5e9cc07a/C5sS0isXOt0-nMfw/langsmith/images/trace-quickstart-llm-call.png?w=1100&fit=max&auto=format&n=C5sS0isXOt0-nMfw&q=85&s=c63599fdd8f12bc1abc80982af376053 1100w, https://mintcdn.com/langchain-5e9cc07a/C5sS0isXOt0-nMfw/langsmith/images/trace-quickstart-llm-call.png?w=1650&fit=max&auto=format&n=C5sS0isXOt0-nMfw&q=85&s=69730a230b5d2cff4d737fab7d965c9f 1650w, https://mintcdn.com/langchain-5e9cc07a/C5sS0isXOt0-nMfw/langsmith/images/trace-quickstart-llm-call.png?w=2500&fit=max&auto=format&n=C5sS0isXOt0-nMfw&q=85&s=d5e73432bdd2f3788f7600364f84c96f 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/langchain-5e9cc07a/C5sS0isXOt0-nMfw/langsmith/images/trace-quickstart-llm-call-dark.png?fit=max&auto=format&n=C5sS0isXOt0-nMfw&q=85&s=a00c55450b4a9937b8e557ef483a4bd6" alt="LangSmith UI showing an LLM call trace called ChatOpenAI with a system and human input followed by an AI Output." data-og-width="728" width="728" data-og-height="549" height="549" data-path="langsmith/images/trace-quickstart-llm-call-dark.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/C5sS0isXOt0-nMfw/langsmith/images/trace-quickstart-llm-call-dark.png?w=280&fit=max&auto=format&n=C5sS0isXOt0-nMfw&q=85&s=9b67fab1c4d3d0e2e45d4a38caa1aa82 280w, https://mintcdn.com/langchain-5e9cc07a/C5sS0isXOt0-nMfw/langsmith/images/trace-quickstart-llm-call-dark.png?w=560&fit=max&auto=format&n=C5sS0isXOt0-nMfw&q=85&s=3090d984d38ebac8d83272d235a11662 560w, https://mintcdn.com/langchain-5e9cc07a/C5sS0isXOt0-nMfw/langsmith/images/trace-quickstart-llm-call-dark.png?w=840&fit=max&auto=format&n=C5sS0isXOt0-nMfw&q=85&s=e515ea96b0e90b2c8dc639bccb7d81b2 840w, https://mintcdn.com/langchain-5e9cc07a/C5sS0isXOt0-nMfw/langsmith/images/trace-quickstart-llm-call-dark.png?w=1100&fit=max&auto=format&n=C5sS0isXOt0-nMfw&q=85&s=85584b64773a98555f22cad5c85ba46e 1100w, https://mintcdn.com/langchain-5e9cc07a/C5sS0isXOt0-nMfw/langsmith/images/trace-quickstart-llm-call-dark.png?w=1650&fit=max&auto=format&n=C5sS0isXOt0-nMfw&q=85&s=bc6e44054bb139c9ed00eeb375eb0f4f 1650w, https://mintcdn.com/langchain-5e9cc07a/C5sS0isXOt0-nMfw/langsmith/images/trace-quickstart-llm-call-dark.png?w=2500&fit=max&auto=format&n=C5sS0isXOt0-nMfw&q=85&s=e2cb7aa22421bc4a7d355658a6371d14 2500w" />
</div>

## 5. Trace an entire application

You can also use the `traceable` decorator for [Python](https://docs.smith.langchain.com/reference/python/run_helpers/langsmith.run_helpers.traceable) or [TypeScript](https://langsmith-docs-bdk0fivr6-langchain.vercel.app/reference/js/functions/traceable.traceable) to trace your entire application instead of just the LLM calls.

1. Include the highlighted code in your app file:

2. Call the application again to create a run:

3. Return to the [LangSmith UI](https://smith.langchain.com), navigate to the **default** Tracing Project for your workspace (or the workspace you specified in [Step 2](#2-set-up-environment-variables)). You'll find a trace of the entire app pipeline with the **rag** step and the **ChatOpenAI** LLM call.

<div style={{ textAlign: 'center' }}>
  <img className="block dark:hidden" src="https://mintcdn.com/langchain-5e9cc07a/C5sS0isXOt0-nMfw/langsmith/images/trace-quickstart-app.png?fit=max&auto=format&n=C5sS0isXOt0-nMfw&q=85&s=204edddb78b671c11a48de751c2e8e19" alt="LangSmith UI showing a trace of the entire application called rag with an input followed by an output." data-og-width="750" width="750" data-og-height="425" height="425" data-path="langsmith/images/trace-quickstart-app.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/C5sS0isXOt0-nMfw/langsmith/images/trace-quickstart-app.png?w=280&fit=max&auto=format&n=C5sS0isXOt0-nMfw&q=85&s=d5ad99d3c107fe3ccb63487f43bf912e 280w, https://mintcdn.com/langchain-5e9cc07a/C5sS0isXOt0-nMfw/langsmith/images/trace-quickstart-app.png?w=560&fit=max&auto=format&n=C5sS0isXOt0-nMfw&q=85&s=9f3cdb47af6471d1e508f1fa76883900 560w, https://mintcdn.com/langchain-5e9cc07a/C5sS0isXOt0-nMfw/langsmith/images/trace-quickstart-app.png?w=840&fit=max&auto=format&n=C5sS0isXOt0-nMfw&q=85&s=d1a496530ed1f98ccabbda138ef10b68 840w, https://mintcdn.com/langchain-5e9cc07a/C5sS0isXOt0-nMfw/langsmith/images/trace-quickstart-app.png?w=1100&fit=max&auto=format&n=C5sS0isXOt0-nMfw&q=85&s=d165f3f228f5ffaeb68b0bacc00a0f6e 1100w, https://mintcdn.com/langchain-5e9cc07a/C5sS0isXOt0-nMfw/langsmith/images/trace-quickstart-app.png?w=1650&fit=max&auto=format&n=C5sS0isXOt0-nMfw&q=85&s=ccc05e2cdbaf9bd71a6bcb8536997f2e 1650w, https://mintcdn.com/langchain-5e9cc07a/C5sS0isXOt0-nMfw/langsmith/images/trace-quickstart-app.png?w=2500&fit=max&auto=format&n=C5sS0isXOt0-nMfw&q=85&s=2ebc113e6930abffa89644ccfd871404 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/langchain-5e9cc07a/C5sS0isXOt0-nMfw/langsmith/images/trace-quickstart-app-dark.png?fit=max&auto=format&n=C5sS0isXOt0-nMfw&q=85&s=2392204346d412554fbda817e082bdcd" alt="LangSmith UI showing a trace of the entire application called rag with an input followed by an output." data-og-width="738" width="738" data-og-height="394" height="394" data-path="langsmith/images/trace-quickstart-app-dark.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/C5sS0isXOt0-nMfw/langsmith/images/trace-quickstart-app-dark.png?w=280&fit=max&auto=format&n=C5sS0isXOt0-nMfw&q=85&s=7987f9046d36d015624c91f06d38efb2 280w, https://mintcdn.com/langchain-5e9cc07a/C5sS0isXOt0-nMfw/langsmith/images/trace-quickstart-app-dark.png?w=560&fit=max&auto=format&n=C5sS0isXOt0-nMfw&q=85&s=4cf9bef39d55a3aa9a31e3911fe7bdba 560w, https://mintcdn.com/langchain-5e9cc07a/C5sS0isXOt0-nMfw/langsmith/images/trace-quickstart-app-dark.png?w=840&fit=max&auto=format&n=C5sS0isXOt0-nMfw&q=85&s=0f0dc65c6705b4239afc33777e214cb7 840w, https://mintcdn.com/langchain-5e9cc07a/C5sS0isXOt0-nMfw/langsmith/images/trace-quickstart-app-dark.png?w=1100&fit=max&auto=format&n=C5sS0isXOt0-nMfw&q=85&s=d2ccb5a3e2e91e45757f57f43b261861 1100w, https://mintcdn.com/langchain-5e9cc07a/C5sS0isXOt0-nMfw/langsmith/images/trace-quickstart-app-dark.png?w=1650&fit=max&auto=format&n=C5sS0isXOt0-nMfw&q=85&s=2aec3b239854f4fd4f38807b991e5812 1650w, https://mintcdn.com/langchain-5e9cc07a/C5sS0isXOt0-nMfw/langsmith/images/trace-quickstart-app-dark.png?w=2500&fit=max&auto=format&n=C5sS0isXOt0-nMfw&q=85&s=b6ee98094ed25e6f2d61fed7241d648e 2500w" />
</div>

Here are some topics you might want to explore next:

* [Tracing integrations](/langsmith/trace-with-langchain) provide support for various LLM providers and agent frameworks.
* [Filtering traces](/langsmith/filter-traces-in-application) can help you effectively navigate and analyze data in tracing projects that contain a significant amount of data.
* [Trace a RAG application](/langsmith/observability-llm-tutorial) is a full tutorial, which adds observability to an application from development through to production.
* [Sending traces to a specific project](/langsmith/log-traces-to-project) changes the destination project of your traces.

<iframe className="w-full aspect-video rounded-xl" src="https://www.youtube.com/embed/fA9b4D8IsPQ?si=0eBb1vzw5AxUtplS" title="YouTube video player" frameBorder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowFullScreen />

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/observability-quickstart.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

## 2. Set up environment variables

Set the following environment variables:

* `LANGSMITH_TRACING`
* `LANGSMITH_API_KEY`
* `OPENAI_API_KEY` (or your LLM provider's API key)
* (optional) `LANGSMITH_WORKSPACE_ID`: If your LangSmith API key is linked to multiple workspaces, set this variable to specify which workspace to use.
```

Example 3 (unknown):
```unknown
If you're using Anthropic, use the [Anthropic wrapper](/langsmith/annotate-code#wrap-the-anthropic-client-python-only) to trace your calls. For other providers, use [the traceable wrapper](/langsmith/annotate-code#use-%40traceable-%2F-traceable).

## 3. Define your application

You can use the example app code outlined in this step to instrument a RAG application. Or, you can use your own application code that includes an LLM call.

This is a minimal RAG app that uses the OpenAI SDK directly without any LangSmith tracing added yet. It has three main parts:

* **Retriever function**: Simulates document retrieval that always returns the same string.
* **OpenAI client**: Instantiates a plain OpenAI client to send a chat completion request.
* **RAG function**: Combines the retrieved documents with the user’s question to form a system prompt, calls the `chat.completions.create()` endpoint with `gpt-4o-mini`, and returns the assistant’s response.

Add the following code into your app file (e.g., `app.py` or `app.ts`):

<CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Transient file (lost after thread ends)

**URL:** llms-txt#transient-file-(lost-after-thread-ends)

agent.invoke({
    "messages": [{"role": "user", "content": "Write draft to /draft.txt"}]
})

---

## Troubleshooting

**URL:** llms-txt#troubleshooting

**Contents:**
- Getting helpful information
- Common issues
  - *DB::Exception: Cannot reserve 1.00 MiB, not enough space: While executing WaitForAsyncInsert. (NOT\_ENOUGH\_SPACE)*
  - *error: Dirty database version 'version'. Fix and force version*
  - *413 - Request Entity Too Large*
  - *Details: code: 497, message: default: Not enough privileges. To execute this query, it's necessary to have the grant CREATE ROW POLICY ON default.feedbacks\_rmt*
  - *ClickHouse fails to start up when running a cluster with AquaSec*

Source: https://docs.langchain.com/langsmith/troubleshooting

This guide will walk you through common issues you may encounter when running a self-hosted instance of LangSmith.

While running LangSmith, you may encounter unexpected 500 errors, slow performance, or other issues. This guide will help you diagnose and resolve these issues.

## Getting helpful information

To diagnose and resolve an issue, you will first need to retrieve some relevant information. The following sections explain how to do this for a Kubernetes or a Docker setup, and how to pull helpful browser information.

Generally, the main services you will want to analyze are the:

* `langsmith-backend`: Handles CRUD API requests, business logic, requests from the frontend and SDK, trace preparation for ingestion, and the hub API.
* `langsmith-platform-backend`: Handles authentication, run ingestion, and other high-volume tasks.
* `langsmith-queue`: Handles incoming traces and feedback, asynchronous ingestion and persistence into the datastore, data integrity checks, and retries during database errors or connection issues.

For more details on these services, refer to the [Architectural overview](/langsmith/architectural-overview).

The first step in troubleshooting is to gather important debugging information about your LangSmith deployment. Service logs, kubernetes events, and resource utilization of containers can help identify the root cause of an issue.

You can run our [k8s troubleshooting script](https://github.com/langchain-ai/helm/blob/main/charts/langsmith/scripts/get_k8s_debugging_info.sh) which will pull all of the relevant kubernetes information and output it to a folder for investigation. The script also compresses this folder into a zip file for sharing. Here is an example of how to run this script, assuming your langsmith deployment was brought up in a `langsmith` namespace:

You can then inspect the contents of the produced folder for any relevant errors or information. If you would like the LangSmith team to assist in debugging, please share this zip file with the team.

If running on Docker, you can check the logs your deployment by running the following command:

If you are experiencing an issue that surfaces as a browser error, it may also be helpful to inspect a HAR file which may include key information. To get the HAR file, you can follow [this guide](https://support.langchain.com/articles/9042697994-how-to-generate-a-har-file-for-troubleshooting) which explains the short process for various browsers.

You can then use [Google's HAR analyzer](https://toolbox.googleapps.com/apps/har_analyzer/) to investigate. You can also send your HAR file to the LangSmith team to help with debugging.

### *DB::Exception: Cannot reserve 1.00 MiB, not enough space: While executing WaitForAsyncInsert. (NOT\_ENOUGH\_SPACE)*

This error occurs when ClickHouse runs out of disk space. You will need to increase the disk space available to ClickHouse.

In Kubernetes, you will need to increase the size of the ClickHouse PVC. To achieve this, you can perform the following steps:

1. Get the storage class of the PVC: `kubectl get pvc data-langsmith-clickhouse-0 -n <namespace> -o jsonpath='{.spec.storageClassName}'`

2. Ensure the storage class has AllowVolumeExpansion: true: `kubectl get sc <storage-class-name> -o jsonpath='{.allowVolumeExpansion}'`

* If it is false, some storage classes can be updated to allow volume expansion.
   * To update the storage class, you can run `kubectl patch sc <storage-class-name> -p '{"allowVolumeExpansion": true}'`
   * If this fails, you may need to create a new storage class with the correct settings.

3. Edit your pvc to have the new size: `kubectl edit pvc data-langsmith-clickhouse-0 -n <namespace>` or `kubectl patch pvc data-langsmith-clickhouse-0 '{"spec":{"resources":{"requests":{"storage":"100Gi"}}}}' -n <namespace>`

4. Update your helm chart `langsmith_config.yaml` to new size(e.g `100 Gi`)

5. Delete the clickhouse statefulset `kubectl delete statefulset langsmith-clickhouse --cascade=orphan -n <namespace>`

6. Apply helm chart with updated size (You can follow the upgrade guide [here](/langsmith/self-host-upgrades))

7. Your pvc should now have the new size. Verify by running `kubectl get pvc` and `kubectl exec langsmith-clickhouse-0 -- bash -c "df"`

In Docker, you will need to increase the size of the ClickHouse volume. To achieve this, you can perform the following steps:

1. Stop your instance of LangSmith. `docker compose down`
2. If using bind mount, you will need to increase the size of the mount point.
3. If using a docker `volume`, you will need to allocate more space to the volume/docker.

### *error: Dirty database version 'version'. Fix and force version*

This error occurs when the ClickHouse database is in an inconsistent state with our migrations. You will need to reset to an earlier database version and then rerun your upgrade/migrations.

1. Force migration to an earlier version, where version = dirty version - 1.

1. Rerun your upgrade/migrations.

1. Force migration to an earlier version, where version = dirty version - 1.

1. Rerun your upgrade/migrations.

### *413 - Request Entity Too Large*

This error occurs when the request size exceeds the maximum allowed size. You will need to increase the maximum request size in your Nginx configuration.

1. Edit your `langsmith_config.yaml` and increase the `frontend.maxBodySize` [value](https://github.com/langchain-ai/helm/blob/main/charts/langsmith/values.yaml#L519). This might look something like this:

2. Apply your changes to the cluster.

### *Details: code: 497, message: default: Not enough privileges. To execute this query, it's necessary to have the grant CREATE ROW POLICY ON default.feedbacks\_rmt*

This error occurs when your user does not have the necessary permissions to create row policies in Clickhouse. When deploying the Docker deployment, you need to copy the `users.xml` file from the github repo as well. This adds the `<access_management>` tag to the `users.xml` file, which allows the user to create row policies. Below is the default `users.xml` file that we expect to be used.

In some environments, your mount point may not be writable by the container. In these cases we suggest building a custom image with the `users.xml` file included.

Example `Dockerfile`:

Then take the following steps:

1. Build your custom image.

2. Update your `docker-compose.yaml` to use the custom image. Make sure to remove the users.xml mount point.

3. Restart your instance of LangSmith.

### *ClickHouse fails to start up when running a cluster with AquaSec*

In some environments, AquaSec may prevent ClickHouse from starting up correctly. This may manifest as the ClickHouse pod not emitting any logs and failing to get marked as ready.
Generally this is due to `LD_PRELOAD` being set by AquaSec, which interferes with ClickHouse. To resolve this, you can add the following environment variable to your ClickHouse deployment:

Edit your `langsmith_config.yaml` (or corresponding config file) and set the `AQUA_SKIP_LD_PRELOAD` environment variable:

Edit your `docker-compose.yaml` and set the `AQUA_SKIP_LD_PRELOAD` environment variable:

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/troubleshooting.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
You can then inspect the contents of the produced folder for any relevant errors or information. If you would like the LangSmith team to assist in debugging, please share this zip file with the team.

#### Docker

If running on Docker, you can check the logs your deployment by running the following command:
```

Example 2 (unknown):
```unknown
#### Browser Errors

If you are experiencing an issue that surfaces as a browser error, it may also be helpful to inspect a HAR file which may include key information. To get the HAR file, you can follow [this guide](https://support.langchain.com/articles/9042697994-how-to-generate-a-har-file-for-troubleshooting) which explains the short process for various browsers.

You can then use [Google's HAR analyzer](https://toolbox.googleapps.com/apps/har_analyzer/) to investigate. You can also send your HAR file to the LangSmith team to help with debugging.

## Common issues

### *DB::Exception: Cannot reserve 1.00 MiB, not enough space: While executing WaitForAsyncInsert. (NOT\_ENOUGH\_SPACE)*

This error occurs when ClickHouse runs out of disk space. You will need to increase the disk space available to ClickHouse.

#### Kubernetes

In Kubernetes, you will need to increase the size of the ClickHouse PVC. To achieve this, you can perform the following steps:

1. Get the storage class of the PVC: `kubectl get pvc data-langsmith-clickhouse-0 -n <namespace> -o jsonpath='{.spec.storageClassName}'`

2. Ensure the storage class has AllowVolumeExpansion: true: `kubectl get sc <storage-class-name> -o jsonpath='{.allowVolumeExpansion}'`

   * If it is false, some storage classes can be updated to allow volume expansion.
   * To update the storage class, you can run `kubectl patch sc <storage-class-name> -p '{"allowVolumeExpansion": true}'`
   * If this fails, you may need to create a new storage class with the correct settings.

3. Edit your pvc to have the new size: `kubectl edit pvc data-langsmith-clickhouse-0 -n <namespace>` or `kubectl patch pvc data-langsmith-clickhouse-0 '{"spec":{"resources":{"requests":{"storage":"100Gi"}}}}' -n <namespace>`

4. Update your helm chart `langsmith_config.yaml` to new size(e.g `100 Gi`)

5. Delete the clickhouse statefulset `kubectl delete statefulset langsmith-clickhouse --cascade=orphan -n <namespace>`

6. Apply helm chart with updated size (You can follow the upgrade guide [here](/langsmith/self-host-upgrades))

7. Your pvc should now have the new size. Verify by running `kubectl get pvc` and `kubectl exec langsmith-clickhouse-0 -- bash -c "df"`

#### Docker

In Docker, you will need to increase the size of the ClickHouse volume. To achieve this, you can perform the following steps:

1. Stop your instance of LangSmith. `docker compose down`
2. If using bind mount, you will need to increase the size of the mount point.
3. If using a docker `volume`, you will need to allocate more space to the volume/docker.

### *error: Dirty database version 'version'. Fix and force version*

This error occurs when the ClickHouse database is in an inconsistent state with our migrations. You will need to reset to an earlier database version and then rerun your upgrade/migrations.

#### Kubernetes

1. Force migration to an earlier version, where version = dirty version - 1.
```

Example 3 (unknown):
```unknown
1. Rerun your upgrade/migrations.

#### Docker

1. Force migration to an earlier version, where version = dirty version - 1.
```

Example 4 (unknown):
```unknown
1. Rerun your upgrade/migrations.

### *413 - Request Entity Too Large*

This error occurs when the request size exceeds the maximum allowed size. You will need to increase the maximum request size in your Nginx configuration.

#### Kubernetes

1. Edit your `langsmith_config.yaml` and increase the `frontend.maxBodySize` [value](https://github.com/langchain-ai/helm/blob/main/charts/langsmith/values.yaml#L519). This might look something like this:
```

---

## Troubleshoot trace nesting

**URL:** llms-txt#troubleshoot-trace-nesting

**Contents:**
- Python
  - Context propagation using asyncio
  - Context propagation using threading

Source: https://docs.langchain.com/langsmith/nest-traces

When tracing with the LangSmith SDK, LangGraph, and LangChain, tracing should automatically propagate the correct context so that code executed within a parent trace will be rendered in the expected location in the UI.

If you see a child run go to a separate trace (and appear on the top level), it may be caused by one of the following known "edge cases".

The following outlines common causes for "split" traces when building with python.

### Context propagation using asyncio

When using async calls (especially with streaming) in Python versions \< 3.11, you may encounter issues with trace nesting. This is because Python's `asyncio` only [added full support for passing context](https://docs.python.org/3/library/asyncio-task.html#asyncio.create_task) in version 3.11.

LangChain and LangSmith SDK use [contextvars](https://docs.python.org/3/library/contextvars.html) to propagate tracing information implicitly. In Python 3.11 and above, this works seamlessly. However, in earlier versions (3.8, 3.9, 3.10), `asyncio` tasks lack proper `contextvar` support, which can lead to disconnected traces.

1. **Upgrade Python Version (Recommended)** If possible, upgrade to Python 3.11 or later for automatic context propagation.

2. **Manual Context Propagation** If upgrading isn't an option, you'll need to manually propagate the tracing context. The method varies depending on your setup:

a) **Using LangGraph or LangChain** Pass the parent `config` to the child call:

b) **Using LangSmith Directly** Pass the run tree directly:

c) **Combining Decorated Code with LangGraph/LangChain** Use a combination of techniques for manual handoff:

### Context propagation using threading

It's common to start tracing and want to apply some parallelism on child tasks all within a single trace. Python's stdlib `ThreadPoolExecutor` by default breaks tracing.

Python's contextvars start empty within new threads. Here are two approaches to handle maintain trace contiguity:

1. **Using LangSmith's ContextThreadPoolExecutor**

LangSmith provides a `ContextThreadPoolExecutor` that automatically handles context propagation:

2. **Manually providing the parent run tree**

Alternatively, you can manually pass the parent run tree to the inner function:

In this approach, we use `get_current_run_tree()` to obtain the current run tree and pass it to the inner function using the `langsmith_extra` parameter.

Both methods ensure that the inner function calls are correctly aggregated under the initial trace stack, even when executed in separate threads.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/nest-traces.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
b) **Using LangSmith Directly** Pass the run tree directly:
```

Example 2 (unknown):
```unknown
c) **Combining Decorated Code with LangGraph/LangChain** Use a combination of techniques for manual handoff:
```

Example 3 (unknown):
```unknown
### Context propagation using threading

It's common to start tracing and want to apply some parallelism on child tasks all within a single trace. Python's stdlib `ThreadPoolExecutor` by default breaks tracing.

#### Why

Python's contextvars start empty within new threads. Here are two approaches to handle maintain trace contiguity:

#### To resolve

1. **Using LangSmith's ContextThreadPoolExecutor**

   LangSmith provides a `ContextThreadPoolExecutor` that automatically handles context propagation:
```

Example 4 (unknown):
```unknown
2. **Manually providing the parent run tree**

   Alternatively, you can manually pass the parent run tree to the inner function:
```

---

## Troubleshoot variable caching

**URL:** llms-txt#troubleshoot-variable-caching

**Contents:**
- 1. Verify your environment variables
- 2. Clear the cache
- 3. Reload the environment variables

Source: https://docs.langchain.com/langsmith/troubleshooting-variable-caching

If you're not seeing traces in your tracing project or notice traces logged to the wrong project/workspace, the issue might be due to LangSmith's default environment variable caching. This is especially common when running LangSmith within a Jupyter notebook. Follow these steps to diagnose and resolve the issue:

## 1. Verify your environment variables

First, check that the environment variables are set correctly by running:

If the output does not match what's defined in your .env file, it's likely due to environment variable caching.

## 2. Clear the cache

Clear the cached environment variables with the following command:

## 3. Reload the environment variables

Reload your environment variables from the .env file by executing:

After reloading, your environment variables should be set correctly.

If you continue to experience issues, please reach out to us via a shared Slack channel or email support (available for Plus and Enterprise plans), or in the [LangChain Forum](https://forum.langchain.com/).

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/troubleshooting-variable-caching.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
If the output does not match what's defined in your .env file, it's likely due to environment variable caching.

## 2. Clear the cache

Clear the cached environment variables with the following command:
```

Example 2 (unknown):
```unknown
## 3. Reload the environment variables

Reload your environment variables from the .env file by executing:
```

---

## Try creating an assistant. This should fail

**URL:** llms-txt#try-creating-an-assistant.-this-should-fail

try:
    await alice.assistants.create("agent")
    print("❌ Alice shouldn't be able to create assistants!")
except Exception as e:
    print("✅ Alice correctly denied access:", e)

---

## Try searching for assistants. This also should fail

**URL:** llms-txt#try-searching-for-assistants.-this-also-should-fail

try:
    await alice.assistants.search()
    print("❌ Alice shouldn't be able to search assistants!")
except Exception as e:
    print("✅ Alice correctly denied access to searching assistants:", e)

---

## Try to access user 1's thread as user 2

**URL:** llms-txt#try-to-access-user-1's-thread-as-user-2

**Contents:**
- Next steps

user2_token = await login(email2, password)
user2_client = get_client(
    url="http://localhost:2024", headers={"Authorization": f"Bearer {user2_token}"}
)

try:
    await user2_client.threads.get(thread["thread_id"])
    print("❌ User 2 shouldn't see User 1's thread!")
except Exception as e:
    print("✅ User 2 blocked from User 1's thread:", e)
shell  theme={null}
✅ User 1 created thread: d6af3754-95df-4176-aa10-dbd8dca40f1a
✅ Unauthenticated access blocked: Client error '403 Forbidden' for url 'http://localhost:2024/threads'
✅ User 2 blocked from User 1's thread: Client error '404 Not Found' for url 'http://localhost:2024/threads/d6af3754-95df-4176-aa10-dbd8dca40f1a'
```

Your authentication and authorization are working together:

1. Users must log in to access the bot
2. Each user can only see their own threads

All users are managed by the Supabase auth provider, so you don't need to implement any additional user management logic.

You've successfully built a production-ready authentication system for your LangGraph application! Let's review what you've accomplished:

1. Set up an authentication provider (Supabase in this case)
2. Added real user accounts with email/password authentication
3. Integrated JWT token validation into your Agent Server
4. Implemented proper authorization to ensure users can only access their own data
5. Created a foundation that's ready to handle your next authentication challenge 🚀

Now that you have production authentication, consider:

1. Building a web UI with your preferred framework (see the [Custom Auth](https://github.com/langchain-ai/custom-auth) template for an example)
2. Learn more about the other aspects of authentication and authorization in the [conceptual guide on authentication](/langsmith/auth).
3. Customize your handlers and setup further after reading the [reference docs](/langsmith/langgraph-python-sdk#langgraph_sdk.auth.Auth).

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/add-auth-server.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
The output should look like this:
```

---

## Try to access without a token

**URL:** llms-txt#try-to-access-without-a-token

unauthenticated_client = get_client(url="http://localhost:2024")
try:
    await unauthenticated_client.threads.create()
    print("❌ Unauthenticated access should fail!")
except Exception as e:
    print("✅ Unauthenticated access blocked:", e)

---

## Try without a token (should fail)

**URL:** llms-txt#try-without-a-token-(should-fail)

client = get_client(url="http://localhost:2024")
try:
    thread = await client.threads.create()
    print("❌ Should have failed without token!")
except Exception as e:
    print("✅ Correctly blocked access:", e)

---

## Try with a valid token

**URL:** llms-txt#try-with-a-valid-token

client = get_client(
    url="http://localhost:2024", headers={"Authorization": "Bearer user1-token"}
)

---

## ttl:

**URL:** llms-txt#ttl:

---

## ttl_period_seconds:

**URL:** llms-txt#ttl_period_seconds:

---

## TypedDict defines the structure of user information for the LLM

**URL:** llms-txt#typeddict-defines-the-structure-of-user-information-for-the-llm

class UserInfo(TypedDict):
    name: str

---

## {"type": "image", "base64": "...", "mime_type": "image/jpeg"},

**URL:** llms-txt#{"type":-"image",-"base64":-"...",-"mime_type":-"image/jpeg"},

---

## {"type": "text", "text": "Here's a picture of a cat"},

**URL:** llms-txt#{"type":-"text",-"text":-"here's-a-picture-of-a-cat"},

---

## [{"type": "text", "text": "The sky is typically blue..."}]

**URL:** llms-txt#[{"type":-"text",-"text":-"the-sky-is-typically-blue..."}]

**Contents:**
  - Batch
- Tool calling
- Structured outputs
- Supported models
- Advanced topics
  - Multimodal

python  theme={null}
    async for event in model.astream_events("Hello"):

if event["event"] == "on_chat_model_start":
            print(f"Input: {event['data']['input']}")

elif event["event"] == "on_chat_model_stream":
            print(f"Token: {event['data']['chunk'].text}")

elif event["event"] == "on_chat_model_end":
            print(f"Full message: {event['data']['output'].text}")

else:
            pass
    txt  theme={null}
    Input: Hello
    Token: Hi
    Token:  there
    Token: !
    Token:  How
    Token:  can
    Token:  I
    ...
    Full message: Hi there! How can I help today?
    python Batch theme={null}
responses = model.batch([
    "Why do parrots have colorful feathers?",
    "How do airplanes fly?",
    "What is quantum computing?"
])
for response in responses:
    print(response)
python Yield batch responses upon completion theme={null}
for response in model.batch_as_completed([
    "Why do parrots have colorful feathers?",
    "How do airplanes fly?",
    "What is quantum computing?"
]):
    print(response)
python Batch with max concurrency theme={null}
  model.batch(
      list_of_inputs,
      config={
          'max_concurrency': 5,  # Limit to 5 parallel calls
      }
  )
  mermaid  theme={null}
sequenceDiagram
    participant U as User
    participant M as Model
    participant T as Tools

U->>M: "What's the weather in SF and NYC?"
    M->>M: Analyze request & decide tools needed

par Parallel Tool Calls
        M->>T: get_weather("San Francisco")
        M->>T: get_weather("New York")
    end

par Tool Execution
        T-->>M: SF weather data
        T-->>M: NYC weather data
    end

M->>M: Process results & generate response
    M->>U: "SF: 72°F sunny, NYC: 68°F cloudy"
python Binding user tools theme={null}
from langchain.tools import tool

@tool
def get_weather(location: str) -> str:
    """Get the weather at a location."""
    return f"It's sunny in {location}."

model_with_tools = model.bind_tools([get_weather])  # [!code highlight]

response = model_with_tools.invoke("What's the weather like in Boston?")
for tool_call in response.tool_calls:
    # View tool calls made by the model
    print(f"Tool: {tool_call['name']}")
    print(f"Args: {tool_call['args']}")
python Tool execution loop theme={null}
    # Bind (potentially multiple) tools to the model
    model_with_tools = model.bind_tools([get_weather])

# Step 1: Model generates tool calls
    messages = [{"role": "user", "content": "What's the weather in Boston?"}]
    ai_msg = model_with_tools.invoke(messages)
    messages.append(ai_msg)

# Step 2: Execute tools and collect results
    for tool_call in ai_msg.tool_calls:
        # Execute the tool with the generated arguments
        tool_result = get_weather.invoke(tool_call)
        messages.append(tool_result)

# Step 3: Pass results back to model for final response
    final_response = model_with_tools.invoke(messages)
    print(final_response.text)
    # "The current weather in Boston is 72°F and sunny."
    python Force use of any tool theme={null}
      model_with_tools = model.bind_tools([tool_1], tool_choice="any")
      python Force use of specific tools theme={null}
      model_with_tools = model.bind_tools([tool_1], tool_choice="tool_1")
      python Parallel tool calls theme={null}
    model_with_tools = model.bind_tools([get_weather])

response = model_with_tools.invoke(
        "What's the weather in Boston and Tokyo?"
    )

# The model may generate multiple tool calls
    print(response.tool_calls)
    # [
    #   {'name': 'get_weather', 'args': {'location': 'Boston'}, 'id': 'call_1'},
    #   {'name': 'get_weather', 'args': {'location': 'Tokyo'}, 'id': 'call_2'},
    # ]

# Execute all tools (can be done in parallel with async)
    results = []
    for tool_call in response.tool_calls:
        if tool_call['name'] == 'get_weather':
            result = get_weather.invoke(tool_call)
        ...
        results.append(result)
    python  theme={null}
      model.bind_tools([get_weather], parallel_tool_calls=False)
      python Streaming tool calls theme={null}
    for chunk in model_with_tools.stream(
        "What's the weather in Boston and Tokyo?"
    ):
        # Tool call chunks arrive progressively
        for tool_chunk in chunk.tool_call_chunks:
            if name := tool_chunk.get("name"):
                print(f"Tool: {name}")
            if id_ := tool_chunk.get("id"):
                print(f"ID: {id_}")
            if args := tool_chunk.get("args"):
                print(f"Args: {args}")

# Output:
    # Tool: get_weather
    # ID: call_SvMlU1TVIZugrFLckFE2ceRE
    # Args: {"lo
    # Args: catio
    # Args: n": "B
    # Args: osto
    # Args: n"}
    # Tool: get_weather
    # ID: call_QMZdy6qInx13oWKE7KhuhOLR
    # Args: {"lo
    # Args: catio
    # Args: n": "T
    # Args: okyo
    # Args: "}
    python Accumulate tool calls theme={null}
    gathered = None
    for chunk in model_with_tools.stream("What's the weather in Boston?"):
        gathered = chunk if gathered is None else gathered + chunk
        print(gathered.tool_calls)
    python  theme={null}
    from pydantic import BaseModel, Field

class Movie(BaseModel):
        """A movie with details."""
        title: str = Field(..., description="The title of the movie")
        year: int = Field(..., description="The year the movie was released")
        director: str = Field(..., description="The director of the movie")
        rating: float = Field(..., description="The movie's rating out of 10")

model_with_structure = model.with_structured_output(Movie)
    response = model_with_structure.invoke("Provide details about the movie Inception")
    print(response)  # Movie(title="Inception", year=2010, director="Christopher Nolan", rating=8.8)
    python  theme={null}
    from typing_extensions import TypedDict, Annotated

class MovieDict(TypedDict):
        """A movie with details."""
        title: Annotated[str, ..., "The title of the movie"]
        year: Annotated[int, ..., "The year the movie was released"]
        director: Annotated[str, ..., "The director of the movie"]
        rating: Annotated[float, ..., "The movie's rating out of 10"]

model_with_structure = model.with_structured_output(MovieDict)
    response = model_with_structure.invoke("Provide details about the movie Inception")
    print(response)  # {'title': 'Inception', 'year': 2010, 'director': 'Christopher Nolan', 'rating': 8.8}
    python  theme={null}
    import json

json_schema = {
        "title": "Movie",
        "description": "A movie with details",
        "type": "object",
        "properties": {
            "title": {
                "type": "string",
                "description": "The title of the movie"
            },
            "year": {
                "type": "integer",
                "description": "The year the movie was released"
            },
            "director": {
                "type": "string",
                "description": "The director of the movie"
            },
            "rating": {
                "type": "number",
                "description": "The movie's rating out of 10"
            }
        },
        "required": ["title", "year", "director", "rating"]
    }

model_with_structure = model.with_structured_output(
        json_schema,
        method="json_schema",
    )
    response = model_with_structure.invoke("Provide details about the movie Inception")
    print(response)  # {'title': 'Inception', 'year': 2010, ...}
    python  theme={null}
  from pydantic import BaseModel, Field

class Movie(BaseModel):
      """A movie with details."""
      title: str = Field(..., description="The title of the movie")
      year: int = Field(..., description="The year the movie was released")
      director: str = Field(..., description="The director of the movie")
      rating: float = Field(..., description="The movie's rating out of 10")

model_with_structure = model.with_structured_output(Movie, include_raw=True)  # [!code highlight]
  response = model_with_structure.invoke("Provide details about the movie Inception")
  response
  # {
  #     "raw": AIMessage(...),
  #     "parsed": Movie(title=..., year=..., ...),
  #     "parsing_error": None,
  # }
  python Pydantic BaseModel theme={null}
    from pydantic import BaseModel, Field

class Actor(BaseModel):
        name: str
        role: str

class MovieDetails(BaseModel):
        title: str
        year: int
        cast: list[Actor]
        genres: list[str]
        budget: float | None = Field(None, description="Budget in millions USD")

model_with_structure = model.with_structured_output(MovieDetails)
    python TypedDict theme={null}
    from typing_extensions import Annotated, TypedDict

class Actor(TypedDict):
        name: str
        role: str

class MovieDetails(TypedDict):
        title: str
        year: int
        cast: list[Actor]
        genres: list[str]
        budget: Annotated[float | None, ..., "Budget in millions USD"]

model_with_structure = model.with_structured_output(MovieDetails)
    python Multimodal output theme={null}
response = model.invoke("Create a picture of a cat")
print(response.content_blocks)

**Examples:**

Example 1 (unknown):
```unknown
The resulting message can be treated the same as a message that was generated with [`invoke()`](#invoke) - for example, it can be aggregated into a message history and passed back to the model as conversational context.

<Warning>
  Streaming only works if all steps in the program know how to process a stream of chunks. For instance, an application that isn't streaming-capable would be one that needs to store the entire output in memory before it can be processed.
</Warning>

<Accordion title="Advanced streaming topics">
  <Accordion title="&#x22;Auto-streaming&#x22; chat models">
    LangChain simplifies streaming from chat models by automatically enabling streaming mode in certain cases, even when you're not explicitly calling the streaming methods. This is particularly useful when you use the non-streaming invoke method but still want to stream the entire application, including intermediate results from the chat model.

    In [LangGraph agents](/oss/python/langchain/agents), for example, you can call `model.invoke()` within nodes, but LangChain will automatically delegate to streaming if running in a streaming mode.

    #### How it works

    When you `invoke()` a chat model, LangChain will automatically switch to an internal streaming mode if it detects that you are trying to stream the overall application. The result of the invocation will be the same as far as the code that was using invoke is concerned; however, while the chat model is being streamed, LangChain will take care of invoking [`on_llm_new_token`](https://reference.langchain.com/python/langchain_core/callbacks/#langchain_core.callbacks.base.AsyncCallbackHandler.on_llm_new_token) events in LangChain's callback system.

    Callback events allow LangGraph `stream()` and [`astream_events()`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel.astream_events) to surface the chat model's output in real-time.
  </Accordion>

  <Accordion title="Streaming events">
    LangChain chat models can also stream semantic events using [`astream_events()`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel.astream_events).

    This simplifies filtering based on event types and other metadata, and will aggregate the full message in the background. See below for an example.
```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
<Tip>
      See the [`astream_events()`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel.astream_events) reference for event types and other details.
    </Tip>
  </Accordion>
</Accordion>

### Batch

Batching a collection of independent requests to a model can significantly improve performance and reduce costs, as the processing can be done in parallel:
```

Example 4 (unknown):
```unknown
<Note>
  This section describes a chat model method [`batch()`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel.batch), which parallelizes model calls client-side.

  It is **distinct** from batch APIs supported by inference providers, such as [OpenAI](https://platform.openai.com/docs/guides/batch) or [Anthropic](https://docs.claude.com/en/docs/build-with-claude/batch-processing#message-batches-api).
</Note>

By default, [`batch()`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel.batch) will only return the final output for the entire batch. If you want to receive the output for each individual input as it finishes generating, you can stream results with [`batch_as_completed()`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel.batch_as_completed):
```

---

## Under the hood, it looks like

**URL:** llms-txt#under-the-hood,-it-looks-like

**Contents:**
  - FilesystemBackend (local disk)
  - StoreBackend (LangGraph Store)
  - CompositeBackend (router)
- Specify a backend
- Route to different backends
- Use a virtual filesystem
- Add policy hooks
- Protocol reference

from deepagents.backends import StateBackend

agent = create_deep_agent(
    backend=(lambda rt: StateBackend(rt))   # Note that the tools access State through the runtime.state
)
python  theme={null}
from deepagents.backends import FilesystemBackend

agent = create_deep_agent(
    backend=FilesystemBackend(root_dir=".", virtual_mode=True)
)
python  theme={null}
from langgraph.store.memory import InMemoryStore
from deepagents.backends import StoreBackend

agent = create_deep_agent(
    backend=(lambda rt: StoreBackend(rt)),   # Note that the tools access Store through the runtime.store
    store=InMemoryStore()
)
python  theme={null}
from deepagents import create_deep_agent
from deepagents.backends import CompositeBackend, StateBackend, StoreBackend
from langgraph.store.memory import InMemoryStore

composite_backend = lambda rt: CompositeBackend(
    default=StateBackend(rt),
    routes={
        "/memories/": StoreBackend(rt),
    }
)

agent = create_deep_agent(
    backend=composite_backend,
    store=InMemoryStore()  # Store passed to create_deep_agent, not backend
)
python  theme={null}
from deepagents import create_deep_agent
from deepagents.backends import CompositeBackend, StateBackend, FilesystemBackend

composite_backend = lambda rt: CompositeBackend(
    default=StateBackend(rt),
    routes={
        "/memories/": FilesystemBackend(root_dir="/deepagents/myagent", virtual_mode=True),
    },
)

agent = create_deep_agent(backend=composite_backend)
python  theme={null}
from deepagents.backends.protocol import BackendProtocol, WriteResult, EditResult
from deepagents.backends.utils import FileInfo, GrepMatch

class S3Backend(BackendProtocol):
    def __init__(self, bucket: str, prefix: str = ""):
        self.bucket = bucket
        self.prefix = prefix.rstrip("/")

def _key(self, path: str) -> str:
        return f"{self.prefix}{path}"

def ls_info(self, path: str) -> list[FileInfo]:
        # List objects under _key(path); build FileInfo entries (path, size, modified_at)
        ...

def read(self, file_path: str, offset: int = 0, limit: int = 2000) -> str:
        # Fetch object; return numbered content or an error string
        ...

def grep_raw(self, pattern: str, path: str | None = None, glob: str | None = None) -> list[GrepMatch] | str:
        # Optionally filter server‑side; else list and scan content
        ...

def glob_info(self, pattern: str, path: str = "/") -> list[FileInfo]:
        # Apply glob relative to path across keys
        ...

def write(self, file_path: str, content: str) -> WriteResult:
        # Enforce create‑only semantics; return WriteResult(path=file_path, files_update=None)
        ...

def edit(self, file_path: str, old_string: str, new_string: str, replace_all: bool = False) -> EditResult:
        # Read → replace (respect uniqueness vs replace_all) → write → return occurrences
        ...
python  theme={null}
from deepagents.backends.filesystem import FilesystemBackend
from deepagents.backends.protocol import WriteResult, EditResult

class GuardedBackend(FilesystemBackend):
    def __init__(self, *, deny_prefixes: list[str], **kwargs):
        super().__init__(**kwargs)
        self.deny_prefixes = [p if p.endswith("/") else p + "/" for p in deny_prefixes]

def write(self, file_path: str, content: str) -> WriteResult:
        if any(file_path.startswith(p) for p in self.deny_prefixes):
            return WriteResult(error=f"Writes are not allowed under {file_path}")
        return super().write(file_path, content)

def edit(self, file_path: str, old_string: str, new_string: str, replace_all: bool = False) -> EditResult:
        if any(file_path.startswith(p) for p in self.deny_prefixes):
            return EditResult(error=f"Edits are not allowed under {file_path}")
        return super().edit(file_path, old_string, new_string, replace_all)
python  theme={null}
from deepagents.backends.protocol import BackendProtocol, WriteResult, EditResult
from deepagents.backends.utils import FileInfo, GrepMatch

class PolicyWrapper(BackendProtocol):
    def __init__(self, inner: BackendProtocol, deny_prefixes: list[str] | None = None):
        self.inner = inner
        self.deny_prefixes = [p if p.endswith("/") else p + "/" for p in (deny_prefixes or [])]

def _deny(self, path: str) -> bool:
        return any(path.startswith(p) for p in self.deny_prefixes)

def ls_info(self, path: str) -> list[FileInfo]:
        return self.inner.ls_info(path)
    def read(self, file_path: str, offset: int = 0, limit: int = 2000) -> str:
        return self.inner.read(file_path, offset=offset, limit=limit)
    def grep_raw(self, pattern: str, path: str | None = None, glob: str | None = None) -> list[GrepMatch] | str:
        return self.inner.grep_raw(pattern, path, glob)
    def glob_info(self, pattern: str, path: str = "/") -> list[FileInfo]:
        return self.inner.glob_info(pattern, path)
    def write(self, file_path: str, content: str) -> WriteResult:
        if self._deny(file_path):
            return WriteResult(error=f"Writes are not allowed under {file_path}")
        return self.inner.write(file_path, content)
    def edit(self, file_path: str, old_string: str, new_string: str, replace_all: bool = False) -> EditResult:
        if self._deny(file_path):
            return EditResult(error=f"Edits are not allowed under {file_path}")
        return self.inner.edit(file_path, old_string, new_string, replace_all)
```

## Protocol reference

Backends must implement the `BackendProtocol`.

* `ls_info(path: str) -> list[FileInfo]`
  * Return entries with at least `path`. Include `is_dir`, `size`, `modified_at` when available. Sort by `path` for deterministic output.
* `read(file_path: str, offset: int = 0, limit: int = 2000) -> str`
  * Return numbered content. On missing file, return `"Error: File '/x' not found"`.
* `grep_raw(pattern: str, path: Optional[str] = None, glob: Optional[str] = None) -> list[GrepMatch] | str`
  * Return structured matches. For an invalid regex, return a string like `"Invalid regex pattern: ..."` (do not raise).
* `glob_info(pattern: str, path: str = "/") -> list[FileInfo]`
  * Return matched files as `FileInfo` entries (empty list if none).
* `write(file_path: str, content: str) -> WriteResult`
  * Create-only. On conflict, return `WriteResult(error=...)`. On success, set `path` and for state backends set `files_update={...}`; external backends should use `files_update=None`.
* `edit(file_path: str, old_string: str, new_string: str, replace_all: bool = False) -> EditResult`
  * Enforce uniqueness of `old_string` unless `replace_all=True`. If not found, return error. Include `occurrences` on success.

* `WriteResult(error, path, files_update)`
* `EditResult(error, path, files_update, occurrences)`
* `FileInfo` with fields: `path` (required), optionally `is_dir`, `size`, `modified_at`.
* `GrepMatch` with fields: `path`, `line`, `text`.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/deepagents/backends.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
**How it works:**

* Stores files in LangGraph agent state for the current thread.
* Persists across multiple agent turns on the same thread via checkpoints.

**Best for:**

* A scratch pad for the agent to write intermediate results.
* Automatic eviction of large tool outputs which the agent can then read back in piece by piece.

### FilesystemBackend (local disk)
```

Example 2 (unknown):
```unknown
**How it works:**

* Reads/writes real files under a configurable `root_dir`.
* You can optionally set `virtual_mode=True` to sandbox and normalize paths under `root_dir`.
* Uses secure path resolution, prevents unsafe symlink traversal when possible, can use ripgrep for fast `grep`.

**Best for:**

* Local projects on your machine
* CI sandboxes
* Mounted persistent volumes

### StoreBackend (LangGraph Store)
```

Example 3 (unknown):
```unknown
**How it works:**

* Stores files in a LangGraph `BaseStore` provided by the runtime, enabling cross‑thread durable storage.

**Best for:**

* When you already run with a configured LangGraph store (for example, Redis, Postgres, or cloud implementations behind `BaseStore`).
* When you're deploying your agent through LangSmith Deployments (a store is automatically provisioned for your agent).

### CompositeBackend (router)
```

Example 4 (unknown):
```unknown
**How it works:**

* Routes file operations to different backends based on path prefix.
* Preserves the original path prefixes in listings and search results.

**Best for:**

* When you want to give your agent both ephemeral and cross-thread storage, a CompositeBackend allows you provide both a StateBackend and StoreBackend
* When you have multiple sources of information that you want to provide to your agent as part of a single filesystem.
  * e.g. You have long-term memories stored under /memories/ in one Store and you also have a custom backend that has documentation accessible at /docs/.

## Specify a backend

* Pass a backend to `create_deep_agent(backend=...)`. The filesystem middleware uses it for all tooling.
* You can pass either:
  * An instance implementing `BackendProtocol` (for example, `FilesystemBackend(root_dir=".")`), or
  * A factory `BackendFactory = Callable[[ToolRuntime], BackendProtocol]` (for backends that need runtime like `StateBackend` or `StoreBackend`).
* If omitted, the default is `lambda rt: StateBackend(rt)`.

## Route to different backends

Route parts of the namespace to different backends. Commonly used to persist `/memories/*` and keep everything else ephemeral.
```

---

## Unified access to content blocks

**URL:** llms-txt#unified-access-to-content-blocks

**Contents:**
  - Benefits
- Simplified package
  - Namespace

for block in response.content_blocks:
    if block["type"] == "reasoning":
        print(f"Model reasoning: {block['reasoning']}")
    elif block["type"] == "text":
        print(f"Response: {block['text']}")
    elif block["type"] == "tool_call":
        print(f"Tool call: {block['name']}({block['args']})")
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
### Benefits

* **Provider agnostic**: Access reasoning traces, citations, built-in tools (web search, code interpreters, etc.), and other features using the same API regardless of provider
* **Type safe**: Full type hints for all content block types
* **Backward compatible**: Standard content can be [loaded lazily](/oss/python/langchain/messages#standard-content-blocks), so there are no associated breaking changes

For more information, see our guide on [content blocks](/oss/python/langchain/messages#standard-content-blocks).

***

## Simplified package

LangChain v1 streamlines the [`langchain`](https://pypi.org/project/langchain/) package namespace to focus on essential building blocks for agents. The refined namespace exposes the most useful and relevant functionality:

### Namespace

| Module                                                                                | What's available                                                                                                                                                                                                                                                          | Notes                                 |
| ------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------- |
| [`langchain.agents`](https://reference.langchain.com/python/langchain/agents)         | [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent), [`AgentState`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.AgentState)                                                            | Core agent creation functionality     |
| [`langchain.messages`](https://reference.langchain.com/python/langchain/messages)     | Message types, [content blocks](https://reference.langchain.com/python/langchain/messages/#langchain.messages.ContentBlock), [`trim_messages`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.trim_messages)                               | Re-exported from @\[`langchain-core`] |
| [`langchain.tools`](https://reference.langchain.com/python/langchain/tools)           | [`@tool`](https://reference.langchain.com/python/langchain/tools/#langchain.tools.tool), [`BaseTool`](https://reference.langchain.com/python/langchain/tools/#langchain.tools.BaseTool), injection helpers                                                                | Re-exported from @\[`langchain-core`] |
| [`langchain.chat_models`](https://reference.langchain.com/python/langchain/models)    | [`init_chat_model`](https://reference.langchain.com/python/langchain/models/#langchain.chat_models.init_chat_model), [`BaseChatModel`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel)   | Unified model initialization          |
| [`langchain.embeddings`](https://reference.langchain.com/python/langchain/embeddings) | [`Embeddings`](https://reference.langchain.com/python/langchain_core/embeddings/#langchain_core.embeddings.embeddings.Embeddings), [`init_embeddings`](https://reference.langchain.com/python/langchain_core/embeddings/#langchain_core.embeddings.embeddings.Embeddings) | Embedding models                      |

Most of these are re-exported from `langchain-core` for convenience, which gives you a focused API surface for building agents.
```

---

## Update memory

**URL:** llms-txt#update-memory

@tool
def save_user_info(user_id: str, user_info: dict[str, Any], runtime: ToolRuntime) -> str:
    """Save user info."""
    store = runtime.store
    store.put(("users",), user_id, user_info)
    return "Successfully saved user info."

store = InMemoryStore()
agent = create_agent(
    model,
    tools=[get_user_info, save_user_info],
    store=store
)

---

## Update these values as needed to connect to your replicated clickhouse cluster.

**URL:** llms-txt#update-these-values-as-needed-to-connect-to-your-replicated-clickhouse-cluster.

clickhouse:
  external:
    # If using a 3 node replicated setup, each replica in the cluster should have resource requests of 14+ cores and 24+ GB memory, and resource limit of 20 cores and 48 GB memory.
    enabled: true
    host: langsmith-ch-clickhouse-replicated.default.svc.cluster.local
    port: "8123"
    nativePort: "9000"
    user: "default"
    password: "password"
    database: "default"
    cluster: "replicated"

commonEnv:
  - name: "CLICKHOUSE_ASYNC_INSERT_WAIT_PCT_FLOAT"
    value: "0"
```

<Note>
  Ensure that the Kubernetes cluster is configured with sufficient resources to scale to the recommended size. After deployment, all of the pods in the Kubernetes cluster should be in a `Running` state. Pods stuck in `Pending` may indicate that you are reaching node pool limits or need larger nodes.

Also, ensure that any ingress controller deployed on the cluster is able to handle the desired load to prevent bottlenecks.
</Note>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/self-host-scale.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Update the conversation history by removing all messages

**URL:** llms-txt#update-the-conversation-history-by-removing-all-messages

@tool
def clear_conversation() -> Command:
    """Clear the conversation history."""

return Command(
        update={
            "messages": [RemoveMessage(id=REMOVE_ALL_MESSAGES)],
        }
    )

---

## Update the user_name in the agent state

**URL:** llms-txt#update-the-user_name-in-the-agent-state

@tool
def update_user_name(
    new_name: str,
    runtime: ToolRuntime
) -> Command:
    """Update the user's name."""
    return Command(update={"user_name": new_name})
python  theme={null}
from dataclasses import dataclass
from langchain_openai import ChatOpenAI
from langchain.agents import create_agent
from langchain.tools import tool, ToolRuntime

USER_DATABASE = {
    "user123": {
        "name": "Alice Johnson",
        "account_type": "Premium",
        "balance": 5000,
        "email": "alice@example.com"
    },
    "user456": {
        "name": "Bob Smith",
        "account_type": "Standard",
        "balance": 1200,
        "email": "bob@example.com"
    }
}

@dataclass
class UserContext:
    user_id: str

@tool
def get_account_info(runtime: ToolRuntime[UserContext]) -> str:
    """Get the current user's account information."""
    user_id = runtime.context.user_id

if user_id in USER_DATABASE:
        user = USER_DATABASE[user_id]
        return f"Account holder: {user['name']}\nType: {user['account_type']}\nBalance: ${user['balance']}"
    return "User not found"

model = ChatOpenAI(model="gpt-4o")
agent = create_agent(
    model,
    tools=[get_account_info],
    context_schema=UserContext,
    system_prompt="You are a financial assistant."
)

result = agent.invoke(
    {"messages": [{"role": "user", "content": "What's my current balance?"}]},
    context=UserContext(user_id="user123")
)
python expandable theme={null}
from typing import Any
from langgraph.store.memory import InMemoryStore
from langchain.agents import create_agent
from langchain.tools import tool, ToolRuntime

**Examples:**

Example 1 (unknown):
```unknown
#### Context

Access immutable configuration and contextual data like user IDs, session details, or application-specific configuration through `runtime.context`.

Tools can access runtime context through `ToolRuntime`:
```

Example 2 (unknown):
```unknown
#### Memory (Store)

Access persistent data across conversations using the store. The store is accessed via `runtime.store` and allows you to save and retrieve user-specific or application-specific data.

Tools can access and update the store through `ToolRuntime`:
```

---

## Upgrade an installation

**URL:** llms-txt#upgrade-an-installation

**Contents:**
- Kubernetes(Helm)
  - Validate your deployment:
- Docker
  - Validate your deployment:

Source: https://docs.langchain.com/langsmith/self-host-upgrades

For general upgrade instructions, please follow the instructions below. Certain versions may have specific upgrade instructions, which will be detailed in more specific upgrade guides.

If you don't have the repo added, run the following command to add it:

Update your local helm repo

Update your helm chart config file with any updates that are needed in the new version. These will be detailed in the release notes for the new version.

Run the following command to upgrade the chart(replace version with the version you want to upgrade to):

<Note>
  If you are using a namespace other than the default namespace, you will need to specify the namespace in the `helm` and `kubectl` commands by using the `-n <namespace` flag.
</Note>

Find the latest version of the chart. You can find this in the [LangSmith Helm Chart GitHub repository](https://github.com/langchain-ai/helm/releases) or by running the following command:

You should see an output similar to this:

Choose the version you want to upgrade to (generally the latest version is recommended) and note the version number.

Verify that the upgrade was successful:

All pods should be in the `Running` state. Verify that clickhouse is running and that both `migrations` jobs have completed.

### Validate your deployment:

1. Run `kubectl get services`

Output should look something like:

2. Curl the external ip of the `langsmith-frontend` service:

Check that the version matches the version you upgraded to.

3. Visit the external ip for the `langsmith-frontend` service on your browser

The LangSmith UI should be visible/operational

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=5310f686e7b9eebaaee4fe2a152a8675" alt="LangSmith UI" data-og-width="2886" width="2886" data-og-height="1698" height="1698" data-path="langsmith/images/langsmith-ui.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=5f155ce778ca848f89fefff237b69bcb 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=1d55d4068a9f53387c129b4688b0971e 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=feb20198d67249ece559e5fd0e6d8e98 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=3e5eba764d911e567d5aaa9e5702327b 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=d45af56632578a8d1b05e546dfc8d01d 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=16a49517a6c224930fdb81c9ccde5527 2500w" />

Upgrading the Docker version of LangSmith is a bit more involved than the Helm version and may require a small amount of downtime. Please follow the instructions below to upgrade your Docker version of LangSmith.

1. Update your `docker-compose.yml` file to the file used in the latest release. You can find this in the [LangSmith SDK GitHub repository](https://github.com/langchain-ai/langsmith-sdk/blob/main/python/langsmith/cli/docker-compose.yaml)
2. Update your `.env` file with any new environment variables that are required in the new version. These will be detailed in the release notes for the new version.
3. Run the following command to stop your current LangSmith instance:

4. Run the following command to start your new LangSmith instance in the background:

If everything ran successfully, you should see all the LangSmith containers running and healthy.

### Validate your deployment:

1. Curl the exposed port of the `cli-langchain-frontend-1` container:

2. Visit the exposed port of the `cli-langchain-frontend-1` container on your browser

The LangSmith UI should be visible/operational

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=5310f686e7b9eebaaee4fe2a152a8675" alt="LangSmith UI" data-og-width="2886" width="2886" data-og-height="1698" height="1698" data-path="langsmith/images/langsmith-ui.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=5f155ce778ca848f89fefff237b69bcb 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=1d55d4068a9f53387c129b4688b0971e 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=feb20198d67249ece559e5fd0e6d8e98 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=3e5eba764d911e567d5aaa9e5702327b 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=d45af56632578a8d1b05e546dfc8d01d 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=16a49517a6c224930fdb81c9ccde5527 2500w" />

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/self-host-upgrades.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
Update your local helm repo
```

Example 2 (unknown):
```unknown
Update your helm chart config file with any updates that are needed in the new version. These will be detailed in the release notes for the new version.

Run the following command to upgrade the chart(replace version with the version you want to upgrade to):

<Note>
  If you are using a namespace other than the default namespace, you will need to specify the namespace in the `helm` and `kubectl` commands by using the `-n <namespace` flag.
</Note>

Find the latest version of the chart. You can find this in the [LangSmith Helm Chart GitHub repository](https://github.com/langchain-ai/helm/releases) or by running the following command:
```

Example 3 (unknown):
```unknown
You should see an output similar to this:
```

Example 4 (unknown):
```unknown
Choose the version you want to upgrade to (generally the latest version is recommended) and note the version number.
```

---

## Upload files with traces

**URL:** llms-txt#upload-files-with-traces

**Contents:**
  - Python

Source: https://docs.langchain.com/langsmith/upload-files-with-traces

<Check>
  Before diving into this content, it would be helpful to read the following guides:

* [Trace with LangSmith using the traceable decorator or wrapper](/langsmith/annotate-code#use-traceable--traceable)
</Check>

<Note>
  The following features are available in the following SDK versions:

* Python SDK: >=0.1.141
  * JS/TS SDK: >=0.2.5
</Note>

LangSmith supports uploading binary files (such as images, audio, videos, PDFs, and CSVs) with your traces. This is particularly useful when working with LLM pipelines using multimodal inputs or outputs.

In both the Python and TypeScript SDKs, attachments can be added to your traces by specifying the MIME type and binary content of each file. This guide explains how to define and trace attachments using the `Attachment` type in Python and `Uint8Array` / `ArrayBuffer` in TypeScript.

In the Python SDK, you can use the `Attachment` type to add files to your traces. Each `Attachment` requires:

* `mime_type` (str): The MIME type of the file (e.g., `"image/png"`).
* `data` (bytes | Path): The binary content of the file, or the file path.

You can also define an attachment with a tuple tuple of the form `(mime_type, data)` for convenience.

Simply decorate a function with `@traceable` and include your `Attachment` instances as arguments. Note that to use the file path instead of the raw bytes, you need to set the `dangerously_allow_filesystem` flag to `True` in your traceable decorator.

```python Python theme={null}
from langsmith import traceable
from langsmith.schemas import Attachment
from pathlib import Path
import os

---

## User management

**URL:** llms-txt#user-management

**Contents:**
- Set up access control
  - Create a role
  - Assign a role to a user
- Set up SAML SSO for your organization
  - Just-in-time (JIT) provisioning
  - Login methods and access
  - Enforce SAML SSO only
  - Prerequisites
  - Initial configuration
  - Entra ID (Azure)

Source: https://docs.langchain.com/langsmith/user-management

This page covers user management features in LangSmith, including access control, authentication, and automated user provisioning:

* [Set up access control](#set-up-access-control): Configure role-based access control (RBAC) to manage user permissions within workspaces, including creating custom roles and assigning them to users.
* [SAML SSO (Enterprise plan)](#set-up-saml-sso-for-your-organization): Set up Single Sign-On authentication for Enterprise customers using SAML 2.0, including configuration for popular identity providers.
* [SCIM User Provisioning (Enterprise plan)](#set-up-scim-for-your-organization): Automate user provisioning and deprovisioning between your identity provider and LangSmith using SCIM.

## Set up access control

<Note>
  RBAC (Role-Based Access Control) is a feature that is only available to Enterprise customers. If you are interested in this feature, [contact our sales team](https://www.langchain.com/contact-sales). Other plans default to using the [`Admin` role](/langsmith/administration-overview) for all users.
</Note>

<Check>
  You may find it helpful to read the [Administration overview](/langsmith/administration-overview) page before setting up access control.
</Check>

LangSmith relies on RBAC to manage user permissions within a [workspace](/langsmith/administration-overview#workspaces). This allows you to control who can access your LangSmith workspace and what they can do within it. Only users with the `workspace:manage` permission can manage access control settings for a workspace.

By default, LangSmith comes with a set of system roles:

* `Admin`: has full access to all resources within the workspace.
* `Viewer`: has read-only access to all resources within the workspace.
* `Editor`: has full permissions except for workspace management (adding/removing users, changing roles, configuring service keys).

If these do not fit your access model, `Organization Admins` can create custom roles to suit your needs.

To create a role, navigate to the **Roles** tab in the **Members and roles** section of the [Organization settings page](https://smith.langchain.com/settings). Note that new roles that you create will be usable across all workspaces within your organization.

Click on the **Create Role** button to create a new role. A **Create role** form will open.

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-role.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=6a413dbec076a37d680fa2ed8a91c495" alt="Create Role" data-og-width="3078" width="3078" data-og-height="1932" height="1932" data-path="langsmith/images/create-role.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-role.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=9b92bb7e4743445999e92faac75163ec 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-role.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=9ca40498a6740d4b73a5856461088af8 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-role.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=77bde5767be13e160a54d06c5c016953 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-role.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=c83fd7dfd2f046073d80682607c6b6e1 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-role.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=318019bbd5d5e272aa6bad083f6713d5 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-role.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=642e5b56a1dc12a29173f969ec08be45 2500w" />

Assign permissions for the different LangSmith resources that you want to control access to.

### Assign a role to a user

Once you have your roles set up, you can assign them to users. To assign a role to a user, navigate to the `Workspace members` tab in the `Workspaces` section of the [Organization settings page](https://smith.langchain.com/settings)

Each user will have a **Role** dropdown that you can use to assign a role to them.

<img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/assign-role.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=ec5748a2c884693a24f984cc517a3860" alt="Assign Role" data-og-width="1888" width="1888" data-og-height="574" height="574" data-path="langsmith/images/assign-role.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/assign-role.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=9530054e9b95d485534b328b13c6df69 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/assign-role.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=864bd5501ab0f253fc5f6db15dec9205 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/assign-role.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=520648bd2bef4ed100193c0031b9b3e9 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/assign-role.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=53bbafe1b595daf0487a2e8daff8ce68 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/assign-role.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=b6b5ea2d4d814c28b243d170f0af6359 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/assign-role.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=6ad29acab6d3c0e06b0f5ddbef793ca9 2500w" />

You can also invite new users with a given role.

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/invite-user.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=3787cb55cbefcd5c95d6c01b6e9f6e75" alt="Invite User" data-og-width="1204" width="1204" data-og-height="886" height="886" data-path="langsmith/images/invite-user.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/invite-user.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=7a300a15bd2572a18b8cc2f37921ffb6 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/invite-user.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=ea044cb3c58da8351dc60f2d3a040c23 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/invite-user.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=a3ab5fd8738cde3f2257e580daf96114 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/invite-user.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=7e8b4ab3a1f758aaa93a8694da3ee9cf 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/invite-user.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=a7642a1b08854289db7bb7939c23db52 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/invite-user.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=33205dfa57d9a51af02c4f53395fc7f6 2500w" />

## Set up SAML SSO for your organization

Single Sign-On (SSO) functionality is **available for Enterprise Cloud** customers to access LangSmith through a single authentication source. This allows administrators to centrally manage team access and keeps information more secure.

LangSmith's SSO configuration is built using the SAML (Security Assertion Markup Language) 2.0 standard. SAML 2.0 enables connecting an Identity Provider (IdP) to your organization for an easier, more secure login experience.

SSO services permit a user to use one set of credentials (for example, a name or email address and password) to access multiple applications. The service authenticates the end user only once for all the applications the user has been given rights to and eliminates further prompts when the user switches applications during the same session. The benefits of SSO include:

* Streamlines user management across systems for organization owners.
* Enables organizations to enforce their own security policies (e.g., MFA).
* Removes the need for end users to remember and manage multiple passwords. Simplifies the end-user experience, by allowing sign in at one single access point across multiple applications.

### Just-in-time (JIT) provisioning

LangSmith supports Just-in-time provisioning when using SAML SSO. This allows someone signing in via SAML SSO to join the organization and selected workspaces automatically as a member.

<Note>
  JIT provisioning only runs for new users, that is, users who do not already have access to the organization with the same email address via a [different login method](/langsmith/authentication-methods#cloud).
</Note>

### Login methods and access

Once you have completed your configuration of SAML SSO for your organization, users will be able to log in via SAML SSO in addition to [other login methods](/langsmith/authentication-methods#cloud), such as username/password or Google Authentication":

* When logged in via SAML SSO, users can only access the corresponding organization with SAML SSO configured.
* Users with SAML SSO as their only login method do not have [personal organizations](/langsmith/administration-overview#organizations).
* When logged in via any other method, users can access the organization with SAML SSO configured along with any other organizations they are a part of.

### Enforce SAML SSO only

<Note>
  User invites are not supported in organizations enforcing SAML SSO only. Initial workspace membership and role is determined by JIT provisioning, and changes afterwards can be managed in the UI.
  For additional flexibility in automated user management, LangSmith supports SCIM.
</Note>

To ensure users can only access the organization when logged in using SAML SSO and no other method, check the **Login via SSO only** checkbox and click **Save**. Once this happens, users accessing the organization that are logged-in via a non-SSO login method are required to log back in using SAML SSO. This setting can be switched back to allow all login methods by unselecting the checkbox and clicking **Save**.

<Note>
  You must be logged in via SAML SSO in order to update this setting to `Only SAML SSO`. This is to ensure the SAML settings are valid and avoid locking users out of your organization.
</Note>

For troubleshooting, refer to the [SAML SSO FAQs](/langsmith/faq#saml-sso-faqs). If you have issues setting up SAML SSO, reach out to the LangChain support team at [support@langchain.dev](mailto:support@langchain.dev).

<Note>
  SAML SSO is available for organizations on the [Enterprise plan](https://www.langchain.com/pricing-langsmith). Please [contact sales](https://www.langchain.com/contact-sales) to learn more.
</Note>

* Your organization must be on an Enterprise plan.
* Your Identity Provider (IdP) must support the SAML 2.0 standard.
* Only [`Organization Admins`](/langsmith/observability-concepts#organization-roles) can configure SAML SSO.

For instructions on using SCIM along with SAML for user provisioning and deprovisioning, refer to the [SCIM setup](#set-up-scim-for-your-organization).

### Initial configuration

<Note>
  For IdP-specific configuration steps, refer to one of the following:

* [Entra ID](#entra-id-azure)
  * [Google](#google)
  * [Okta](#okta)
</Note>

1. In your IdP: Configure a SAML application with the following details, then copy the metadata URL or XML for step 3.

<Note>
     The following URLs are different for the US and EU regions. Ensure you select the correct link.
   </Note>

1. Single sign-on URL (or ACS URL):
      * US: [https://auth.langchain.com/auth/v1/sso/saml/acs](https://auth.langchain.com/auth/v1/sso/saml/acs)
      * EU: [https://eu.auth.langchain.com/auth/v1/sso/saml/acs](https://eu.auth.langchain.com/auth/v1/sso/saml/acs)
   2. Audience URI (or SP Entity ID):
      * US: [https://auth.langchain.com/auth/v1/sso/saml/metadata](https://auth.langchain.com/auth/v1/sso/saml/metadata)
      * EU: [https://eu.auth.langchain.com/auth/v1/sso/saml/metadata](https://eu.auth.langchain.com/auth/v1/sso/saml/metadata)
   3. Name ID format: email address.
   4. Application username: email address.
   5. Required claims: `sub` and `email`.

2. In LangSmith: Go to **Settings** -> **Members and roles** -> **SSO Configuration**. Fill in the required information and submit to activate SSO login:

1. Fill in either the `SAML metadata URL` or `SAML metadata XML`.
   2. Select the `Default workspace role` and `Default workspaces`. New users logging in via SSO will be added to the specified workspaces with the selected role.

* `Default workspace role` and `Default workspaces` are editable. The updated settings will apply to new users only, not existing users.
* (Coming soon) `SAML metadata URL` and `SAML metadata XML` are editable. This is usually only necessary when cryptographic keys are rotated/expired or the metadata URL has changed but the same IdP is still used.

For additional information, see Microsoft's [documentation](https://learn.microsoft.com/en-us/entra/identity/enterprise-apps/add-application-portal-setup-sso).

<div id="create-application-entra-id" />

**Step 1: Create a new Entra ID application integration**

1. Log in to the [Azure portal](https://portal.azure.com/#home) with a privileged role (e.g., `Global Administrator`). On the left navigation pane, select the `Entra ID` service.

2. Navigate to **Enterprise Applications** and then select **All Applications**.

3. Click **Create your own application**.

4. In the **Create your own application** window:

1. Enter a name for your application (e.g., `LangSmith`).
   2. Select \*Integrate any other application you don't find in the gallery (Non-gallery)\*\*.

**Step 2: Configure the Entra ID application and obtain the SAML Metadata**

1. Open the enterprise application that you created.

2. In the left-side navigation, select **Manage** > **Single sign-on**.

3. On the Single sign-on page, click **SAML**.

4. Update the **Basic SAML Configuration**:

1. `Identifier (Entity ID)`:
      * US: [https://auth.langchain.com/auth/v1/sso/saml/metadata](https://auth.langchain.com/auth/v1/sso/saml/metadata)
      * EU: [https://eu.auth.langchain.com/auth/v1/sso/saml/metadata](https://eu.auth.langchain.com/auth/v1/sso/saml/metadata)
   2. `Reply URL (Assertion Consumer Service URL)`:
      * US: [https://auth.langchain.com/auth/v1/sso/saml/acs](https://auth.langchain.com/auth/v1/sso/saml/acs)
      * EU: [https://eu.auth.langchain.com/auth/v1/sso/saml/acs](https://eu.auth.langchain.com/auth/v1/sso/saml/acs)
   3. Leave `Relay State`, `Logout Url`, and `Sign on URL` empty.
   4. Click **Save**.

5. Ensure required claims are present with **Namespace**: `http://schemas.xmlsoap.org/ws/2005/05/identity/claims`:

1. `sub`: `user.objectid`.
   2. `emailaddress`: `user.userprincipalname` or `user.mail` (if using the latter, ensure all users have the `Email` field filled in under `Contact Information`).
   3. (Optional) For SCIM, see the [setup documentation](/langsmith/user-management) for specific instructions about `Unique User Identifier (Name ID)`.

6. On the SAML-based Sign-on page, under **SAML Certificates**, copy the **App Federation Metadata URL**.

**Step 3: Set up LangSmith SSO Configuration**

Follow the instructions under [initial configuration](#initial-configuration) in the `Fill in required information` step, using the metadata URL from the previous step.

**Step 4: Verify the SSO setup**

1. Assign the application to users/groups in Entra ID:

1. Select **Manage** > **Users and groups**.

2. Click **Add user/group**.

3. In the **Add Assignment** window:

1. Under **Users**, click **None Selected**.
      2. Search for the user you want to assign to the enterprise application, and then click **Select**.
      3. Verify that the user is selected, and click **Assign**.

2. Have the user sign in via the unique login URL from the **SSO Configuration** page, or go to **Manage** > **Single sign-on** and select **Test single sign-on with (application name)**.

For additional information, see Google's [documentation](https://support.google.com/a/answer/6087519).

**Step 1: Create and configure the Google Workspace SAML application**

1. Make sure you're signed into an administrator account with the appropriate permissions.

2. In the Admin console, go to **Menu** -> **Apps** -> **Web and mobile apps**.

3. Click **Add App** and then **Add custom SAML app**.

4. Enter the app name and, optionally, upload an icon. Click **Continue**.

5. On the Google Identity Provider details page, download the **IDP metadata** and save it for Step 2. Click **Continue**.

6. In the `Service Provider Details` window, enter:

1. `ACS URL`:
      * US: [https://auth.langchain.com/auth/v1/sso/saml/acs](https://auth.langchain.com/auth/v1/sso/saml/acs)
      * EU: [https://eu.auth.langchain.com/auth/v1/sso/saml/acs](https://eu.auth.langchain.com/auth/v1/sso/saml/acs)
   2. `Entity ID`:
      * US: [https://auth.langchain.com/auth/v1/sso/saml/metadata](https://auth.langchain.com/auth/v1/sso/saml/metadata)
      * EU: [https://eu.auth.langchain.com/auth/v1/sso/saml/metadata](https://eu.auth.langchain.com/auth/v1/sso/saml/metadata)
   3. Leave `Start URL` and the `Signed response` box empty.
   4. Set `Name ID` format to `EMAIL` and leave `Name ID` as the default (`Basic Information > Primary email`).
   5. Click `Continue`.

7. Use `Add mapping` to ensure required claims are present:
   1. `Basic Information > Primary email` -> `email`

**Step 2: Set up LangSmith SSO Configuration**

Follow the instructions under [initial configuration](#initial-configuration) in the `Fill in required information` step, using the `IDP metadata` from the previous step as the metadata XML.

**Step 3: Turn on the SAML app in Google**

1. Select the SAML app under `Menu -> Apps -> Web and mobile apps`

2. Click `User access`.

3. Turn on the service:

1. To turn the service on for everyone in your organization, click `On for everyone`, and then click `Save`.

2. To turn the service on for an organizational unit:

1. At the left, select the organizational unit then `On`.
      2. If the Service status is set to `Inherited` and you want to keep the updated setting, even if the parent setting changes, click `Override`.
      3. If the Service status is set to `Overridden`, either click `Inherit` to revert to the same setting as its parent, or click `Save` to keep the new setting, even if the parent setting changes.

3. To turn on a service for a set of users across or within organizational units, select an access group. For details, go to [Use groups to customize service access](https://support.google.com/a/answer/9050643).

4. Ensure that the email addresses your users use to sign in to LangSmith match the email addresses they use to sign in to your Google domain.

**Step 4: Verify the SSO setup**

Have a user with access sign in via the unique login URL from the **SSO Configuration** page, or go to the SAML application page in Google and click **TEST SAML LOGIN**.

#### Supported features

* IdP-initiated SSO (Single Sign-On)
* SP-initiated SSO
* Just-In-Time provisioning
* Enforce SSO only

#### Configuration steps

For additional information, see Okta's [documentation](https://help.okta.com/en-us/content/topics/apps/apps_app_integration_wizard_saml.htm).

**Step 1: Create and configure the Okta SAML application**

<div id="via-okta-integration-network">
  <b>Via Okta Integration Network (recommended)</b>
</div>

1. Sign in to [Okta](https://login.okta.com/).
2. In the upper-right corner, select Admin. The button is not visible from the Admin area.
3. Select `Browse App Integration Catalog`.
4. Find and select the LangSmith application.
5. On the application overview page, select Add Integration.
6. Leave `ApiUrlBase` empty.
7. Fill in `AuthHost`:
   * US: `auth.langchain.com`
   * EU: `eu.auth.langchain.com`
8. (Optional, if planning to use [SCIM](#set-up-scim-for-your-organization) as well) Fill in `LangSmithUrl`:
   * US: `api.smith.langchain.com`
   * EU: `eu.api.smith.langchain.com`
9. Under Application Visibility, keep the box unchecked.
10. Select Next.
11. Select `SAML 2.0`.
12. Fill in `Sign-On Options`:
    * `Application username format`: `Email`
    * `Update application username on`: `Create and update`
    * `Allow users to securely see their password`: leave **unchecked**.
13. Copy the **Metadata URL** from the **Sign On Options** page to use in the next step.

**Via Custom App Integration**

<Warning>
  SCIM is not compatible with this method of configuration. Refer to [**Via Okta Integration Network**](#via-okta-integration-network).
</Warning>

1. Log in to Okta as an administrator, and go to the **Okta Admin console**.

2. Under **Applications** > **Applications** click **Create App Integration**.

3. Select **SAML 2.0**.

4. Enter an `App name` (e.g., `LangSmith`) and optionally an **App logo**, then click **Next**.

5. Enter the following information in the **Configure SAML** page:

1. `Single sign-on URL` (`ACS URL`). Keep `Use this for Recipient URL and Destination URL` checked:
      * US: [https://auth.langchain.com/auth/v1/sso/saml/acs](https://auth.langchain.com/auth/v1/sso/saml/acs)
      * EU: [https://eu.auth.langchain.com/auth/v1/sso/saml/acs](https://eu.auth.langchain.com/auth/v1/sso/saml/acs)
   2. `Audience URI (SP Entity ID)`:
      * US: [https://auth.langchain.com/auth/v1/sso/saml/metadata](https://auth.langchain.com/auth/v1/sso/saml/metadata)
      * EU: [https://eu.auth.langchain.com/auth/v1/sso/saml/metadata](https://eu.auth.langchain.com/auth/v1/sso/saml/metadata)
   3. `Name ID format`: **Persistent**.
   4. `Application username`: `email`.
   5. Leave the rest of the fields empty or set to their default.
   6. Click **Next**.

7. Copy the **Metadata URL** from the **Sign On** page to use in the next step.

**Step 2: Set up LangSmith SSO Configuration**

Follow the instructions under [initial configuration](#initial-configuration) in the **Fill in required information** step, using the metadata URL from the previous step.

**Step 3: Assign users to LangSmith in Okta**

1. Under **Applications** > **Applications**, select the SAML application created in Step 1.
2. Under the **Assignments** tab, click **Assign** then either **Assign to People** or **Assign to Groups**.
3. Make the desired selection(s), then **Assign** and **Done**.

**Step 4: Verify the SSO setup**

Have a user with access sign in via the unique login URL from the `SSO Configuration` page, or have a user select the application from their Okta dashboard.

#### SP-initiated SSO

Once service-provider–initiated SSO is configured, users can sign in using a unique login URL. You can find this in the LangSmith UI under **Organization members and roles** then **SSO configuration**.

## Set up SCIM for your organization

System for Cross-domain Identity Management (SCIM) is an open standard that allows for the automation of user provisioning. Using SCIM, you can automatically provision and de-provision users in your LangSmith [organization and workspaces](/langsmith/administration-overview), keeping user access synchronized with your organization's identity provider.

<Note>
  SCIM is available for organizations on the [Enterprise plan](https://www.langchain.com/pricing). [Contact sales](https://www.langchain.com/contact-sales) to learn more.

SCIM is available on Helm chart versions 0.10.41 (application version 0.10.108) and later.

SCIM support is API-only (see instructions below).
</Note>

SCIM eliminates the need for manual user management and ensures that user access is always up-to-date with your organization's identity system. This allows for:

* **Automated user management**: Users are automatically added, updated, and removed from LangSmith based on their status in your IdP.
* **Reduced administrative overhead**: No need to manage user access manually across multiple systems.
* **Improved security**: Users who leave your organization are automatically deprovisioned from LangSmith.
* **Consistent access control**: User attributes and group memberships are synchronized between systems.
* **Scaling team access control**: Efficiently manage large teams with many workspaces and custom roles.
* **Role assignment**: Select specific [Organization Roles](/langsmith/observability-concepts#organization-roles) and [Workspace Roles](/langsmith/observability-concepts#workspace-roles) for groups of users.

* Your organization must be on an Enterprise plan.
* Your Identity Provider (IdP) must support SCIM 2.0.
* Only [Organization Admins](/langsmith/administration-overview#organization-roles) can configure SCIM.
* For cloud customers: [SAML SSO](#set-up-saml-sso-for-your-organization) must be configurable for your organization.
* For self-hosted customers: [OAuth with Client Secret](/langsmith/self-host-sso#with-secret) authentication mode must be enabled.
* For self-hosted customers, network traffic must be allowed from the identity provider to LangSmith:
  * Microsoft Entra ID supports allowlisting IP ranges or an agent-based solution to provide connectivity.
    ([details](https://learn.microsoft.com/en-us/entra/identity/app-provisioning/use-scim-to-provision-users-and-groups#ip-ranges)).
  * Okta supports allow-listing IPs or domains ([details](https://help.okta.com/en-us/content/topics/security/ip-address-allow-listing.htm))
    or an agent-based solution ([details](https://help.okta.com/en-us/content/topics/provisioning/opp/opp-main.htm)) to provide connectivity.

When a user belongs to multiple groups for the same workspace, the following precedence applies:

1. **Organization Admin groups** take highest precedence. Users in these groups will be `Admin` in all workspaces.
2. **Most recently created workspace-specific group** takes precedence over other workspace groups.

<Note>
  When a group is deleted or a user is removed from a group, their access is updated according to their remaining group membership, following the precedence rules.

SCIM group membership will override manually assigned roles or roles assigned via Just-in-time (JIT) provisioning. We recommend disabling JIT provisioning to avoid conflicts.
</Note>

#### Email verification

In cloud only, creating a new user with SCIM triggers an email to the user.
They must verify their email address by clicking the link in this email.
The link expires in 24 hours, and can be resent if needed by removing and re-adding the user via SCIM.

### Attributes and Mapping

#### Group Naming Convention

<Warning>
  Renaming groups is **not** supported via SCIM. Group names are persistent because they must match role names and/or workspace names in LangSmith.
</Warning>

Group membership maps to LangSmith workspace membership and workspace roles with a specific naming convention:

**Organization Admin Groups**

Format: `<optional_prefix>Organization Admin` or `<optional_prefix>Organization Admins`

* `LS:Organization Admins`
* `Groups-Organization Admins`
* `Organization Admin`

**Workspace-Specific Groups**

Format: `<optional_prefix><org_role_name>:<workspace_name>:<workspace_role_name>`

* `LS:Organization User:Production:Annotators`
* `Groups-Organization User:Engineering:Developers`
* `Organization User:Marketing:Viewers`

While specific instructions depending on the identity provider may vary, these mappings show what is supported by the LangSmith SCIM integration:

| **LangSmith App Attribute**    | **Identity Provider Attribute**                       | **Matching Precedence** |
| ------------------------------ | ----------------------------------------------------- | ----------------------- |
| `userName`<sup>1</sup>         | email address                                         |                         |
| `active`                       | `!deactivated`                                        |                         |
| `emails[type eq "work"].value` | email address<sup>2</sup>                             |                         |
| `name.formatted`               | `displayName` OR `givenName + familyName`<sup>3</sup> |                         |
| `givenName`                    | `givenName`                                           |                         |
| `familyName`                   | `familyName`                                          |                         |
| `externalId`                   | `sub`<sup>4</sup>                                     | 1                       |

1. `userName` is not required by LangSmith
2. Email address is required
3. Use the computed expression if your `displayName` does not match the format of `Firstname Lastname`
4. To avoid inconsistency, this should match the SAML `NameID` assertion for cloud customers, or the `sub` OAuth2.0 claim for self-hosted.

#### Group Attributes

| **LangSmith App Attribute** | **Identity Provider Attribute** | **Matching Precedence** |
| --------------------------- | ------------------------------- | ----------------------- |
| `displayName`               | `displayName`<sup>1</sup>       | 1                       |
| `externalId`                | `objectId`                      |                         |
| `members`                   | `members`                       |                         |

1. Groups must follow the naming convention described in the [Group Naming Convention](#group-naming-convention) section.
   If your company has a group naming policy, you should instead map from the `description` identity provider attribute and
   set the description based on the [Group Naming Convention](#group-naming-convention) section.

### Step 1 - Configure SAML SSO (Cloud only)

There are two scenarios for [SAML SSO](#set-up-saml-sso-for-your-organization) configuration:

1. If SAML SSO is already configured for your organization, you should skip the steps to initially add the application ([Add application from Okta Integration Network](#add-application-okta-oin) or [Create a new Entra ID application integration](#create-application-entra-id)), as you already have an application configured and just need to enable provisioning.
2. If you are configuring SAML SSO for the first time alongside SCIM, first follow the instructions to [set up SAML SSO](#set-up-saml-sso-for-your-organization), *then* follow the instructions here to enable SCIM.

LangSmith uses the SAML NameID to identify users. The NameID is a required field in the SAML response and is case-insensitive.

1. Be unique to each user.
2. Be a persistent value that never changes, such as a randomly generated unique user ID.
3. Match exactly on each sign-in attempt. It should not rely on user input.

The NameID should not be an email address or username because email addresses and usernames are more likely to change over time and can be case-sensitive.

The NameID format must be `Persistent`, unless you are using a field, like email, that requires a different format.

### Step 2 - Disable JIT provisioning

Before enabling SCIM, disable [Just-in-time (JIT) provisioning](/langsmith/user-management#just-in-time-jit-provisioning) to prevent conflicts between automatic and manual user provisioning.

#### Disabling JIT for Cloud

Use the `PATCH /orgs/current/info` [endpoint](https://api.smith.langchain.com/redoc#tag/orgs/operation/update_current_organization_info_api_v1_orgs_current_info_patch):

#### Disabling JIT for Self-Hosted

As of LangSmith chart version **0.11.14**, you can disable JIT provisioning for your self-hosted organization using SSO. To disable, set the following values:

### Step 3 - Generate SCIM bearer token

<Note>
  In self-hosted environments, the full URL below may look like `https://langsmith.yourdomain.com/api/v1/platform/orgs/current/scim/tokens` (without a subdomain, note the `/api/v1` path prefix) or `https://langsmith.yourdomain.com/subdomain/api/v1/platform/orgs/current/scim/tokens` (with a subdomain) - see the [ingress docs](/langsmith/self-host-ingress) for more details.
</Note>

Generate a SCIM Bearer Token for your organization. This token will be used by your IdP to authenticate SCIM API requests. Ensure env vars are set appropriately, for example:

Note that the SCIM Bearer Token value is not available outside of the response to this request. These additional endpoints are present:

* `GET /v1/platform/orgs/current/scim/tokens`
* `GET /v1/platform/orgs/current/scim/tokens/{scim_token_id}`
* `PATCH /v1/platform/orgs/current/scim/tokens/{scim_token_id}` (only the `description` field is supported)
* `DELETE /v1/platform/orgs/current/scim/tokens/{scim_token_id}`

### Step 4 - Configure your Identity Provider

<Note>
  If you use Azure Entra ID (formerly Azure AD) or Okta, there are specific instructions for identity provider setup (refer to [Azure Entra ID](#azure-entra-id-configuration-steps), [Okta](#okta)). The requirements and steps above are applicable for all identity providers.
</Note>

#### Azure Entra ID configuration steps

For additional information, see Microsoft's [documentation](https://learn.microsoft.com/en-us/entra/identity/app-provisioning/user-provisioning).

<Note>
  In self-hosted installations, the `oid` JWT claim is used as the `sub`.
  See [this Microsoft Learn link](https://learn.microsoft.com/en-us/answers/questions/5546297/how-to-link-oidc-users-with-scim)
  and [the related configuration instructions](/langsmith/self-host-sso#override-sub-claim) for additional details.
</Note>

**Step 1: Configure SCIM in your Enterprise Application**

1. Log in to the [Azure portal](https://portal.azure.com/#home) with a privileged role (e.g., `Global Administrator`).
2. Navigate to your existing LangSmith Enterprise Application.
3. In the left-side navigation, select **Manage** > **Provisioning**.
4. Click **Get started**.

**Step 2: Configure Admin credentials**

1. Under **Admin Credentials**:

* US: `https://api.smith.langchain.com/scim/v2`
     * EU: `https://eu.api.smith.langchain.com/scim/v2`
     * Self-hosted: `<langsmith_url>/scim/v2`

* **Secret Token**: Enter the SCIM Bearer Token generated in Step 3.

2. Click **Test Connection** to verify the configuration.

**Step 3: Configure Attribute Mappings**

Configure the following attribute mappings under `Mappings`:

Set **Target Object Actions** to `Create` and `Update` (start with `Delete` disabled for safety):

|   **LangSmith App Attribute**  |            **Microsoft Entra ID Attribute**           | **Matching Precedence** |
| :----------------------------: | :---------------------------------------------------: | :---------------------: |
|           `userName`           |                  `userPrincipalName`                  |                         |
|            `active`            |                 `Not([IsSoftDeleted])`                |                         |
| `emails[type eq "work"].value` |                        `mail`1                        |                         |
|        `name.formatted`        | `displayName` OR `Join(" ", [givenName], [surname])`2 |                         |
|          `externalId`          |                      `objectId`3                      |            1            |

1. User's email address must be present in Entra ID.
2. Use the `Join` expression if your `displayName` does not match the format of `Firstname Lastname`.
3. To avoid inconsistency, this should match the SAML NameID assertion and the `sub` OAuth2.0 claim. For SAML SSO in cloud, the `Unique User Identifier (Name ID)` required claim should be `user.objectID` and the `Name identifier format` should be `persistent`.

Set **Target Object Actions** to `Create` and `Update` only (start with `Delete` disabled for safety):

| **LangSmith App Attribute** | **Microsoft Entra ID Attribute** | **Matching Precedence** |
| :-------------------------: | :------------------------------: | :---------------------: |
|        `displayName`        |          `displayName`1          |            1            |
|         `externalId`        |            `objectId`            |                         |
|          `members`          |             `members`            |                         |

1. Groups must follow the naming convention described in the [Group Naming Convention](#group-naming-convention) section.
   If your company has a group naming policy, you should instead map from the `description` Microsoft Entra ID Attribute and
   set the description based on the [Group Naming Convention](#group-naming-convention) section.

**Step 4: Assign Users and Groups**

1. Under **Applications** > **Applications**, select your LangSmith Enterprise Application.
2. Under the **Assignments** tab, click **Assign** then either **Assign to People** or **Assign to Groups**.
3. Make the desired selection(s), then **Assign** and **Done**.

**Step 5: Enable Provisioning**

1. Set **Provisioning Status** to `On` under **Provisioning**.
2. Monitor the initial sync to ensure users and groups are provisioned correctly.
3. Once verified, enable `Delete` actions for both User and Group mappings.

For troubleshooting, refer to the [SAML SSO FAQs](/langsmith/faq#saml-sso-faqs). If you have issues setting up SCIM, reach out to the LangChain support team at [support@langchain.dev](mailto:support@langchain.dev).

#### Okta configuration steps

<Note>
  You must use the [Okta Lifecycle Management](https://www.okta.com/products/lifecycle-management/) product. This product tier is required to use SCIM on Okta.
</Note>

<div id="supported-features">
  <b>Supported features</b>
</div>

* Create users
* Update user attributes
* Deactivate users
* Group push (**without group renaming**)
* Import users
* Import groups

<div id="add-application-okta-oin">
  <b>Step 1: Add application from Okta Integration Network</b>
</div>

<Note>
  If you have already configured SSO login via SAML (cloud) or OAuth2.0 with OIDC (self-hosted), skip this step.
</Note>

See [SAML SSO setup](#okta) for cloud or [OAuth2.0 setup](/langsmith/self-host-sso#okta-idp-setup) for self-hosted.

**Step 2: Configure API Integration**

1. In the General tab, ensure the `LangSmithUrl` is filled in according to the instructions from [Step 1](#add-application-okta-oin)
2. In the Provisioning tab, select `Integration`.
3. Select `Edit` then `Enable API integration`.
4. For API Token, paste the SCIM token you [generated above](#step-3-generate-scim-bearer-token).
5. Keep `Import Groups` checked.
6. To verify the configuration, select Test API Credentials.
7. Select Save.
8. After saving the API integration details, new settings tabs appear on the left. Select `To App`.
9. Select Edit.
10. Select the Enable checkbox for Create Users, Update Users, and Deactivate Users.
11. Select Save.
12. Assign users and/or groups in the Assignments tab. Assigned users are created and managed in your LangSmith group.

**Step 3: Configure User Provisioning Settings**

1. Configure provisioning: under `Provisioning > To App > Provisioning to App`, click `Edit`, then check `Create Users`, `Update User Attributes`, and `Deactivate Users`.
2. Under `<application_name> Attribute Mappings`, set the user attribute mappings as shown below, and delete the rest:

<img src="https://mintcdn.com/langchain-5e9cc07a/Fd2sBZqE3DLR7pWt/langsmith/images/scim_okta_user_attributes.png?fit=max&auto=format&n=Fd2sBZqE3DLR7pWt&q=85&s=4951533e29e0f0f75e7aac74dcfab3bb" alt="SCIM Okta User Attributes Mapping" data-og-width="748" width="748" data-og-height="467" height="467" data-path="langsmith/images/scim_okta_user_attributes.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fd2sBZqE3DLR7pWt/langsmith/images/scim_okta_user_attributes.png?w=280&fit=max&auto=format&n=Fd2sBZqE3DLR7pWt&q=85&s=f883ed0bf6e0876126e5a7f79dadfbb1 280w, https://mintcdn.com/langchain-5e9cc07a/Fd2sBZqE3DLR7pWt/langsmith/images/scim_okta_user_attributes.png?w=560&fit=max&auto=format&n=Fd2sBZqE3DLR7pWt&q=85&s=0dcd59e75cc4e7b5532dfc9ad81347fa 560w, https://mintcdn.com/langchain-5e9cc07a/Fd2sBZqE3DLR7pWt/langsmith/images/scim_okta_user_attributes.png?w=840&fit=max&auto=format&n=Fd2sBZqE3DLR7pWt&q=85&s=3c9ee64fb3d1357bb67685ca85ada751 840w, https://mintcdn.com/langchain-5e9cc07a/Fd2sBZqE3DLR7pWt/langsmith/images/scim_okta_user_attributes.png?w=1100&fit=max&auto=format&n=Fd2sBZqE3DLR7pWt&q=85&s=fb7274973d558c7973c6c00195d77341 1100w, https://mintcdn.com/langchain-5e9cc07a/Fd2sBZqE3DLR7pWt/langsmith/images/scim_okta_user_attributes.png?w=1650&fit=max&auto=format&n=Fd2sBZqE3DLR7pWt&q=85&s=7830d972b9ad3cb036d8c08623bc891b 1650w, https://mintcdn.com/langchain-5e9cc07a/Fd2sBZqE3DLR7pWt/langsmith/images/scim_okta_user_attributes.png?w=2500&fit=max&auto=format&n=Fd2sBZqE3DLR7pWt&q=85&s=f1e137b9faa8d6422cff5e3673cf98fb 2500w" />

**Step 4: Push Groups**

<Note>
  Okta does not support group attributes besides the group name itself, so group name *must* follow the naming convention described in the [Group Naming Convention](#group-naming-convention) section.
</Note>

Follow Okta's [Enable Group Push](https://help.okta.com/en-us/content/topics/users-groups-profiles/usgp-enable-group-push.htm) instructions to configure groups to push by name or by rule.

#### Other Identity Providers

Other identity providers have not been tested but may function depending on their SCIM implementation.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/user-management.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
#### Disabling JIT for Self-Hosted

As of LangSmith chart version **0.11.14**, you can disable JIT provisioning for your self-hosted organization using SSO. To disable, set the following values:
```

Example 2 (unknown):
```unknown
### Step 3 - Generate SCIM bearer token

<Note>
  In self-hosted environments, the full URL below may look like `https://langsmith.yourdomain.com/api/v1/platform/orgs/current/scim/tokens` (without a subdomain, note the `/api/v1` path prefix) or `https://langsmith.yourdomain.com/subdomain/api/v1/platform/orgs/current/scim/tokens` (with a subdomain) - see the [ingress docs](/langsmith/self-host-ingress) for more details.
</Note>

Generate a SCIM Bearer Token for your organization. This token will be used by your IdP to authenticate SCIM API requests. Ensure env vars are set appropriately, for example:
```

---

## Use annotation queues

**URL:** llms-txt#use-annotation-queues

**Contents:**
- Create an annotation queue
  - Basic Details
  - Annotation Rubric
  - Collaborator Settings
- Assign runs to an annotation queue
- Review runs in an annotation queue
- Video guide

Source: https://docs.langchain.com/langsmith/annotation-queues

*Annotation queues* provide a streamlined, directed view for human annotators to attach feedback to specific [runs](/langsmith/observability-concepts#runs). While you can always annotate [traces](/langsmith/observability-concepts#traces) inline, annotation queues provide another option to group runs together, then have annotators review and provide [feedback](/langsmith/observability-concepts#feedback) on them.

## Create an annotation queue

To create an annotation queue:

1. Navigate to the **Annotation queues** section on the left-hand navigation panel of the [LangSmith UI](https://smith.langchain.com).
2. Click **+ New annotation queue** in the top right corner.

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-annotation-queue-new.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=c5c28c10a5522af0a37f40236ed57510" alt="Create Annotation Queue form with Basic Details, Annotation Rubric, and Feedback sections." data-og-width="3456" width="3456" data-og-height="1912" height="1912" data-path="langsmith/images/create-annotation-queue-new.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-annotation-queue-new.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=daa5c44976804eae5ca8bbfef1d0a9d0 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-annotation-queue-new.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=167955e0202671425e6cd1476c31a756 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-annotation-queue-new.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=71627eeab271c6d4581f00506731cc09 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-annotation-queue-new.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=cd30341efa9d5eea82d85b63518b53a0 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-annotation-queue-new.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=3e71d31b42b3411946f73d79e8735599 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-annotation-queue-new.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=832e7c8b99d332176bc9d9de702a6bac 2500w" />

1. Fill in the form with the **Name** and **Description** of the queue. You can also assign a **default dataset** to queue, which will streamline the process of sending the inputs and outputs of certain runs to datasets in your LangSmith [workspace](/langsmith/administration-overview#workspaces).

### Annotation Rubric

1. Draft some high-level instructions for your annotators, which will be shown in the sidebar on every run.
2. Click **+ Desired Feedback** to add feedback keys to your annotation queue. Annotators will be presented with these feedback keys on each run.
3. Add a description for each, as well as a short description of each category, if the feedback is categorical.

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-annotation-rubric.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=8adfdba2649847f82543674978b0d1b1" alt="Annotation queue rubric form with instructions and desired feedback entered." data-og-width="3456" width="3456" data-og-height="1914" height="1914" data-path="langsmith/images/create-annotation-rubric.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-annotation-rubric.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=5d73a7688b61b3b9489aacac1223f7c6 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-annotation-rubric.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=ede9b22be4e3ce82e4feabf86575e8a5 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-annotation-rubric.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=768747aa9e314c66631f27e794d9174b 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-annotation-rubric.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=f3061b6ba68c4d9cab997bbed2efe76e 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-annotation-rubric.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=27545c5b64b3b82ae9ebed853ff02168 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-annotation-rubric.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=801397056da06004808b6c38df30c139 2500w" />

For example, with the descriptions in the previous screenshot, reviewers will see the **Annotation Rubric** details in the right-hand pane of the UI.

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rubric-for-annotators.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=44452f7da89329acc06672beba4e4c0e" alt="The rendered rubric for reviewers from the example instructions." data-og-width="3456" width="3456" data-og-height="1912" height="1912" data-path="langsmith/images/rubric-for-annotators.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rubric-for-annotators.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=fa18a86229854c27a85c341da2638501 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rubric-for-annotators.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=67f5880b9d79e05e2dcd3703db92f79a 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rubric-for-annotators.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=01436264374f87f9baab4ac4f3f8c161 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rubric-for-annotators.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=80b8569a8ef3acc5b7d94a9276dc0d26 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rubric-for-annotators.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=b4c6f43fe2e91f57de4a2790757a2083 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rubric-for-annotators.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=aae4a68208b5b6838c3fdf77f2c57efe 2500w" />

### Collaborator Settings

When there are multiple annotators for a run:

* **Number of reviewers per run**: This determines the number of reviewers that must mark a run as **Done** for it to be removed from the queue. If you check **All workspace members review each run**, then a run will remain in the queue until all [workspace](/langsmith/administration-overview#workspaces) members have marked their review as **Done**.

* Reviewers cannot view the feedback left by other reviewers.
  * Comments on runs are visible to all reviewers.

* **Enable reservations on runs**: When a reviewer views a run, the run is reserved for that reviewer for the specified **Reservation length**. If there are multiple reviewers per run as specified above, the run can be reserved by multiple reviewers (up to the number of reviewers per run) at the same time.

<Tip>
    We recommend enabling reservations. This will prevent multiple annotators from reviewing the same run at the same time.
  </Tip>

If a reviewer has viewed a run and then leaves the run without marking it **Done**, the reservation will expire after the specified **Reservation length**. The run is then released back into the queue and can be reserved by another reviewer.

<Note>
    Clicking **Requeue** for a run's annotation will only move the current run to the end of the current user's queue; it won't affect the queue order of any other user. It will also release the reservation that the current user has on that run.
  </Note>

As a result of the **Collaborator settings**, it's possible (and likely) that the number of runs visible to an individual in an annotation queue differs from the total number of runs in the queue compared to another user's queue size.

You can update these settings at any time by clicking on the pencil icon <Icon icon="pencil" /> in the **Annotation Queues** section.

## Assign runs to an annotation queue

To assign runs to an annotation queue, do one of the following:

* Click on **Add to Annotation Queue** in top right corner of any [trace](/langsmith/observability-concepts#traces) view. You can add any intermediate [run](/langsmith/observability-concepts#runs) (span) of the trace to an annotation queue, but not the root span.

<img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-to-annotation-queue.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=fc604c7f91bc8795dc688c4f9db73ce9" alt="Trace view with the Add to Annotation Queue button highglighted at the top of the screen." data-og-width="1373" width="1373" data-og-height="1028" height="1028" data-path="langsmith/images/add-to-annotation-queue.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-to-annotation-queue.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=0ff1545d09984dfb766067ad65ecbfb9 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-to-annotation-queue.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=055554579c01cc8c48471e0d74fe27d6 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-to-annotation-queue.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=10412c26d4042358e098631386cddbf2 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-to-annotation-queue.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=be2c6162599ab18ef34975a439f16e93 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-to-annotation-queue.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=4e0b51a16246b1e28d415f23d86643c7 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-to-annotation-queue.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=c7744f75389cd270f60bbbee9571582a 2500w" />

* Select multiple runs in the runs table then click **Add to Annotation Queue** at the bottom of the page.

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-select-annotation-queue.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=c6781e6a7345ef7e16ea7a0bb306a474" alt="View of the runs table with runs selected. Add to Annotation Queue button at the botton of the page." data-og-width="1323" width="1323" data-og-height="1317" height="1317" data-path="langsmith/images/multi-select-annotation-queue.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-select-annotation-queue.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=03fff2a1f8cc40bf86f4b9251dacc0e1 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-select-annotation-queue.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=2cc50c1d4f1b9ec24f9e3bc4d5fabbff 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-select-annotation-queue.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=b37ce7b181457582652a22405f240750 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-select-annotation-queue.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=badc9ef126c220b9aa8f5e8212421b93 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-select-annotation-queue.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=ae02333d194a955c1a6c86a2bf87c75f 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-select-annotation-queue.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=9dce63a0eedfc4a02a5ad822cee67bce 2500w" />

* [Set up an automation rule](/langsmith/rules) that automatically assigns runs that pass a certain filter and sampling condition to an annotation queue.

* Navigate to the **Datasets & Experiments** page and select a dataset. On the dataset's page select one or multiple [experiments](/langsmith/evaluation-concepts#experiment). At the bottom of the page, click **<Icon icon="pencil" /> Annotate**. From the resulting popup, you can either create a new queue or add the runs to an existing one.

<img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/annotate-experiment.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=7622e6db855711542de24270ddc129dc" alt="Selected experiments with the Annotate button at the bottom of the page." data-og-width="3456" width="3456" data-og-height="1914" height="1914" data-path="langsmith/images/annotate-experiment.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/annotate-experiment.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=6bc0abf70504c439b413dfb6a3ff59f7 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/annotate-experiment.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=c511826c68b2ecf9fb75addc50df48c3 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/annotate-experiment.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=450345b069f8de91a982d66bfe6ce8a9 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/annotate-experiment.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=fb67c24cd3f6fee0042ad3ef94fc9a59 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/annotate-experiment.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=83aab2b2cd3cc7a84440caa0d57a68be 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/annotate-experiment.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=08a045816ca42285ad46570280ee7061 2500w" />

<Check>
  It is often a good idea to assign runs that have a particular type of user feedback score (e.g., thumbs up, thumbs down) from the application to an annotation queue. This way, you can identify and address issues that are causing user dissatisfaction. To learn more about how to capture user feedback from your LLM application, follow the guide on [attaching user feedback](/langsmith/attach-user-feedback).
</Check>

## Review runs in an annotation queue

To review runs in an annotation queue:

1. Navigate to the **Annotation Queues** section through the left-hand navigation bar.
2. Click on the queue you want to review. This will take you to a focused, cyclical view of the runs in the queue that require review.
3. You can attach a comment, attach a score for a particular [feedback](/langsmith/observability-concepts#feedback) criteria, add the run to a dataset or mark the run as reviewed. You can also remove the run from the queue for all users, despite any current reservations or settings for the queue, by clicking the **Trash** icon <Icon icon="trash" /> next to **View run**.

<Tip>
     The keyboard shortcuts that are next to each option can help streamline the review process.
   </Tip>

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/review-runs.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=9065d4b85e6165084b65d3908d61778a" alt="View or a run with the Annotate side panel. Keyboard shortcuts visible for options." data-og-width="1532" width="1532" data-og-height="1080" height="1080" data-path="langsmith/images/review-runs.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/review-runs.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=f69f94916e247ff498e1d9e5ed2755a2 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/review-runs.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=64f16b8b83ffc56d1ad078ae1bfacd65 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/review-runs.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=085b9e1ea9b3797bc10117c326a80018 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/review-runs.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=4321324e5d276d12864e446cd2a97c82 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/review-runs.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=7eeb685e4d4064518613283e502c9395 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/review-runs.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=059f57fb238aa0cd1ebcf6a2fb0ede95 2500w" />

<iframe className="w-full aspect-video rounded-xl" src="https://www.youtube.com/embed/rxKYHA-2KS0?si=V4EnrUmzJaUVJh0m" title="YouTube video player" frameBorder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowFullScreen />

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/annotation-queues.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Use an existing secret for your installation (Kubernetes)

**URL:** llms-txt#use-an-existing-secret-for-your-installation-(kubernetes)

**Contents:**
- Requirements
- Parameters
- Configuration

Source: https://docs.langchain.com/langsmith/self-host-using-an-existing-secret

By default, LangSmith will provision several Kubernetes secrets to store sensitive information such as license keys, salts, and other configuration parameters. However, you may want to use an existing secret that you have already created in your Kubernetes cluster (or provisioned via some sort of secrets operator). This can be useful if you want to manage sensitive information in a centralized way or if you have specific security requirements.

By default we will provision the following secrets corresponding to different components of LangSmith:

* `langsmith-secrets`: This secret contains the license key and some other basic configuration parameters. You can see the template for this secret [here](https://github.com/langchain-ai/helm/blob/main/charts/langsmith/templates/secrets.yaml)
* `langsmith-redis`: This secret contains the Redis connection string and password. You can see the template for this secret [here](https://github.com/langchain-ai/helm/blob/main/charts/langsmith/templates/redis/secrets.yaml)
* `langsmith-postgres`: This secret contains the Postgres connection string and password. You can see the template for this secret [here](https://github.com/langchain-ai/helm/blob/main/charts/langsmith/templates/postgres/secrets.yaml)
* `langsmith-clickhouse`: This secret contains the ClickHouse connection string and password. You can see the template for this secret [here](https://github.com/langchain-ai/helm/blob/main/charts/langsmith/templates/clickhouse/secrets.yaml)

* An existing Kubernetes cluster
* A way to create Kubernetes secrets in your cluster. This can be done using `kubectl`, a Helm chart, or a secrets operator like [Sealed Secrets](https://github.com/bitnami-labs/sealed-secrets)

You will need to create your own Kubernetes secrets that adhere to the structure of the secrets provisioned by the LangSmith Helm Chart.

<Warning>
  The secrets must have the same structure as the ones provisioned by the LangSmith Helm Chart (refer to the links above to see the specific secrets). If you miss any of the required keys, your LangSmith instance may not work correctly.
</Warning>

An example secret may look like this:

With these secrets provisioned, you can configure your LangSmith instance to use the secrets directly to avoid passing in secret values through plaintext. You can do this by modifying the `langsmith_config.yaml` file for your LangSmith Helm Chart installation.

Once configured, you will need to update your LangSmith installation. You can follow our upgrade guide [here](/langsmith/self-host-upgrades). If everything is configured correctly, your LangSmith instance should now be accessible via the Ingress. You can run the following to check that your secrets are being used correctly:

You should see something like this in the output:

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/self-host-using-an-existing-secret.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
## Configuration

With these secrets provisioned, you can configure your LangSmith instance to use the secrets directly to avoid passing in secret values through plaintext. You can do this by modifying the `langsmith_config.yaml` file for your LangSmith Helm Chart installation.
```

Example 2 (unknown):
```unknown
Once configured, you will need to update your LangSmith installation. You can follow our upgrade guide [here](/langsmith/self-host-upgrades). If everything is configured correctly, your LangSmith instance should now be accessible via the Ingress. You can run the following to check that your secrets are being used correctly:
```

Example 3 (unknown):
```unknown
You should see something like this in the output:
```

---

## Use cron jobs

**URL:** llms-txt#use-cron-jobs

**Contents:**
- Setup
- Cron job on a thread
- Cron job stateless

Source: https://docs.langchain.com/langsmith/cron-jobs

There are many situations in which it is useful to run an assistant on a schedule.

For example, say that you're building an assistant that runs daily and sends an email summary
of the day's news. You could use a cron job to run the assistant every day at 8:00 PM.

LangSmith Deployment supports cron jobs, which run on a user-defined schedule. The user specifies a schedule, an assistant, and some input. After that, on the specified schedule, the server will:

* Create a new thread with the specified assistant
* Send the specified input to that thread

Note that this sends the same input to the thread every time.

The LangSmith Deployment API provides several endpoints for creating and managing cron jobs. See the [API reference](https://langchain-ai.github.io/langgraph/cloud/reference/api/api_ref/) for more details.

Sometimes you don't want to run your graph based on user interaction, but rather you would like to schedule your graph to run on a schedule - for example if you wish for your graph to compose and send out a weekly email of to-dos for your team. LangSmith Deployment allows you to do this without having to write your own script by using the `Crons` client. To schedule a graph job, you need to pass a [cron expression](https://crontab.cronhub.io/) to inform the client when you want to run the graph. `Cron` jobs are run in the background and do not interfere with normal invocations of the graph.

First, let's set up our SDK client, assistant, and thread:

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

## Cron job on a thread

To create a cron job associated with a specific thread, you can write:

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

Note that it is **very** important to delete `Cron` jobs that are no longer useful. Otherwise you could rack up unwanted API charges to the LLM! You can delete a `Cron` job using the following code:

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

## Cron job stateless

You can also create stateless cron jobs by using the following code:

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

Again, remember to delete your job once you are done with it!

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/cron-jobs.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
</Tab>

  <Tab title="Javascript">
```

Example 2 (unknown):
```unknown
</Tab>

  <Tab title="CURL">
```

Example 3 (unknown):
```unknown
</Tab>
</Tabs>

Output:
```

Example 4 (unknown):
```unknown
## Cron job on a thread

To create a cron job associated with a specific thread, you can write:

<Tabs>
  <Tab title="Python">
```

---

## Use decorators in agent

**URL:** llms-txt#use-decorators-in-agent

**Contents:**
  - Available decorators
  - When to use decorators
- Class-based middleware
  - Two hook styles
  - Custom state schema

agent = create_agent(
    model="gpt-4o",
    middleware=[log_before_model, validate_output, retry_model, personalized_prompt],
    tools=[...],
)
python  theme={null}
from langchain.agents.middleware import AgentMiddleware, AgentState
from langgraph.runtime import Runtime
from typing import Any

class LoggingMiddleware(AgentMiddleware):
    def before_model(self, state: AgentState, runtime: Runtime) -> dict[str, Any] | None:
        print(f"About to call model with {len(state['messages'])} messages")
        return None

def after_model(self, state: AgentState, runtime: Runtime) -> dict[str, Any] | None:
        print(f"Model returned: {state['messages'][-1].content}")
        return None
python  theme={null}
from langchain.agents.middleware import AgentMiddleware, AgentState
from langchain.messages import AIMessage
from langgraph.runtime import Runtime
from typing import Any

class MessageLimitMiddleware(AgentMiddleware):
    def __init__(self, max_messages: int = 50):
        super().__init__()
        self.max_messages = max_messages

def before_model(self, state: AgentState, runtime: Runtime) -> dict[str, Any] | None:
        if len(state["messages"]) == self.max_messages:
            return {
                "messages": [AIMessage("Conversation limit reached.")],
                "jump_to": "end"
            }
        return None
python  theme={null}
from langchain.agents.middleware import AgentMiddleware, ModelRequest, ModelResponse
from typing import Callable

class RetryMiddleware(AgentMiddleware):
    def __init__(self, max_retries: int = 3):
        super().__init__()
        self.max_retries = max_retries

def wrap_model_call(
        self,
        request: ModelRequest,
        handler: Callable[[ModelRequest], ModelResponse],
    ) -> ModelResponse:
        for attempt in range(self.max_retries):
            try:
                return handler(request)
            except Exception as e:
                if attempt == self.max_retries - 1:
                    raise
                print(f"Retry {attempt + 1}/{self.max_retries} after error: {e}")
python  theme={null}
from langchain.agents.middleware import AgentMiddleware, ModelRequest, ModelResponse
from langchain.chat_models import init_chat_model
from typing import Callable

class DynamicModelMiddleware(AgentMiddleware):
    def wrap_model_call(
        self,
        request: ModelRequest,
        handler: Callable[[ModelRequest], ModelResponse],
    ) -> ModelResponse:
        # Use different model based on conversation length
        if len(request.messages) > 10:
            request.model = init_chat_model("gpt-4o")
        else:
            request.model = init_chat_model("gpt-4o-mini")

return handler(request)
python  theme={null}
from langchain.tools.tool_node import ToolCallRequest
from langchain.agents.middleware import AgentMiddleware
from langchain_core.messages import ToolMessage
from langgraph.types import Command
from typing import Callable

class ToolMonitoringMiddleware(AgentMiddleware):
    def wrap_tool_call(
        self,
        request: ToolCallRequest,
        handler: Callable[[ToolCallRequest], ToolMessage | Command],
    ) -> ToolMessage | Command:
        print(f"Executing tool: {request.tool_call['name']}")
        print(f"Arguments: {request.tool_call['args']}")

try:
            result = handler(request)
            print(f"Tool completed successfully")
            return result
        except Exception as e:
            print(f"Tool failed: {e}")
            raise
python  theme={null}
from langchain.agents.middleware import AgentState, AgentMiddleware
from typing_extensions import NotRequired
from typing import Any

class CustomState(AgentState):
    model_call_count: NotRequired[int]
    user_id: NotRequired[str]

class CallCounterMiddleware(AgentMiddleware[CustomState]):
    state_schema = CustomState

def before_model(self, state: CustomState, runtime) -> dict[str, Any] | None:
        # Access custom state properties
        count = state.get("model_call_count", 0)

if count > 10:
            return {"jump_to": "end"}

def after_model(self, state: CustomState, runtime) -> dict[str, Any] | None:
        # Update custom state
        return {"model_call_count": state.get("model_call_count", 0) + 1}
python  theme={null}
agent = create_agent(
    model="gpt-4o",
    middleware=[CallCounterMiddleware()],
    tools=[...],
)

**Examples:**

Example 1 (unknown):
```unknown
### Available decorators

**Node-style** (run at specific execution points):

* `@before_agent` - Before agent starts (once per invocation)
* [`@before_model`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.before_model) - Before each model call
* [`@after_model`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.after_model) - After each model response
* `@after_agent` - After agent completes (once per invocation)

**Wrap-style** (intercept and control execution):

* [`@wrap_model_call`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.wrap_model_call) - Around each model call
* [`@wrap_tool_call`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.wrap_tool_call) - Around each tool call

**Convenience decorators**:

* [`@dynamic_prompt`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.dynamic_prompt) - Generates dynamic system prompts (equivalent to [`@wrap_model_call`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.wrap_model_call) that modifies the prompt)

### When to use decorators

<CardGroup cols={2}>
  <Card title="Use decorators when" icon="check">
    • You need a single hook<br />
    • No complex configuration
  </Card>

  <Card title="Use classes when" icon="code">
    • Multiple hooks needed<br />
    • Complex configuration<br />
    • Reuse across projects (config on init)
  </Card>
</CardGroup>

## Class-based middleware

### Two hook styles

<CardGroup cols={2}>
  <Card title="Node-style hooks" icon="diagram-project">
    Run sequentially at specific execution points. Use for logging, validation, and state updates.
  </Card>

  <Card title="Wrap-style hooks" icon="arrows-rotate">
    Intercept execution with full control over handler calls. Use for retries, caching, and transformation.
  </Card>
</CardGroup>

#### Node-style hooks

Run at specific points in the execution flow:

* `before_agent` - Before agent starts (once per invocation)
* `before_model` - Before each model call
* `after_model` - After each model response
* `after_agent` - After agent completes (up to once per invocation)

**Example: Logging middleware**
```

Example 2 (unknown):
```unknown
**Example: Conversation length limit**
```

Example 3 (unknown):
```unknown
#### Wrap-style hooks

Intercept execution and control when the handler is called:

* `wrap_model_call` - Around each model call
* `wrap_tool_call` - Around each tool call

You decide if the handler is called zero times (short-circuit), once (normal flow), or multiple times (retry logic).

**Example: Model retry middleware**
```

Example 4 (unknown):
```unknown
**Example: Dynamic model selection**
```

---

## Use different sampling rates for different operations

**URL:** llms-txt#use-different-sampling-rates-for-different-operations

with tracing_context(client=client_1):
    # Your code here - will be traced with 50% sampling rate
    agent_1.invoke(...)

with tracing_context(client=client_2):
    # Your code here - will be traced with 25% sampling rate
    agent_1.invoke(...)

with tracing_context(client=client_no_trace):
    # Your code here - will not be traced
    agent_1.invoke(...)
```

This allows you to control sampling rates at the operation level.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/sample-traces.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Use DuckDuckGo if you don't have a Tavily API key:

**URL:** llms-txt#use-duckduckgo-if-you-don't-have-a-tavily-api-key:

---

## Use environment variables for model providers

**URL:** llms-txt#use-environment-variables-for-model-providers

**Contents:**
- Requirements
- Configuration

Source: https://docs.langchain.com/langsmith/self-host-playground-environment-settings

<Note>
  This feature is only available on Helm chart versions 0.10.27 (application version 0.10.74) and later.
</Note>

Many model providers support setting credentials and other configuration options through environment variables. This is useful for self-hosted deployments where you want to avoid hardcoding sensitive information in your code or configuration files. In LangSmith, most model interactions are done through the `playground` service, which allows you to configure many of those environment variables directly on the pod itself. This can be useful to avoid having to set credentials in the UI.

* A self-hosted LangSmith instance with the `playground` service running.
* The provider you want to configure must support environment variables for configuration. Check the provider's Chat Model [documentation](https://python.langchain.com/docs/integrations/providers/) for more information.
* The secrets/roles you may want to attach to the `playground` service.
  * Note that for IRSA you may need to grant the `langsmith-playground` service account the necessary permissions to access the secrets or roles in your cloud provider.

With the parameters from above, you can configure your LangSmith instance to use environment variables for model providers. You can do this by modifying the `langsmith_config.yaml` file for your LangSmith Helm Chart installation or the `docker-compose.yaml` file for your Docker installation.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/self-host-playground-environment-settings.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

---

## Use instead

**URL:** llms-txt#use-instead

**Contents:**
  - Tools
  - Structured output
  - Streaming node name rename
  - Runtime context
- Standard content
  - What changed
  - Read standardized content
  - Create multimodal messages
  - Example block shapes

agent = create_agent("gpt-4o-mini", tools=[some_tool])
python v1 (new) theme={null}
  from langchain.agents import create_agent

agent = create_agent(
      model="claude-sonnet-4-5-20250929",
      tools=[check_weather, search_web]
  )
  python v0 (old) theme={null}
  from langgraph.prebuilt import create_react_agent, ToolNode

agent = create_react_agent(
      model="claude-sonnet-4-5-20250929",
      tools=ToolNode([check_weather, search_web]) # [!code highlight]
  )
  python v1 (new) theme={null}
  # Example coming soon
  python v0 (old) theme={null}
  # Example coming soon
  python v1 (new) theme={null}
  from langchain.agents import create_agent
  from langchain.agents.structured_output import ToolStrategy, ProviderStrategy
  from pydantic import BaseModel

class OutputSchema(BaseModel):
      summary: str
      sentiment: str

# Using ToolStrategy
  agent = create_agent(
      model="gpt-4o-mini",
      tools=tools,
      # explicitly using tool strategy
      response_format=ToolStrategy(OutputSchema)  # [!code highlight]
  )
  python v0 (old) theme={null}
  from langgraph.prebuilt import create_react_agent
  from pydantic import BaseModel

class OutputSchema(BaseModel):
      summary: str
      sentiment: str

agent = create_react_agent(
      model="gpt-4o-mini",
      tools=tools,
      # using tool strategy by default with no option for provider strategy
      response_format=OutputSchema  # [!code highlight]
  )

agent = create_react_agent(
      model="gpt-4o-mini",
      tools=tools,
      # using a custom prompt to instruct the model to generate the output schema
      response_format=("please generate ...", OutputSchema)  # [!code highlight]
  )
  python v1 (new) theme={null}
  from dataclasses import dataclass

from langchain.agents import create_agent

@dataclass
  class Context:
      user_id: str
      session_id: str

agent = create_agent(
      model=model,
      tools=tools,
      context_schema=Context  # [!code highlight]
  )

result = agent.invoke(
      {"messages": [{"role": "user", "content": "Hello"}]},
      context=Context(user_id="123", session_id="abc")  # [!code highlight]
  )
  python v0 (old) theme={null}
  from langgraph.prebuilt import create_react_agent

agent = create_react_agent(model, tools)

# Pass context via configurable
  result = agent.invoke(
      {"messages": [{"role": "user", "content": "Hello"}]},
      config={  # [!code highlight]
          "configurable": {  # [!code highlight]
              "user_id": "123",  # [!code highlight]
              "session_id": "abc"  # [!code highlight]
          }  # [!code highlight]
      }  # [!code highlight]
  )
  python v1 (new) theme={null}
  from langchain.chat_models import init_chat_model

model = init_chat_model("gpt-5-nano")
  response = model.invoke("Explain AI")

for block in response.content_blocks:
      if block["type"] == "reasoning":
          print(block.get("reasoning"))
      elif block["type"] == "text":
          print(block.get("text"))
  python v0 (old) theme={null}
  # Provider-native formats vary; you needed per-provider handling
  response = model.invoke("Explain AI")
  for item in response.content:
      if item.get("type") == "reasoning":
          ...  # OpenAI-style reasoning
      elif item.get("type") == "thinking":
          ...  # Anthropic-style thinking
      elif item.get("type") == "text":
          ...  # Text
  python v1 (new) theme={null}
  from langchain.messages import HumanMessage

message = HumanMessage(content_blocks=[
      {"type": "text", "text": "Describe this image."},
      {"type": "image", "url": "https://example.com/image.jpg"},
  ])
  res = model.invoke([message])
  python v0 (old) theme={null}
  from langchain.messages import HumanMessage

message = HumanMessage(content=[
      # Provider-native structure
      {"type": "text", "text": "Describe this image."},
      {"type": "image_url", "image_url": {"url": "https://example.com/image.jpg"}},
  ])
  res = model.invoke([message])
  python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
<Note>
  Dynamic model functions can return pre-bound models if structured output is *not* used.
</Note>

### Tools

The [`tools`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent\(tools\)) argument to [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent) accepts a list of:

* LangChain [`BaseTool`](https://reference.langchain.com/python/langchain/tools/#langchain.tools.BaseTool) instances (functions decorated with [`@tool`](https://reference.langchain.com/python/langchain/tools/#langchain.tools.tool))
* Callable objects (functions) with proper type hints and a docstring
* `dict` that represents a built-in provider tools

The argument will no longer accept [`ToolNode`](https://reference.langchain.com/python/langgraph/agents/#langgraph.prebuilt.tool_node.ToolNode) instances.

<CodeGroup>
```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
</CodeGroup>

#### Handling tool errors

You can now configure the handling of tool errors with middleware implementing the `wrap_tool_call` method.

<CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Use in an async context

**URL:** llms-txt#use-in-an-async-context

results = await search_store()
```

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/semantic-search.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## use in our SQL queries.

**URL:** llms-txt#use-in-our-sql-queries.

def index_fields() -> tuple[InMemoryVectorStore, InMemoryVectorStore, InMemoryVectorStore]: ...

track_store, artist_store, album_store = index_fields()

---

## Use it as a custom subagent

**URL:** llms-txt#use-it-as-a-custom-subagent

**Contents:**
- The general-purpose subagent
  - When to use it
- Best practices
  - Write clear descriptions
  - Keep system prompts detailed
  - Minimize tool sets

custom_subagent = CompiledSubAgent(
    name="data-analyzer",
    description="Specialized agent for complex data analysis tasks",
    runnable=custom_graph
)

subagents = [custom_subagent]

agent = create_deep_agent(
    model="claude-sonnet-4-5-20250929",
    tools=[internet_search],
    system_prompt=research_instructions,
    subagents=subagents
)
python  theme={null}
research_subagent = {
    "name": "research-agent",
    "description": "Conducts in-depth research using web search and synthesizes findings",
    "system_prompt": """You are a thorough researcher. Your job is to:

1. Break down the research question into searchable queries
    2. Use internet_search to find relevant information
    3. Synthesize findings into a comprehensive but concise summary
    4. Cite sources when making claims

Output format:
    - Summary (2-3 paragraphs)
    - Key findings (bullet points)
    - Sources (with URLs)

Keep your response under 500 words to maintain clean context.""",
    "tools": [internet_search],
}
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
## The general-purpose subagent

In addition to any user-defined subagents, deep agents have access to a `general-purpose` subagent at all times. This subagent:

* Has the same system prompt as the main agent
* Has access to all the same tools
* Uses the same model (unless overridden)

### When to use it

The general-purpose subagent is ideal for context isolation without specialized behavior. The main agent can delegate a complex multi-step task to this subagent and get a concise result back without bloat from intermediate tool calls.

<Card title="Example">
  Instead of the main agent making 10 web searches and filling its context with results, it delegates to the general-purpose subagent: `task(name="general-purpose", task="Research quantum computing trends")`. The subagent performs all the searches internally and returns only a summary.
</Card>

## Best practices

### Write clear descriptions

The main agent uses descriptions to decide which subagent to call. Be specific:

✅ **Good:** `"Analyzes financial data and generates investment insights with confidence scores"`

❌ **Bad:** `"Does finance stuff"`

### Keep system prompts detailed

Include specific guidance on how to use tools and format outputs:
```

Example 2 (unknown):
```unknown
### Minimize tool sets

Only give subagents the tools they need. This improves focus and security:
```

---

## Use multiple limiters together

**URL:** llms-txt#use-multiple-limiters-together

**Contents:**
  - Model fallback
  - PII detection
  - To-do list

agent = create_agent(
    model="gpt-4o",
    tools=[search_tool, database_tool, scraper_tool],
    middleware=[
        global_limiter,
        search_limiter,
        database_limiter,
        web_scraper_limiter
    ],
)
python  theme={null}
from langchain.agents import create_agent
from langchain.agents.middleware import ModelFallbackMiddleware

agent = create_agent(
    model="gpt-4o",  # Primary model
    tools=[...],
    middleware=[
        ModelFallbackMiddleware(
            "gpt-4o-mini",  # Try first on error
            "claude-3-5-sonnet-20241022",  # Then this
        ),
    ],
)
python  theme={null}
from langchain.agents import create_agent
from langchain.agents.middleware import PIIMiddleware

agent = create_agent(
    model="gpt-4o",
    tools=[...],
    middleware=[
        # Redact emails in user input
        PIIMiddleware("email", strategy="redact", apply_to_input=True),
        # Mask credit cards (show last 4 digits)
        PIIMiddleware("credit_card", strategy="mask", apply_to_input=True),
        # Custom PII type with regex
        PIIMiddleware(
            "api_key",
            detector=r"sk-[a-zA-Z0-9]{32}",
            strategy="block",  # Raise error if detected
        ),
    ],
)
python  theme={null}
from langchain.agents import create_agent
from langchain.agents.middleware import TodoListMiddleware
from langchain_core.messages import HumanMessage
from langchain_core.tools import tool

@tool
def read_file(file_path: str) -> str:
    """Read contents of a file."""
    with open(file_path) as f:
        return f.read()

@tool
def write_file(file_path: str, content: str) -> str:
    """Write content to a file."""
    with open(file_path, 'w') as f:
        f.write(content)
    return f"Wrote {len(content)} characters to {file_path}"

@tool
def run_tests(test_path: str) -> str:
    """Run tests and return results."""
    # Simplified for example
    return "All tests passed!"

agent = create_agent(
    model="gpt-4o",
    tools=[read_file, write_file, run_tests],
    middleware=[TodoListMiddleware()],
)

result = agent.invoke({
    "messages": [HumanMessage("Refactor the authentication module to use async/await and ensure all tests pass")]
})

**Examples:**

Example 1 (unknown):
```unknown
<Accordion title="Configuration options">
  <ParamField body="tool_name" type="string">
    Name of specific tool to limit. If not provided, limits apply to **all tools globally**.
  </ParamField>

  <ParamField body="thread_limit" type="number">
    Maximum tool calls across all runs in a thread (conversation). Persists across multiple invocations with the same thread ID. Requires a checkpointer to maintain state. `None` means no thread limit.
  </ParamField>

  <ParamField body="run_limit" type="number">
    Maximum tool calls per single invocation (one user message → response cycle). Resets with each new user message. `None` means no run limit.

    **Note:** At least one of `thread_limit` or `run_limit` must be specified.
  </ParamField>

  <ParamField body="exit_behavior" type="string" default="continue">
    Behavior when limit is reached:

    * `"continue"` (default) - Block exceeded tool calls with error messages, let other tools and the model continue. The model decides when to end based on the error messages.
    * `"error"` - Raise a `ToolCallLimitExceededError` exception, stopping execution immediately
    * `"end"` - Stop execution immediately with a ToolMessage and AI message for the exceeded tool call. Only works when limiting a single tool; raises `NotImplementedError` if other tools have pending calls.
  </ParamField>
</Accordion>

### Model fallback

Automatically fallback to alternative models when the primary model fails.

<Tip>
  **Perfect for:**

  * Building resilient agents that handle model outages
  * Cost optimization by falling back to cheaper models
  * Provider redundancy across OpenAI, Anthropic, etc.
</Tip>
```

Example 2 (unknown):
```unknown
<Accordion title="Configuration options">
  <ParamField body="first_model" type="string | BaseChatModel" required>
    First fallback model to try when the primary model fails. Can be a model string (e.g., `"openai:gpt-4o-mini"`) or a `BaseChatModel` instance.
  </ParamField>

  <ParamField body="*additional_models" type="string | BaseChatModel">
    Additional fallback models to try in order if previous models fail
  </ParamField>
</Accordion>

### PII detection

Detect and handle Personally Identifiable Information in conversations.

<Tip>
  **Perfect for:**

  * Healthcare and financial applications with compliance requirements
  * Customer service agents that need to sanitize logs
  * Any application handling sensitive user data
</Tip>
```

Example 3 (unknown):
```unknown
<Accordion title="Configuration options">
  <ParamField body="pii_type" type="string" required>
    Type of PII to detect. Can be a built-in type (`email`, `credit_card`, `ip`, `mac_address`, `url`) or a custom type name.
  </ParamField>

  <ParamField body="strategy" type="string" default="redact">
    How to handle detected PII. Options:

    * `"block"` - Raise exception when detected
    * `"redact"` - Replace with `[REDACTED_TYPE]`
    * `"mask"` - Partially mask (e.g., `****-****-****-1234`)
    * `"hash"` - Replace with deterministic hash
  </ParamField>

  <ParamField body="detector" type="function | regex">
    Custom detector function or regex pattern. If not provided, uses built-in detector for the PII type.
  </ParamField>

  <ParamField body="apply_to_input" type="boolean" default="True">
    Check user messages before model call
  </ParamField>

  <ParamField body="apply_to_output" type="boolean" default="False">
    Check AI messages after model call
  </ParamField>

  <ParamField body="apply_to_tool_results" type="boolean" default="False">
    Check tool result messages after execution
  </ParamField>
</Accordion>

### To-do list

Equip agents with task planning and tracking capabilities for complex multi-step tasks.

<Tip>
  **Perfect for:**

  * Complex multi-step tasks requiring coordination across multiple tools
  * Long-running operations where progress visibility is important
</Tip>

Just as humans are more effective when they write down and track tasks, agents benefit from structured task management to break down complex problems, adapt plans as new information emerges, and provide transparency into their workflow.

You may have noticed patterns like this in Claude Code, which writes out a to-do list before tackling complex, multi-part tasks.

<Note>
  This middleware automatically provides agents with a `write_todos` tool and system prompts to guide effective task planning.
</Note>
```

---

## Use the functional API

**URL:** llms-txt#use-the-functional-api

**Contents:**
- Creating a simple workflow
- Parallel execution
- Calling graphs
- Call other entrypoints
- Streaming
- Retry policy
- Caching Tasks
- Resuming after an error
- Human-in-the-loop
  - Basic human-in-the-loop workflow

Source: https://docs.langchain.com/oss/javascript/langgraph/use-functional-api

The [**Functional API**](/oss/javascript/langgraph/functional-api) allows you to add LangGraph's key features — [persistence](/oss/javascript/langgraph/persistence), [memory](/oss/javascript/langgraph/add-memory), [human-in-the-loop](/oss/javascript/langgraph/interrupts), and [streaming](/oss/javascript/langgraph/streaming) — to your applications with minimal changes to your existing code.

<Tip>
  For conceptual information on the functional API, see [Functional API](/oss/javascript/langgraph/functional-api).
</Tip>

## Creating a simple workflow

When defining an `entrypoint`, input is restricted to the first argument of the function. To pass multiple inputs, you can use a dictionary.

<Accordion title="Extended example: simple workflow">
  
</Accordion>

<Accordion title="Extended example: Compose an essay with an LLM">
  This example demonstrates how to use the `@task` and `@entrypoint` decorators
  syntactically. Given that a checkpointer is provided, the workflow results will
  be persisted in the checkpointer.

## Parallel execution

Tasks can be executed in parallel by invoking them concurrently and waiting for the results. This is useful for improving performance in IO bound tasks (e.g., calling APIs for LLMs).

<Accordion title="Extended example: parallel LLM calls">
  This example demonstrates how to run multiple LLM calls in parallel using `@task`. Each call generates a paragraph on a different topic, and results are joined into a single text output.

This example uses LangGraph's concurrency model to improve execution time, especially when tasks involve I/O like LLM completions.
</Accordion>

The **Functional API** and the [**Graph API**](/oss/javascript/langgraph/graph-api) can be used together in the same application as they share the same underlying runtime.

<Accordion title="Extended example: calling a simple graph from the functional API">
  
</Accordion>

## Call other entrypoints

You can call other **entrypoints** from within an **entrypoint** or a **task**.

<Accordion title="Extended example: calling another entrypoint">
  
</Accordion>

The **Functional API** uses the same streaming mechanism as the **Graph API**. Please
read the [**streaming guide**](/oss/javascript/langgraph/streaming) section for more details.

Example of using the streaming API to stream both updates and custom data.

1. Emit custom data before computation begins.
2. Emit another custom message after computing the result.
3. Use `.stream()` to process streamed output.
4. Specify which streaming modes to use.

1. `ttl` is specified in seconds. The cache will be invalidated after this time.

## Resuming after an error

When we resume execution, we won't need to re-run the `slowTask` as its result is already saved in the checkpoint.

The functional API supports [human-in-the-loop](/oss/javascript/langgraph/interrupts) workflows using the [`interrupt`](https://langchain-ai.github.io/langgraphjs/reference/functions/langgraph.interrupt-2.html) function and the `Command` primitive.

### Basic human-in-the-loop workflow

We will create three [tasks](/oss/javascript/langgraph/functional-api#task):

1. Append `"bar"`.
2. Pause for human input. When resuming, append human input.
3. Append `"qux"`.

We can now compose these tasks in an [entrypoint](/oss/javascript/langgraph/functional-api#entrypoint):

[interrupt()](/oss/javascript/langgraph/interrupts#pause-using-interrupt) is called inside a task, enabling a human to review and edit the output of the previous task. The results of prior tasks-- in this case `step_1`-- are persisted, so that they are not run again following the [`interrupt`](https://langchain-ai.github.io/langgraphjs/reference/functions/langgraph.interrupt-2.html).

Let's send in a query string:

Note that we've paused with an [`interrupt`](https://langchain-ai.github.io/langgraphjs/reference/functions/langgraph.interrupt-2.html) after `step_1`. The interrupt provides instructions to resume the run. To resume, we issue a [`Command`](/oss/javascript/langgraph/interrupts#resuming-interrupts) containing the data expected by the `human_feedback` task.

After resuming, the run proceeds through the remaining step and terminates as expected.

### Review tool calls

To review tool calls before execution, we add a `review_tool_call` function that calls [`interrupt`](/oss/javascript/langgraph/interrupts#pause-using-interrupt). When this function is called, execution will be paused until we issue a command to resume it.

Given a tool call, our function will [`interrupt`](https://langchain-ai.github.io/langgraphjs/reference/functions/langgraph.interrupt-2.html) for human review. At that point we can either:

* Accept the tool call
* Revise the tool call and continue
* Generate a custom tool message (e.g., instructing the model to re-format its tool call)

We can now update our [entrypoint](/oss/javascript/langgraph/functional-api#entrypoint) to review the generated tool calls. If a tool call is accepted or revised, we execute in the same way as before. Otherwise, we just append the @\[`ToolMessage`] supplied by the human. The results of prior tasks — in this case the initial model call — are persisted, so that they are not run again following the [`interrupt`](https://langchain-ai.github.io/langgraphjs/reference/functions/langgraph.interrupt-2.html).

Short-term memory allows storing information across different **invocations** of the same **thread id**. See [short-term memory](/oss/javascript/langgraph/functional-api#short-term-memory) for more details.

### Manage checkpoints

You can view and delete the information stored by the checkpointer.

<a id="checkpoint" />

#### View thread state

<a id="checkpoints" />

#### View the history of the thread

### Decouple return value from saved value

Use `entrypoint.final` to decouple what is returned to the caller from what is persisted in the checkpoint. This is useful when:

* You want to return a computed result (e.g., a summary or status), but save a different internal value for use on the next invocation.
* You need to control what gets passed to the previous parameter on the next run.

An example of a simple chatbot using the functional API and the @\[`InMemorySaver`] checkpointer.

The bot is able to remember the previous conversation and continue from where it left off.

[long-term memory](/oss/javascript/concepts/memory#long-term-memory) allows storing information across different **thread ids**. This could be useful for learning information about a given user in one conversation and using it in another.

* [Workflows and agent](/oss/javascript/langgraph/workflows-agents) guide for more examples of how to build workflows using the Functional API.

## Integrate with other libraries

* [Add LangGraph's features to other frameworks using the functional API](/langsmith/autogen-integration): Add LangGraph features like persistence, memory and streaming to other agent frameworks that do not provide them out of the box.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/use-functional-api.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
<Accordion title="Extended example: simple workflow">
```

Example 2 (unknown):
```unknown
</Accordion>

<Accordion title="Extended example: Compose an essay with an LLM">
  This example demonstrates how to use the `@task` and `@entrypoint` decorators
  syntactically. Given that a checkpointer is provided, the workflow results will
  be persisted in the checkpointer.
```

Example 3 (unknown):
```unknown
</Accordion>

## Parallel execution

Tasks can be executed in parallel by invoking them concurrently and waiting for the results. This is useful for improving performance in IO bound tasks (e.g., calling APIs for LLMs).
```

Example 4 (unknown):
```unknown
<Accordion title="Extended example: parallel LLM calls">
  This example demonstrates how to run multiple LLM calls in parallel using `@task`. Each call generates a paragraph on a different topic, and results are joined into a single text output.
```

---

## Use the graph API

**URL:** llms-txt#use-the-graph-api

**Contents:**
- Setup
- Define and update state
  - Define state
  - Update state
  - Process state updates with reducers
  - Define input and output schemas
  - Pass private state between nodes
  - Alternative state definitions
- Add runtime configuration
- Add retry policies

Source: https://docs.langchain.com/oss/javascript/langgraph/use-graph-api

This guide demonstrates the basics of LangGraph's Graph API. It walks through [state](#define-and-update-state), as well as composing common graph structures such as [sequences](#create-a-sequence-of-steps), [branches](#create-branches), and [loops](#create-and-control-loops). It also covers LangGraph's control features, including the [Send API](#map-reduce-and-the-send-api) for map-reduce workflows and the [Command API](#combine-control-flow-and-state-updates-with-command) for combining state updates with "hops" across nodes.

<Tip>
  **Set up LangSmith for better debugging**
  Sign up for [LangSmith](https://smith.langchain.com) to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph — read more about how to get started in the [docs](https://docs.smith.langchain.com).
</Tip>

## Define and update state

Here we show how to define and update [state](/oss/javascript/langgraph/graph-api#state) in LangGraph. We will demonstrate:

1. How to use state to define a graph's [schema](/oss/javascript/langgraph/graph-api#schema)
2. How to use [reducers](/oss/javascript/langgraph/graph-api#reducers) to control how state updates are processed.

[State](/oss/javascript/langgraph/graph-api#state) in LangGraph can be defined using Zod schemas. Below we will use Zod. See [this section](#alternative-state-definitions) for detail on using alternative approaches.

By default, graphs will have the same input and output schema, and the state determines that schema. See [this section](#define-input-and-output-schemas) for how to define distinct input and output schemas.

Let's consider a simple example using [messages](/oss/javascript/langgraph/graph-api#messagesstate). This represents a versatile formulation of state for many LLM applications. See our [concepts page](/oss/javascript/langgraph/graph-api#working-with-messages-in-graph-state) for more detail.

This state tracks a list of [message](https://js.langchain.com/docs/concepts/messages/) objects, as well as an extra integer field.

Let's build an example graph with a single node. Our [node](/oss/javascript/langgraph/graph-api#nodes) is just a TypeScript function that reads our graph's state and makes updates to it. The first argument to this function will always be the state:

This node simply appends a message to our message list, and populates an extra field.

<Warning>
  Nodes should return updates to the state directly, instead of mutating the state.
</Warning>

Let's next define a simple graph containing this node. We use [`StateGraph`](/oss/javascript/langgraph/graph-api#stategraph) to define a graph that operates on this state. We then use [`addNode`](/oss/javascript/langgraph/graph-api#nodes) populate our graph.

LangGraph provides built-in utilities for visualizing your graph. Let's inspect our graph. See [this section](#visualize-your-graph) for detail on visualization.

In this case, our graph just executes a single node. Let's proceed with a simple invocation:

* We kicked off invocation by updating a single key of the state.
* We receive the entire state in the invocation result.

For convenience, we frequently inspect the content of [message objects](https://js.langchain.com/docs/concepts/messages/) via logging:

### Process state updates with reducers

Each key in the state can have its own independent [reducer](/oss/javascript/langgraph/graph-api#reducers) function, which controls how updates from nodes are applied. If no reducer function is explicitly specified then it is assumed that all updates to the key should override it.

For Zod state schemas, we can define reducers by using the special `.langgraph.reducer()` method on the schema field.

In the earlier example, our node updated the `"messages"` key in the state by appending a message to it. Below, we add a reducer to this key, such that updates are automatically appended:

Now our node can be simplified:

In practice, there are additional considerations for updating lists of messages:

* We may wish to update an existing message in the state.
* We may want to accept short-hands for [message formats](/oss/javascript/langgraph/graph-api#using-messages-in-your-graph), such as [OpenAI format](https://python.langchain.com/docs/concepts/messages/#openai-format).

LangGraph includes a built-in `MessagesZodMeta` that handles these considerations:

This is a versatile representation of state for applications involving [chat models](https://js.langchain.com/docs/concepts/chat_models/). LangGraph includes this pre-built `MessagesZodMeta` for convenience, so that we can have:

### Define input and output schemas

By default, `StateGraph` operates with a single schema, and all nodes are expected to communicate using that schema. However, it's also possible to define distinct input and output schemas for a graph.

When distinct schemas are specified, an internal schema will still be used for communication between nodes. The input schema ensures that the provided input matches the expected structure, while the output schema filters the internal data to return only the relevant information according to the defined output schema.

Below, we'll see how to define distinct input and output schema.

Notice that the output of invoke only includes the output schema.

### Pass private state between nodes

In some cases, you may want nodes to exchange information that is crucial for intermediate logic but doesn't need to be part of the main schema of the graph. This private data is not relevant to the overall input/output of the graph and should only be shared between certain nodes.

Below, we'll create an example sequential graph consisting of three nodes (node\_1, node\_2 and node\_3), where private data is passed between the first two steps (node\_1 and node\_2), while the third step (node\_3) only has access to the public overall state.

### Alternative state definitions

While Zod schemas are the recommended approach, LangGraph also supports other ways to define state schemas:

## Add runtime configuration

Sometimes you want to be able to configure your graph when calling it. For example, you might want to be able to specify what LLM or system prompt to use at runtime, *without polluting the graph state with these parameters*.

To add runtime configuration:

1. Specify a schema for your configuration
2. Add the configuration to the function signature for nodes or conditional edges
3. Pass the configuration into the graph.

See below for a simple example:

<Accordion title="Extended example: specifying LLM at runtime">
  Below we demonstrate a practical example in which we configure what LLM to use at runtime. We will use both OpenAI and Anthropic models.

<Accordion title="Extended example: specifying model and system message at runtime">
  Below we demonstrate a practical example in which we configure two parameters: the LLM and system message to use at runtime.

## Add retry policies

There are many use cases where you may wish for your node to have a custom retry policy, for example if you are calling an API, querying a database, or calling an LLM, etc. LangGraph lets you add retry policies to nodes.

To configure a retry policy, pass the `retryPolicy` parameter to the [`addNode`](https://reference.langchain.com/javascript/classes/_langchain_langgraph.index.Graph.html#addnode). The `retryPolicy` parameter takes in a `RetryPolicy` object. Below we instantiate a `RetryPolicy` object with the default parameters and associate it with a node:

By default, the retry policy retries on any exception except for the following:

* `TypeError`
* `SyntaxError`
* `ReferenceError`

<Accordion title="Extended example: customizing retry policies">
  Consider an example in which we are reading from a SQL database. Below we pass two different retry policies to nodes:

## Create a sequence of steps

<Info>
  **Prerequisites**
  This guide assumes familiarity with the above section on [state](#define-and-update-state).
</Info>

Here we demonstrate how to construct a simple sequence of steps. We will show:

1. How to build a sequential graph
2. Built-in short-hand for constructing similar graphs.

To add a sequence of nodes, we use the `.addNode` and `.addEdge` methods of our [graph](/oss/javascript/langgraph/graph-api#stategraph):

<Accordion title="Why split application steps into a sequence with LangGraph?">
  LangGraph makes it easy to add an underlying persistence layer to your application.
  This allows state to be checkpointed in between the execution of nodes, so your LangGraph nodes govern:

* How state updates are [checkpointed](/oss/javascript/langgraph/persistence)
  * How interruptions are resumed in [human-in-the-loop](/oss/javascript/langgraph/interrupts) workflows
  * How we can "rewind" and branch-off executions using LangGraph's [time travel](/oss/javascript/langgraph/use-time-travel) features

They also determine how execution steps are [streamed](/oss/javascript/langgraph/streaming), and how your application is visualized and debugged using [Studio](/langsmith/studio).

Let's demonstrate an end-to-end example. We will create a sequence of three steps:

1. Populate a value in a key of the state
  2. Update the same value
  3. Populate a different value

Let's first define our [state](/oss/javascript/langgraph/graph-api#state). This governs the [schema of the graph](/oss/javascript/langgraph/graph-api#schema), and can also specify how to apply updates. See [this section](#process-state-updates-with-reducers) for more detail.

In our case, we will just keep track of two values:

Our [nodes](/oss/javascript/langgraph/graph-api#nodes) are just TypeScript functions that read our graph's state and make updates to it. The first argument to this function will always be the state:

<Note>
    Note that when issuing updates to the state, each node can just specify the value of the key it wishes to update.

By default, this will **overwrite** the value of the corresponding key. You can also use [reducers](/oss/javascript/langgraph/graph-api#reducers) to control how updates are processed— for example, you can append successive updates to a key instead. See [this section](#process-state-updates-with-reducers) for more detail.
  </Note>

Finally, we define the graph. We use [StateGraph](/oss/javascript/langgraph/graph-api#stategraph) to define a graph that operates on this state.

We will then use [addNode](/oss/javascript/langgraph/graph-api#nodes) and [addEdge](/oss/javascript/langgraph/graph-api#edges) to populate our graph and define its control flow.

<Tip>
    **Specifying custom names**
    You can specify custom names for nodes using `.addNode`:

* `.addEdge` takes the names of nodes, which for functions defaults to `node.name`.
  * We must specify the entry point of the graph. For this we add an edge with the [START node](/oss/javascript/langgraph/graph-api#start-node).
  * The graph halts when there are no more nodes to execute.

We next [compile](/oss/javascript/langgraph/graph-api#compiling-your-graph) our graph. This provides a few basic checks on the structure of the graph (e.g., identifying orphaned nodes). If we were adding persistence to our application via a [checkpointer](/oss/javascript/langgraph/persistence), it would also be passed in here.

LangGraph provides built-in utilities for visualizing your graph. Let's inspect our sequence. See [this guide](#visualize-your-graph) for detail on visualization.

Let's proceed with a simple invocation:

* We kicked off invocation by providing a value for a single state key. We must always provide a value for at least one key.
  * The value we passed in was overwritten by the first node.
  * The second node updated the value.
  * The third node populated a different value.
</Accordion>

Parallel execution of nodes is essential to speed up overall graph operation. LangGraph offers native support for parallel execution of nodes, which can significantly enhance the performance of graph-based workflows. This parallelization is achieved through fan-out and fan-in mechanisms, utilizing both standard edges and [conditional\_edges](https://langchain-ai.github.io/langgraph/reference/graphs.md#langgraph.graph.MessageGraph.add_conditional_edges). Below are some examples showing how to add create branching dataflows that work for you.

### Run graph nodes in parallel

In this example, we fan out from `Node A` to `B and C` and then fan in to `D`. With our state, [we specify the reducer add operation](/oss/javascript/langgraph/graph-api#reducers). This will combine or accumulate values for the specific key in the State, rather than simply overwriting the existing value. For lists, this means concatenating the new list with the existing list. See the above section on [state reducers](#process-state-updates-with-reducers) for more detail on updating state with reducers.

With the reducer, you can see that the values added in each node are accumulated.

<Note>
  In the above example, nodes `"b"` and `"c"` are executed concurrently in the same [superstep](/oss/javascript/langgraph/graph-api#graphs). Because they are in the same step, node `"d"` executes after both `"b"` and `"c"` are finished.

Importantly, updates from a parallel superstep may not be ordered consistently. If you need a consistent, predetermined ordering of updates from a parallel superstep, you should write the outputs to a separate field in the state together with a value with which to order them.
</Note>

<Accordion title="Exception handling?">
  LangGraph executes nodes within [supersteps](/oss/javascript/langgraph/graph-api#graphs), meaning that while parallel branches are executed in parallel, the entire superstep is **transactional**. If any of these branches raises an exception, **none** of the updates are applied to the state (the entire superstep errors).

Importantly, when using a [checkpointer](/oss/javascript/langgraph/persistence), results from successful nodes within a superstep are saved, and don't repeat when resumed.

If you have error-prone (perhaps want to handle flakey API calls), LangGraph provides two ways to address this:

1. You can write regular python code within your node to catch and handle exceptions.
  2. You can set a **[retry\_policy](https://langchain-ai.github.io/langgraph/reference/types/#langgraph.types.RetryPolicy)** to direct the graph to retry nodes that raise certain types of exceptions. Only failing branches are retried, so you needn't worry about performing redundant work.

Together, these let you perform parallel execution and fully control exception handling.
</Accordion>

<Tip>
  **Set max concurrency**
  You can control the maximum number of concurrent tasks by setting `max_concurrency` in the [configuration](https://reference.langchain.com/javascript/interfaces/_langchain_langgraph.index.LangGraphRunnableConfig.html) when invoking the graph.

### Conditional branching

If your fan-out should vary at runtime based on the state, you can use [`addConditionalEdges`](https://reference.langchain.com/javascript/classes/_langchain_langgraph.index.StateGraph.html#addconditionaledges) to select one or more paths using the graph state. See example below, where node `a` generates a state update that determines the following node.

<Tip>
  Your conditional edges can route to multiple destination nodes. For example:

## Map-Reduce and the Send API

LangGraph supports map-reduce and other advanced branching patterns using the Send API. Here is an example of how to use it:

## Create and control loops

When creating a graph with a loop, we require a mechanism for terminating execution. This is most commonly done by adding a [conditional edge](/oss/javascript/langgraph/graph-api#conditional-edges) that routes to the [END](/oss/javascript/langgraph/graph-api#end-node) node once we reach some termination condition.

You can also set the graph recursion limit when invoking or streaming the graph. The recursion limit sets the number of [supersteps](/oss/javascript/langgraph/graph-api#graphs) that the graph is allowed to execute before it raises an error. Read more about the concept of recursion limits [here](/oss/javascript/langgraph/graph-api#recursion-limit).

Let's consider a simple graph with a loop to better understand how these mechanisms work.

<Tip>
  To return the last value of your state instead of receiving a recursion limit error, see the [next section](#impose-a-recursion-limit).
</Tip>

When creating a loop, you can include a conditional edge that specifies a termination condition:

To control the recursion limit, specify `"recursionLimit"` in the config. This will raise a `GraphRecursionError`, which you can catch and handle:

Let's define a graph with a simple loop. Note that we use a conditional edge to implement a termination condition.

This architecture is similar to a [ReAct agent](/oss/javascript/langgraph/workflows-agents) in which node `"a"` is a tool-calling model, and node `"b"` represents the tools.

In our `route` conditional edge, we specify that we should end after the `"aggregate"` list in the state passes a threshold length.

Invoking the graph, we see that we alternate between nodes `"a"` and `"b"` before terminating once we reach the termination condition.

### Impose a recursion limit

In some applications, we may not have a guarantee that we will reach a given termination condition. In these cases, we can set the graph's [recursion limit](/oss/javascript/langgraph/graph-api#recursion-limit). This will raise a `GraphRecursionError` after a given number of [supersteps](/oss/javascript/langgraph/graph-api#graphs). We can then catch and handle this exception:

## Combine control flow and state updates with `Command`

It can be useful to combine control flow (edges) and state updates (nodes). For example, you might want to BOTH perform state updates AND decide which node to go to next in the SAME node. LangGraph provides a way to do so by returning a [Command](https://langchain-ai.github.io/langgraph/reference/types/#langgraph.types.Command) object from node functions:

We show an end-to-end example below. Let's create a simple graph with 3 nodes: A, B and C. We will first execute node A, and then decide whether to go to Node B or Node C next based on the output of node A.

We can now create the `StateGraph` with the above nodes. Notice that the graph doesn't have [conditional edges](/oss/javascript/langgraph/graph-api#conditional-edges) for routing! This is because control flow is defined with `Command` inside `nodeA`.

<Warning>
  You might have noticed that we used `ends` to specify which nodes `nodeA` can navigate to. This is necessary for the graph rendering and tells LangGraph that `nodeA` can navigate to `nodeB` and `nodeC`.
</Warning>

If we run the graph multiple times, we'd see it take different paths (A -> B or A -> C) based on the random choice in node A.

### Navigate to a node in a parent graph

If you are using [subgraphs](/oss/javascript/langgraph/use-subgraphs), you might want to navigate from a node within a subgraph to a different subgraph (i.e. a different node in the parent graph). To do so, you can specify `graph=Command.PARENT` in `Command`:

Let's demonstrate this using the above example. We'll do so by changing `nodeA` in the above example into a single-node graph that we'll add as a subgraph to our parent graph.

<Warning>
  **State updates with `Command.PARENT`**
  When you send updates from a subgraph node to a parent graph node for a key that's shared by both parent and subgraph [state schemas](/oss/javascript/langgraph/graph-api#schema), you **must** define a [reducer](/oss/javascript/langgraph/graph-api#reducers) for the key you're updating in the parent graph state. See the example below.
</Warning>

A common use case is updating graph state from inside a tool. For example, in a customer support application you might want to look up customer information based on their account number or ID in the beginning of the conversation. To update the graph state from the tool, you can return `Command(update={"my_custom_key": "foo", "messages": [...]})` from the tool:

<Warning>
  You MUST include `messages` (or any state key used for the message history) in `Command.update` when returning [`Command`](https://langchain-ai.github.io/langgraphjs/reference/classes/langgraph.Command.html) from a tool and the list of messages in `messages` MUST contain a `ToolMessage`. This is necessary for the resulting message history to be valid (LLM providers require AI messages with tool calls to be followed by the tool result messages).
</Warning>

If you are using tools that update state via [`Command`](https://langchain-ai.github.io/langgraphjs/reference/classes/langgraph.Command.html), we recommend using prebuilt [`ToolNode`](https://reference.langchain.com/javascript/classes/langchain.index.ToolNode.html) which automatically handles tools returning [`Command`](https://langchain-ai.github.io/langgraphjs/reference/classes/langgraph.Command.html) objects and propagates them to the graph state. If you're writing a custom node that calls tools, you would need to manually propagate [`Command`](https://langchain-ai.github.io/langgraphjs/reference/classes/langgraph.Command.html) objects returned by the tools as the update from the node.

## Visualize your graph

Here we demonstrate how to visualize the graphs you create.

You can visualize any arbitrary [Graph](https://langchain-ai.github.io/langgraph/reference/graphs/), including [StateGraph](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph).

Let's create a simple example graph to demonstrate visualization.

We can also convert a graph class into Mermaid syntax.

If preferred, we could render the Graph into a `.png`. This uses the Mermaid.ink API to generate the diagram.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/use-graph-api.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
<Tip>
  **Set up LangSmith for better debugging**
  Sign up for [LangSmith](https://smith.langchain.com) to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph — read more about how to get started in the [docs](https://docs.smith.langchain.com).
</Tip>

## Define and update state

Here we show how to define and update [state](/oss/javascript/langgraph/graph-api#state) in LangGraph. We will demonstrate:

1. How to use state to define a graph's [schema](/oss/javascript/langgraph/graph-api#schema)
2. How to use [reducers](/oss/javascript/langgraph/graph-api#reducers) to control how state updates are processed.

### Define state

[State](/oss/javascript/langgraph/graph-api#state) in LangGraph can be defined using Zod schemas. Below we will use Zod. See [this section](#alternative-state-definitions) for detail on using alternative approaches.

By default, graphs will have the same input and output schema, and the state determines that schema. See [this section](#define-input-and-output-schemas) for how to define distinct input and output schemas.

Let's consider a simple example using [messages](/oss/javascript/langgraph/graph-api#messagesstate). This represents a versatile formulation of state for many LLM applications. See our [concepts page](/oss/javascript/langgraph/graph-api#working-with-messages-in-graph-state) for more detail.
```

Example 2 (unknown):
```unknown
This state tracks a list of [message](https://js.langchain.com/docs/concepts/messages/) objects, as well as an extra integer field.

### Update state

Let's build an example graph with a single node. Our [node](/oss/javascript/langgraph/graph-api#nodes) is just a TypeScript function that reads our graph's state and makes updates to it. The first argument to this function will always be the state:
```

Example 3 (unknown):
```unknown
This node simply appends a message to our message list, and populates an extra field.

<Warning>
  Nodes should return updates to the state directly, instead of mutating the state.
</Warning>

Let's next define a simple graph containing this node. We use [`StateGraph`](/oss/javascript/langgraph/graph-api#stategraph) to define a graph that operates on this state. We then use [`addNode`](/oss/javascript/langgraph/graph-api#nodes) populate our graph.
```

Example 4 (unknown):
```unknown
LangGraph provides built-in utilities for visualizing your graph. Let's inspect our graph. See [this section](#visualize-your-graph) for detail on visualization.
```

---

## Use threads

**URL:** llms-txt#use-threads

**Contents:**
- Create a thread
  - Empty thread
  - Copy thread
  - Prepopulated State
- List threads
  - LangGraph SDK
  - LangSmith UI
- Inspect threads
  - LangGraph SDK
  - LangSmith UI

Source: https://docs.langchain.com/langsmith/use-threads

In this guide, we will show how to create, view, and inspect [threads](/oss/python/langgraph/persistence#threads).

To run your graph and the state persisted, you must first create a thread.

To create a new thread, use the [LangGraph SDK](/langsmith/sdk) `create` method. See the [Python](/langsmith/langgraph-python-sdk#langgraph_sdk.client.ThreadsClient.create) and [JS](/langsmith/langgraph-js-ts-sdk#create_3) SDK reference docs for more information.

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

Alternatively, if you already have a thread in your application whose state you wish to copy, you can use the `copy` method. This will create an independent thread whose history is identical to the original thread at the time of the operation. See the [Python](/langsmith/langgraph-python-sdk#langgraph_sdk.client.ThreadsClient.copy) and [JS](/langsmith/langgraph-js-ts-sdk#copy) SDK reference docs for more information.

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

### Prepopulated State

Finally, you can create a thread with an arbitrary pre-defined state by providing a list of `supersteps` into the `create` method. The `supersteps` describe a list of a sequence of state updates. For example:

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

To list threads, use the [LangGraph SDK](/langsmith/sdk) `search` method. This will list the threads in the application that match the provided filters. See the [Python](/langsmith/langgraph-python-sdk#langgraph_sdk.client.ThreadsClient.search) and [JS](/langsmith/langgraph-js-ts-sdk#search_2) SDK reference docs for more information.

#### Filter by thread status

Use the `status` field to filter threads based on their status. Supported values are `idle`, `busy`, `interrupted`, and `error`. See [here](/langsmith/langgraph-python-sdk?h=thread+status#langgraph_sdk.auth.types.ThreadStatus) for information on each status. For example, to view `idle` threads:

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

#### Filter by metadata

The `search` method allows you to filter on metadata:

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

The SDK also supports sorting threads by `thread_id`, `status`, `created_at`, and `updated_at` using the `sort_by` and `sort_order` params.

You can also view threads in a deployment via the LangSmith UI.

Inside your deployment, select the "Threads" tab. This will load a table of all of the threads in your deployment.

To filter by thread status, select a status in the top bar. To sort by a supported property, click on the arrow icon for the desired column.

To view a specific thread given its `thread_id`, use the `get` method:

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

#### Inspect thread state

To view the current state of a given thread, use the `get_state` method:

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

Optionally, to view the state of a thread at a given checkpoint, simply pass in the checkpoint id (or the entire checkpoint object):

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

#### Inspect full thread history

To view a thread's history, use the `get_history` method. This returns a list of every state the thread experienced. For more information see the [Python](/langsmith/langgraph-python-sdk?h=thread+status#langgraph_sdk.client.ThreadsClient.get_history) and [JS](/langsmith/langgraph-js-ts-sdk#gethistory) reference docs.

You can also view threads in a deployment via the LangSmith UI.

Inside your deployment, select the "Threads" tab. This will load a table of all of the threads in your deployment.

Select a thread to inspect its current state. To view its full history and for further debugging, open the thread in [Studio](/langsmith/studio).

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/use-threads.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
</Tab>

  <Tab title="Javascript">
```

Example 2 (unknown):
```unknown
</Tab>

  <Tab title="CURL">
```

Example 3 (unknown):
```unknown
</Tab>
</Tabs>

Output:
```

Example 4 (unknown):
```unknown
### Copy thread

Alternatively, if you already have a thread in your application whose state you wish to copy, you can use the `copy` method. This will create an independent thread whose history is identical to the original thread at the time of the operation. See the [Python](/langsmith/langgraph-python-sdk#langgraph_sdk.client.ThreadsClient.copy) and [JS](/langsmith/langgraph-js-ts-sdk#copy) SDK reference docs for more information.

<Tabs>
  <Tab title="Python">
```

---

## Use time-travel

**URL:** llms-txt#use-time-travel

**Contents:**
- In a workflow
  - Setup
  - 1. Run the graph

Source: https://docs.langchain.com/oss/javascript/langgraph/use-time-travel

When working with non-deterministic systems that make model-based decisions (e.g., agents powered by LLMs), it can be useful to examine their decision-making process in detail:

1. <Icon icon="lightbulb" size={16} /> **Understand reasoning**: Analyze the steps that led to a successful result.
2. <Icon icon="bug" size={16} /> **Debug mistakes**: Identify where and why errors occurred.
3. <Icon icon="magnifying-glass" size={16} /> **Explore alternatives**: Test different paths to uncover better solutions.

LangGraph provides [time travel](/oss/javascript/langgraph/use-time-travel) functionality to support these use cases. Specifically, you can resume execution from a prior checkpoint — either replaying the same state or modifying it to explore alternatives. In all cases, resuming past execution produces a new fork in the history.

To use [time-travel](/oss/javascript/langgraph/use-time-travel) in LangGraph:

1. [Run the graph](#1-run-the-graph) with initial inputs using [`invoke`](https://langchain-ai.github.io/langgraphjs/reference/classes/langgraph.CompiledStateGraph.html#invoke) or [`stream`](https://langchain-ai.github.io/langgraphjs/reference/classes/langgraph.CompiledStateGraph.html#stream) methods.
2. [Identify a checkpoint in an existing thread](#2-identify-a-checkpoint): Use the @\[`getStateHistory`] method to retrieve the execution history for a specific `thread_id` and locate the desired `checkpoint_id`.
   Alternatively, set a [breakpoint](/oss/javascript/langgraph/interrupts) before the node(s) where you want execution to pause. You can then find the most recent checkpoint recorded up to that breakpoint.
3. [Update the graph state (optional)](#3-update-the-state-optional): Use the @\[`updateState`] method to modify the graph's state at the checkpoint and resume execution from alternative state.
4. [Resume execution from the checkpoint](#4-resume-execution-from-the-checkpoint): Use the `invoke` or `stream` methods with an input of `null` and a configuration containing the appropriate `thread_id` and `checkpoint_id`.

<Tip>
  For a conceptual overview of time-travel, see [Time travel](/oss/javascript/langgraph/use-time-travel).
</Tip>

This example builds a simple LangGraph workflow that generates a joke topic and writes a joke using an LLM. It demonstrates how to run the graph, retrieve past execution checkpoints, optionally modify the state, and resume execution from a chosen checkpoint to explore alternate outcomes.

First we need to install the packages required

Next, we need to set API keys for Anthropic (the LLM we will use)

<Tip>
  Sign up for [LangSmith](https://smith.langchain.com) to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph.
</Tip>

```
How about "The Secret Life of Socks in the Dryer"? You know, exploring the mysterious phenomenon of how socks go into the laundry as pairs but come out as singles. Where do they go? Are they starting new lives elsewhere? Is there a sock paradise we don't know about? There's a lot of comedic potential in the everyday mystery that unites us all!

**Examples:**

Example 1 (unknown):
```unknown
Next, we need to set API keys for Anthropic (the LLM we will use)
```

Example 2 (unknown):
```unknown
<Tip>
  Sign up for [LangSmith](https://smith.langchain.com) to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph.
</Tip>
```

Example 3 (unknown):
```unknown
### 1. Run the graph
```

Example 4 (unknown):
```unknown
**Output:**
```

---

## Use tools in a prompt

**URL:** llms-txt#use-tools-in-a-prompt

**Contents:**
- When to use tools
- Built-in tools
  - OpenAI Tools
  - Anthropic Tools
- Adding and using tools
  - Add a tool
  - Use a built-in tool
  - Create a custom tool
- Tool choice settings

Source: https://docs.langchain.com/langsmith/use-tools

Tools allow language models to interact with external systems and perform actions beyond just generating text. In the LangSmith playground, you can use two types of tools:

1. **Built-in tools**: Pre-configured tools provided by model providers (like OpenAI and Anthropic) that are ready to use. These include capabilities like web search, code interpretation, and more.

2. **Custom tools**: Functions you define to perform specific tasks. These are useful when you need to integrate with your own systems or create specialized functionality. When you define custom tools within the LangSmith Playground, you can verify that the model correctly identifies and calls these tools with the correct arguments. Soon we plan to support executing these custom tool calls directly.

* Use **built-in tools** when you need common capabilities like web search or code interpretation. These are built and maintained by the model providers.

* Use **custom tools** when you want to test and validate your own tool designs, including:

* Validating which tools the model chooses to use and seeing the specific arguments it provides in tool calls
  * Simulating tool interactions

The LangSmith Playground has native support for a variety of tools from OpenAI and Anthropic. If you want to use a tool that isn't explicitly listed in the Playground, you can still add it by manually specifying its `type` and any required arguments.

* **Web search**: [Search the web for real-time information](https://platform.openai.com/docs/guides/tools-web-search?api-mode=responses)
* **Image generation**: [Generate images based on a text prompt](https://platform.openai.com/docs/guides/tools-image-generation)
* **MCP**: [Gives the model access to tools hosted on a remote MCP server](https://platform.openai.com/docs/guides/tools-remote-mcp)
* [View all OpenAI tools](https://platform.openai.com/docs/guides/tools?api-mode=responses)

* **Web search**: [Search the web for up-to-date information](https://docs.claude.com/en/docs/agents-and-tools/tool-use/web-search-tool)
* [View all Anthropic tools](https://docs.claude.com/en/docs/agents-and-tools/tool-use/overview)

## Adding and using tools

To add a tool to your prompt, click the `+ Tool` button at the bottom of the prompt editor. <img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-tool.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=b922d9b219cff1bbc726bc2f6b82d6b6" alt="" data-og-width="753" width="753" data-og-height="351" height="351" data-path="langsmith/images/add-tool.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-tool.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=d08cf08da323465b8340f5395f93cb94 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-tool.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=c54feb8a827ef4d7858061f252200fc1 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-tool.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=59fbdea1f87fc501812417a5457ff249 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-tool.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=f850e2a4dacb21cba0939bd017ce2106 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-tool.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=5b40ff17dd48d154c83b5063bab9f673 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-tool.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=6c48a7e9d6c921f206fc9ff777d94ed6 2500w" />

### Use a built-in tool

1. In the tool section, select the built-in tool you want to use. You'll only see the tools that are compatible with the provider and model you've chosen.
2. When the model calls the tool, the playground will display the response

<img src="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/web-search-tool.gif?s=2fb882f785abc26d0e5412557cc982ca" alt="" data-og-width="1036" width="1036" data-og-height="720" height="720" data-path="langsmith/images/web-search-tool.gif" data-optimize="true" data-opv="3" />

### Create a custom tool

To create a custom tool, you'll need to provide:

* Name: A descriptive name for your tool
* Description: Clear explanation of what the tool does
* Arguments: The inputs your tool requires

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/custom-tool.gif?s=69638e515cd7c5be413cf13348e10974" alt="" data-og-width="1028" width="1028" data-og-height="720" height="720" data-path="langsmith/images/custom-tool.gif" data-optimize="true" data-opv="3" />

Note: When running a custom tool in the playground, the model will respond with a JSON object containing the tool name and the tool call. Currently, there's no way to connect this to a hosted tool via MCP.

<img src="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tool-call.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=44458ff5a2790122ffd7e8b62bf14032" alt="" data-og-width="1488" width="1488" data-og-height="747" height="747" data-path="langsmith/images/tool-call.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tool-call.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=add552eba8c2254b2b8631420e8a92ee 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tool-call.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=807f9c730d6386d884a8b263d07de8c3 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tool-call.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=6190625854497044bea5a486f155c1ab 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tool-call.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=eeee3567477764d44857923ff49d4d75 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tool-call.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=ec4aaf0657a0b200d8822b346762d443 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tool-call.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=1c6b56541dd156fdc5c06f3d9fb79648 2500w" />

## Tool choice settings

Some models provide control over which tools are called. To configure this:

1. Go to prompt settings
2. Navigate to tool settings
3. Select tool choice

To understand the available tool choice options, check the documentation for your specific provider. For example, [OpenAI's documentation on tool choice](https://platform.openai.com/docs/guides/function-calling/function-calling-behavior?api-mode=responses#tool-choice).

<img src="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tool-choice.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=8685dbafdab37ed9529f8dbeceab72e6" alt="" data-og-width="942" width="942" data-og-height="867" height="867" data-path="langsmith/images/tool-choice.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tool-choice.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=4d040577bbfae3816d23de7d16b2017d 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tool-choice.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=e8b1220627a1dd668a6c7e66bb8881b2 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tool-choice.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=f99a56b4e513d33c4595b9665e5a3d0e 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tool-choice.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=c62de52814917a4d6f65a05c9b44a670 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tool-choice.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=6d4ff53eb7fa66b94c24e285dc89ba5f 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tool-choice.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=591da66f1402ffa38645cec70ba12e0a 2500w" />

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/use-tools.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Use webhooks

**URL:** llms-txt#use-webhooks

**Contents:**
- Supported endpoints
- Set up your assistant and thread
- Use a webhook with a graph run
- Webhook payload
- Secure webhooks
- Disable webhooks
- Test webhooks

Source: https://docs.langchain.com/langsmith/use-webhooks

Webhooks enable event-driven communication from your LangSmith application to external services. For example, you may want to issue an update to a separate service once an API call to LangSmith has finished running.

Many LangSmith endpoints accept a `webhook` parameter. If this parameter is specified by an endpoint that can accept POST requests, LangSmith will send a request at the completion of a run.

When working with LangSmith, you may want to use webhooks to receive updates after an API call completes. Webhooks are useful for triggering actions in your service once a run has finished processing. To implement this, you need to expose an endpoint that can accept `POST` requests and pass this endpoint as a `webhook` parameter in your API request.

Currently, the SDK does not provide built-in support for defining webhook endpoints, but you can specify them manually using API requests.

## Supported endpoints

The following API endpoints accept a `webhook` parameter:

| Operation            | HTTP Method | Endpoint                          |
| -------------------- | ----------- | --------------------------------- |
| Create Run           | `POST`      | `/thread/{thread_id}/runs`        |
| Create Thread Cron   | `POST`      | `/thread/{thread_id}/runs/crons`  |
| Stream Run           | `POST`      | `/thread/{thread_id}/runs/stream` |
| Wait Run             | `POST`      | `/thread/{thread_id}/runs/wait`   |
| Create Cron          | `POST`      | `/runs/crons`                     |
| Stream Run Stateless | `POST`      | `/runs/stream`                    |
| Wait Run Stateless   | `POST`      | `/runs/wait`                      |

In this guide, we’ll show how to trigger a webhook after streaming a run.

## Set up your assistant and thread

Before making API calls, set up your assistant and thread.

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="JavaScript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

## Use a webhook with a graph run

To use a webhook, specify the `webhook` parameter in your API request. When the run completes, LangSmith sends a `POST` request to the specified webhook URL.

For example, if your server listens for webhook events at `https://my-server.app/my-webhook-endpoint`, include this in your request:

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="JavaScript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

LangSmith sends webhook notifications in the format of a [Run](/langsmith/assistants#execution). See the [API Reference](https://langchain-ai.github.io/langgraph/cloud/reference/api/api_ref.html#tag/assistants) for details. The request payload includes run input, configuration, and other metadata in the `kwargs` field.

To ensure only authorized requests hit your webhook endpoint, consider adding a security token as a query parameter:

Your server should extract and validate this token before processing requests.

As of `langgraph-api>=0.2.78`, developers can disable webhooks in the `langgraph.json` file:

This feature is primarily intended for self-hosted deployments, where platform administrators or developers may prefer to disable webhooks to simplify their security posture—especially if they are not configuring firewall rules or other network controls. Disabling webhooks helps prevent untrusted payloads from being sent to internal endpoints.

For full configuration details, refer to the [configuration file reference](/langsmith/cli?h=disable_webhooks#configuration-file).

You can test your webhook using online services like:

* **[Beeceptor](https://beeceptor.com/)** – Quickly create a test endpoint and inspect incoming webhook payloads.
* **[Webhook.site](https://webhook.site/)** – View, debug, and log incoming webhook requests in real time.

These tools help you verify that LangSmith is correctly triggering and sending webhooks to your service.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/use-webhooks.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
</Tab>

  <Tab title="JavaScript">
```

Example 2 (unknown):
```unknown
</Tab>

  <Tab title="CURL">
```

Example 3 (unknown):
```unknown
</Tab>
</Tabs>

Example response:
```

Example 4 (unknown):
```unknown
## Use a webhook with a graph run

To use a webhook, specify the `webhook` parameter in your API request. When the run completes, LangSmith sends a `POST` request to the specified webhook URL.

For example, if your server listens for webhook events at `https://my-server.app/my-webhook-endpoint`, include this in your request:

<Tabs>
  <Tab title="Python">
```

---

## Use with chat models

**URL:** llms-txt#use-with-chat-models

**Contents:**
  - Text prompts
  - Message prompts
  - Dictionary format
- Message types
  - System Message
  - Human Message
  - AI Message

messages = [system_msg, human_msg]
response = model.invoke(messages)  # Returns AIMessage
python  theme={null}
response = model.invoke("Write a haiku about spring")
python  theme={null}
from langchain.messages import SystemMessage, HumanMessage, AIMessage

messages = [
    SystemMessage("You are a poetry expert"),
    HumanMessage("Write a haiku about spring"),
    AIMessage("Cherry blossoms bloom...")
]
response = model.invoke(messages)
python  theme={null}
messages = [
    {"role": "system", "content": "You are a poetry expert"},
    {"role": "user", "content": "Write a haiku about spring"},
    {"role": "assistant", "content": "Cherry blossoms bloom..."}
]
response = model.invoke(messages)
python Basic instructions theme={null}
system_msg = SystemMessage("You are a helpful coding assistant.")

messages = [
    system_msg,
    HumanMessage("How do I create a REST API?")
]
response = model.invoke(messages)
python Detailed persona theme={null}
from langchain.messages import SystemMessage, HumanMessage

system_msg = SystemMessage("""
You are a senior Python developer with expertise in web frameworks.
Always provide code examples and explain your reasoning.
Be concise but thorough in your explanations.
""")

messages = [
    system_msg,
    HumanMessage("How do I create a REST API?")
]
response = model.invoke(messages)
python Message object theme={null}
  response = model.invoke([
    HumanMessage("What is machine learning?")
  ])
  python String shortcut theme={null}
  # Using a string is a shortcut for a single HumanMessage
  response = model.invoke("What is machine learning?")
  python Add metadata theme={null}
human_msg = HumanMessage(
    content="Hello!",
    name="alice",  # Optional: identify different users
    id="msg_123",  # Optional: unique identifier for tracing
)
python  theme={null}
response = model.invoke("Explain AI")
print(type(response))  # <class 'langchain_core.messages.ai.AIMessage'>
python  theme={null}
from langchain.messages import AIMessage, SystemMessage, HumanMessage

**Examples:**

Example 1 (unknown):
```unknown
### Text prompts

Text prompts are strings - ideal for straightforward generation tasks where you don't need to retain conversation history.
```

Example 2 (unknown):
```unknown
**Use text prompts when:**

* You have a single, standalone request
* You don't need conversation history
* You want minimal code complexity

### Message prompts

Alternatively, you can pass in a list of messages to the model by providing a list of message objects.
```

Example 3 (unknown):
```unknown
**Use message prompts when:**

* Managing multi-turn conversations
* Working with multimodal content (images, audio, files)
* Including system instructions

### Dictionary format

You can also specify messages directly in OpenAI chat completions format.
```

Example 4 (unknown):
```unknown
## Message types

* <Icon icon="gear" size={16} /> [System message](#system-message) - Tells the model how to behave and provide context for interactions
* <Icon icon="user" size={16} /> [Human message](#human-message) - Represents user input and interactions with the model
* <Icon icon="robot" size={16} /> [AI message](#ai-message) - Responses generated by the model, including text content, tool calls, and metadata
* <Icon icon="wrench" size={16} /> [Tool message](#tool-message) - Represents the outputs of [tool calls](/oss/python/langchain/models#tool-calling)

### System Message

A [`SystemMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.SystemMessage) represent an initial set of instructions that primes the model's behavior. You can use a system message to set the tone, define the model's role, and establish guidelines for responses.
```

---

## Using standard tests

**URL:** llms-txt#using-standard-tests

Source: https://docs.langchain.com/oss/javascript/contributing/standard-tests-langgraph

**Standard tests ensure your integration works as expected.**

When creating either a custom class for yourself or to publish in a LangGraph integration, it is important to add standard tests to ensure it works as expected. This guide will show you how to add standard tests to each integration type.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/contributing/standard-tests-langgraph.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Vector stores

**URL:** llms-txt#vector-stores

**Contents:**
- Overview
  - Interface
  - Initialization
  - Adding docuemnts
  - Deleting documents
  - Similarity search
  - Similarity metrics & indexing
  - Metadata filtering
- Top integrations
- All vector stores

Source: https://docs.langchain.com/oss/javascript/integrations/vectorstores/index

A [vector store](/oss/javascript/integrations/vectorstores) stores [embedded](/oss/javascript/integrations/text_embedding) data and performs similarity search.

LangChain provides a unified interface for vector stores, allowing you to:

* `addDocuments` - Add documents to the store.
* `delete` - Remove stored documents by ID.
* `similaritySearch` - Query for semantically similar documents.

This abstraction lets you switch between different implementations without altering your application logic.

Most vectorstores in LangChain accept an embedding model as an argument when initializing the vector store.

You can add documents to the vector store by using the `addDocuments` function.

### Deleting documents

You can delete documents from the vector store by using the `delete` function.

### Similarity search

Issue a semantic query using `similaritySearch`, which returns the closest embedded documents:

Many vector stores support parameters like:

* `k` — number of results to return
* `filter` — conditional filtering based on metadata

### Similarity metrics & indexing

Embedding similarity may be computed using:

* **Cosine similarity**
* **Euclidean distance**
* **Dot product**

Efficient search often employs indexing methods such as HNSW (Hierarchical Navigable Small World), though specifics depend on the vector store.

### Metadata filtering

Filtering by metadata (e.g., source, date) can refine search results:

<important>
  Support for metadata-based filtering varies between implementations.
  Check the documentation of your chosen vector store for details.
</important>

**Select embedding model:**

<AccordionGroup>
  <Accordion title="OpenAI">
    Install dependencies:

Add environment variables:

Instantiate the model:

<Accordion title="Azure">
    Install dependencies

Add environment variables:

Instantiate the model:

<Accordion title="AWS">
    Install dependencies:

Add environment variables:

Instantiate the model:

<Accordion title="Google Gemini">
    Install dependencies:

Add environment variables:

Instantiate the model:

<Accordion title="Google Vertex">
    Install dependencies:

Add environment variables:

Instantiate the model:

<Accordion title="MistralAI">
    Install dependencies:

Add environment variables:

Instantiate the model:

<Accordion title="Cohere">
    Install dependencies:

Add environment variables:

Instantiate the model:

<Accordion title="Ollama">
    Install dependencies:

Instantiate the model:

</Accordion>
</AccordionGroup>

**Select vector store:**

<AccordionGroup>
  <Accordion title="Memory">
    <CodeGroup>

<Accordion title="Chroma">
    <CodeGroup>

<Accordion title="FAISS">
    <CodeGroup>

<Accordion title="MongoDB">
    <CodeGroup>

<Accordion title="PGVector">
    <CodeGroup>

<Accordion title="Pinecone">
    <CodeGroup>

<Accordion title="Qdrant">
    <CodeGroup>

</Accordion>
</AccordionGroup>

LangChain.js integrates with a variety of vector stores. You can check out a full list below:

<Columns cols={3}>
  <Card title="AnalyticDB" icon="link" href="/oss/javascript/integrations/vectorstores/analyticdb" arrow="true" cta="View guide" />

<Card title="Astra DB" icon="link" href="/oss/javascript/integrations/vectorstores/astradb" arrow="true" cta="View guide" />

<Card title="Azion EdgeSQL" icon="link" href="/oss/javascript/integrations/vectorstores/azion-edgesql" arrow="true" cta="View guide" />

<Card title="Azure AI Search" icon="link" href="/oss/javascript/integrations/vectorstores/azure_aisearch" arrow="true" cta="View guide" />

<Card title="Azure Cosmos DB for MongoDB vCore" icon="link" href="/oss/javascript/integrations/vectorstores/azure_cosmosdb_mongodb" arrow="true" cta="View guide" />

<Card title="Azure Cosmos DB for NoSQL" icon="link" href="/oss/javascript/integrations/vectorstores/azure_cosmosdb_nosql" arrow="true" cta="View guide" />

<Card title="Cassandra" icon="link" href="/oss/javascript/integrations/vectorstores/cassandra" arrow="true" cta="View guide" />

<Card title="Chroma" icon="link" href="/oss/javascript/integrations/vectorstores/chroma" arrow="true" cta="View guide" />

<Card title="ClickHouse" icon="link" href="/oss/javascript/integrations/vectorstores/clickhouse" arrow="true" cta="View guide" />

<Card title="CloseVector" icon="link" href="/oss/javascript/integrations/vectorstores/closevector" arrow="true" cta="View guide" />

<Card title="Cloudflare Vectorize" icon="link" href="/oss/javascript/integrations/vectorstores/cloudflare_vectorize" arrow="true" cta="View guide" />

<Card title="Convex" icon="link" href="/oss/javascript/integrations/vectorstores/convex" arrow="true" cta="View guide" />

<Card title="Couchbase Query" icon="link" href="/oss/javascript/integrations/vectorstores/couchbase_query" arrow="true" cta="View guide" />

<Card title="Couchbase Search" icon="link" href="/oss/javascript/integrations/vectorstores/couchbase_search" arrow="true" cta="View guide" />

<Card title="Elasticsearch" icon="link" href="/oss/javascript/integrations/vectorstores/elasticsearch" arrow="true" cta="View guide" />

<Card title="Faiss" icon="link" href="/oss/javascript/integrations/vectorstores/faiss" arrow="true" cta="View guide" />

<Card title="Google Cloud SQL for PostgreSQL" icon="link" href="/oss/javascript/integrations/vectorstores/google_cloudsql_pg" arrow="true" cta="View guide" />

<Card title="Google Vertex AI Matching Engine" icon="link" href="/oss/javascript/integrations/vectorstores/googlevertexai" arrow="true" cta="View guide" />

<Card title="SAP HANA Cloud Vector Engine" icon="link" href="/oss/javascript/integrations/vectorstores/hanavector" arrow="true" cta="View guide" />

<Card title="HNSWLib" icon="link" href="/oss/javascript/integrations/vectorstores/hnswlib" arrow="true" cta="View guide" />

<Card title="LanceDB" icon="link" href="/oss/javascript/integrations/vectorstores/lancedb" arrow="true" cta="View guide" />

<Card title="libSQL" icon="link" href="/oss/javascript/integrations/vectorstores/libsql" arrow="true" cta="View guide" />

<Card title="MariaDB" icon="link" href="/oss/javascript/integrations/vectorstores/mariadb" arrow="true" cta="View guide" />

<Card title="In-memory" icon="link" href="/oss/javascript/integrations/vectorstores/memory" arrow="true" cta="View guide" />

<Card title="Milvus" icon="link" href="/oss/javascript/integrations/vectorstores/milvus" arrow="true" cta="View guide" />

<Card title="Momento Vector Index (MVI)" icon="link" href="/oss/javascript/integrations/vectorstores/momento_vector_index" arrow="true" cta="View guide" />

<Card title="MongoDB Atlas" icon="link" href="/oss/javascript/integrations/vectorstores/mongodb_atlas" arrow="true" cta="View guide" />

<Card title="MyScale" icon="link" href="/oss/javascript/integrations/vectorstores/myscale" arrow="true" cta="View guide" />

<Card title="Neo4j Vector Index" icon="link" href="/oss/javascript/integrations/vectorstores/neo4jvector" arrow="true" cta="View guide" />

<Card title="Neon Postgres" icon="link" href="/oss/javascript/integrations/vectorstores/neon" arrow="true" cta="View guide" />

<Card title="OpenSearch" icon="link" href="/oss/javascript/integrations/vectorstores/opensearch" arrow="true" cta="View guide" />

<Card title="PGVector" icon="link" href="/oss/javascript/integrations/vectorstores/pgvector" arrow="true" cta="View guide" />

<Card title="Pinecone" icon="link" href="/oss/javascript/integrations/vectorstores/pinecone" arrow="true" cta="View guide" />

<Card title="Prisma" icon="link" href="/oss/javascript/integrations/vectorstores/prisma" arrow="true" cta="View guide" />

<Card title="Qdrant" icon="link" href="/oss/javascript/integrations/vectorstores/qdrant" arrow="true" cta="View guide" />

<Card title="Redis" icon="link" href="/oss/javascript/integrations/vectorstores/redis" arrow="true" cta="View guide" />

<Card title="Rockset" icon="link" href="/oss/javascript/integrations/vectorstores/rockset" arrow="true" cta="View guide" />

<Card title="SingleStore" icon="link" href="/oss/javascript/integrations/vectorstores/singlestore" arrow="true" cta="View guide" />

<Card title="Supabase" icon="link" href="/oss/javascript/integrations/vectorstores/supabase" arrow="true" cta="View guide" />

<Card title="Tigris" icon="link" href="/oss/javascript/integrations/vectorstores/tigris" arrow="true" cta="View guide" />

<Card title="Turbopuffer" icon="link" href="/oss/javascript/integrations/vectorstores/turbopuffer" arrow="true" cta="View guide" />

<Card title="TypeORM" icon="link" href="/oss/javascript/integrations/vectorstores/typeorm" arrow="true" cta="View guide" />

<Card title="Typesense" icon="link" href="/oss/javascript/integrations/vectorstores/typesense" arrow="true" cta="View guide" />

<Card title="Upstash Vector" icon="link" href="/oss/javascript/integrations/vectorstores/upstash" arrow="true" cta="View guide" />

<Card title="USearch" icon="link" href="/oss/javascript/integrations/vectorstores/usearch" arrow="true" cta="View guide" />

<Card title="Vectara" icon="link" href="/oss/javascript/integrations/vectorstores/vectara" arrow="true" cta="View guide" />

<Card title="Vercel Postgres" icon="link" href="/oss/javascript/integrations/vectorstores/vercel_postgres" arrow="true" cta="View guide" />

<Card title="Voy" icon="link" href="/oss/javascript/integrations/vectorstores/voy" arrow="true" cta="View guide" />

<Card title="Weaviate" icon="link" href="/oss/javascript/integrations/vectorstores/weaviate" arrow="true" cta="View guide" />

<Card title="Xata" icon="link" href="/oss/javascript/integrations/vectorstores/xata" arrow="true" cta="View guide" />

<Card title="Zep Open Source" icon="link" href="/oss/javascript/integrations/vectorstores/zep" arrow="true" cta="View guide" />

<Card title="Zep Cloud" icon="link" href="/oss/javascript/integrations/vectorstores/zep_cloud" arrow="true" cta="View guide" />
</Columns>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/javascript/integrations/vectorstores/index.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
### Interface

LangChain provides a unified interface for vector stores, allowing you to:

* `addDocuments` - Add documents to the store.
* `delete` - Remove stored documents by ID.
* `similaritySearch` - Query for semantically similar documents.

This abstraction lets you switch between different implementations without altering your application logic.

### Initialization

Most vectorstores in LangChain accept an embedding model as an argument when initializing the vector store.
```

Example 2 (unknown):
```unknown
### Adding docuemnts

You can add documents to the vector store by using the `addDocuments` function.
```

Example 3 (unknown):
```unknown
### Deleting documents

You can delete documents from the vector store by using the `delete` function.
```

Example 4 (unknown):
```unknown
### Similarity search

Issue a semantic query using `similaritySearch`, which returns the closest embedded documents:
```

---

## Verify the variables are set

**URL:** llms-txt#verify-the-variables-are-set

**Contents:**
  - Initial export

echo "Customer ID: $CUSTOMER_ID"
echo "Customer Name: $CUSTOMER_NAME"
bash  theme={null}
curl -s $LANGSMITH_URL/api/v1/info
export CUSTOMER_ID="<id>"
export CUSTOMER_NAME="<name>"
bash  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
You can then use these environment variables in your export scripts or other commands.

If you don't have `jq`, run these commands to set the environment variables based on the curl output:
```

Example 2 (unknown):
```unknown
### Initial export

These scripts export usage data to a CSV for reporting to LangChain. They additionally track the export by assigning a backfill ID and timestamp.

To export LangSmith trace usage:
```

---

## Versioning

**URL:** llms-txt#versioning

**Contents:**
- Version numbering
- API stability
  - Stable APIs
  - Beta APIs
  - Alpha APIs
  - Deprecated APIs
  - Internal APIs
- Release cycles
- Version support policy
  - Long-term support (LTS) releases

Source: https://docs.langchain.com/oss/javascript/versioning

Each LangChain and LangGraph version number follows the format: `MAJOR.MINOR.PATCH`

* **Major**: Breaking API updates that require code changes.
* **Minor**: New features and improvements that maintain backward compatibility.
* **Patch**: Bug fixes and minor improvements.

LangChain and LangGraph follow [Semantic Versioning](https://semver.org/) principles:

* `1.0.0`: First stable release with production-ready APIs
* `1.1.0`: New features added in a backward-compatible manner
* `1.0.1`: Backward-compatible bug fixes

We communicate the stability of our APIs as follows:

All APIs without special prefixes are considered stable and ready for production use. We maintain backward compatibility for stable features and only introduce breaking changes in major releases.

APIs marked as `beta` are feature-complete but may undergo minor changes based on user feedback. They are safe for production use but may require small adjustments in future releases.

APIs marked as `alpha` are experimental and subject to significant changes. Use these with caution in production environments.

APIs marked as `deprecated` will be removed in future major releases. When possible, we specify the intended version of removal. To handle deprecations:

1. Switch to the recommended alternative API
2. Follow the migration guide (released alongside major releases)
3. Use automated migration tools when available

Certain APIs are explicitly marked as "internal" in a couple of ways:

* Some documentation refers to internals and mentions them as such. If the documentation says that something is internal, it may change.
* Functions, methods, and other objects prefixed by a leading underscore (**`_`**). This is the standard Python convention of indicating that something is private; if any method starts with a single **`_`**, it's an internal API.
  * **Exception:** Certain methods are prefixed with `_` , but do not contain an implementation. These methods are *meant* to be overridden by sub-classes that provide the implementation. Such methods are generally part of the **Public API** of LangChain.

<AccordionGroup>
  <Accordion title="Major releases">
    Major releases (e.g., `1.0.0` → `2.0.0`) may include:

* Breaking API changes
    * Removal of deprecated features
    * Significant architectural improvements

* Detailed migration guides
    * Automated migration tools when possible
    * Extended support period for the previous major version
  </Accordion>

<Accordion title="Minor releases">
    Minor releases (e.g., `1.0.0` → `1.1.0`) include:

* New features and capabilities
    * Performance improvements
    * New optional parameters
    * Backward-compatible enhancements
  </Accordion>

<Accordion title="Patch releases">
    Patch releases (e.g., `1.0.0` → `1.0.1`) include:

* Bug fixes
    * Security updates
    * Documentation improvements
    * Performance optimizations without API changes
  </Accordion>
</AccordionGroup>

## Version support policy

* **Latest major version**: Full support with active development (ACTIVE status)
* **Previous major version**: Security updates and critical bug fixes for 12 months after the next major release (MAINTENANCE status)
* **Older versions**: Community support only

### Long-term support (LTS) releases

Both LangChain and LangGraph 1.0 are designated as LTS releases:

* Version 1.0 will remain in ACTIVE status until version 2.0 is released
* After version 2.0 is released, version 1.0 will enter MAINTENANCE mode for at least 1 year
* LTS releases follow semantic versioning (semver), allowing safe upgrades between minor versions
* Legacy versions (LangChain 0.3 and LangGraph 0.4) are in MAINTENANCE mode until December 2026

For detailed information about release status and support timelines, see the [Release policy](/oss/javascript/release-policy).

## Check your version

To check your installed version:

## Pre-release versions

We occasionally release alpha and beta versions for early testing:

* **Alpha** (e.g., `1.0.0a1`): Early preview, significant changes expected
* **Beta** (e.g., `1.0.0b1`): Feature-complete, minor changes possible
* **Release Candidate** (e.g., `1.0.0rc1`): Final testing before stable release

* [Release policy](/oss/javascript/release-policy) - Detailed release and deprecation policies
* [Releases](/oss/javascript/releases) - Version-specific release notes and migration guides

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/versioning.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

## Upgrade

<CodeGroup>
```

Example 3 (unknown):
```unknown

```

---

## View server logs for a trace

**URL:** llms-txt#view-server-logs-for-a-trace

**Contents:**
- Access server logs from trace view
- Server logs view
- Filtering logs by trace ID

Source: https://docs.langchain.com/langsmith/platform-logs

When viewing a trace that was generated by a run in LangSmith, you can access the associated server logs directly from the trace view.

<Note>
  Viewing server logs for a trace only works with the [Cloud SaaS](https://langchain-ai.github.io/langgra/langsmith/observability-concepts/deployment_options/#cloud-saas) and [fully self-hosted](https://langchain-ai.github.io/langgra/langsmith/observability-concepts/deployment_options/#self-hosted-control-plane) deployment options.
</Note>

## Access server logs from trace view

In the trace view, use the **See Logs** button in the top right corner, next to the **Run in Studio** button.

<img src="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-server-logs-button.png?fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=87e85d7f38169b5259ba9b335aa89d2f" alt="" data-og-width="1595" width="1595" data-og-height="821" height="821" data-path="langsmith/images/view-server-logs-button.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-server-logs-button.png?w=280&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=d70119d610bea0b05196653a2ab13cdd 280w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-server-logs-button.png?w=560&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=2b395732952d78a269e9fa776b102465 560w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-server-logs-button.png?w=840&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=b69563779a026c19efed909fc1fc8cd3 840w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-server-logs-button.png?w=1100&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=13b31e6677cc41c18f5f7d6826489423 1100w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-server-logs-button.png?w=1650&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=3ea7803aa4ee13332c14809b953bb03a 1650w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-server-logs-button.png?w=2500&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=f260eb50eae4ee0e5c094391fa180f5c 2500w" />

Clicking this button will take you to the server logs view for the associated deployment in LangSmith.

The server logs view displays logs from both:

* **Agent Server's own operational logs** - Internal server operations, API calls, and system events
* **User application logs** - Logs written in your graph with:
  * Python: Use the `logging` or `structlog` libraries
  * JavaScript: Use the re-exported Winston logger from `@langchain/langgraph-sdk/logging`:

## Filtering logs by trace ID

When you navigate from the trace view, the **Filters** box will automatically pre-fill with the Trace ID from the trace you just viewed.

This allows you to quickly filter the logs to see only those related to your specific trace execution.

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/lgp-server-logs-filters.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=cb6f3c15ca3c8d462ee10c5fd190c73e" alt="" data-og-width="1348" width="1348" data-og-height="681" height="681" data-path="langsmith/images/lgp-server-logs-filters.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/lgp-server-logs-filters.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=8eecae6d552d9e58b4b59949afe5cdf4 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/lgp-server-logs-filters.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=543817c9df78b3fa98d063956189ea6f 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/lgp-server-logs-filters.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=16989edf602b9f025464c0cd7b83bfa5 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/lgp-server-logs-filters.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=5f109fbba34ebd01b856baf6be0d4d18 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/lgp-server-logs-filters.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=d456efb3eb004361f2d2a8c7c9d99e34 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/lgp-server-logs-filters.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=fb86476afe8c3f573bdb3a80d51075aa 2500w" />

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/platform-logs.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## View trace counts across your organization

**URL:** llms-txt#view-trace-counts-across-your-organization

**Contents:**
- Programmatically fetch trace counts
  - Method 1: Use the LangSmith REST API
  - Method 2: Use PostgreSQL support queries

Source: https://docs.langchain.com/langsmith/self-host-organization-charts

<Note>
  This feature is available on Helm chart versions 0.9.5 and later.
</Note>

LangSmith automatically generates and syncs organization usage charts for self-hosted installations.

These charts are available under `Settings > Usage and billing > Usage graph`:

* Usage by Workspace: this counts traces (root runs) by workspace
* Organization Usage: this counts all traces (root runs) for the organization

The charts are refreshed to include any new workspaces every 5 minutes. Note that the charts are not editable.

## Programmatically fetch trace counts

You can retrieve trace counts programmatically using two different methods:

### Method 1: Use the LangSmith REST API

If your self-hosted installation uses an online key, you can use the [LangSmith REST API](https://api.smith.langchain.com/redoc?_gl=1*w68t81*_gcl_au*MTgyNTQ5MDUxNy4xNzU2NzI3MDky*_ga*MTU3NDY5MzQyNC4xNzQyOTMyMTQ2*_ga_47WX3HKKY2*czE3NTgyMDAxMDAkbzM0MSRnMCR0MTc1ODIwMDEwMCRqNjAkbDAkaDA.#tag/orgs/operation/get_org_usage_api_v1_orgs_current_billing_usage_get) to fetch organization usage data.

### Method 2: Use PostgreSQL support queries

For installations using offline keys or when you need more detailed export capabilities, you can run support queries directly against the PostgreSQL database. All available scripts are in the [support queries repository](https://github.com/langchain-ai/helm/tree/main/charts/langsmith/scripts/support_queries/postgres).

For more detailed information about running support queries, see the [Run support queries against PostgreSQL](/langsmith/script-running-pg-support-queries) guide.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/self-host-organization-charts.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
### Method 2: Use PostgreSQL support queries

For installations using offline keys or when you need more detailed export capabilities, you can run support queries directly against the PostgreSQL database. All available scripts are in the [support queries repository](https://github.com/langchain-ai/helm/tree/main/charts/langsmith/scripts/support_queries/postgres).
```

---

## We'll use structured outputs to enforce that the model returns only

**URL:** llms-txt#we'll-use-structured-outputs-to-enforce-that-the-model-returns-only

---

## we can add them as nodes directly.

**URL:** llms-txt#we-can-add-them-as-nodes-directly.

**Contents:**
- Evaluations
  - Final response evaluator

graph_builder.add_node("refund_agent", refund_graph)
graph_builder.add_node("question_answering_agent", qa_graph)
graph_builder.add_node(compile_followup)

graph_builder.set_entry_point("intent_classifier")
graph_builder.add_edge("refund_agent", "compile_followup")
graph_builder.add_edge("question_answering_agent", "compile_followup")
graph_builder.add_edge("compile_followup", END)

graph = graph_builder.compile()
python  theme={null}
display(Image(graph.get_graph().draw_mermaid_png()))
python  theme={null}
state = await graph.ainvoke(
    {"messages": [{"role": "user", "content": "what james brown songs do you have"}]}
)
print(state["followup"])

I found 20 James Brown songs in the database, all from the album "Sex Machine". Here they are: ...
python  theme={null}
state = await graph.ainvoke({"messages": [
    {
        "role": "user",
        "content": "my name is Aaron Mitchell and my number is +1 (204) 452-6452. I bought some songs by Led Zeppelin that i'd like refunded",
    }
]})
print(state["followup"])

Which of the following purchases would you like to be refunded for? ...
python  theme={null}
from langsmith import Client

**Examples:**

Example 1 (unknown):
```unknown
We can visualize our compiled parent graph including all of its subgraphs:
```

Example 2 (unknown):
```unknown
<img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/agent-tutorial-graph.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=619f9b540ea69b1662b2a599ce78241b" alt="graph" data-og-width="646" width="646" data-og-height="680" height="680" data-path="langsmith/images/agent-tutorial-graph.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/agent-tutorial-graph.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=227790d90780a4c56233650b957130df 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/agent-tutorial-graph.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=30ae6a9b1bc367152a57d4a0c3e41af7 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/agent-tutorial-graph.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=37f29b6e783cf2a80714c29ab0be3c5f 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/agent-tutorial-graph.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=423ad48e0266ac257b6d76962697b45d 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/agent-tutorial-graph.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=581821d6b377b98108507712d6b08c51 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/agent-tutorial-graph.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=126ef194ff5042f691c8f52cf3a1cb75 2500w" />

#### Try it out

Let's give our custom support agent a whirl!
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown

```

---

## We need this because we want to enable threads (conversations)

**URL:** llms-txt#we-need-this-because-we-want-to-enable-threads-(conversations)

checkpointer = InMemorySaver()

---

## We need to pass the `tool_call_id` to the sub agent so it can use it to respond with the tool call result

**URL:** llms-txt#we-need-to-pass-the-`tool_call_id`-to-the-sub-agent-so-it-can-use-it-to-respond-with-the-tool-call-result

def call_subagent1(
    query: str,
    tool_call_id: Annotated[str, InjectedToolCallId],

---

## We now add a conditional edge

**URL:** llms-txt#we-now-add-a-conditional-edge

workflow.add_conditional_edges(
    # First, we define the start node. We use 'agent'.
    # This means these are the edges taken after the 'agent' node is called.
    "agent",
    # Next, we pass in the function that will determine which node is called next.
    should_continue,
)

---

## We now add a normal edge from 'tools' to 'agent'.

**URL:** llms-txt#we-now-add-a-normal-edge-from-'tools'-to-'agent'.

---

## We set up a `secret` query parameter

**URL:** llms-txt#we-set-up-a-`secret`-query-parameter

**Contents:**
  - Hooking it up

def f(data: dict, secret: str = Query(...)):
    # You can import dependencies you don't have locally inside Modal functions
    from langsmith import Client

# First, we validate the secret key we pass
    import os

if secret != os.environ["LS_WEBHOOK"]:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Incorrect bearer token",
            headers={"WWW-Authenticate": "Bearer"},
        )

# This is where we put the logic for what should happen inside this webhook
    ls_client = Client()
    runs = data["runs"]
    ids = [r["id"] for r in runs]
    feedback = list(ls_client.list_feedback(run_ids=ids))
    for r, f in zip(runs, feedback):
        try:
            ls_client.create_example(
                inputs=r["inputs"],
                outputs={"output": f.correction},
                dataset_name="classifier-github-issues",
            )
        except Exception:
            raise ValueError(f"{r} and {f}")
    # Function body
    return "success!"

✓ Created objects.
├── 🔨 Created mount /Users/harrisonchase/workplace/langsmith-docs/example-webhook.py
├── 🔨 Created mount PythonPackage:langsmith
└── 🔨 Created f => https://hwchase17--auth-example-f.modal.run
✓ App deployed! 🎉

View Deployment: https://modal.com/apps/hwchase17/auth-example

https://hwchase17--auth-example-f-dev.modal.run?secret={SECRET}
```

Replace `{SECRET}` with the secret key you created to access the Modal service.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/webhooks.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
We can now deploy this easily with `modal deploy ...` (see docs [here](https://modal.com/docs/guide/managing-deployments)).

You should now get something like:
```

Example 2 (unknown):
```unknown
The important thing to remember is `https://hwchase17--auth-example-f.modal.run` - the function we created to run.
NOTE: this is NOT the final deployment URL, make sure not to accidentally use that.

### Hooking it up

We can now take the function URL we create above and add it as a webhook.
We have to remember to also pass in the secret key as a query parameter.
Putting it all together, it should look something like:
```

---

## We strongly recommend setting up a replicated clickhouse cluster for this load.

**URL:** llms-txt#we-strongly-recommend-setting-up-a-replicated-clickhouse-cluster-for-this-load.

---

## We use LCEL declarative syntax here.

**URL:** llms-txt#we-use-lcel-declarative-syntax-here.

---

## We want this to be a `POST` endpoint since we will post data here

**URL:** llms-txt#we-want-this-to-be-a-`post`-endpoint-since-we-will-post-data-here

@web_endpoint(method="POST")

---

## We will use GPT-3.5 Turbo as the baseline and compare against GPT-4o

**URL:** llms-txt#we-will-use-gpt-3.5-turbo-as-the-baseline-and-compare-against-gpt-4o

gpt_3_5_turbo = init_chat_model(
    "gpt-3.5-turbo",
    temperature=1,
    configurable_fields=("model", "model_provider"),
)

---

## What's new in v1

**URL:** llms-txt#what's-new-in-v1

**Contents:**
- `createAgent`
  - Middleware
  - Built on LangGraph
  - Structured output
- Standard content blocks
  - Benefits
- Simplified package
  - `@langchain/classic`
- Reporting issues
- Additional resources

Source: https://docs.langchain.com/oss/javascript/releases/langchain-v1

**LangChain v1 is a focused, production-ready foundation for building agents.** We've streamlined the framework around three core improvements:

<CardGroup cols={1}>
  <Card title="createAgent" icon="robot" href="#create-agent" arrow>
    A new standard way to build agents in LangChain, replacing `createReactAgent` from LangGraph with a cleaner, more powerful API.
  </Card>

<Card title="Standard content blocks" icon="cube" href="#standard-content-blocks" arrow>
    A new `contentBlocks` property that provides unified access to modern LLM features across all providers.
  </Card>

<Card title="Simplified package" icon="sitemap" href="#simplified-package" arrow>
    The `langchain` package has been streamlined to focus on essential building blocks for agents, with legacy functionality moved to `@langchain/classic`.
  </Card>
</CardGroup>

For a complete list of changes, see the [migration guide](/oss/javascript/migrate/langchain-v1).

`createAgent` is the standard way to build agents in LangChain 1.0. It provides a simpler interface than the prebuilt `createReactAgent` exported from LangGraph while offering greater customization potential by using middleware.

Under the hood, `createAgent` is built on the basic agent loop -- calling a model, letting it choose tools to execute, and then finishing when it calls no more tools:

<div style={{ display: "flex", justifyContent: "center" }}>
  <img src="https://mintcdn.com/langchain-5e9cc07a/Tazq8zGc0yYUYrDl/oss/images/core_agent_loop.png?fit=max&auto=format&n=Tazq8zGc0yYUYrDl&q=85&s=ac72e48317a9ced68fd1be64e89ec063" alt="Core agent loop diagram" className="rounded-lg" data-og-width="300" width="300" data-og-height="268" height="268" data-path="oss/images/core_agent_loop.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Tazq8zGc0yYUYrDl/oss/images/core_agent_loop.png?w=280&fit=max&auto=format&n=Tazq8zGc0yYUYrDl&q=85&s=a4c4b766b6678ef52a6ed556b1a0b032 280w, https://mintcdn.com/langchain-5e9cc07a/Tazq8zGc0yYUYrDl/oss/images/core_agent_loop.png?w=560&fit=max&auto=format&n=Tazq8zGc0yYUYrDl&q=85&s=111869e6e99a52c0eff60a1ef7ddc49c 560w, https://mintcdn.com/langchain-5e9cc07a/Tazq8zGc0yYUYrDl/oss/images/core_agent_loop.png?w=840&fit=max&auto=format&n=Tazq8zGc0yYUYrDl&q=85&s=6c1e21de7b53bd0a29683aca09c6f86e 840w, https://mintcdn.com/langchain-5e9cc07a/Tazq8zGc0yYUYrDl/oss/images/core_agent_loop.png?w=1100&fit=max&auto=format&n=Tazq8zGc0yYUYrDl&q=85&s=88bef556edba9869b759551c610c60f4 1100w, https://mintcdn.com/langchain-5e9cc07a/Tazq8zGc0yYUYrDl/oss/images/core_agent_loop.png?w=1650&fit=max&auto=format&n=Tazq8zGc0yYUYrDl&q=85&s=9b0bdd138e9548eeb5056dc0ed2d4a4b 1650w, https://mintcdn.com/langchain-5e9cc07a/Tazq8zGc0yYUYrDl/oss/images/core_agent_loop.png?w=2500&fit=max&auto=format&n=Tazq8zGc0yYUYrDl&q=85&s=41eb4f053ed5e6b0ba5bad2badf6d755 2500w" />
</div>

For more information, see [Agents](/oss/javascript/langchain/agents).

Middleware is the defining feature of `createAgent`. It makes `createAgent` highly customizable, raising the ceiling for what you can build.

Great agents require [context engineering](/oss/javascript/langchain/context-engineering): getting the right information to the model at the right time. Middleware helps you control dynamic prompts, conversation summarization, selective tool access, state management, and guardrails through a composable abstraction.

#### Prebuilt middleware

LangChain provides a few [prebuilt middlewares](/oss/javascript/langchain/middleware#built-in-middleware) for common patterns, including:

* `summarizationMiddleware`: Condense conversation history when it gets too long
* `humanInTheLoopMiddleware`: Require approval for sensitive tool calls
* `piiRedactionMiddleware`: Redact sensitive information before sending to the model

#### Custom middleware

You can also build custom middleware to fit your specific needs.

Build custom middleware by implementing any of these hooks using the `createMiddleware` function:

| Hook            | When it runs             | Use cases                               |
| --------------- | ------------------------ | --------------------------------------- |
| `beforeAgent`   | Before calling the agent | Load memory, validate input             |
| `beforeModel`   | Before each LLM call     | Update prompts, trim messages           |
| `wrapModelCall` | Around each LLM call     | Intercept and modify requests/responses |
| `wrapToolCall`  | Around each tool call    | Intercept and modify tool execution     |
| `afterModel`    | After each LLM response  | Validate output, apply guardrails       |
| `afterAgent`    | After agent completes    | Save results, cleanup                   |

<div style={{ display: "flex", justifyContent: "center" }}>
  <img src="https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=eb4404b137edec6f6f0c8ccb8323eaf1" alt="Middleware flow diagram" className="rounded-lg" data-og-width="500" width="500" data-og-height="560" height="560" data-path="oss/images/middleware_final.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?w=280&fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=483413aa87cf93323b0f47c0dd5528e8 280w, https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?w=560&fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=41b7dd647447978ff776edafe5f42499 560w, https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?w=840&fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=e9b14e264f68345de08ae76f032c52d4 840w, https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?w=1100&fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=ec45e1932d1279b1beee4a4b016b473f 1100w, https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?w=1650&fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=3bca5ebf8aa56632b8a9826f7f112e57 1650w, https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?w=2500&fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=437f141d1266f08a95f030c2804691d9 2500w" />
</div>

Example custom middleware:

For more information, see [the complete middleware guide](/oss/javascript/langchain/middleware).

### Built on LangGraph

Because `createAgent` is built on LangGraph, you automatically get built in support for long running and reliable agents via:

<CardGroup cols={2}>
  <Card title="Persistence" icon="database">
    Conversations automatically persist across sessions with built-in checkpointing
  </Card>

<Card title="Streaming" icon="water">
    Stream tokens, tool calls, and reasoning traces in real-time
  </Card>

<Card title="Human-in-the-loop" icon="hand">
    Pause agent execution for human approval before sensitive actions
  </Card>

<Card title="Time travel" icon="clock-rotate-left">
    Rewind conversations to any point and explore alternate paths and prompts
  </Card>
</CardGroup>

You don't need to learn LangGraph to use these features—they work out of the box.

### Structured output

`createAgent` has improved structured output generation:

* **Main loop integration**: Structured output is now generated in the main loop instead of requiring an additional LLM call
* **Structured output strategy**: Models can choose between calling tools or using provider-side structured output generation
* **Cost reduction**: Eliminates extra expense from additional LLM calls

**Error handling**: Control error handling via the `handleErrors` parameter to `ToolStrategy`:

* **Parsing errors**: Model generates data that doesn't match desired structure
* **Multiple tool calls**: Model generates 2+ tool calls for structured output schemas

## Standard content blocks

<Note>
  1.0 releases are available for most packages. Only the following currently support new content blocks:

* `langchain`
  * `@langchain/core`
  * `@langchain/anthropic`
  * `@langchain/openai`

Broader support for content blocks is planned.
</Note>

* **Provider agnostic**: Access reasoning traces, citations, built-in tools (web search, code interpreters, etc.), and other features using the same API regardless of provider
* **Type safe**: Full type hints for all content block types
* **Backward compatible**: Standard content can be [loaded lazily](/oss/javascript/langchain/messages#standard-content-blocks), so there are no associated breaking changes

For more information, see our guide on [content blocks](/oss/javascript/langchain/messages#message-content)

## Simplified package

LangChain v1 streamlines the `langchain` package namespace to focus on essential building blocks for agents. The package exposes only the most useful and relevant functionality:

Most of these are re-exported from `@langchain/core` for convenience, which gives you a focused API surface for building agents.

### `@langchain/classic`

Legacy functionality has moved to [`@langchain/classic`](https://www.npmjs.com/package/@langchain/classic) to keep the core package lean and focused.

#### What's in `@langchain/classic`

* Legacy chains and chain implementations
* Retrievers
* The indexing API
* [`@langchain/community`](https://www.npmjs.com/package/@langchain/community) exports
* Other deprecated functionality

If you use any of this functionality, install [`@langchain/classic`](https://www.npmjs.com/package/@langchain/classic):

Then update your imports:

Please report any issues discovered with 1.0 on [GitHub](https://github.com/langchain-ai/langchainjs/issues) using the [`'v1'` label](https://github.com/langchain-ai/langchainjs/issues?q=state%3Aopen%20label%3Av1).

## Additional resources

<CardGroup cols={3}>
  <Card title="LangChain 1.0" icon="rocket" href="https://blog.langchain.com/langchain-langchain-1-0-alpha-releases/">
    Read the announcement
  </Card>

<Card title="Middleware Guide" icon="puzzle-piece" href="https://blog.langchain.com/agent-middleware/">
    Deep dive into middleware
  </Card>

<Card title="Agents Documentation" icon="book" href="/oss/javascript/langchain/agents" arrow>
    Full agent documentation
  </Card>

<Card title="Message Content" icon="message" href="/oss/javascript/langchain/messages#message-content" arrow>
    New content blocks API
  </Card>

<Card title="Migration guide" icon="arrow-right-arrow-left" href="/oss/javascript/migrate/langchain-v1" arrow>
    How to migrate to LangChain v1
  </Card>

<Card title="GitHub" icon="github" href="https://github.com/langchain-ai/langchainjs">
    Report issues or contribute
  </Card>
</CardGroup>

* [Versioning](/oss/javascript/versioning) - Understanding version numbers
* [Release policy](/oss/javascript/release-policy) - Detailed release policies

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/javascript/releases/langchain-v1.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>

For a complete list of changes, see the [migration guide](/oss/javascript/migrate/langchain-v1).

## `createAgent`

`createAgent` is the standard way to build agents in LangChain 1.0. It provides a simpler interface than the prebuilt `createReactAgent` exported from LangGraph while offering greater customization potential by using middleware.
```

---

## When user provides PII, it will be handled according to the strategy

**URL:** llms-txt#when-user-provides-pii,-it-will-be-handled-according-to-the-strategy

**Contents:**
  - Human-in-the-loop

result = agent.invoke({
    "messages": [{"role": "user", "content": "My email is john.doe@example.com and card is 4532-1234-5678-9010"}]
})
python  theme={null}
from langchain.agents import create_agent
from langchain.agents.middleware import HumanInTheLoopMiddleware
from langgraph.checkpoint.memory import InMemorySaver
from langgraph.types import Command

agent = create_agent(
    model="gpt-4o",
    tools=[search_tool, send_email_tool, delete_database_tool],
    middleware=[
        HumanInTheLoopMiddleware(
            interrupt_on={
                # Require approval for sensitive operations
                "send_email": True,
                "delete_database": True,
                # Auto-approve safe operations
                "search": False,
            }
        ),
    ],
    # Persist the state across interrupts
    checkpointer=InMemorySaver(),
)

**Examples:**

Example 1 (unknown):
```unknown
<Accordion title="Built-in PII types and configuration">
  **Built-in PII types:**

  * `email` - Email addresses
  * `credit_card` - Credit card numbers (Luhn validated)
  * `ip` - IP addresses
  * `mac_address` - MAC addresses
  * `url` - URLs

  **Configuration options:**

  | Parameter               | Description                                                            | Default                |
  | ----------------------- | ---------------------------------------------------------------------- | ---------------------- |
  | `pii_type`              | Type of PII to detect (built-in or custom)                             | Required               |
  | `strategy`              | How to handle detected PII (`"block"`, `"redact"`, `"mask"`, `"hash"`) | `"redact"`             |
  | `detector`              | Custom detector function or regex pattern                              | `None` (uses built-in) |
  | `apply_to_input`        | Check user messages before model call                                  | `True`                 |
  | `apply_to_output`       | Check AI messages after model call                                     | `False`                |
  | `apply_to_tool_results` | Check tool result messages after execution                             | `False`                |
</Accordion>

See the [middleware documentation](/oss/python/langchain/middleware#pii-detection) for complete details on PII detection capabilities.

### Human-in-the-loop

LangChain provides built-in middleware for requiring human approval before executing sensitive operations. This is one of the most effective guardrails for high-stakes decisions.

Human-in-the-loop middleware is helpful for cases such as financial transactions and transfers, deleting or modifying production data, sending communications to external parties, and any operation with significant business impact.
```

---

## where message_chunk is the token streamed by the LLM and metadata is a dictionary

**URL:** llms-txt#where-message_chunk-is-the-token-streamed-by-the-llm-and-metadata-is-a-dictionary

---

## with information about the graph node where the LLM was called and other information

**URL:** llms-txt#with-information-about-the-graph-node-where-the-llm-was-called-and-other-information

**Contents:**
- Stream custom data
- Use with any LLM

for msg, metadata in graph.stream(
    inputs,
    stream_mode="messages",  # [!code highlight]
):
    # Filter the streamed tokens by the langgraph_node field in the metadata
    # to only include the tokens from the specified node
    if msg.content and metadata["langgraph_node"] == "some_node_name":
        ...
python  theme={null}
  from typing import TypedDict
  from langgraph.graph import START, StateGraph
  from langchain_openai import ChatOpenAI

model = ChatOpenAI(model="gpt-4o-mini")

class State(TypedDict):
        topic: str
        joke: str
        poem: str

def write_joke(state: State):
        topic = state["topic"]
        joke_response = model.invoke(
              [{"role": "user", "content": f"Write a joke about {topic}"}]
        )
        return {"joke": joke_response.content}

def write_poem(state: State):
        topic = state["topic"]
        poem_response = model.invoke(
              [{"role": "user", "content": f"Write a short poem about {topic}"}]
        )
        return {"poem": poem_response.content}

graph = (
        StateGraph(State)
        .add_node(write_joke)
        .add_node(write_poem)
        # write both the joke and the poem concurrently
        .add_edge(START, "write_joke")
        .add_edge(START, "write_poem")
        .compile()
  )

# The "messages" stream mode returns a tuple of (message_chunk, metadata)
  # where message_chunk is the token streamed by the LLM and metadata is a dictionary
  # with information about the graph node where the LLM was called and other information
  for msg, metadata in graph.stream(
      {"topic": "cats"},
      stream_mode="messages",  # [!code highlight]
  ):
      # Filter the streamed tokens by the langgraph_node field in the metadata
      # to only include the tokens from the write_poem node
      if msg.content and metadata["langgraph_node"] == "write_poem":
          print(msg.content, end="|", flush=True)
  python  theme={null}
    from typing import TypedDict
    from langgraph.config import get_stream_writer
    from langgraph.graph import StateGraph, START

class State(TypedDict):
        query: str
        answer: str

def node(state: State):
        # Get the stream writer to send custom data
        writer = get_stream_writer()
        # Emit a custom key-value pair (e.g., progress update)
        writer({"custom_key": "Generating custom data inside node"})
        return {"answer": "some data"}

graph = (
        StateGraph(State)
        .add_node(node)
        .add_edge(START, "node")
        .compile()
    )

inputs = {"query": "example"}

# Set stream_mode="custom" to receive the custom data in the stream
    for chunk in graph.stream(inputs, stream_mode="custom"):
        print(chunk)
    python  theme={null}
    from langchain.tools import tool
    from langgraph.config import get_stream_writer

@tool
    def query_database(query: str) -> str:
        """Query the database."""
        # Access the stream writer to send custom data
        writer = get_stream_writer()  # [!code highlight]
        # Emit a custom key-value pair (e.g., progress update)
        writer({"data": "Retrieved 0/100 records", "type": "progress"})  # [!code highlight]
        # perform query
        # Emit another custom key-value pair
        writer({"data": "Retrieved 100/100 records", "type": "progress"})
        return "some-answer"

graph = ... # define a graph that uses this tool

# Set stream_mode="custom" to receive the custom data in the stream
    for chunk in graph.stream(inputs, stream_mode="custom"):
        print(chunk)
    python  theme={null}
from langgraph.config import get_stream_writer

def call_arbitrary_model(state):
    """Example node that calls an arbitrary model and streams the output"""
    # Get the stream writer to send custom data
    writer = get_stream_writer()  # [!code highlight]
    # Assume you have a streaming client that yields chunks
    # Generate LLM tokens using your custom streaming client
    for chunk in your_custom_streaming_client(state["topic"]):
        # Use the writer to send custom data to the stream
        writer({"custom_llm_chunk": chunk})  # [!code highlight]
    return {"result": "completed"}

graph = (
    StateGraph(State)
    .add_node(call_arbitrary_model)
    # Add other nodes and edges as needed
    .compile()
)

**Examples:**

Example 1 (unknown):
```unknown
<Accordion title="Extended example: streaming LLM tokens from specific nodes">
```

Example 2 (unknown):
```unknown
</Accordion>

## Stream custom data

To send **custom user-defined data** from inside a LangGraph node or tool, follow these steps:

1. Use [`get_stream_writer`](https://reference.langchain.com/python/langgraph/config/#langgraph.config.get_stream_writer) to access the stream writer and emit custom data.
2. Set `stream_mode="custom"` when calling `.stream()` or `.astream()` to get the custom data in the stream. You can combine multiple modes (e.g., `["updates", "custom"]`), but at least one must be `"custom"`.

<Warning>
  **No [`get_stream_writer`](https://reference.langchain.com/python/langgraph/config/#langgraph.config.get_stream_writer) in async for Python \< 3.11**
  In async code running on Python \< 3.11, [`get_stream_writer`](https://reference.langchain.com/python/langgraph/config/#langgraph.config.get_stream_writer) will not work.
  Instead, add a `writer` parameter to your node or tool and pass it manually.
  See [Async with Python \< 3.11](#async) for usage examples.
</Warning>

<Tabs>
  <Tab title="node">
```

Example 3 (unknown):
```unknown
</Tab>

  <Tab title="tool">
```

Example 4 (unknown):
```unknown
</Tab>
</Tabs>

## Use with any LLM

You can use `stream_mode="custom"` to stream data from **any LLM API** — even if that API does **not** implement the LangChain chat model interface.

This lets you integrate raw LLM clients or external services that provide their own streaming interfaces, making LangGraph highly flexible for custom setups.
```

---

## Worker state

**URL:** llms-txt#worker-state

class WorkerState(TypedDict):
    section: Section
    completed_sections: Annotated[list, operator.add]

---

## Workflows and agents

**URL:** llms-txt#workflows-and-agents

**Contents:**
- Setup
- LLMs and augmentations
- Prompt chaining
- Parallelization
- Routing
- Orchestrator-worker
  - Creating workers in LangGraph
- Evaluator-optimizer
- Agents

Source: https://docs.langchain.com/oss/javascript/langgraph/workflows-agents

This guide reviews common workflow and agent patterns.

* Workflows have predetermined code paths and are designed to operate in a certain order.
* Agents are dynamic and define their own processes and tool usage.

<img src="https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/agent_workflow.png?fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=c217c9ef517ee556cae3fc928a21dc55" alt="Agent Workflow" data-og-width="4572" width="4572" data-og-height="2047" height="2047" data-path="oss/images/agent_workflow.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/agent_workflow.png?w=280&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=290e50cff2f72d524a107421ec8e3ff0 280w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/agent_workflow.png?w=560&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=a2bfc87080aee7dd4844f7f24035825e 560w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/agent_workflow.png?w=840&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=ae1fa9087b33b9ff8bc3446ccaa23e3d 840w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/agent_workflow.png?w=1100&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=06003ee1fe07d7a1ea8cf9200e7d0a10 1100w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/agent_workflow.png?w=1650&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=bc98b459a9b1fb226c2887de1696bde0 1650w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/agent_workflow.png?w=2500&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=1933bcdfd5c5b69b98ce96aafa456848 2500w" />

LangGraph offers several benefits when building agents and workflows, including [persistence](/oss/javascript/langgraph/persistence), [streaming](/oss/javascript/langgraph/streaming), and support for debugging as well as [deployment](/oss/javascript/langgraph/deploy).

To build a workflow or agent, you can use [any chat model](/oss/javascript/integrations/chat) that supports structured outputs and tool calling. The following example uses Anthropic:

1. Install dependencies

2. Initialize the LLM:

## LLMs and augmentations

Workflows and agentic systems are based on LLMs and the various augmentations you add to them. [Tool calling](/oss/javascript/langchain/tools), [structured outputs](/oss/javascript/langchain/structured-output), and [short term memory](/oss/javascript/langchain/short-term-memory) are a few options for tailoring LLMs to your needs.

<img src="https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/augmented_llm.png?fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=7ea9656f46649b3ebac19e8309ae9006" alt="LLM augmentations" data-og-width="1152" width="1152" data-og-height="778" height="778" data-path="oss/images/augmented_llm.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/augmented_llm.png?w=280&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=53613048c1b8bd3241bd27900a872ead 280w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/augmented_llm.png?w=560&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=7ba1f4427fd847bd410541ae38d66d40 560w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/augmented_llm.png?w=840&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=503822cf29a28500deb56f463b4244e4 840w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/augmented_llm.png?w=1100&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=279e0440278d3a26b73c72695636272e 1100w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/augmented_llm.png?w=1650&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=d936838b98bc9dce25168e2b2cfd23d0 1650w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/augmented_llm.png?w=2500&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=fa2115f972bc1152b5e03ae590600fa3 2500w" />

Prompt chaining is when each LLM call processes the output of the previous call. It's often used for performing well-defined tasks that can be broken down into smaller, verifiable steps. Some examples include:

* Translating documents into different languages
* Verifying generated content for consistency

<img src="https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/prompt_chain.png?fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=762dec147c31b8dc6ebb0857e236fc1f" alt="Prompt chaining" data-og-width="1412" width="1412" data-og-height="444" height="444" data-path="oss/images/prompt_chain.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/prompt_chain.png?w=280&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=fda27cf4f997e350d4ce48be16049c47 280w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/prompt_chain.png?w=560&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=1374b6de11900d394fc73722a3a6040e 560w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/prompt_chain.png?w=840&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=25246c7111a87b5df5a2af24a0181efe 840w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/prompt_chain.png?w=1100&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=0c57da86a49cf966cc090497ade347f1 1100w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/prompt_chain.png?w=1650&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=a1b5c8fc644d7a80c0792b71769c97da 1650w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/prompt_chain.png?w=2500&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=8a3f66f0e365e503a85b30be48bc1a76 2500w" />

With parallelization, LLMs work simultaneously on a task. This is either done by running multiple independent subtasks at the same time, or running the same task multiple times to check for different outputs. Parallelization is commonly used to:

* Split up subtasks and run them in parallel, which increases speed
* Run tasks multiple times to check for different outputs, which increases confidence

Some examples include:

* Running one subtask that processes a document for keywords, and a second subtask to check for formatting errors
* Running a task multiple times that scores a document for accuracy based on different criteria, like the number of citations, the number of sources used, and the quality of the sources

<img src="https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/parallelization.png?fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=8afe3c427d8cede6fed1e4b2a5107b71" alt="parallelization.png" data-og-width="1020" width="1020" data-og-height="684" height="684" data-path="oss/images/parallelization.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/parallelization.png?w=280&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=88e51062b14d9186a6f0ea246bc48635 280w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/parallelization.png?w=560&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=934941ca52019b7cbce7fbdd31d00f0f 560w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/parallelization.png?w=840&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=30b5c86c545d0e34878ff0a2c367dd0a 840w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/parallelization.png?w=1100&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=6227d2c39f332eaeda23f7db66871dd7 1100w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/parallelization.png?w=1650&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=283f3ee2924a385ab88f2cbfd9c9c48c 1650w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/parallelization.png?w=2500&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=69f6a97716b38998b7b399c3d8ac7d9c 2500w" />

Routing workflows process inputs and then directs them to context-specific tasks. This allows you to define specialized flows for complex tasks. For example, a workflow built to answer product related questions might process the type of question first, and then route the request to specific processes for pricing, refunds, returns, etc.

<img src="https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/routing.png?fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=272e0e9b681b89cd7d35d5c812c50ee6" alt="routing.png" data-og-width="1214" width="1214" data-og-height="678" height="678" data-path="oss/images/routing.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/routing.png?w=280&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=ab85efe91d20c816f9a4e491e92a61f7 280w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/routing.png?w=560&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=769e29f9be058a47ee85e0c9228e6e44 560w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/routing.png?w=840&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=3711ee40746670731a0ce3e96b7cfeb1 840w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/routing.png?w=1100&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=9aaa28410da7643f4a2587f7bfae0f21 1100w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/routing.png?w=1650&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=6706326c7fef0511805c684d1e4f7082 1650w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/routing.png?w=2500&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=f6d603145ca33791b18c8c8afec0bb4d 2500w" />

## Orchestrator-worker

In an orchestrator-worker configuration, the orchestrator:

* Breaks down tasks into subtasks
* Delegates subtasks to workers
* Synthesizes worker outputs into a final result

<img src="https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/worker.png?fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=2e423c67cd4f12e049cea9c169ff0676" alt="worker.png" data-og-width="1486" width="1486" data-og-height="548" height="548" data-path="oss/images/worker.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/worker.png?w=280&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=037222991ea08f889306be035c4730b6 280w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/worker.png?w=560&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=081f3ff05cc1fe50770c864d74084b5b 560w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/worker.png?w=840&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=0ef6c1b9ceb5159030aa34d0f05f1ada 840w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/worker.png?w=1100&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=92ec7353a89ae96e221a5a8f65c88adf 1100w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/worker.png?w=1650&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=71b201dd99fa234ebfb918915aac3295 1650w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/worker.png?w=2500&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=4f7b6e2064db575027932394a3658fbd 2500w" />

Orchestrator-worker workflows provide more flexibility and are often used when subtasks cannot be predefined the way they can with [parallelization](#parallelization). This is common with workflows that write code or need to update content across multiple files. For example, a workflow that needs to update installation instructions for multiple Python libraries across an unknown number of documents might use this pattern.

### Creating workers in LangGraph

Orchestrator-worker workflows are common and LangGraph has built-in support for them. The `Send` API lets you dynamically create worker nodes and send them specific inputs. Each worker has its own state, and all worker outputs are written to a shared state key that is accessible to the orchestrator graph. This gives the orchestrator access to all worker output and allows it to synthesize them into a final output. The example below iterates over a list of sections and uses the `Send` API to send a section to each worker.

## Evaluator-optimizer

In evaluator-optimizer workflows, one LLM call creates a response and the other evaluates that response. If the evaluator or a [human-in-the-loop](/oss/javascript/langgraph/interrupts) determines the response needs refinement, feedback is provided and the response is recreated. This loop continues until an acceptable response is generated.

Evaluator-optimizer workflows are commonly used when there's particular success criteria for a task, but iteration is required to meet that criteria. For example, there's not always a perfect match when translating text between two languages. It might take a few iterations to generate a translation with the same meaning across the two languages.

<img src="https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/evaluator_optimizer.png?fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=9bd0474f42b6040b14ed6968a9ab4e3c" alt="evaluator_optimizer.png" data-og-width="1004" width="1004" data-og-height="340" height="340" data-path="oss/images/evaluator_optimizer.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/evaluator_optimizer.png?w=280&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=ab36856e5f9a518b22e71278aa8b1711 280w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/evaluator_optimizer.png?w=560&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=3ec597c92270278c2bac203d36b611c2 560w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/evaluator_optimizer.png?w=840&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=3ad3bfb734a0e509d9b87fdb4e808bfd 840w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/evaluator_optimizer.png?w=1100&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=e82bd25a463d3cdf76036649c03358a9 1100w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/evaluator_optimizer.png?w=1650&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=d31717ae3e76243dd975a53f46e8c1f6 1650w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/evaluator_optimizer.png?w=2500&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=a9bb4fb1583f6ad06c0b13602cd14811 2500w" />

Agents are typically implemented as an LLM performing actions using [tools](/oss/javascript/langchain/tools). They operate in continuous feedback loops, and are used in situations where problems and solutions are unpredictable. Agents have more autonomy than workflows, and can make decisions about the tools they use and how to solve problems. You can still define the available toolset and guidelines for how agents behave.

<img src="https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/agent.png?fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=bd8da41dbf8b5e6fc9ea6bb10cb63e38" alt="agent.png" data-og-width="1732" width="1732" data-og-height="712" height="712" data-path="oss/images/agent.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/agent.png?w=280&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=f7a590604edc49cfa273b5856f3a3ee3 280w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/agent.png?w=560&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=dff9b17d345fe0fea25616b3b0dc6ebf 560w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/agent.png?w=840&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=bd932835b919f5e58be77221b6d0f194 840w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/agent.png?w=1100&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=d53318b0c9c898a6146991691cbac058 1100w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/agent.png?w=1650&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=ea66fb96bc07c595d321b8b71e651ddb 1650w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/agent.png?w=2500&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=b02599a3c9ba2a5c830b9a346f9d26c9 2500w" />

<Note>
  To get started with agents, see the [quickstart](/oss/javascript/langchain/quickstart) or read more about [how they work](/oss/javascript/langchain/agents) in LangChain.
</Note>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/workflows-agents.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>

2. Initialize the LLM:
```

---

## Workflow execution configuration with a unique thread identifier

**URL:** llms-txt#workflow-execution-configuration-with-a-unique-thread-identifier

config = {
    "configurable": {
        "thread_id": "1"  # Unique identifier to track workflow execution
    }
}

---

## Wrap it in a CompiledSubAgent

**URL:** llms-txt#wrap-it-in-a-compiledsubagent

weather_subagent = CompiledSubAgent(
    name="weather",
    description="This subagent can get weather in cities.",
    runnable=weather_graph
)

agent = create_agent(
    model="claude-sonnet-4-5-20250929",
    middleware=[
        SubAgentMiddleware(
            default_model="claude-sonnet-4-5-20250929",
            default_tools=[],
            subagents=[weather_subagent],
        )
    ],
)
```

In addition to any user-defined subagents, the main agent has access to a `general-purpose` subagent at all times. This subagent has the same instructions as the main agent and all the tools it has access to. The primary purpose of the `general-purpose` subagent is context isolation—the main agent can delegate a complex task to this subagent and get a concise answer back without bloat from intermediate tool calls.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/deepagents/middleware.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Wrap-style: dynamic prompts

**URL:** llms-txt#wrap-style:-dynamic-prompts

@dynamic_prompt
def personalized_prompt(request: ModelRequest) -> str:
    user_id = request.runtime.context.get("user_id", "guest")
    return f"You are a helpful assistant for user {user_id}. Be concise and friendly."

---

## Wrap-style: retry logic

**URL:** llms-txt#wrap-style:-retry-logic

@wrap_model_call
def retry_model(
    request: ModelRequest,
    handler: Callable[[ModelRequest], ModelResponse],
) -> ModelResponse:
    for attempt in range(3):
        try:
            return handler(request)
        except Exception as e:
            if attempt == 2:
                raise
            print(f"Retry {attempt + 1}/3 after error: {e}")

---

## Write sample data to the store using the put method

**URL:** llms-txt#write-sample-data-to-the-store-using-the-put-method

store.put( # [!code highlight]
    ("users",),  # Namespace to group related data together (users namespace for user data)
    "user_123",  # Key within the namespace (user ID as key)
    {
        "name": "John Smith",
        "language": "English",
    }  # Data to store for the given user
)

@tool
def get_user_info(runtime: ToolRuntime[Context]) -> str:
    """Look up user info."""
    # Access the store - same as that provided to `create_agent`
    store = runtime.store # [!code highlight]
    user_id = runtime.context.user_id
    # Retrieve data from store - returns StoreValue object with value and metadata
    user_info = store.get(("users",), user_id) # [!code highlight]
    return str(user_info.value) if user_info else "Unknown user"

agent = create_agent(
    model="claude-sonnet-4-5-20250929",
    tools=[get_user_info],
    # Pass store to agent - enables agent to access store when running tools
    store=store, # [!code highlight]
    context_schema=Context
)

---

## Write your prompt with AI

**URL:** llms-txt#write-your-prompt-with-ai

**Contents:**
- Chat sidebar
- Quick actions
- Custom quick actions
- Diffing
- Saving and using prompts

Source: https://docs.langchain.com/langsmith/write-prompt-with-ai

The prompt canvas makes it easy to edit a prompt with the help of an LLM. This allows you to iterate faster on long prompts and also makes it easier to make overarching stylisting or tonal changes to your prompt. You can enter the promp canvas by clicking the glowing wand over any message in your prompt:

<img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-canvas-open.gif?s=480b38abe2797436e0b7969e2d961e23" alt="" data-og-width="1000" width="1000" data-og-height="539" height="539" data-path="langsmith/images/prompt-canvas-open.gif" data-optimize="true" data-opv="3" />

You can use the chat sidebar to ask questions about your prompt, or to give instructions in natural language to the LLM for how to rewrite your prompt.

<img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-canvas-rewrite.gif?s=91ba55b2e7b1e3a18a799265250dedb0" alt="" data-og-width="1000" width="1000" data-og-height="539" height="539" data-path="langsmith/images/prompt-canvas-rewrite.gif" data-optimize="true" data-opv="3" />

<Note>
  You can also edit the prompt directly - you don't **need** to use the LLM. This is useful if you know what edits you want to make and just want to make them directly
</Note>

There are quick actions to change the reading level or length of the prompt with a single mouse click:

<img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-canvas-quick-actions.gif?s=6d2bb4ee78ec98fe551ce7f1a0e94ad9" alt="" data-og-width="1000" width="1000" data-og-height="539" height="539" data-path="langsmith/images/prompt-canvas-quick-actions.gif" data-optimize="true" data-opv="3" />

## Custom quick actions

You can also save your own custom quick actions, for ease of use across all the prompts you are working on in LangSmith:

<img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-canvas-custom-quick-action.gif?s=3fd706f2fdd87abe339f83a2639ce708" alt="" data-og-width="1000" width="1000" data-og-height="539" height="539" data-path="langsmith/images/prompt-canvas-custom-quick-action.gif" data-optimize="true" data-opv="3" />

You can also see the specific differences between each version of your prompt by selecting the diff slider in the top right of the canvas:

<img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-canvas-diff.gif?s=be6259e6ac773c4a01c5b1ac4fe79e92" alt="" data-og-width="1000" width="1000" data-og-height="539" height="539" data-path="langsmith/images/prompt-canvas-diff.gif" data-optimize="true" data-opv="3" />

## Saving and using prompts

Lastly, you can save the prompt you have created in the canvas by clicking the "Use this Version" button in the bottom right:

<img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-canvas-save.gif?s=f54b0d1b6374e6b69750a6489c787172" alt="" data-og-width="1000" width="1000" data-og-height="539" height="539" data-path="langsmith/images/prompt-canvas-save.gif" data-optimize="true" data-opv="3" />

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/write-prompt-with-ai.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## Your application code using multiple frameworks

**URL:** llms-txt#your-application-code-using-multiple-frameworks

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/trace-with-semantic-kernel.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## You can access the store directly to get the value

**URL:** llms-txt#you-can-access-the-store-directly-to-get-the-value

store.get(("users",), "user_123").value
```

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/long-term-memory.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## You can also wrap the async client as well

**URL:** llms-txt#you-can-also-wrap-the-async-client-as-well

---

## You can customize it if building a custom agent

**URL:** llms-txt#you-can-customize-it-if-building-a-custom-agent

**Contents:**
  - Short-term vs. long-term filesystem
- Subagent middleware

agent = create_agent(
    model="claude-sonnet-4-5-20250929",
    middleware=[
        FilesystemMiddleware(
            backend=None,  # Optional: custom backend (defaults to StateBackend)
            system_prompt="Write to the filesystem when...",  # Optional custom addition to the system prompt
            custom_tool_descriptions={
                "ls": "Use the ls tool when...",
                "read_file": "Use the read_file tool to..."
            }  # Optional: Custom descriptions for filesystem tools
        ),
    ],
)
python  theme={null}
from langchain.agents import create_agent
from deepagents.middleware import FilesystemMiddleware
from deepagents.backends import CompositeBackend, StateBackend, StoreBackend
from langgraph.store.memory import InMemoryStore

store = InMemoryStore()

agent = create_agent(
    model="claude-sonnet-4-5-20250929",
    store=store,
    middleware=[
        FilesystemMiddleware(
            backend=lambda rt: CompositeBackend(
                default=StateBackend(rt),
                routes={"/memories/": StoreBackend(rt)}
            ),
            custom_tool_descriptions={
                "ls": "Use the ls tool when...",
                "read_file": "Use the read_file tool to..."
            }  # Optional: Custom descriptions for filesystem tools
        ),
    ],
)
python  theme={null}
from langchain_core.tools import tool
from langchain.agents import create_agent
from deepagents.middleware.subagents import SubAgentMiddleware

@tool
def get_weather(city: str) -> str:
    """Get the weather in a city."""
    return f"The weather in {city} is sunny."

agent = create_agent(
    model="claude-sonnet-4-5-20250929",
    middleware=[
        SubAgentMiddleware(
            default_model="claude-sonnet-4-5-20250929",
            default_tools=[],
            subagents=[
                {
                    "name": "weather",
                    "description": "This subagent can get weather in cities.",
                    "system_prompt": "Use the get_weather tool to get the weather in a city.",
                    "tools": [get_weather],
                    "model": "gpt-4o",
                    "middleware": [],
                }
            ],
        )
    ],
)
python  theme={null}
from langchain.agents import create_agent
from deepagents.middleware.subagents import SubAgentMiddleware
from deepagents import CompiledSubAgent
from langgraph.graph import StateGraph

**Examples:**

Example 1 (unknown):
```unknown
### Short-term vs. long-term filesystem

By default, these tools write to a local "filesystem" in your graph state. To enable persistent storage across threads, configure a `CompositeBackend` that routes specific paths (like `/memories/`) to a `StoreBackend`.
```

Example 2 (unknown):
```unknown
When you configure a `CompositeBackend` with a `StoreBackend` for `/memories/`, any files prefixed with **/memories/** are saved to persistent storage and survive across different threads. Files without this prefix remain in ephemeral state storage.

## Subagent middleware

Handing off tasks to subagents isolates context, keeping the main (supervisor) agent's context window clean while still going deep on a task.

The subagents middleware allows you to supply subagents through a `task` tool.
```

Example 3 (unknown):
```unknown
A subagent is defined with a **name**, **description**, **system prompt**, and **tools**. You can also provide a subagent with a custom **model**, or with additional **middleware**. This can be particularly useful when you want to give the subagent an additional state key to share with the main agent.

For more complex use cases, you can also provide your own pre-built LangGraph graph as a subagent.
```

---

## You can customize the run name with the `name` keyword argument

**URL:** llms-txt#you-can-customize-the-run-name-with-the-`name`-keyword-argument

@traceable(name="Extract User Details")
def my_function(text: str) -> UserDetail:
    return client.chat.completions.create(
        model="gpt-4o-mini",
        response_model=UserDetail,
        messages=[
            {"role": "user", "content": f"Extract {text}"},
        ]
    )

my_function("Jason is 25 years old")
```

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/trace-with-instructor.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## You can install them using pip:

**URL:** llms-txt#you-can-install-them-using-pip:

---

## You can iterate over the runs in the experiments belonging to the comparative experiment and preferentially rank the outputs

**URL:** llms-txt#you-can-iterate-over-the-runs-in-the-experiments-belonging-to-the-comparative-experiment-and-preferentially-rank-the-outputs

---

## You can tag a specific dataset version with a semantic name, like "prod"

**URL:** llms-txt#you-can-tag-a-specific-dataset-version-with-a-semantic-name,-like-"prod"

**Contents:**
- Evaluate on a specific dataset version
  - Use `list_examples`
- Evaluate on a split / filtered view of a dataset
  - Evaluate on a filtered view of a dataset
  - Evaluate on a dataset split
- Share a dataset
  - Share a dataset publicly
  - Unshare a dataset
- Export a dataset
- Export filtered traces from experiment to dataset

client.update_dataset_tag(
    dataset_name=toxic_dataset_name, as_of=initial_time, tag="prod"
)
python Python theme={null}
  from langsmith import Client

# Assumes actual outputs have a 'class' key.
  # Assumes example outputs have a 'label' key.
  def correct(outputs: dict, reference_outputs: dict) -> bool:
    return outputs["class"] == reference_outputs["label"]

results = ls_client.evaluate(
      lambda inputs: {"class": "Not toxic"},
      # Pass in filtered data here:
      data=ls_client.list_examples(
        dataset_name="Toxic Queries",
        as_of="latest",  # specify version here
      ),
      evaluators=[correct],
  )
  typescript TypeScript theme={null}
  import { evaluate } from "langsmith/evaluation";

await evaluate((inputs) => labelText(inputs["input"]), {
    data: langsmith.listExamples({
      datasetName: datasetName,
      asOf: "latest",
    }),
    evaluators: [correctLabel],
  });
  python Python theme={null}
  from langsmith import evaluate

results = evaluate(
      lambda inputs: label_text(inputs["text"]),
      data=client.list_examples(dataset_name=dataset_name, metadata={"desired_key": "desired_value"}),
      evaluators=[correct_label],
      experiment_prefix="Toxic Queries",
  )
  typescript TypeScript theme={null}
  import { evaluate } from "langsmith/evaluation";

await evaluate((inputs) => labelText(inputs["input"]), {
    data: langsmith.listExamples({
      datasetName: datasetName,
      metadata: {"desired_key": "desired_value"},
    }),
    evaluators: [correctLabel],
    experimentPrefix: "Toxic Queries",
  });
  python Python theme={null}
  from langsmith import evaluate

results = evaluate(
      lambda inputs: label_text(inputs["text"]),
      data=client.list_examples(dataset_name=dataset_name, splits=["test", "training"]),
      evaluators=[correct_label],
      experiment_prefix="Toxic Queries",
  )
  typescript TypeScript theme={null}
  import { evaluate } from "langsmith/evaluation";

await evaluate((inputs) => labelText(inputs["input"]), {
    data: langsmith.listExamples({
      datasetName: datasetName,
      splits: ["test", "training"],
    }),
    evaluators: [correctLabel],
    experimentPrefix: "Toxic Queries",
  });
  ```
</CodeGroup>

For more details on fetching views of a dataset, refer to the guide on [fetching datasets](/langsmith/manage-datasets-programmatically#fetch-datasets).

### Share a dataset publicly

<Warning>
  Sharing a dataset publicly will make the **dataset examples, experiments and associated runs, and feedback on this dataset accessible to anyone with the link**, even if they don't have a LangSmith account. Make sure you're not sharing sensitive information.

This feature is only available in the cloud-hosted version of LangSmith.
</Warning>

From the **Dataset & Experiments** tab, select a dataset, click **⋮** (top right of the page), click **Share Dataset**. This will open a dialog where you can copy the link to the dataset.

<img src="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/share-dataset.gif?s=3788767dadf1c265968fe61d96bacc2d" alt="Share Dataset" data-og-width="1086" width="1086" data-og-height="720" height="720" data-path="langsmith/images/share-dataset.gif" data-optimize="true" data-opv="3" />

### Unshare a dataset

1. Click on **Unshare** by clicking on **Public** in the upper right hand corner of any publicly shared dataset, then **Unshare** in the dialog. <img src="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/unshare-dataset.png?fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=98e1807ba9f9a510f56a435b7f81287c" alt="Unshare Dataset" data-og-width="1312" width="1312" data-og-height="803" height="803" data-path="langsmith/images/unshare-dataset.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/unshare-dataset.png?w=280&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=e5dc54489da489188e26e742fc797f71 280w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/unshare-dataset.png?w=560&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=356b04f4abe97829de039db79ddc5567 560w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/unshare-dataset.png?w=840&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=4bac2ca43d13295f5e16f29b9da90553 840w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/unshare-dataset.png?w=1100&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=768b989e2e27f932d4611cfd5b1d3bc0 1100w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/unshare-dataset.png?w=1650&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=b588cc2aad65b4ad721e05563567e2e3 1650w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/unshare-dataset.png?w=2500&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=605d68739465a17ac63c9f40f00776f1 2500w" />

2. Navigate to your organization's list of publicly shared datasets, by clicking on **Settings** -> **Shared URLs** or [this link](https://smith.langchain.com/settings/shared), then click on **Unshare** next to the dataset you want to unshare.

<img src="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/unshare-trace-list.png?fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=8a7762947b85af17b36f2c71857badf7" alt="Unshare Trace List" data-og-width="1125" width="1125" data-og-height="519" height="519" data-path="langsmith/images/unshare-trace-list.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/unshare-trace-list.png?w=280&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=1b4d7b5ec44a3ec6bcdad04df29bf417 280w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/unshare-trace-list.png?w=560&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=c35f8f9690fd578920a74754ed65eec1 560w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/unshare-trace-list.png?w=840&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=3383e115479d7e7cab4721a571aa52e3 840w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/unshare-trace-list.png?w=1100&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=a5a303b2de245b83369fdfd40390e00b 1100w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/unshare-trace-list.png?w=1650&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=77b9562fb148ec7dac8f5ae04afc86ea 1650w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/unshare-trace-list.png?w=2500&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=3ddac9fb12453cb0647f6b0a6279b85c 2500w" />

You can export your LangSmith dataset to a CSV, JSONL, or [OpenAI's fine tuning format](https://platform.openai.com/docs/guides/fine-tuning#example-format) from the LangSmith UI.

From the **Dataset & Experiments** tab, select a dataset, click **⋮** (top right of the page), click **Download Dataset**.

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/export-dataset-button.gif?s=e71b7c55d70528df0a8985b8884f7597" alt="Export Dataset Button" data-og-width="1086" width="1086" data-og-height="720" height="720" data-path="langsmith/images/export-dataset-button.gif" data-optimize="true" data-opv="3" />

## Export filtered traces from experiment to dataset

After running an [offline evaluation](/langsmith/evaluation-concepts#offline-evaluation) in LangSmith, you may want to export [traces](/langsmith/observability-concepts#traces) that met some evaluation criteria to a dataset.

### View experiment traces

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/export-filtered-trace-to-dataset.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=d05263ae403f7f04e8a00ab956313c01" alt="Export filtered traces" data-og-width="3452" width="3452" data-og-height="1224" height="1224" data-path="langsmith/images/export-filtered-trace-to-dataset.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/export-filtered-trace-to-dataset.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=165c814edfb48cb4f0ae996b654a8b1d 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/export-filtered-trace-to-dataset.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=d097b2b992b036e3f9b1c27a9a5a354e 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/export-filtered-trace-to-dataset.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=d2e9824cdc9d6dccdb2b0e0d8cd09dab 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/export-filtered-trace-to-dataset.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=a7b9df2d826e18396002aa52fabe9465 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/export-filtered-trace-to-dataset.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=e160dbc0be517b4d9ba3e818a62fb628 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/export-filtered-trace-to-dataset.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=542ba924541f3c6cc970f37e84c07efb 2500w" />

To do so, first click on the arrow next to your experiment name. This will direct you to a project that contains the traces generated from your experiment.

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/experiment-tracing-project.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=6ef0c958b5af1f2fe113b8717e698584" alt="Export filtered traces" data-og-width="3452" width="3452" data-og-height="1638" height="1638" data-path="langsmith/images/experiment-tracing-project.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/experiment-tracing-project.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=384a1febf8595c2cbf87488a38f7b60a 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/experiment-tracing-project.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=66fbf297dad35d8379572614c17b0c0a 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/experiment-tracing-project.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=849acb7d46cf795441c5d983dc7034fb 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/experiment-tracing-project.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=31bdc68e882b0b94a86f3d28032f2eab 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/experiment-tracing-project.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=6567c99736d666c3a0e616b0cd474663 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/experiment-tracing-project.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=3331bc1c1eefe3189c6257be0a7f48b8 2500w" />

From there, you can filter the traces based on your evaluation criteria. In this example, we're filtering for all traces that received an accuracy score greater than 0.5.

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filtered-traces-from-experiment.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=0a0edc120c230511d10285f80248dab0" alt="Export filtered traces" data-og-width="3438" width="3438" data-og-height="1844" height="1844" data-path="langsmith/images/filtered-traces-from-experiment.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filtered-traces-from-experiment.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=2bfcc0f940f0becbe41050683dd97787 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filtered-traces-from-experiment.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=702fcaaab265f22c7a6e86c4738aecc2 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filtered-traces-from-experiment.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=352cfc459b7b30c3aaa5b0968b8ce961 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filtered-traces-from-experiment.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=173ce1d23621756731a4df05956117c9 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filtered-traces-from-experiment.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=a7f00715f4e840504d19b6f4daba41ef 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filtered-traces-from-experiment.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=0f29eb2c1b85e37af88178ffe69eda39 2500w" />

After applying the filter on the project, we can multi-select runs to add to the dataset, and click **Add to Dataset**.

<img src="https://mintcdn.com/langchain-5e9cc07a/Xbr8HuVd9jPi6qTU/langsmith/images/add-filtered-traces-to-dataset.png?fit=max&auto=format&n=Xbr8HuVd9jPi6qTU&q=85&s=d2488fe04acef624c3528ad01c5bedaa" alt="Export filtered traces" data-og-width="3364" width="3364" data-og-height="1834" height="1834" data-path="langsmith/images/add-filtered-traces-to-dataset.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Xbr8HuVd9jPi6qTU/langsmith/images/add-filtered-traces-to-dataset.png?w=280&fit=max&auto=format&n=Xbr8HuVd9jPi6qTU&q=85&s=4e41c751ac2fec4a2fd5550af8e5538d 280w, https://mintcdn.com/langchain-5e9cc07a/Xbr8HuVd9jPi6qTU/langsmith/images/add-filtered-traces-to-dataset.png?w=560&fit=max&auto=format&n=Xbr8HuVd9jPi6qTU&q=85&s=79e7789b3543800aec927e96e60f6530 560w, https://mintcdn.com/langchain-5e9cc07a/Xbr8HuVd9jPi6qTU/langsmith/images/add-filtered-traces-to-dataset.png?w=840&fit=max&auto=format&n=Xbr8HuVd9jPi6qTU&q=85&s=5fa95115dec4ef56c924260dc3008aef 840w, https://mintcdn.com/langchain-5e9cc07a/Xbr8HuVd9jPi6qTU/langsmith/images/add-filtered-traces-to-dataset.png?w=1100&fit=max&auto=format&n=Xbr8HuVd9jPi6qTU&q=85&s=4aed75b0e0ef7f1f365ba84860311019 1100w, https://mintcdn.com/langchain-5e9cc07a/Xbr8HuVd9jPi6qTU/langsmith/images/add-filtered-traces-to-dataset.png?w=1650&fit=max&auto=format&n=Xbr8HuVd9jPi6qTU&q=85&s=84e44cd484135ba5718da7d7c5c68f81 1650w, https://mintcdn.com/langchain-5e9cc07a/Xbr8HuVd9jPi6qTU/langsmith/images/add-filtered-traces-to-dataset.png?w=2500&fit=max&auto=format&n=Xbr8HuVd9jPi6qTU&q=85&s=22468eea737aaec2ef2faa8e0e34f2ec 2500w" />

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/manage-datasets.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
To run an evaluation on a particular tagged version of a dataset, refer to the [Evaluate on a specific dataset version section](#evaluate-on-specific-dataset-version).

## Evaluate on a specific dataset version

<Check>
  You may find it helpful to refer to the following content before you read this section:

  * [Version a dataset](#version-a-dataset).
  * [Fetching examples](/langsmith/manage-datasets-programmatically#fetch-examples).
</Check>

### Use `list_examples`

You can use `evaluate` / `aevaluate` to pass in an iterable of examples to evaluate on a particular version of a dataset. Use `list_examples` / `listExamples` to fetch examples from a particular version tag using `as_of` / `asOf` and pass that into the `data` argument.

<CodeGroup>
```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
</CodeGroup>

Learn more about how to fetch views of a dataset on the [Create and manage datasets programmatically](/langsmith/manage-datasets-programmatically#fetch-datasets) page.

## Evaluate on a split / filtered view of a dataset

<Check>
  You may find it helpful to refer to the following content before you read this section:

  * [Fetching examples](/langsmith/manage-datasets-programmatically#fetch-examples).
  * [Creating and managing dataset splits](/langsmith/manage-datasets-in-application#create-and-manage-dataset-splits).
</Check>

### Evaluate on a filtered view of a dataset

You can use the `list_examples` / `listExamples` method to [fetch](/langsmith/manage-datasets-programmatically#fetch-examples) a subset of examples from a dataset to evaluate on.

One common workflow is to fetch examples that have a certain metadata key-value pair.

<CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## You can then create edges to/from this node by referencing it as `"my_node"`

**URL:** llms-txt#you-can-then-create-edges-to/from-this-node-by-referencing-it-as-`"my_node"`

**Contents:**
  - `START` Node
  - `END` Node
  - Node Caching

python  theme={null}
from langgraph.graph import START

graph.add_edge(START, "node_a")
python  theme={null}
from langgraph.graph import END

graph.add_edge("node_a", END)
python  theme={null}
import time
from typing_extensions import TypedDict
from langgraph.graph import StateGraph
from langgraph.cache.memory import InMemoryCache
from langgraph.types import CachePolicy

class State(TypedDict):
    x: int
    result: int

builder = StateGraph(State)

def expensive_node(state: State) -> dict[str, int]:
    # expensive computation
    time.sleep(2)
    return {"result": state["x"] * 2}

builder.add_node("expensive_node", expensive_node, cache_policy=CachePolicy(ttl=3))
builder.set_entry_point("expensive_node")
builder.set_finish_point("expensive_node")

graph = builder.compile(cache=InMemoryCache())

print(graph.invoke({"x": 5}, stream_mode='updates'))    # [!code highlight]

**Examples:**

Example 1 (unknown):
```unknown
### `START` Node

The [`START`](https://reference.langchain.com/python/langgraph/constants/#langgraph.constants.START) Node is a special node that represents the node that sends user input to the graph. The main purpose for referencing this node is to determine which nodes should be called first.
```

Example 2 (unknown):
```unknown
### `END` Node

The `END` Node is a special node that represents a terminal node. This node is referenced when you want to denote which edges have no actions after they are done.
```

Example 3 (unknown):
```unknown
### Node Caching

LangGraph supports caching of tasks/nodes based on the input to the node. To use caching:

* Specify a cache when compiling a graph (or specifying an entrypoint)
* Specify a cache policy for nodes. Each cache policy supports:
  * `key_func` used to generate a cache key based on the input to a node, which defaults to a `hash` of the input with pickle.
  * `ttl`, the time to live for the cache in seconds. If not specified, the cache will never expire.

For example:
```

---

## You must provide a thread ID to associate the execution with a conversation thread,

**URL:** llms-txt#you-must-provide-a-thread-id-to-associate-the-execution-with-a-conversation-thread,

---

## You need to return a `Command` object to include more than just a final tool call

**URL:** llms-txt#you-need-to-return-a-`command`-object-to-include-more-than-just-a-final-tool-call

**Contents:**
- Handoffs
  - Implementation (Coming soon)

) -> Command:
    result = subagent1.invoke({
        "messages": [{"role": "user", "content": query}]
    })
    return Command(update={
        # This is the example state key we are passing back
        "example_state_key": result["example_state_key"],
        "messages": [
            ToolMessage(
                content=result["messages"][-1].content,
                # We need to include the tool call id so it matches up with the right tool call
                tool_call_id=tool_call_id
            )
        ]
    })
mermaid  theme={null}
graph LR
    A[User] --> B[Agent A]
    B --> C[Agent B]
    C --> A
```

### Implementation (Coming soon)

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/multi-agent.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
## Handoffs

In **handoffs**, agents can directly pass control to each other. The “active” agent changes, and the user interacts with whichever agent currently has control.

Flow:

1. The **current agent** decides it needs help from another agent.
2. It passes control (and state) to the **next agent**.
3. The **new agent** interacts directly with the user until it decides to hand off again or finish.
```

---

## ]

**URL:** llms-txt#]

**Contents:**
  - Reasoning
  - Local models
  - Prompt caching
  - Server-side tool use
  - Rate limiting
  - Base URL or proxy
  - Log probabilities
  - Token usage
  - Invocation config
  - Configurable models

python Stream reasoning output theme={null}
  for chunk in model.stream("Why do parrots have colorful feathers?"):
      reasoning_steps = [r for r in chunk.content_blocks if r["type"] == "reasoning"]
      print(reasoning_steps if reasoning_steps else chunk.text)
  python Complete reasoning output theme={null}
  response = model.invoke("Why do parrots have colorful feathers?")
  reasoning_steps = [b for b in response.content_blocks if b["type"] == "reasoning"]
  print(" ".join(step["reasoning"] for step in reasoning_steps))
  python Invoke with server-side tool use theme={null}
from langchain.chat_models import init_chat_model

model = init_chat_model("gpt-4.1-mini")

tool = {"type": "web_search"}
model_with_tools = model.bind_tools([tool])

response = model_with_tools.invoke("What was a positive news story from today?")
response.content_blocks
python Result expandable theme={null}
[
    {
        "type": "server_tool_call",
        "name": "web_search",
        "args": {
            "query": "positive news stories today",
            "type": "search"
        },
        "id": "ws_abc123"
    },
    {
        "type": "server_tool_result",
        "tool_call_id": "ws_abc123",
        "status": "success"
    },
    {
        "type": "text",
        "text": "Here are some positive news stories from today...",
        "annotations": [
            {
                "end_index": 410,
                "start_index": 337,
                "title": "article title",
                "type": "citation",
                "url": "..."
            }
        ]
    }
]
python Define a rate limiter theme={null}
  from langchain_core.rate_limiters import InMemoryRateLimiter

rate_limiter = InMemoryRateLimiter(
      requests_per_second=0.1,  # 1 request every 10s
      check_every_n_seconds=0.1,  # Check every 100ms whether allowed to make a request
      max_bucket_size=10,  # Controls the maximum burst size.
  )

model = init_chat_model(
      model="gpt-5",
      model_provider="openai",
      rate_limiter=rate_limiter  # [!code highlight]
  )
  python  theme={null}
  model = init_chat_model(
      model="MODEL_NAME",
      model_provider="openai",
      base_url="BASE_URL",
      api_key="YOUR_API_KEY",
  )
  python  theme={null}
  from langchain_openai import ChatOpenAI

model = ChatOpenAI(
      model="gpt-4o",
      openai_proxy="http://proxy.example.com:8080"
  )
  python  theme={null}
model = init_chat_model(
    model="gpt-4o",
    model_provider="openai"
).bind(logprobs=True)

response = model.invoke("Why do parrots talk?")
print(response.response_metadata["logprobs"])
python  theme={null}
    from langchain.chat_models import init_chat_model
    from langchain_core.callbacks import UsageMetadataCallbackHandler

model_1 = init_chat_model(model="gpt-4o-mini")
    model_2 = init_chat_model(model="claude-haiku-4-5-20251001")

callback = UsageMetadataCallbackHandler()
    result_1 = model_1.invoke("Hello", config={"callbacks": [callback]})
    result_2 = model_2.invoke("Hello", config={"callbacks": [callback]})
    callback.usage_metadata
    python  theme={null}
    {
        'gpt-4o-mini-2024-07-18': {
            'input_tokens': 8,
            'output_tokens': 10,
            'total_tokens': 18,
            'input_token_details': {'audio': 0, 'cache_read': 0},
            'output_token_details': {'audio': 0, 'reasoning': 0}
        },
        'claude-haiku-4-5-20251001': {
            'input_tokens': 8,
            'output_tokens': 21,
            'total_tokens': 29,
            'input_token_details': {'cache_read': 0, 'cache_creation': 0}
        }
    }
    python  theme={null}
    from langchain.chat_models import init_chat_model
    from langchain_core.callbacks import get_usage_metadata_callback

model_1 = init_chat_model(model="gpt-4o-mini")
    model_2 = init_chat_model(model="claude-haiku-4-5-20251001")

with get_usage_metadata_callback() as cb:
        model_1.invoke("Hello")
        model_2.invoke("Hello")
        print(cb.usage_metadata)
    python  theme={null}
    {
        'gpt-4o-mini-2024-07-18': {
            'input_tokens': 8,
            'output_tokens': 10,
            'total_tokens': 18,
            'input_token_details': {'audio': 0, 'cache_read': 0},
            'output_token_details': {'audio': 0, 'reasoning': 0}
        },
        'claude-haiku-4-5-20251001': {
            'input_tokens': 8,
            'output_tokens': 21,
            'total_tokens': 29,
            'input_token_details': {'cache_read': 0, 'cache_creation': 0}
        }
    }
    python Invocation with config theme={null}
response = model.invoke(
    "Tell me a joke",
    config={
        "run_name": "joke_generation",      # Custom name for this run
        "tags": ["humor", "demo"],          # Tags for categorization
        "metadata": {"user_id": "123"},     # Custom metadata
        "callbacks": [my_callback_handler], # Callback handlers
    }
)
python  theme={null}
from langchain.chat_models import init_chat_model

configurable_model = init_chat_model(temperature=0)

configurable_model.invoke(
    "what's your name",
    config={"configurable": {"model": "gpt-5-nano"}},  # Run with GPT-5-Nano
)
configurable_model.invoke(
    "what's your name",
    config={"configurable": {"model": "claude-sonnet-4-5-20250929"}},  # Run with Claude
)
python  theme={null}
  first_model = init_chat_model(
          model="gpt-4.1-mini",
          temperature=0,
          configurable_fields=("model", "model_provider", "temperature", "max_tokens"),
          config_prefix="first",  # Useful when you have a chain with multiple models
  )

first_model.invoke("what's your name")
  python  theme={null}
  first_model.invoke(
      "what's your name",
      config={
          "configurable": {
              "first_model": "claude-sonnet-4-5-20250929",
              "first_temperature": 0.5,
              "first_max_tokens": 100,
          }
      },
  )
  python  theme={null}
  from pydantic import BaseModel, Field

class GetWeather(BaseModel):
      """Get the current weather in a given location"""

location: str = Field(..., description="The city and state, e.g. San Francisco, CA")

class GetPopulation(BaseModel):
      """Get the current population in a given location"""

location: str = Field(..., description="The city and state, e.g. San Francisco, CA")

model = init_chat_model(temperature=0)
  model_with_tools = model.bind_tools([GetWeather, GetPopulation])

model_with_tools.invoke(
      "what's bigger in 2024 LA or NYC", config={"configurable": {"model": "gpt-4.1-mini"}}
  ).tool_calls
  
  [
      {
          'name': 'GetPopulation',
          'args': {'location': 'Los Angeles, CA'},
          'id': 'call_Ga9m8FAArIyEjItHmztPYA22',
          'type': 'tool_call'
      },
      {
          'name': 'GetPopulation',
          'args': {'location': 'New York, NY'},
          'id': 'call_jh2dEvBaAHRaw5JUDthOs7rt',
          'type': 'tool_call'
      }
  ]
  python  theme={null}
  model_with_tools.invoke(
      "what's bigger in 2024 LA or NYC",
      config={"configurable": {"model": "claude-sonnet-4-5-20250929"}},
  ).tool_calls
  
  [
      {
          'name': 'GetPopulation',
          'args': {'location': 'Los Angeles, CA'},
          'id': 'toolu_01JMufPf4F4t2zLj7miFeqXp',
          'type': 'tool_call'
      },
      {
          'name': 'GetPopulation',
          'args': {'location': 'New York City, NY'},
          'id': 'toolu_01RQBHcE8kEEbYTuuS8WqY1u',
          'type': 'tool_call'
      }
  ]
  ```
</Accordion>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/models.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
See the [integrations page](/oss/python/integrations/providers/overview) for details on specific providers.

### Reasoning

Newer models are capable of performing multi-step reasoning to arrive at a conclusion. This involves breaking down complex problems into smaller, more manageable steps.

**If supported by the underlying model,** you can surface this reasoning process to better understand how the model arrived at its final answer.

<CodeGroup>
```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
</CodeGroup>

Depending on the model, you can sometimes specify the level of effort it should put into reasoning. Similarly, you can request that the model turn off reasoning entirely. This may take the form of categorical "tiers" of reasoning (e.g., `'low'` or `'high'`) or integer token budgets.

For details, see the [integrations page](/oss/python/integrations/providers/overview) or [reference](https://reference.langchain.com/python/integrations/) for your respective chat model.

### Local models

LangChain supports running models locally on your own hardware. This is useful for scenarios where either data privacy is critical, you want to invoke a custom model, or when you want to avoid the costs incurred when using a cloud-based model.

[Ollama](/oss/python/integrations/chat/ollama) is one of the easiest ways to run models locally. See the full list of local integrations on the [integrations page](/oss/python/integrations/providers/overview).

### Prompt caching

Many providers offer prompt caching features to reduce latency and cost on repeat processing of the same tokens. These features can be **implicit** or **explicit**:

* **Implicit prompt caching:** providers will automatically pass on cost savings if a request hits a cache. Examples: [OpenAI](/oss/python/integrations/chat/openai) and [Gemini](/oss/python/integrations/chat/google_generative_ai) (Gemini 2.5 and above).
* **Explicit caching:** providers allow you to manually indicate cache points for greater control or to guarantee cost savings. Examples: [`ChatOpenAI`](https://reference.langchain.com/python/integrations/langchain_openai/ChatOpenAI/) (via `prompt_cache_key`), Anthropic's [`AnthropicPromptCachingMiddleware`](/oss/python/integrations/chat/anthropic#prompt-caching) and [`cache_control`](https://docs.langchain.com/oss/python/integrations/chat/anthropic#prompt-caching) options, [AWS Bedrock](/oss/python/integrations/chat/bedrock#prompt-caching), [Gemini](https://python.langchain.com/api_reference/google_genai/chat_models/langchain_google_genai.chat_models.ChatGoogleGenerativeAI.html).

<Warning>
  Prompt caching is often only engaged above a minimum input token threshold. See [provider pages](/oss/python/integrations/chat) for details.
</Warning>

Cache usage will be reflected in the [usage metadata](/oss/python/langchain/messages#token-usage) of the model response.

### Server-side tool use

Some providers support server-side [tool-calling](#tool-calling) loops: models can interact with web search, code interpreters, and other tools and analyze the results in a single conversational turn.

If a model invokes a tool server-side, the content of the response message will include content representing the invocation and result of the tool. Accessing the [content blocks](/oss/python/langchain/messages#standard-content-blocks) of the response will return the server-side tool calls and results in a provider-agnostic format:
```

Example 4 (unknown):
```unknown

```

---

## ...

**URL:** llms-txt#...

print(full.content_blocks)

---

## >                'action_name': 'execute_sql',

**URL:** llms-txt#>----------------'action_name':-'execute_sql',

---

## >          'action_requests': [

**URL:** llms-txt#>----------'action_requests':-[

---

## - Age: 25

**URL:** llms-txt#--age:-25

---

## >                'allowed_decisions': ['approve', 'reject']

**URL:** llms-txt#>----------------'allowed_decisions':-['approve',-'reject']

---

## >                'arguments': {'query': 'DELETE FROM records WHERE created_at < NOW() - INTERVAL \'30 days\';'},

**URL:** llms-txt#>----------------'arguments':-{'query':-'delete-from-records-where-created_at-<-now()---interval-\'30-days\';'},

---

## ❌ Bad: Too many tools

**URL:** llms-txt#❌-bad:-too-many-tools

**Contents:**
  - Choose models by task
  - Return concise results
- Common patterns
  - Multiple specialized subagents
- Troubleshooting
  - Subagent not being called
  - Context still getting bloated
  - Wrong subagent being selected

email_agent = {
    "name": "email-sender",
    "tools": [send_email, web_search, database_query, file_upload],  # Unfocused
}
python  theme={null}
subagents = [
    {
        "name": "contract-reviewer",
        "description": "Reviews legal documents and contracts",
        "system_prompt": "You are an expert legal reviewer...",
        "tools": [read_document, analyze_contract],
        "model": "claude-sonnet-4-5-20250929",  # Large context for long documents
    },
    {
        "name": "financial-analyst",
        "description": "Analyzes financial data and market trends",
        "system_prompt": "You are an expert financial analyst...",
        "tools": [get_stock_price, analyze_fundamentals],
        "model": "openai:gpt-5",  # Better for numerical analysis
    },
]
python  theme={null}
data_analyst = {
    "system_prompt": """Analyze the data and return:
    1. Key insights (3-5 bullet points)
    2. Overall confidence score
    3. Recommended next actions

Do NOT include:
    - Raw data
    - Intermediate calculations
    - Detailed tool outputs

Keep response under 300 words."""
}
python  theme={null}
from deepagents import create_deep_agent

subagents = [
    {
        "name": "data-collector",
        "description": "Gathers raw data from various sources",
        "system_prompt": "Collect comprehensive data on the topic",
        "tools": [web_search, api_call, database_query],
    },
    {
        "name": "data-analyzer",
        "description": "Analyzes collected data for insights",
        "system_prompt": "Analyze data and extract key insights",
        "tools": [statistical_analysis],
    },
    {
        "name": "report-writer",
        "description": "Writes polished reports from analysis",
        "system_prompt": "Create professional reports from insights",
        "tools": [format_document],
    },
]

agent = create_deep_agent(
    model="claude-sonnet-4-5-20250929",
    system_prompt="You coordinate data analysis and reporting. Use subagents for specialized tasks.",
    subagents=subagents
)
python  theme={null}
   # ✅ Good
   {"name": "research-specialist", "description": "Conducts in-depth research on specific topics using web search. Use when you need detailed information that requires multiple searches."}

# ❌ Bad
   {"name": "helper", "description": "helps with stuff"}
   python  theme={null}
   agent = create_deep_agent(
       system_prompt="""...your instructions...

IMPORTANT: For complex tasks, delegate to your subagents using the task() tool.
       This keeps your context clean and improves results.""",
       subagents=[...]
   )
   python  theme={null}
   system_prompt="""...

IMPORTANT: Return only the essential summary.
   Do NOT include raw data, intermediate search results, or detailed tool outputs.
   Your response should be under 500 words."""
   python  theme={null}
   system_prompt="""When you gather large amounts of data:
   1. Save raw data to /data/raw_results.txt
   2. Process and analyze the data
   3. Return only the analysis summary

This keeps context clean."""
   python  theme={null}
subagents = [
    {
        "name": "quick-researcher",
        "description": "For simple, quick research questions that need 1-2 searches. Use when you need basic facts or definitions.",
    },
    {
        "name": "deep-researcher",
        "description": "For complex, in-depth research requiring multiple searches, synthesis, and analysis. Use for comprehensive reports.",
    }
]
```

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/deepagents/subagents.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
### Choose models by task

Different models excel at different tasks:
```

Example 2 (unknown):
```unknown
### Return concise results

Instruct subagents to return summaries, not raw data:
```

Example 3 (unknown):
```unknown
## Common patterns

### Multiple specialized subagents

Create specialized subagents for different domains:
```

Example 4 (unknown):
```unknown
**Workflow:**

1. Main agent creates high-level plan
2. Delegates data collection to data-collector
3. Passes results to data-analyzer
4. Sends insights to report-writer
5. Compiles final output

Each subagent works with clean context focused only on its task.

## Troubleshooting

### Subagent not being called

**Problem**: Main agent tries to do work itself instead of delegating.

**Solutions**:

1. **Make descriptions more specific:**
```

---

## ... can add custom routes if needed.

**URL:** llms-txt#...-can-add-custom-routes-if-needed.

**Contents:**
- Configure `langgraph.json`
- Start server
- Deploying
- Next steps

json  theme={null}
{
  "dependencies": ["."],
  "graphs": {
    "agent": "./src/agent/graph.py:graph"
  },
  "env": ".env",
  "http": {
    "app": "./src/agent/webapp.py:app"
  }
  // Other configuration options like auth, store, etc.
}
bash  theme={null}
langgraph dev --no-browser
```

You should see your startup message printed when the server starts, and your cleanup message when you stop it with `Ctrl+C`.

You can deploy your app as-is to cloud or to your self-hosted platform.

Now that you've added lifespan events to your deployment, you can use similar techniques to add [custom routes](/langsmith/custom-routes) or [custom middleware](/langsmith/custom-middleware) to further customize your server's behavior.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/custom-lifespan.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
## Configure `langgraph.json`

Add the following to your `langgraph.json` configuration file. Make sure the path points to the `webapp.py` file you created above.
```

Example 2 (unknown):
```unknown
## Start server

Test the server out locally:
```

---

## ... database connection and query code

**URL:** llms-txt#...-database-connection-and-query-code

**Contents:**
  - Define the customer support agent

[(1, 'AC/DC'), (2, 'Accept'), (3, 'Aerosmith'), (4, 'Alanis Morissette'), (5, 'Alice In Chains'), (6, 'Antônio Carlos Jobim'), (7, 'Apocalyptica'), (8, 'Audioslave'), (9, 'BackBeat'), (10, 'Billy Cobham')]
python  theme={null}
import sqlite3

def _refund(invoice_id: int | None, invoice_line_ids: list[int] | None, mock: bool = False) -> float:
    ...

def _lookup( ...
`python  theme={null}
from typing import Literal
import json

from langchain.chat_models import init_chat_model
from langchain_core.runnables import RunnableConfig
from langgraph.graph import END, StateGraph
from langgraph.graph.message import AnyMessage, add_messages
from langgraph.types import Command, interrupt
from tabulate import tabulate
from typing_extensions import Annotated, TypedDict

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
And here's the database schema (image from [https://github.com/lerocha/chinook-database](https://github.com/lerocha/chinook-database)):

<img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/chinook-diagram.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=5da2a8dcca68f02dfcec11f9c472d341" alt="Chinook DB" data-og-width="1672" width="1672" data-og-height="1132" height="1132" data-path="langsmith/images/chinook-diagram.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/chinook-diagram.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=ea7b3a27e9780b556aa90f6914dcef30 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/chinook-diagram.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=d9cf3ddad46562213014ffb1a77b1e45 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/chinook-diagram.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=2c9e1e70e9be2cf07111b2211e1ef9b7 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/chinook-diagram.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=970f7f48c80222219b493211331ee22f 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/chinook-diagram.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=3188acb57c4abc0156f8687fa9e229d8 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/chinook-diagram.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=86b9655b9fd1bc38fcf9e054b11833df 2500w" />

### Define the customer support agent

We'll create a [LangGraph](https://langchain-ai.github.io/langgraph/) agent with limited access to our database. For demo purposes, our agent will support two basic types of requests:

* Lookup: The customer can look up song titles, artist names, and albums based on other identifying information. For example: "What songs do you have by Jimi Hendrix?"
* Refund: The customer can request a refund on their past purchases. For example: "My name is Claude Shannon and I'd like a refund on a purchase I made last week, could you help me?"

For simplicity in this demo, we'll implement refunds by deleting the corresponding database records. We'll skip implementing user authentication and other production security measures.

The agent's logic will be structured as two separate subgraphs (one for lookups and one for refunds), with a parent graph that routes requests to the appropriate subgraph.

#### Refund agent

Let's build the refund processing agent. This agent needs to:

1. Find the customer's purchase records in the database
2. Delete the relevant Invoice and InvoiceLine records to process the refund

We'll create two SQL helper functions:

1. A function to execute the refund by deleting records
2. A function to look up a customer's purchase history

To make testing easier, we'll add a "mock" mode to these functions. When mock mode is enabled, the functions will simulate database operations without actually modifying any data.
```

Example 3 (unknown):
```unknown
Now let's define our graph. We'll use a simple architecture with three main paths:

1. Extract customer and purchase information from the conversation

2. Route the request to one of three paths:

   * Refund path: If we have sufficient purchase details (Invoice ID or Invoice Line IDs) to process a refund
   * Lookup path: If we have enough customer information (name and phone) to search their purchase history
   * Response path: If we need more information, respond to the user requesting the specific details needed

The graph's state will track:

* The conversation history (messages between user and agent)
* All customer and purchase information extracted from the conversation
* The next message to send to the user (followup text)
```

---

## [

**URL:** llms-txt#[

---

## ... Define the graph ...

**URL:** llms-txt#...-define-the-graph-...

**Contents:**
- Capabilities
  - Human-in-the-loop
  - Memory
  - Time Travel
  - Fault-tolerance

graph.compile(
    checkpointer=InMemorySaver(serde=JsonPlusSerializer(pickle_fallback=True))
)
python  theme={null}
import sqlite3

from langgraph.checkpoint.serde.encrypted import EncryptedSerializer
from langgraph.checkpoint.sqlite import SqliteSaver

serde = EncryptedSerializer.from_pycryptodome_aes()  # reads LANGGRAPH_AES_KEY
checkpointer = SqliteSaver(sqlite3.connect("checkpoint.db"), serde=serde)
python  theme={null}
from langgraph.checkpoint.serde.encrypted import EncryptedSerializer
from langgraph.checkpoint.postgres import PostgresSaver

serde = EncryptedSerializer.from_pycryptodome_aes()
checkpointer = PostgresSaver.from_conn_string("postgresql://...", serde=serde)
checkpointer.setup()
```

When running on LangSmith, encryption is automatically enabled whenever `LANGGRAPH_AES_KEY` is present, so you only need to provide the environment variable. Other encryption schemes can be used by implementing [`CipherProtocol`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.serde.base.CipherProtocol) and supplying it to [`EncryptedSerializer`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.serde.encrypted.EncryptedSerializer).

### Human-in-the-loop

First, checkpointers facilitate [human-in-the-loop workflows](/oss/python/langgraph/interrupts) workflows by allowing humans to inspect, interrupt, and approve graph steps. Checkpointers are needed for these workflows as the human has to be able to view the state of a graph at any point in time, and the graph has to be to resume execution after the human has made any updates to the state. See [the how-to guides](/oss/python/langgraph/interrupts) for examples.

Second, checkpointers allow for ["memory"](/oss/python/concepts/memory) between interactions. In the case of repeated human interactions (like conversations) any follow up messages can be sent to that thread, which will retain its memory of previous ones. See [Add memory](/oss/python/langgraph/add-memory) for information on how to add and manage conversation memory using checkpointers.

Third, checkpointers allow for ["time travel"](/oss/python/langgraph/use-time-travel), allowing users to replay prior graph executions to review and / or debug specific graph steps. In addition, checkpointers make it possible to fork the graph state at arbitrary checkpoints to explore alternative trajectories.

Lastly, checkpointing also provides fault-tolerance and error recovery: if one or more nodes fail at a given superstep, you can restart your graph from the last successful step. Additionally, when a graph node fails mid-execution at a given superstep, LangGraph stores pending checkpoint writes from any other nodes that completed successfully at that superstep, so that whenever we resume graph execution from that superstep we don't re-run the successful nodes.

Additionally, when a graph node fails mid-execution at a given superstep, LangGraph stores pending checkpoint writes from any other nodes that completed successfully at that superstep, so that whenever we resume graph execution from that superstep we don't re-run the successful nodes.

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/persistence.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
#### Encryption

Checkpointers can optionally encrypt all persisted state. To enable this, pass an instance of [`EncryptedSerializer`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.serde.encrypted.EncryptedSerializer) to the `serde` argument of any [`BaseCheckpointSaver`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.base.BaseCheckpointSaver) implementation. The easiest way to create an encrypted serializer is via [`from_pycryptodome_aes`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.serde.encrypted.EncryptedSerializer.from_pycryptodome_aes), which reads the AES key from the `LANGGRAPH_AES_KEY` environment variable (or accepts a `key` argument):
```

Example 2 (unknown):
```unknown

```

---

## >                'description': 'Tool execution pending approval\n\nTool: execute_sql\nArgs: {...}'

**URL:** llms-txt#>----------------'description':-'tool-execution-pending-approval\n\ntool:-execute_sql\nargs:-{...}'

---

## - Email: foo@langchain.dev

**URL:** llms-txt#--email:-foo@langchain.dev

python  theme={null}
from langchain.tools import tool, ToolRuntime

@tool
def get_weather(city: str, runtime: ToolRuntime) -> str:
    """Get weather for a given city."""
    writer = runtime.stream_writer

# Stream custom updates as the tool executes
    writer(f"Looking up data for city: {city}")
    writer(f"Acquired data for city: {city}")

return f"It's always sunny in {city}!"
```

<Note>
  If you use `runtime.stream_writer` inside your tool, the tool must be invoked within a LangGraph execution context. See [Streaming](/oss/python/langchain/streaming) for more details.
</Note>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/tools.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
#### Stream Writer

Stream custom updates from tools as they execute using `runtime.stream_writer`. This is useful for providing real-time feedback to users about what a tool is doing.
```

---

## ✅ Good: Focused tool set

**URL:** llms-txt#✅-good:-focused-tool-set

email_agent = {
    "name": "email-sender",
    "tools": [send_email, validate_email],  # Only email-related
}

---

## > [Interrupt(value='Do you approve this action?')]

**URL:** llms-txt#>-[interrupt(value='do-you-approve-this-action?')]

---

## >    Interrupt(

**URL:** llms-txt#>----interrupt(

---

## >                'name': 'execute_sql',

**URL:** llms-txt#>----------------'name':-'execute_sql',

---

## - Name: Foo

**URL:** llms-txt#--name:-foo

---

## >          'review_configs': [

**URL:** llms-txt#>----------'review_configs':-[

---

## ... same as above

**URL:** llms-txt#...-same-as-above

**Contents:**
- Distributed tracing in TypeScript

@app.post("/my-route")
async def fake_route(request: Request):
    # request.headers:  {"langsmith-trace": "..."}
    my_application(langsmith_extra={"parent": request.headers})
typescript  theme={null}
// client.mts
import { getCurrentRunTree, traceable } from "langsmith/traceable";

const client = traceable(
    async () => {
        const runTree = getCurrentRunTree();
        return await fetch("...", {
            method: "POST",
            headers: runTree.toHeaders(),
        }).then((a) => a.text());
    },
    { name: "client" }
);

await client();
typescript Express.JS theme={null}
  // server.mts
  import { RunTree } from "langsmith";
  import { traceable, withRunTree } from "langsmith/traceable";
  import express from "express";
  import bodyParser from "body-parser";

const server = traceable(
          (text: string) => `Hello from the server! Received "${text}"`,
          { name: "server" }
      );

const app = express();
      app.use(bodyParser.text());

app.post("/", async (req, res) => {
      const runTree = RunTree.fromHeaders(req.headers);
      const result = await withRunTree(runTree, () => server(req.body));
      res.send(result);
  });
  typescript Hono theme={null}
  // server.mts
  import { RunTree } from "langsmith";
  import { traceable, withRunTree } from "langsmith/traceable";
  import { Hono } from "hono";

const server = traceable(
          (text: string) => `Hello from the server! Received "${text}"`,
          { name: "server" }
      );

const app = new Hono();

app.post("/", async (c) => {
      const body = await c.req.text();
      const runTree = RunTree.fromHeaders(c.req.raw.headers);
      const result = await withRunTree(runTree, () => server(body));
      return c.body(result);
  });
  ```
</CodeGroup>

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/distributed-tracing.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
## Distributed tracing in TypeScript

<Note>
  Distributed tracing in TypeScript requires `langsmith` version `>=0.1.31`
</Note>

First, we obtain the current run tree from the client and convert it to `langsmith-trace` and `baggage` header values, which we can pass to the server:
```

Example 2 (unknown):
```unknown
Then, the server converts the headers back to a run tree, which it uses to further continue the tracing.

To pass the newly created run tree to a traceable function, we can use the `withRunTree` helper, which will ensure the run tree is propagated within traceable invocations.

<CodeGroup>
```

Example 3 (unknown):
```unknown

```

---

## ... Same as before

**URL:** llms-txt#...-same-as-before

---

## ... Setup authenticate, etc.

**URL:** llms-txt#...-setup-authenticate,-etc.

**Contents:**
- Learn more

@auth.on
async def add_owner(
    ctx: Auth.types.AuthContext,
    value: dict  # The payload being sent to this access method
) -> dict:  # Returns a filter dict that restricts access to resources
    if is_studio_user(ctx.user):
        return {}

filters = {"owner": ctx.user.identity}
    metadata = value.setdefault("metadata", {})
    metadata.update(filters)
    return filters
```

Only use this if you want to permit developer access to a graph deployed on the managed LangSmith SaaS.

* [Authentication & Access Control](/langsmith/auth)
* [Setting up custom authentication tutorial](/langsmith/set-up-custom-auth)

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/langsmith/custom-auth.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

---

## ... the rest is the same as before

**URL:** llms-txt#...-the-rest-is-the-same-as-before

---

## -- This code should be in a separate file or service --

**URL:** llms-txt#---this-code-should-be-in-a-separate-file-or-service---

**Contents:**
- Interoperability between LangChain (Python) and LangSmith SDK

@chain
def parent_chain(inputs):
    rt = get_current_run_tree()
    headers = rt.to_headers()
    # ... make a request to another service with the headers
    # The headers should be passed to the other service, eventually to the child_wrapper function

parent_chain.invoke({"test": 1})
python  theme={null}
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langsmith import traceable

prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant. Please respond to the user's request only based on the given context."),
    ("user", "Question: {question}\nContext: {context}")
])

model = ChatOpenAI(model="gpt-4o-mini")
output_parser = StrOutputParser()
chain = prompt | model | output_parser

**Examples:**

Example 1 (unknown):
```unknown
## Interoperability between LangChain (Python) and LangSmith SDK

If you are using LangChain for part of your application and the LangSmith SDK (see [this guide](/langsmith/annotate-code)) for other parts, you can still trace the entire application seamlessly.

LangChain objects will be traced when invoked within a `traceable` function and be bound as a child run of the `traceable` function.
```

---

## > User is John Smith.

**URL:** llms-txt#>-user-is-john-smith.

**Contents:**
  - Prompt
  - Before model
  - After model

python  theme={null}
from langchain.tools import tool, ToolRuntime
from langchain_core.runnables import RunnableConfig
from langchain.messages import ToolMessage
from langchain.agents import create_agent, AgentState
from langgraph.types import Command
from pydantic import BaseModel

class CustomState(AgentState):  # [!code highlight]
    user_name: str

class CustomContext(BaseModel):
    user_id: str

@tool
def update_user_info(
    runtime: ToolRuntime[CustomContext, CustomState],
) -> Command:
    """Look up and update user info."""
    user_id = runtime.context.user_id 
    name = "John Smith" if user_id == "user_123" else "Unknown user"
    return Command(update={  # [!code highlight]
        "user_name": name,
        # update the message history
        "messages": [
            ToolMessage(
                "Successfully looked up user information",
                tool_call_id=runtime.tool_call_id
            )
        ]
    })

@tool
def greet(
    runtime: ToolRuntime[CustomContext, CustomState]
) -> str:
    """Use this to greet the user once you found their info."""
    user_name = runtime.state["user_name"]
    return f"Hello {user_name}!"

agent = create_agent(
    model="gpt-5-nano",
    tools=[update_user_info, greet],
    state_schema=CustomState, # [!code highlight]
    context_schema=CustomContext,  
)

agent.invoke(
    {"messages": [{"role": "user", "content": "greet the user"}]},
    context=CustomContext(user_id="user_123"),
)
python  theme={null}
from langchain.agents import create_agent
from typing import TypedDict
from langchain.agents.middleware import dynamic_prompt, ModelRequest

class CustomContext(TypedDict):
    user_name: str

def get_weather(city: str) -> str:
    """Get the weather in a city."""
    return f"The weather in {city} is always sunny!"

@dynamic_prompt
def dynamic_system_prompt(request: ModelRequest) -> str:
    user_name = request.runtime.context["user_name"]
    system_prompt = f"You are a helpful assistant. Address the user as {user_name}."
    return system_prompt

agent = create_agent(
    model="gpt-5-nano",
    tools=[get_weather],
    middleware=[dynamic_system_prompt],
    context_schema=CustomContext,
)

result = agent.invoke(
    {"messages": [{"role": "user", "content": "What is the weather in SF?"}]},
    context=CustomContext(user_name="John Smith"),
)
for msg in result["messages"]:
    msg.pretty_print()

shell title="Output" theme={null}
================================ Human Message =================================

What is the weather in SF?
================================== Ai Message ==================================
Tool Calls:
  get_weather (call_WFQlOGn4b2yoJrv7cih342FG)
 Call ID: call_WFQlOGn4b2yoJrv7cih342FG
  Args:
    city: San Francisco
================================= Tool Message =================================
Name: get_weather

The weather in San Francisco is always sunny!
================================== Ai Message ==================================

Hi John Smith, the weather in San Francisco is always sunny!
mermaid  theme={null}
%%{
    init: {
        "fontFamily": "monospace",
        "flowchart": {
        "curve": "basis"
        },
        "themeVariables": {"edgeLabelBackground": "transparent"}
    }
}%%
graph TD
    S(["\_\_start\_\_"])
    PRE(before_model)
    MODEL(model)
    TOOLS(tools)
    END(["\_\_end\_\_"])
    S --> PRE
    PRE --> MODEL
    MODEL -.-> TOOLS
    MODEL -.-> END
    TOOLS --> PRE
    classDef blueHighlight fill:#0a1c25,stroke:#0a455f,color:#bae6fd;
    class S blueHighlight;
    class END blueHighlight;
python  theme={null}
from langchain.messages import RemoveMessage
from langgraph.graph.message import REMOVE_ALL_MESSAGES
from langgraph.checkpoint.memory import InMemorySaver
from langchain.agents import create_agent, AgentState
from langchain.agents.middleware import before_model
from langgraph.runtime import Runtime
from typing import Any

@before_model
def trim_messages(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:
    """Keep only the last few messages to fit context window."""
    messages = state["messages"]

if len(messages) <= 3:
        return None  # No changes needed

first_msg = messages[0]
    recent_messages = messages[-3:] if len(messages) % 2 == 0 else messages[-4:]
    new_messages = [first_msg] + recent_messages

return {
        "messages": [
            RemoveMessage(id=REMOVE_ALL_MESSAGES),
            *new_messages
        ]
    }

agent = create_agent(
    model,
    tools=tools,
    middleware=[trim_messages]
)

config: RunnableConfig = {"configurable": {"thread_id": "1"}}

agent.invoke({"messages": "hi, my name is bob"}, config)
agent.invoke({"messages": "write a short poem about cats"}, config)
agent.invoke({"messages": "now do the same but for dogs"}, config)
final_response = agent.invoke({"messages": "what's my name?"}, config)

final_response["messages"][-1].pretty_print()
"""
================================== Ai Message ==================================

Your name is Bob. You told me that earlier.
If you'd like me to call you a nickname or use a different name, just say the word.
"""
mermaid  theme={null}
%%{
    init: {
        "fontFamily": "monospace",
        "flowchart": {
        "curve": "basis"
        },
        "themeVariables": {"edgeLabelBackground": "transparent"}
    }
}%%
graph TD
    S(["\_\_start\_\_"])
    MODEL(model)
    POST(after_model)
    TOOLS(tools)
    END(["\_\_end\_\_"])
    S --> MODEL
    MODEL --> POST
    POST -.-> END
    POST -.-> TOOLS
    TOOLS --> MODEL
    classDef blueHighlight fill:#0a1c25,stroke:#0a455f,color:#bae6fd;
    class S blueHighlight;
    class END blueHighlight;
    class POST greenHighlight;
python  theme={null}
from langchain.messages import RemoveMessage
from langgraph.checkpoint.memory import InMemorySaver
from langchain.agents import create_agent, AgentState
from langchain.agents.middleware import after_model
from langgraph.runtime import Runtime

@after_model
def validate_response(state: AgentState, runtime: Runtime) -> dict | None:
    """Remove messages containing sensitive words."""
    STOP_WORDS = ["password", "secret"]
    last_message = state["messages"][-1]
    if any(word in last_message.content for word in STOP_WORDS):
        return {"messages": [RemoveMessage(id=last_message.id)]}
    return None

agent = create_agent(
    model="gpt-5-nano",
    tools=[],
    middleware=[validate_response],
    checkpointer=InMemorySaver(),
)
```

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/short-term-memory.mdx)
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for    real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
#### Write short-term memory from tools

To modify the agent's short-term memory (state) during execution, you can return state updates directly from the tools.

This is useful for persisting intermediate results or making information accessible to subsequent tools or prompts.
```

Example 2 (unknown):
```unknown
### Prompt

Access short term memory (state) in middleware to create dynamic prompts based on conversation history or custom state fields.
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
### Before model

Access short term memory (state) in [`@before_model`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.before_model) middleware to process messages before model calls.
```

---

## >       value={

**URL:** llms-txt#>-------value={

---

## >          ]

**URL:** llms-txt#>----------]

---

## >          ],

**URL:** llms-txt#>----------],

---

## > ]

**URL:** llms-txt#>-]

---

## >             {

**URL:** llms-txt#>-------------{

---

## >       }

**URL:** llms-txt#>-------}

---

## >             }

**URL:** llms-txt#>-------------}

---

## >    )

**URL:** llms-txt#>----)

---

## > [

**URL:** llms-txt#>-[

---

## __interrupt__ contains the payload that was passed to interrupt()

**URL:** llms-txt#__interrupt__-contains-the-payload-that-was-passed-to-interrupt()

print(result["__interrupt__"])

---
