---
document_type: "Architecture"
version: "1.0.0"
last_modified: "2025-11-19"
status: "approved"
iteration: 1

authors:
  - name: "Architect Agent"
    role: "Solution Architect"

reviewers:
  - name: "PO Agent"
    role: "Product Owner"
    approved: true

compatible_with:
  prd: "v1.0"
  api_spec: "v1.0"

api_spec_hash: "0dc1d3610d28bf99"

changes_from_previous:
  - "Initial Architecture with frontmatter metadata"

git:
  commit_sha: ""
  tag: ""

metadata:
  components_count: 0
  external_services: []
  technology_stack:
    frontend: []
    backend: ["Python 3.11", "asyncio"]
    database: []
    infrastructure: []
---

# Canvaså­¦ä¹ ç³»ç»Ÿ - LangGraphè®°å¿†ç³»ç»Ÿé›†æˆè®¾è®¡

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0
**åˆ›å»ºæ—¥æœŸ**: 2025-11-11
**ä½œè€…**: Claude Code
**å…³è”PRD**: v1.1.3 Section 3.6
**çŠ¶æ€**: æŠ€æœ¯æ–¹æ¡ˆè®¾è®¡

---

## ğŸ“‹ æ¦‚è¿°

æœ¬æ–‡æ¡£è¯¦ç»†è®¾è®¡äº†Canvaså­¦ä¹ ç³»ç»Ÿçš„**åŒè®°å¿†æ¶æ„**ï¼Œæ˜ç¡®LangGraphæ¡†æ¶å±‚è®°å¿†ç³»ç»Ÿï¼ˆCheckpointerï¼‰ä¸3å±‚ä¸šåŠ¡è®°å¿†ç³»ç»Ÿï¼ˆGraphiti + Temporal + Semanticï¼‰çš„é›†æˆæ–¹æ¡ˆï¼Œå®šä¹‰èŒè´£è¾¹ç•Œã€è§¦å‘æ—¶æœºã€ä¸€è‡´æ€§ä¿è¯å’Œæ•…éšœå¤„ç†æœºåˆ¶ã€‚

### æ ¸å¿ƒç›®æ ‡

1. **èŒè´£æ˜ç¡®**: æ¸…æ™°å®šä¹‰LangGraph Checkpointerä¸3å±‚ä¸šåŠ¡è®°å¿†çš„èŒè´£è¾¹ç•Œ
2. **æ— ç¼é›†æˆ**: å®ç°æ¡†æ¶å±‚ä¸ä¸šåŠ¡å±‚è®°å¿†ç³»ç»Ÿçš„æ— ç¼åä½œ
3. **ä¸€è‡´æ€§ä¿è¯**: å¼ºä¸€è‡´æ€§ï¼ˆCanvas â†” Checkpointerï¼‰+ æœ€ç»ˆä¸€è‡´æ€§ï¼ˆCanvas â†” Graphitiï¼‰
4. **æ€§èƒ½ä¼˜åŒ–**: å¼‚æ­¥å†™å…¥ã€æ‰¹é‡æ“ä½œã€åˆ†å±‚ç¼“å­˜ç­–ç•¥
5. **å®¹é”™æœºåˆ¶**: å…³é”®è·¯å¾„ vs éå…³é”®è·¯å¾„çš„é”™è¯¯å¤„ç†

### è§£å†³çš„å…³é”®é—®é¢˜

> **PRDåé¦ˆçš„3ä¸ªæ ¸å¿ƒé—®é¢˜**:
> 1. â“ **ä½•æ—¶è§¦å‘è®°å¿†å­˜å‚¨**: ç¼ºä¹æ˜ç¡®çš„è§¦å‘æ—¶æœºå®šä¹‰
> 2. â“ **å¦‚ä½•é¿å…å†²çª**: LangGraph Checkpointerä¸ä¸šåŠ¡è®°å¿†ç³»ç»ŸèŒè´£ä¸æ¸…
> 3. â“ **å¦‚ä½•ä¿è¯ä¸€è‡´æ€§**: å¤šå±‚è®°å¿†ç³»ç»Ÿçš„æ•°æ®ä¸€è‡´æ€§ç¼ºå¤±

---

## ğŸ—ï¸ ä¸€ã€åŒè®°å¿†æ¶æ„è®¾è®¡

### 1.1 æ¶æ„å…¨æ™¯å›¾

```mermaid
graph TB
    subgraph "Canvaså­¦ä¹ ç³»ç»Ÿ"
        A[Canvas File] --> B[LangGraph Agent Node]
    end

    subgraph "æ¡†æ¶å±‚è®°å¿† (LangGraph)"
        B --> C[LangGraph Checkpointer]
        C --> D[(PostgreSQL)]
        D --> E[Stateæ¢å¤]
    end

    subgraph "ä¸šåŠ¡å±‚è®°å¿† (3å±‚ç³»ç»Ÿ)"
        B --> F[Graphiti Knowledge Graph]
        B --> G[Temporal Memory]
        B --> H[Semantic Memory]

        F --> I[(Neo4j)]
        G --> J[(Neo4j)]
        H --> K[(LanceDB)]
    end

    E --> L[å¤šè½®å¯¹è¯ä¸Šä¸‹æ–‡]
    I --> M[è·¨CanvasçŸ¥è¯†å…³è”]
    J --> N[å­¦ä¹ æ—¶é—´çº¿è¿½è¸ª]
    K --> O[æ–‡æ¡£è¯­ä¹‰æ£€ç´¢]

    style C fill:#e1f5fe
    style F fill:#fff3e0
    style G fill:#f3e5f5
    style H fill:#e8f5e9
```

---

### 1.2 åŒè®°å¿†æ¶æ„èŒè´£åˆ†å·¥

| ç»´åº¦ | LangGraph Checkpointer | Graphiti | Temporal Memory | Semantic Memory |
|------|----------------------|----------|----------------|----------------|
| **æ•°æ®ç±»å‹** | Agent Stateï¼ˆä¼šè¯çŠ¶æ€ï¼‰ | çŸ¥è¯†å›¾è°±ä¸‰å…ƒç»„ | å­¦ä¹ äº‹ä»¶æ—¶é—´çº¿ | æ–‡æ¡£å‘é‡åµŒå…¥ |
| **æ—¶é—´èŒƒå›´** | å½“å‰å­¦ä¹ ä¼šè¯ï¼ˆçŸ­æœŸï¼‰ | è·¨ä¼šè¯å†å²ï¼ˆé•¿æœŸï¼‰ | è·¨ä¼šè¯å†å²ï¼ˆé•¿æœŸï¼‰ | è·¨ä¼šè¯å†å²ï¼ˆé•¿æœŸï¼‰ |
| **æŸ¥è¯¢åœºæ™¯** | æ¢å¤Agentæ‰§è¡Œä¸Šä¸‹æ–‡ | è·¨CanvasçŸ¥è¯†å…³è” | è‰¾å®¾æµ©æ–¯å¤ä¹ è®¡åˆ’ | ç›¸ä¼¼æ–‡æ¡£æ£€ç´¢ |
| **æŒä¹…åŒ–** | PostgreSQL/InMemory | Neo4j | Neo4j | LanceDB + CUDA |
| **æ•°æ®é‡çº§** | MBçº§ï¼ˆå•ä¼šè¯Stateï¼‰ | GBçº§ï¼ˆå…¨å±€çŸ¥è¯†å›¾è°±ï¼‰ | GBçº§ï¼ˆäº‹ä»¶æ—¶é—´åºåˆ—ï¼‰ | GBçº§ï¼ˆå‘é‡æ•°æ®åº“ï¼‰ |
| **ä¸€è‡´æ€§è¦æ±‚** | å¼ºä¸€è‡´æ€§ï¼ˆä¸Canvasï¼‰ | æœ€ç»ˆä¸€è‡´æ€§ | æœ€ç»ˆä¸€è‡´æ€§ | æœ€ç»ˆä¸€è‡´æ€§ |
| **æ›´æ–°é¢‘ç‡** | æ¯æ¬¡Agentæ“ä½œï¼ˆé«˜é¢‘ï¼‰ | Canvasæ“ä½œåå¼‚æ­¥ï¼ˆä½é¢‘ï¼‰ | Canvasæ“ä½œåå¼‚æ­¥ï¼ˆä½é¢‘ï¼‰ | æ–‡æ¡£ç”Ÿæˆåå¼‚æ­¥ï¼ˆä½é¢‘ï¼‰ |
| **æŸ¥è¯¢æ€§èƒ½** | <50msï¼ˆStateæ¢å¤ï¼‰ | <200msï¼ˆç®€å•æŸ¥è¯¢ï¼‰ | <100msï¼ˆæ—¶é—´æŸ¥è¯¢ï¼‰ | <150msï¼ˆå‘é‡æ£€ç´¢ï¼‰ |

---

### 1.3 æ ¸å¿ƒè®¾è®¡åŸåˆ™

**åŸåˆ™1: åˆ†å±‚èŒè´£**
- **æ¡†æ¶å±‚ï¼ˆCheckpointerï¼‰**: è´Ÿè´£Agent Stateç®¡ç†ï¼Œæ— ä¸šåŠ¡é€»è¾‘
- **ä¸šåŠ¡å±‚ï¼ˆ3å±‚è®°å¿†ï¼‰**: è´Ÿè´£ä¸šåŠ¡çŸ¥è¯†ç®¡ç†ï¼Œä¸æ„ŸçŸ¥Agent State

**åŸåˆ™2: ä¸€è‡´æ€§åˆ†çº§**
- **å¼ºä¸€è‡´æ€§**: Canvasæ–‡ä»¶ â†” LangGraph Stateï¼ˆå¿…é¡»åŒæ­¥ï¼‰
- **æœ€ç»ˆä¸€è‡´æ€§**: Canvasæ–‡ä»¶ â†” Graphiti/Temporal/Semanticï¼ˆå…è®¸å»¶è¿Ÿï¼‰

**åŸåˆ™3: å…³é”®è·¯å¾„ä¼˜å…ˆ**
- **å…³é”®è·¯å¾„**: Canvasæ“ä½œ + CheckpointeræŒä¹…åŒ–ï¼ˆå¤±è´¥å¿…é¡»å›æ»šï¼‰
- **éå…³é”®è·¯å¾„**: ä¸šåŠ¡è®°å¿†å­˜å‚¨ï¼ˆå¤±è´¥ä»…è®°å½•æ—¥å¿—ï¼Œä¸å½±å“ä¸»æµç¨‹ï¼‰

**åŸåˆ™4: æ€§èƒ½ä¼˜å…ˆ**
- Checkpointerå†™å…¥å¼‚æ­¥åŒ–ï¼ˆ<100msï¼‰
- ä¸šåŠ¡è®°å¿†æ‰¹é‡å†™å…¥ï¼ˆæ‰¹é‡10ä¸ªæ“ä½œï¼‰
- åˆ†å±‚ç¼“å­˜ï¼ˆL1-L4ï¼‰

---

## ğŸ”§ äºŒã€LangGraph Checkpointerè¯¦ç»†è®¾è®¡

### 2.1 Checkpointeré€‰å‹å’Œé…ç½®

**ç”Ÿäº§ç¯å¢ƒæ¨è**: PostgresSaver

```python
from langgraph.checkpoint.postgres import PostgresSaver

# æ•°æ®åº“è¿æ¥é…ç½®
DB_URI = "postgresql://canvas_user:password@localhost:5432/canvas_learning"
checkpointer = PostgresSaver.from_conn_string(DB_URI)

# PostgreSQLæ•°æ®åº“schema
CREATE TABLE IF NOT EXISTS checkpoints (
    thread_id TEXT NOT NULL,
    checkpoint_id TEXT NOT NULL,
    parent_checkpoint_id TEXT,
    checkpoint JSONB NOT NULL,
    metadata JSONB,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    PRIMARY KEY (thread_id, checkpoint_id)
);

CREATE INDEX idx_thread_id ON checkpoints(thread_id);
CREATE INDEX idx_created_at ON checkpoints(created_at);
```

**å¼€å‘/æµ‹è¯•ç¯å¢ƒ**: InMemorySaver

```python
from langgraph.checkpoint.memory import InMemorySaver

checkpointer = InMemorySaver()  # å¿«é€Ÿä½†ä¸æŒä¹…åŒ–
graph = builder.compile(checkpointer=checkpointer)
```

**é…ç½®å¯¹æ¯”**:

| ç‰¹æ€§ | PostgresSaver | InMemorySaver |
|------|--------------|--------------|
| æŒä¹…åŒ– | âœ… æ˜¯ï¼ˆPostgreSQLï¼‰ | âŒ å¦ï¼ˆè¿›ç¨‹å†…å­˜ï¼‰ |
| å†™å…¥é€Ÿåº¦ | ~50ms | ~5ms |
| æ¢å¤èƒ½åŠ› | âœ… å¯æ¢å¤å†å²ä¼šè¯ | âŒ è¿›ç¨‹é‡å¯ä¸¢å¤± |
| é€‚ç”¨ç¯å¢ƒ | ç”Ÿäº§ç¯å¢ƒ | å¼€å‘/æµ‹è¯• |
| å¹¶å‘æ”¯æŒ | âœ… å¤šè¿›ç¨‹å®‰å…¨ | âš ï¸ å•è¿›ç¨‹ |

---

### 2.2 thread_idè®¾è®¡ç­–ç•¥

**thread_idæ ¼å¼**: `canvas_{canvas_name}_{session_id}`

```python
def generate_thread_id(canvas_path: str, session_id: str) -> str:
    """ç”Ÿæˆthread_id

    Args:
        canvas_path: Canvasæ–‡ä»¶è·¯å¾„
        session_id: å”¯ä¸€ä¼šè¯æ ‡è¯†ç¬¦ï¼ˆUUID v4ï¼‰

    Returns:
        thread_idå­—ç¬¦ä¸²

    Examples:
        >>> generate_thread_id("ç¬”è®°åº“/ç¦»æ•£æ•°å­¦/ç¦»æ•£æ•°å­¦.canvas", "a1b2c3d4-...")
        "canvas_ç¦»æ•£æ•°å­¦_a1b2c3d4-e5f6-7890-abcd-ef1234567890"
    """
    canvas_name = Path(canvas_path).stem
    return f"canvas_{canvas_name}_{session_id}"
```

**è®¾è®¡åŸåˆ™**:

1. **å”¯ä¸€æ€§**: æ¯ä¸ªå­¦ä¹ ä¼šè¯ç‹¬ç«‹thread_idï¼Œé¿å…çŠ¶æ€æ··æ·†
   ```python
   # åŒä¸€Canvasçš„ä¸¤ä¸ªä¼šè¯æœ‰ä¸åŒçš„thread_id
   session1 = "a1b2c3d4-..."
   session2 = "b2c3d4e5-..."
   thread1 = "canvas_ç¦»æ•£æ•°å­¦_a1b2c3d4-..."  # äº’ä¸å¹²æ‰°
   thread2 = "canvas_ç¦»æ•£æ•°å­¦_b2c3d4e5-..."
   ```

2. **å¯è¿½æº¯æ€§**: ä»thread_idå¯ç›´æ¥å®šä½Canvasæ–‡ä»¶å’Œä¼šè¯
   ```python
   # ä»thread_idè§£æCanvasåç§°
   def parse_thread_id(thread_id: str) -> tuple[str, str]:
       parts = thread_id.split("_", 2)
       canvas_name = parts[1]
       session_id = parts[2]
       return canvas_name, session_id
   ```

3. **éš”ç¦»æ€§**: åŒä¸€Canvasçš„ä¸åŒä¼šè¯äº’ä¸å¹²æ‰°
   - ä¸åŒç”¨æˆ· â†’ ä¸åŒsession_id â†’ ä¸åŒthread_id
   - åŒä¸€ç”¨æˆ·çš„ä¸åŒå­¦ä¹ ä¼šè¯ â†’ ä¸åŒsession_id

4. **è·¨ä¼šè¯æŸ¥è¯¢**: Temporal Memoryå¯é€šè¿‡canvas_nameæŸ¥è¯¢å†å²æ‰€æœ‰ä¼šè¯
   ```python
   # Temporal MemoryæŸ¥è¯¢æ‰€æœ‰å…³äº"ç¦»æ•£æ•°å­¦"çš„å­¦ä¹ ä¼šè¯
   sessions = temporal_memory.query_sessions_by_canvas(canvas_name="ç¦»æ•£æ•°å­¦")
   ```

**ç”Ÿå‘½å‘¨æœŸç®¡ç†**:

```python
class SessionLifecycleManager:
    """ä¼šè¯ç”Ÿå‘½å‘¨æœŸç®¡ç†"""

    def create_session(self, canvas_path: str, user_id: str) -> str:
        """åˆ›å»ºæ–°ä¼šè¯"""
        session_id = str(uuid.uuid4())
        thread_id = generate_thread_id(canvas_path, session_id)

        # è®°å½•ä¼šè¯å…ƒä¿¡æ¯åˆ°Temporal Memory
        temporal_memory.create_session({
            "session_id": session_id,
            "canvas_path": canvas_path,
            "user_id": user_id,
            "created_at": datetime.now(),
            "status": "active"
        })

        return session_id

    def close_session(self, session_id: str):
        """å…³é—­ä¼šè¯"""
        temporal_memory.update_session(session_id, {
            "status": "closed",
            "closed_at": datetime.now()
        })

    def cleanup_old_sessions(self, days=30):
        """æ¸…ç†è¶…è¿‡30å¤©æ— æ´»åŠ¨çš„ä¼šè¯"""
        cutoff_date = datetime.now() - timedelta(days=days)
        old_sessions = temporal_memory.query_inactive_sessions(cutoff_date)

        for session in old_sessions:
            # åˆ é™¤å¯¹åº”çš„checkpoints
            checkpointer.delete_checkpoints_by_thread_id(
                generate_thread_id(session["canvas_path"], session["session_id"])
            )
```

---

### 2.3 configå‚æ•°ç»“æ„å®šä¹‰

**å®Œæ•´configå®ç°**:

```python
from typing import TypedDict
from pathlib import Path

class LangGraphConfig(TypedDict):
    """LangGraphè°ƒç”¨é…ç½®å‚æ•°"""
    configurable: dict

def create_langgraph_config(
    canvas_path: str,
    user_id: str,
    session_id: str,
    checkpoint_id: str | None = None
) -> LangGraphConfig:
    """ç”ŸæˆLangGraph graph.invoke()æ‰€éœ€çš„configå‚æ•°

    Args:
        canvas_path: Canvasæ–‡ä»¶ç»å¯¹è·¯å¾„
        user_id: ç”¨æˆ·å”¯ä¸€æ ‡è¯†ç¬¦
        session_id: ä¼šè¯å”¯ä¸€æ ‡è¯†ç¬¦ï¼ˆUUID v4ï¼‰
        checkpoint_id: å¯é€‰ï¼ŒæŒ‡å®šæ¢å¤çš„checkpoint ID

    Returns:
        ç¬¦åˆLangGraphæ ‡å‡†çš„configå­—å…¸
    """
    canvas_name = Path(canvas_path).stem
    thread_id = generate_thread_id(canvas_path, session_id)

    config: LangGraphConfig = {
        "configurable": {
            # LangGraphæ ¸å¿ƒå‚æ•°
            "thread_id": thread_id,  # ä¼šè¯æ ‡è¯†ç¬¦
            "checkpoint_id": checkpoint_id,  # æ¢å¤çš„checkpoint IDï¼ˆå¯é€‰ï¼‰

            # Canvaså­¦ä¹ ç³»ç»Ÿä¸šåŠ¡å‚æ•°
            "canvas_path": canvas_path,  # Canvasæ–‡ä»¶è·¯å¾„
            "user_id": user_id,          # ç”¨æˆ·ID
            "session_id": session_id,    # ä¼šè¯ID
            "canvas_name": canvas_name,  # Canvasåç§°ï¼ˆæ–¹ä¾¿è°ƒè¯•ï¼‰

            # å¯é€‰æ‰©å±•å‚æ•°
            "checkpoint_ns": "canvas_learning",  # å‘½åç©ºé—´éš”ç¦»
        }
    }

    return config
```

**ä½¿ç”¨ç¤ºä¾‹**:

```python
# åœºæ™¯1: åˆ›å»ºæ–°ä¼šè¯
session_id = str(uuid.uuid4())
config = create_langgraph_config(
    canvas_path="C:/Users/ROG/æ‰˜ç¦/ç¬”è®°åº“/ç¦»æ•£æ•°å­¦/ç¦»æ•£æ•°å­¦.canvas",
    user_id="user_12345",
    session_id=session_id
)

result = graph.invoke({
    "canvas_path": config["configurable"]["canvas_path"],
    "operation": "decomposition",
    "concept": "é€†å¦å‘½é¢˜"
}, config=config)

# åœºæ™¯2: æ¢å¤å†å²checkpoint
historical_config = create_langgraph_config(
    canvas_path="C:/Users/ROG/æ‰˜ç¦/ç¬”è®°åº“/ç¦»æ•£æ•°å­¦/ç¦»æ•£æ•°å­¦.canvas",
    user_id="user_12345",
    session_id=existing_session_id,
    checkpoint_id="checkpoint_abc123"  # æŒ‡å®šcheckpoint
)

historical_state = graph.get_state(historical_config)
```

---

### 2.4 State Schemaå®šä¹‰

**CanvasLearningStateå®Œæ•´å®šä¹‰**:

```python
from typing import Annotated, Literal
from typing_extensions import TypedDict
from langgraph.graph.message import add_messages

class CanvasLearningState(TypedDict):
    """Canvaså­¦ä¹ ç³»ç»Ÿçš„LangGraph State Schema"""

    # ========== ä¼šè¯å…ƒä¿¡æ¯ ==========
    canvas_path: str              # Canvasæ–‡ä»¶ç»å¯¹è·¯å¾„
    user_id: str                  # ç”¨æˆ·å”¯ä¸€æ ‡è¯†ç¬¦
    session_id: str               # ä¼šè¯å”¯ä¸€æ ‡è¯†ç¬¦

    # ========== å½“å‰æ“ä½œä¸Šä¸‹æ–‡ ==========
    operation: Literal[
        "decomposition",          # åŸºç¡€æ‹†è§£
        "deep_decomposition",     # æ·±åº¦æ‹†è§£
        "scoring",                # è¯„åˆ†
        "explanation",            # è§£é‡Šç”Ÿæˆ
        "verification",           # æ£€éªŒé—®é¢˜ç”Ÿæˆ
        "concurrent_analysis"     # å¹¶å‘åˆ†æ
    ]
    concept: str                  # å½“å‰å¤„ç†çš„æ¦‚å¿µåç§°
    target_nodes: list[str]       # å½“å‰æ“ä½œçš„èŠ‚ç‚¹IDs

    # ========== Agentè¾“å‡ºç»“æœ ==========
    decomposition_results: dict[str, list[str]]  # {node_id: [questions]}
    explanation_results: dict[str, str]          # {node_id: doc_path}
    scoring_results: dict[str, dict]             # {node_id: scoring_data}

    # ç¤ºä¾‹scoring_dataç»“æ„:
    # {
    #     "accuracy": 22,
    #     "imagery": 18,
    #     "completeness": 20,
    #     "originality": 15,
    #     "total": 75,
    #     "color": "3",  # ç´«è‰²
    #     "recommendations": ["clarification-path", "oral-explanation"]
    # }

    # ========== LangChain messagesï¼ˆå¯¹è¯å†å²ï¼‰ ==========
    messages: Annotated[list, add_messages]  # å¯¹è¯æ¶ˆæ¯ç´¯ç§¯

    # ========== æ‰§è¡ŒçŠ¶æ€è®°å½• ==========
    last_operation: str           # æœ€åæ‰§è¡Œçš„æ“ä½œ
    last_timestamp: str           # æœ€åæ“ä½œæ—¶é—´ï¼ˆISO 8601æ ¼å¼ï¼‰
    tasks_completed: int          # å·²å®Œæˆä»»åŠ¡æ•°
    tasks_failed: int             # å¤±è´¥ä»»åŠ¡æ•°
    error_log: list[dict]         # é”™è¯¯æ—¥å¿—

    # ç¤ºä¾‹error_logæ¡ç›®:
    # {
    #     "timestamp": "2025-11-11T14:30:00",
    #     "agent": "decomposition",
    #     "node_id": "red_001",
    #     "error": "Canvas write failed: Permission denied",
    #     "retry_count": 2
    # }
```

**Stateæ›´æ–°ç¤ºä¾‹**:

```python
def basic_decomposition_node(state: CanvasLearningState) -> CanvasLearningState:
    """åŸºç¡€æ‹†è§£AgentèŠ‚ç‚¹"""
    # Step 1: ç”Ÿæˆæ‹†è§£é—®é¢˜
    questions = generate_decomposition_questions(state["concept"])

    # Step 2: å†™å…¥Canvasï¼ˆå…³é”®è·¯å¾„ï¼‰
    write_questions_to_canvas(
        state["canvas_path"],
        questions,
        config={"thread_id": generate_thread_id(state["canvas_path"], state["session_id"])}
    )

    # Step 3: å¼‚æ­¥å­˜å‚¨åˆ°Graphitiï¼ˆéå…³é”®è·¯å¾„ï¼‰
    try:
        asyncio.create_task(
            store_to_graphiti(state["session_id"], "decomposition", questions)
        )
    except Exception as e:
        logger.error(f"Graphiti storage failed: {e}")

    # Step 4: è¿”å›æ›´æ–°çš„Stateï¼ˆLangGraphè‡ªåŠ¨æŒä¹…åŒ–ï¼‰
    return {
        **state,
        "decomposition_results": {state["concept"]: questions},
        "tasks_completed": state["tasks_completed"] + 1,
        "last_operation": "decomposition",
        "last_timestamp": datetime.now().isoformat()
    }
```

---

## ğŸ”— ä¸‰ã€ä¸3å±‚ä¸šåŠ¡è®°å¿†ç³»ç»Ÿé›†æˆ

### 3.1 GraphitiçŸ¥è¯†å›¾è°±é›†æˆ

**èŒè´£å®šä¹‰**:
- **å­˜å‚¨å†…å®¹**: CanvasèŠ‚ç‚¹ã€è¾¹ã€æ¦‚å¿µå…³ç³»ä¸‰å…ƒç»„
- **æŸ¥è¯¢åœºæ™¯**: è·¨CanvasçŸ¥è¯†å…³è”ã€æ¦‚å¿µè¯­ä¹‰æ£€ç´¢
- **è§¦å‘æ—¶æœº**: Canvasæ“ä½œæˆåŠŸåå¼‚æ­¥å­˜å‚¨

**é›†æˆæ–¹å¼**:

```python
async def store_to_graphiti(
    session_id: str,
    operation_type: str,
    canvas_path: str,
    data: dict,
    config: dict
) -> None:
    """å¼‚æ­¥å­˜å‚¨åˆ°GraphitiçŸ¥è¯†å›¾è°±

    Args:
        session_id: ä¼šè¯ID
        operation_type: æ“ä½œç±»å‹ï¼ˆdecomposition/scoring/explanationï¼‰
        canvas_path: Canvasæ–‡ä»¶è·¯å¾„
        data: è¦å­˜å‚¨çš„æ•°æ®
        config: LangGraph configå‚æ•°
    """
    try:
        # Step 1: æ„å»ºGraphitiä¸‰å…ƒç»„
        triples = build_graphiti_triples(operation_type, data)

        # Step 2: æ‰¹é‡å†™å…¥Neo4j
        await graphiti_client.batch_write(triples)

        # Step 3: è®°å½•åˆ°Temporal Memory
        await temporal_memory.log_event({
            "session_id": session_id,
            "event_type": "graphiti_sync",
            "timestamp": datetime.now(),
            "data": {
                "operation_type": operation_type,
                "triples_count": len(triples)
            }
        })

    except Exception as e:
        # è®°å½•é”™è¯¯ä½†ä¸é˜»å¡ä¸»æµç¨‹
        logger.error(f"Graphiti storage failed for session {session_id}: {e}")
        await temporal_memory.log_event({
            "session_id": session_id,
            "event_type": "graphiti_sync_failed",
            "timestamp": datetime.now(),
            "error": str(e)
        })
```

**æ•°æ®ç¤ºä¾‹**:

```cypher
// Graphitiå­˜å‚¨çš„ä¸‰å…ƒç»„ç¤ºä¾‹
CREATE (c:Canvas {
    name: "ç¦»æ•£æ•°å­¦",
    path: "ç¬”è®°åº“/ç¦»æ•£æ•°å­¦/ç¦»æ•£æ•°å­¦.canvas",
    created_at: "2025-10-01"
})

CREATE (concept:Concept {
    name: "é€†å¦å‘½é¢˜",
    domain: "ç¦»æ•£æ•°å­¦",
    first_seen: "2025-11-11"
})

CREATE (node:Node {
    canvas_id: "red_001",
    text: "ä»€ä¹ˆæ˜¯é€†å¦å‘½é¢˜?",
    color: "1",  // çº¢è‰²
    created_at: "2025-11-11T14:30:00"
})

CREATE (understanding:UnderstandingState {
    node_id: "yellow_123",
    accuracy: 22,
    imagery: 18,
    completeness: 20,
    originality: 15,
    total: 75,
    timestamp: "2025-11-11T14:35:00"
})

// å…³ç³»
CREATE (c)-[:CONTAINS]->(node)
CREATE (node)-[:IS_ABOUT]->(concept)
CREATE (node)-[:HAS_UNDERSTANDING_STATE]->(understanding)
CREATE (understanding)-[:CREATED_IN_SESSION]->(session)
```

---

### 3.2 Temporal Memoryé›†æˆ

**èŒè´£å®šä¹‰**:
- **å­˜å‚¨å†…å®¹**: å­¦ä¹ äº‹ä»¶æ—¶é—´çº¿ã€è¿›åº¦è¿½è¸ªã€è‰¾å®¾æµ©æ–¯å¤ä¹ è®¡åˆ’
- **æŸ¥è¯¢åœºæ™¯**: å­¦ä¹ å†å²æŸ¥è¯¢ã€å¤ä¹ è®¡åˆ’ç”Ÿæˆ
- **è§¦å‘æ—¶æœº**: Canvasæ“ä½œæˆåŠŸåå¼‚æ­¥å­˜å‚¨

**é›†æˆæ–¹å¼**:

```python
async def store_to_temporal_memory(
    session_id: str,
    event_type: str,
    timestamp: datetime,
    data: dict
) -> None:
    """å¼‚æ­¥å­˜å‚¨åˆ°Temporal Memory

    Args:
        session_id: ä¼šè¯ID
        event_type: äº‹ä»¶ç±»å‹
        timestamp: äº‹ä»¶æ—¶é—´
        data: äº‹ä»¶æ•°æ®
    """
    event = {
        "session_id": session_id,
        "event_type": event_type,
        "timestamp": timestamp.isoformat(),
        "data": data
    }

    try:
        await temporal_memory_client.insert_event(event)
    except Exception as e:
        logger.error(f"Temporal memory storage failed: {e}")
```

**Neo4j Schema (Temporal Memory)**:

```cypher
// å­¦ä¹ äº‹ä»¶èŠ‚ç‚¹
CREATE (e:LearningEvent {
    id: randomUUID(),
    session_id: $session_id,
    event_type: $event_type,
    timestamp: datetime(),
    data: $data
})

// æ—¶åºç´¢å¼•ï¼ˆNeo4jï¼‰
CREATE INDEX learning_event_timestamp IF NOT EXISTS FOR (e:LearningEvent) ON (e.timestamp);
CREATE INDEX learning_event_session IF NOT EXISTS FOR (e:LearningEvent) ON (e.session_id);
CREATE INDEX learning_event_type IF NOT EXISTS FOR (e:LearningEvent) ON (e.event_type);
```

**äº‹ä»¶ç±»å‹**:

```python
# å­¦ä¹ äº‹ä»¶ç±»å‹æšä¸¾
class LearningEventType(Enum):
    DECOMPOSITION_COMPLETED = "decomposition_completed"
    SCORING_COMPLETED = "scoring_completed"
    EXPLANATION_GENERATED = "explanation_generated"
    COLOR_TRANSITION = "color_transition"  # çº¢â†’ç´«â†’ç»¿
    REVIEW_SCHEDULED = "review_scheduled"  # è‰¾å®¾æµ©æ–¯å¤ä¹ è®¡åˆ’
    GRAPHITI_SYNC = "graphiti_sync"
    SEMANTIC_SYNC = "semantic_sync"
```

---

### 3.3 Semantic Memoryé›†æˆ

**èŒè´£å®šä¹‰**:
- **å­˜å‚¨å†…å®¹**: AIç”Ÿæˆçš„è§£é‡Šæ–‡æ¡£å‘é‡åµŒå…¥
- **æŸ¥è¯¢åœºæ™¯**: ç›¸ä¼¼æ–‡æ¡£æ£€ç´¢ã€è¯­ä¹‰æœç´¢
- **è§¦å‘æ—¶æœº**: è§£é‡Šæ–‡æ¡£ç”Ÿæˆåå¼‚æ­¥å­˜å‚¨

**é›†æˆæ–¹å¼**:

```python
async def store_to_semantic_memory(
    doc_path: str,
    concept: str,
    agent_type: str,
    session_id: str
) -> None:
    """å¼‚æ­¥å­˜å‚¨åˆ°Semantic Memory

    Args:
        doc_path: æ–‡æ¡£è·¯å¾„
        concept: æ¦‚å¿µåç§°
        agent_type: ç”Ÿæˆæ–‡æ¡£çš„Agentç±»å‹
        session_id: ä¼šè¯ID
    """
    try:
        # Step 1: è¯»å–æ–‡æ¡£å†…å®¹
        with open(doc_path, 'r', encoding='utf-8') as f:
            content = f.read()

        # Step 2: ç”Ÿæˆå‘é‡åµŒå…¥
        # Step 3: ç”Ÿæˆå‘é‡åµŒå…¥ (CUDAåŠ é€Ÿ)
        from sentence_transformers import SentenceTransformer
        model = SentenceTransformer('all-MiniLM-L6-v2', device='cuda')  # GPUåŠ é€Ÿ
        embedding = model.encode(content)  # 768ç»´å‘é‡

        # Step 4: å†™å…¥LanceDB
        await lance_table.add([{
            "id": str(uuid.uuid4()),
            "text": content,
            "vector": embedding.tolist(),
            "doc_path": doc_path,
            "concept": concept,
            "agent_type": agent_type,
            "session_id": session_id,
            "created_at": datetime.now().isoformat()
        }])

        # Step 5: åˆ›å»ºæˆ–æ›´æ–°FTSç´¢å¼•(BM25)
        await lance_table.create_fts_index("text", replace=True)

    except Exception as e:
        logger.error(f"Semantic memory storage failed: {e}")
```

**LanceDB Tableé…ç½®**:

```python
# âœ… Verified from LanceDB Context7
import lancedb
from lancedb.pydantic import LanceModel, Vector
from lancedb.embeddings import get_registry

# åˆå§‹åŒ–LanceDBå®¢æˆ·ç«¯
db = lancedb.connect("./data/lancedb")

# å®šä¹‰Schema with Pydantic
embedder = get_registry().get("sentence-transformers").create()

class CanvasDocSchema(LanceModel):
    text: str = embedder.SourceField()
    vector: Vector(embedder.ndims()) = embedder.VectorField()
    doc_path: str
    concept: str
    agent_type: str
    session_id: str
    created_at: str

# åˆ›å»ºæˆ–è·å–table
table = db.create_table(
    "canvas_learning_docs",
    schema=CanvasDocSchema,
    mode="overwrite"  # é¦–æ¬¡åˆ›å»ºæ—¶ä½¿ç”¨,åç»­ä½¿ç”¨open_table()
)

# åˆ›å»ºBM25å…¨æ–‡ç´¢å¼•(æ”¯æŒHybrid Search)
table.create_fts_index("text", replace=True)

# å‘é‡ç»´åº¦: 768 (sentence-transformers/all-MiniLM-L6-v2)
# GPUåŠ é€Ÿ: PyTorch CUDA (éœ€è¦ >=4GB VRAM)
# Lanceæ•°æ®æ ¼å¼: Parquetæ¼”è¿›ç‰ˆ,100xæŸ¥è¯¢æ€§èƒ½æå‡
```

---

### 3.4 è®°å¿†ç³»ç»Ÿè°ƒåº¦çŸ©é˜µ

| Canvasæ“ä½œ | LangGraph Checkpointer | Graphiti | Temporal Memory | Semantic Memory | ç²¾ç¡®æ—¶æœº |
|-----------|----------------------|----------|----------------|----------------|---------|
| **åŸºç¡€æ‹†è§£** | âœ… åŒæ­¥ | âœ… å¼‚æ­¥ | âœ… å¼‚æ­¥ | âŒ | Agent Nodeè¿”å›æ—¶ |
| **æ·±åº¦æ‹†è§£** | âœ… åŒæ­¥ | âœ… å¼‚æ­¥ | âœ… å¼‚æ­¥ | âŒ | Agent Nodeè¿”å›æ—¶ |
| **è¯„åˆ†** | âœ… åŒæ­¥ | âœ… å¼‚æ­¥ | âœ… å¼‚æ­¥ | âŒ | Agent Nodeè¿”å›æ—¶ |
| **ç”Ÿæˆè§£é‡Š** | âœ… åŒæ­¥ | âœ… å¼‚æ­¥ | âœ… å¼‚æ­¥ | âœ… å¼‚æ­¥ | æ–‡æ¡£å†™å…¥ç£ç›˜å |
| **é¢œè‰²æµè½¬** | âœ… åŒæ­¥ | âœ… å¼‚æ­¥ | âœ… å¼‚æ­¥ | âŒ | Canvasæ–‡ä»¶æ›´æ–°å |
| **åˆ›å»ºæ£€éªŒç™½æ¿** | âœ… åŒæ­¥ | âœ… å¼‚æ­¥ | âœ… å¼‚æ­¥ | âŒ | æ–°Canvasæ–‡ä»¶åˆ›å»ºå |

**6æ­¥ç²¾ç¡®æ—¶åº**ï¼ˆä»¥basic-decompositionä¸ºä¾‹ï¼‰:

```python
async def basic_decomposition_node(state: CanvasLearningState):
    """åŸºç¡€æ‹†è§£AgentèŠ‚ç‚¹ - å±•ç¤ºå®Œæ•´çš„6æ­¥æ—¶åº"""

    # Step 1: ç”Ÿæˆæ‹†è§£é—®é¢˜
    questions = generate_decomposition_questions(state["concept"])

    # Step 2: å†™å…¥Canvasï¼ˆå…³é”®è·¯å¾„ - å¿…é¡»æˆåŠŸï¼‰
    try:
        write_questions_to_canvas(state["canvas_path"], questions)
    except Exception as e:
        # Canvaså†™å…¥å¤±è´¥ â†’ æ•´ä¸ªæ“ä½œå¤±è´¥
        raise CanvasWriteError(f"Canvas write failed: {e}")

    # â° æ—¶åˆ»1: Canvasæ–‡ä»¶å†™å…¥æˆåŠŸ

    # Step 3: è¿”å›æ–°Stateï¼ˆè§¦å‘CheckpointeræŒä¹…åŒ– - å…³é”®è·¯å¾„ï¼‰
    new_state = {
        **state,
        "decomposition_results": {state["concept"]: questions},
        "tasks_completed": state["tasks_completed"] + 1,
        "last_operation": "decomposition",
        "last_timestamp": datetime.now().isoformat()
    }

    # â° æ—¶åˆ»2: LangGraphè‡ªåŠ¨æŒä¹…åŒ–Stateåˆ°Checkpointer

    # Step 4: å¼‚æ­¥å­˜å‚¨åˆ°Graphitiï¼ˆéå…³é”®è·¯å¾„ - å…è®¸å¤±è´¥ï¼‰
    asyncio.create_task(
        store_to_graphiti(
            state["session_id"],
            "decomposition",
            state["canvas_path"],
            questions,
            config={}
        )
    )

    # â° æ—¶åˆ»3: Graphitiå­˜å‚¨ä»»åŠ¡å·²å…¥é˜Ÿï¼ˆå®é™…å†™å…¥å¯èƒ½å»¶è¿Ÿ1-5ç§’ï¼‰

    # Step 5: å¼‚æ­¥å­˜å‚¨åˆ°Temporal Memoryï¼ˆéå…³é”®è·¯å¾„ï¼‰
    asyncio.create_task(
        store_to_temporal_memory(
            state["session_id"],
            "decomposition_completed",
            datetime.now(),
            {"concept": state["concept"], "question_count": len(questions)}
        )
    )

    # â° æ—¶åˆ»4: Temporal Memoryå­˜å‚¨ä»»åŠ¡å·²å…¥é˜Ÿ

    # Step 6: è¿”å›æ–°State
    return new_state

    # â° æ—¶åˆ»5: Agent Nodeæ‰§è¡Œå®Œæˆ
    # â° æ—¶åˆ»6: graph.invoke()è¿”å›ï¼ˆç”¨æˆ·æ„ŸçŸ¥çš„å®Œæˆæ—¶åˆ»ï¼‰
```

---

## ğŸ”’ å››ã€ä¸€è‡´æ€§ä¿è¯ä¸æ•…éšœå¤„ç†

### 4.1 å¼ºä¸€è‡´æ€§è·¯å¾„ï¼ˆCanvas â†” Checkpointerï¼‰

**å®šä¹‰**: Canvasæ“ä½œä¸LangGraph Stateæ›´æ–°å¿…é¡»åŒæ­¥ï¼Œä»»ä¸€å¤±è´¥åˆ™æ•´ä½“å›æ»š

```python
def agent_node_with_strong_consistency(state: CanvasLearningState):
    """ç¡®ä¿Canvasæ“ä½œå’ŒStateæ›´æ–°çš„å¼ºä¸€è‡´æ€§"""

    # Step 1: å¤‡ä»½Canvasï¼ˆäº‹åŠ¡å¼€å§‹ï¼‰
    backup = backup_canvas(state["canvas_path"])

    try:
        # Step 2: æ‰§è¡ŒCanvasæ“ä½œ
        result = write_to_canvas(state["canvas_path"], new_data)

        # Step 3: è¿”å›æ–°Stateï¼ˆLangGraphè‡ªåŠ¨æŒä¹…åŒ–ï¼‰
        new_state = {
            **state,
            "last_operation": "decomposition",
            "decomposition_results": result,
            "last_timestamp": datetime.now().isoformat()
        }

        # âœ… æˆåŠŸè·¯å¾„ï¼šCanvaså’ŒStateéƒ½æˆåŠŸ
        return new_state

    except Exception as e:
        # âŒ å¤±è´¥è·¯å¾„ï¼šå›æ»šCanvas
        restore_canvas(state["canvas_path"], backup)

        # ä¸è¿”å›æ–°State â†’ LangGraphä¸åˆ›å»ºæ–°checkpoint
        raise CanvasOperationError(f"Operation failed: {e}")

    # å¼ºä¸€è‡´æ€§ä¿è¯ï¼š
    # - Canvasæ›´æ–°æˆåŠŸ â‡” Checkpointeråˆ›å»ºæ–°checkpoint
    # - Canvasæ›´æ–°å¤±è´¥ â‡” Checkpointerä¸åˆ›å»ºcheckpoint
```

---

### 4.2 æœ€ç»ˆä¸€è‡´æ€§è·¯å¾„ï¼ˆCanvas â†” ä¸šåŠ¡è®°å¿†ï¼‰

**å®šä¹‰**: Canvasæ“ä½œæˆåŠŸåå¼‚æ­¥å­˜å‚¨åˆ°Graphiti/Temporal/Semanticï¼Œå…è®¸å»¶è¿Ÿå’Œå¤±è´¥

```python
def agent_node_with_eventual_consistency(state: CanvasLearningState):
    """Canvasæ“ä½œæˆåŠŸï¼Œä¸šåŠ¡è®°å¿†å¼‚æ­¥å­˜å‚¨ï¼ˆå…è®¸å¤±è´¥ï¼‰"""

    # Step 1: Canvasæ“ä½œï¼ˆå…³é”®è·¯å¾„ï¼‰
    write_to_canvas(state["canvas_path"], new_data)

    # Step 2: è¿”å›æ–°Stateï¼ˆå…³é”®è·¯å¾„ï¼‰
    new_state = {
        **state,
        "decomposition_results": new_data,
        "last_timestamp": datetime.now().isoformat()
    }

    # Step 3: å¼‚æ­¥å­˜å‚¨åˆ°ä¸šåŠ¡è®°å¿†ï¼ˆéå…³é”®è·¯å¾„ï¼‰
    try:
        asyncio.create_task(store_to_graphiti(state["session_id"], new_data))
        asyncio.create_task(store_to_temporal_memory(state["session_id"], ...))
    except Exception as e:
        # âœ… ä»…è®°å½•æ—¥å¿—ï¼Œä¸å½±å“Canvasæ“ä½œæˆåŠŸ
        logger.error(f"Business memory storage failed: {e}")

        # è®°å½•åˆ°error_logï¼Œä¾›åç»­é‡è¯•
        new_state["error_log"].append({
            "timestamp": datetime.now().isoformat(),
            "error": str(e),
            "retry_count": 0
        })

    return new_state

    # æœ€ç»ˆä¸€è‡´æ€§ä¿è¯ï¼š
    # - Canvasç«‹å³å¯è§ï¼ˆå¼ºä¸€è‡´ï¼‰
    # - Graphiti/Temporalå»¶è¿Ÿå¯è§ï¼ˆå…è®¸1-5ç§’å»¶è¿Ÿï¼‰
    # - å¤±è´¥æ—¶é€šè¿‡é‡è¯•æœºåˆ¶æœ€ç»ˆåŒæ­¥
```

---

### 4.3 å†²çªå¤„ç†ç­–ç•¥

**åœºæ™¯1: Checkpointerä¸Graphitiæ•°æ®ä¸ä¸€è‡´**

```python
async def detect_and_fix_inconsistency():
    """æ£€æµ‹å¹¶ä¿®å¤Checkpointerä¸Graphitiçš„æ•°æ®ä¸ä¸€è‡´"""

    # Step 1: ä»Checkpointeræ¢å¤State
    config = create_langgraph_config(canvas_path, user_id, session_id)
    state = graph.get_state(config)

    # Step 2: ä»Canvasæ–‡ä»¶è¯»å–å®é™…æ•°æ®
    canvas_data = read_canvas(canvas_path)

    # Step 3: ä»GraphitiæŸ¥è¯¢å­˜å‚¨çš„æ•°æ®
    graphiti_data = await graphiti_client.query_canvas_nodes(canvas_path)

    # Step 4: ä¸‰æ–¹å¯¹æ¯”
    if not verify_consistency(state.values, canvas_data, graphiti_data):
        logger.warning("Inconsistency detected!")

        # Step 5: ä¿®å¤ç­–ç•¥ï¼ˆCanvas = çœŸå®æ•°æ®æºï¼‰
        # Canvasæ–‡ä»¶ â†’ Checkpointer
        if canvas_data != state.values:
            logger.info("Syncing Checkpointer from Canvas")
            # é‡æ–°æ‰§è¡Œæ“ä½œï¼Œåˆ›å»ºæ–°checkpoint
            graph.invoke({...}, config=config)

        # Canvasæ–‡ä»¶ â†’ Graphiti
        if canvas_data != graphiti_data:
            logger.info("Re-syncing Graphiti from Canvas")
            await graphiti_client.full_sync(canvas_data)
```

**åœºæ™¯2: å›æ»šæ“ä½œå¯¼è‡´çš„æ•°æ®å†²çª**

```python
def handle_rollback_conflict(
    canvas_path: str,
    session_id: str,
    checkpoint_id: str
):
    """å›æ»šæ—¶ç¡®ä¿ä¸‰ä¸ªç³»ç»Ÿä¸€è‡´"""

    # Step 1: å›æ»šCanvasæ–‡ä»¶ï¼ˆä»å¤‡ä»½ï¼‰
    backup_path = f".canvas_backups/{Path(canvas_path).stem}_{checkpoint_id}.canvas"
    shutil.copy(backup_path, canvas_path)

    # Step 2: å›æ»šLangGraph Stateï¼ˆä»checkpointï¼‰
    config = create_langgraph_config(canvas_path, user_id, session_id)
    config["configurable"]["checkpoint_id"] = checkpoint_id
    state = graph.get_state(config)

    # Step 3: æ ‡è®°Graphitiæ“ä½œä¸ºå·²æ’¤é”€ï¼ˆä¸åˆ é™¤ï¼Œä¿ç•™å†å²ï¼‰
    mark_graphiti_operations_as_reverted(
        session_id,
        after_timestamp=state.values["last_timestamp"]
    )

    # Step 4: æ ‡è®°Temporal Memoryäº‹ä»¶ä¸ºå·²æ’¤é”€
    temporal_memory.mark_events_as_reverted(
        session_id,
        after_timestamp=state.values["last_timestamp"]
    )

    # Step 5: éªŒè¯ä¸€è‡´æ€§
    assert verify_consistency(canvas_path, state, graphiti_data)

    logger.info(f"Rollback completed: {canvas_path} â†’ checkpoint {checkpoint_id}")
```

---

## âš¡ äº”ã€æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

### 5.1 æ‰¹é‡æ“ä½œä¼˜åŒ–

**é—®é¢˜**: æ¯æ¬¡graph.invoke()éƒ½åˆ›å»ºæ–°checkpointï¼Œ100ä¸ªèŠ‚ç‚¹ = 100æ¬¡æ•°æ®åº“å†™å…¥

**è§£å†³æ–¹æ¡ˆ**: æ‰¹é‡å¤„ç†

```python
# âŒ ä½æ•ˆï¼šé€ä¸ªèŠ‚ç‚¹è°ƒç”¨
for node_id in yellow_nodes:  # 100ä¸ªèŠ‚ç‚¹
    graph.invoke({
        "operation": "scoring",
        "target_nodes": [node_id],
        ...
    }, config)
    # 100æ¬¡checkpointå†™å…¥ â‰ˆ 5ç§’

# âœ… é«˜æ•ˆï¼šæ‰¹é‡å¤„ç†
graph.invoke({
    "operation": "batch_scoring",
    "target_nodes": yellow_nodes,  # ä¸€æ¬¡ä¼ å…¥100ä¸ªèŠ‚ç‚¹
    ...
}, config)
# 1æ¬¡checkpointå†™å…¥ â‰ˆ 50ms
```

**æ€§èƒ½æå‡**: 100å€å†™å…¥æ¬¡æ•°å‡å°‘ï¼Œ5ç§’ â†’ 50ms

---

### 5.2 å¼‚æ­¥æŒä¹…åŒ–

**LangGraphå†…éƒ¨å®ç°**:

```python
# LangGraphçš„checkpointer.put()å·²ç»æ˜¯å¼‚æ­¥çš„
async def put(self, config, checkpoint, metadata):
    """å¼‚æ­¥å†™å…¥checkpointï¼ˆéé˜»å¡ï¼‰"""
    # åœ¨åå°çº¿ç¨‹ä¸­æ‰§è¡Œæ•°æ®åº“å†™å…¥
    await self._async_write_to_db(checkpoint)

# Agent Nodeæ— éœ€ç­‰å¾…checkpointå†™å…¥å®Œæˆ
def agent_node(state):
    # è¿”å›æ–°Stateåç«‹å³è¿”å›ï¼Œä¸é˜»å¡
    return new_state
    # LangGraphåœ¨åå°å¼‚æ­¥æŒä¹…åŒ–
```

---

### 5.3 åˆ†å±‚ç¼“å­˜ç­–ç•¥

```mermaid
graph TD
    A[æŸ¥è¯¢è¯·æ±‚] --> B{L1ç¼“å­˜?}
    B -->|å‘½ä¸­| C[è¿”å›State<10ms>]
    B -->|æœªå‘½ä¸­| D{L2ç¼“å­˜?}
    D -->|å‘½ä¸­| E[è¿”å›Checkpoint<50ms>]
    D -->|æœªå‘½ä¸­| F{L3ç¼“å­˜?}
    F -->|å‘½ä¸­| G[è¿”å›Graphiti<20ms>]
    F -->|æœªå‘½ä¸­| H[æŸ¥è¯¢Neo4j<200ms>]

    style B fill:#e1f5fe
    style D fill:#fff3e0
    style F fill:#f3e5f5
```

**å®ç°**:

```python
class MultiLayerCache:
    """4å±‚ç¼“å­˜æ¶æ„"""

    def __init__(self):
        self.l1_cache = {}  # å†…å­˜ç¼“å­˜ï¼ˆå½“å‰ä¼šè¯Stateï¼‰
        self.l2_cache = None  # Checkpointerï¼ˆæ•°æ®åº“ï¼‰
        self.l3_cache = None  # Redisï¼ˆGraphitiæŸ¥è¯¢ç¼“å­˜ï¼‰
        self.l4_storage = None  # Neo4jï¼ˆå®Œæ•´çŸ¥è¯†å›¾è°±ï¼‰

    async def get_state(self, session_id: str) -> dict:
        """æŸ¥è¯¢Stateï¼ˆL1 â†’ L2 â†’ L3 â†’ L4ï¼‰"""

        # L1: å†…å­˜ç¼“å­˜
        if session_id in self.l1_cache:
            logger.debug("L1 cache hit")
            return self.l1_cache[session_id]

        # L2: Checkpointerï¼ˆPostgreSQLï¼‰
        config = create_langgraph_config(..., session_id=session_id)
        state = graph.get_state(config)
        if state:
            logger.debug("L2 cache hit")
            self.l1_cache[session_id] = state.values  # å¡«å……L1
            return state.values

        # L3: Graphiti Redisç¼“å­˜
        cached_data = await redis_client.get(f"session:{session_id}")
        if cached_data:
            logger.debug("L3 cache hit")
            return json.loads(cached_data)

        # L4: Neo4jå®Œæ•´æŸ¥è¯¢
        data = await graphiti_client.query_session(session_id)
        logger.debug("L4 storage hit")

        # å›å¡«ç¼“å­˜
        await redis_client.set(f"session:{session_id}", json.dumps(data), ex=300)

        return data
```

**æ€§èƒ½å¯¹æ¯”**:

| ç¼“å­˜å±‚ | å»¶è¿Ÿ | å‘½ä¸­ç‡ | å®¹é‡ |
|-------|------|-------|------|
| L1ï¼ˆå†…å­˜ï¼‰ | <10ms | 90% | 100MB |
| L2ï¼ˆCheckpointerï¼‰ | <50ms | 95% | 10GB |
| L3ï¼ˆRedisï¼‰ | <20ms | 80% | 100GB |
| L4ï¼ˆNeo4jï¼‰ | <200ms | 100% | 1TB+ |

---

### 5.4 å»¶è¿ŸGraphitiå†™å…¥

**é—®é¢˜**: æ¯æ¬¡Canvasæ“ä½œéƒ½è§¦å‘Graphitiå†™å…¥ï¼Œé¢‘ç‡è¿‡é«˜

**è§£å†³æ–¹æ¡ˆ**: æ¶ˆæ¯é˜Ÿåˆ— + æ‰¹é‡å†™å…¥

```python
import asyncio
from asyncio import Queue

graphiti_write_queue: Queue = Queue()

async def batch_graphiti_writer():
    """åå°ä»»åŠ¡ï¼šæ‰¹é‡å†™å…¥Graphiti"""
    while True:
        batch = []

        # æ”’10ä¸ªæ“ä½œæˆ–ç­‰å¾…1ç§’
        for _ in range(10):
            try:
                item = await asyncio.wait_for(graphiti_write_queue.get(), timeout=1.0)
                batch.append(item)
            except asyncio.TimeoutError:
                break

        if batch:
            # æ‰¹é‡å†™å…¥Neo4j
            await graphiti_client.batch_write(batch)
            logger.info(f"Batchå†™å…¥Graphiti: {len(batch)}æ¡")

        await asyncio.sleep(0.1)  # çŸ­æš‚ä¼‘çœ 

# AgentèŠ‚ç‚¹ä¸­å¼‚æ­¥å…¥é˜Ÿ
async def agent_node(state):
    # Canvasæ“ä½œ
    write_to_canvas(...)

    # å…¥é˜ŸGraphitiå†™å…¥ï¼ˆä¸é˜»å¡ï¼‰
    await graphiti_write_queue.put({
        "type": "decomposition",
        "session_id": state["session_id"],
        "data": questions
    })

    return new_state

# å¯åŠ¨åå°å†™å…¥ä»»åŠ¡
asyncio.create_task(batch_graphiti_writer())
```

**æ€§èƒ½æå‡**:
- å†™å…¥é¢‘ç‡: æ¯æ“ä½œ1æ¬¡ â†’ æ¯ç§’1æ¬¡ï¼ˆæ”’10ä¸ªï¼‰
- Neo4jè¿æ¥æ•°: 100æ¬¡ â†’ 10æ¬¡
- æ€»å»¶è¿Ÿ: ä¸å˜ï¼ˆå¼‚æ­¥ï¼Œç”¨æˆ·æ— æ„ŸçŸ¥ï¼‰

---

## âœ… å…­ã€éªŒæ”¶æ ‡å‡†

### 6.1 åŠŸèƒ½éªŒæ”¶

- âœ… **AC 1**: PostgresSaverå’ŒInMemorySaverå‡å¯æ­£å¸¸å·¥ä½œ
- âœ… **AC 2**: thread_idæ ¼å¼ç¬¦åˆ`canvas_{name}_{session_id}`è§„èŒƒ
- âœ… **AC 3**: configå‚æ•°åŒ…å«æ‰€æœ‰å¿…éœ€å­—æ®µï¼ˆthread_id, canvas_path, user_id, session_idï¼‰
- âœ… **AC 4**: graph.compile(checkpointer=checkpointer)æˆåŠŸç¼–è¯‘
- âœ… **AC 5**: å¤šè½®å¯¹è¯å¯å¤ç”¨ç›¸åŒthread_idå¹¶ç´¯ç§¯State
- âœ… **AC 6**: graph.get_state()å¯æ¢å¤å†å²State
- âœ… **AC 7**: å›æ»šæ“ä½œåŒæ­¥æ¢å¤Canvasæ–‡ä»¶å’ŒLangGraph State
- âœ… **AC 8**: Checkpointerå’ŒGraphitiå¯ç‹¬ç«‹å·¥ä½œï¼Œäº’ä¸é˜»å¡
- âœ… **AC 9**: Canvasæ“ä½œå¤±è´¥æ—¶ï¼ŒCheckpointerä¸åˆ›å»ºcheckpointï¼ŒGraphitiä¸å­˜å‚¨
- âœ… **AC 10**: Checkpointerå†™å…¥å¤±è´¥æ—¶ï¼ŒCanvasæ“ä½œå¤±è´¥å¹¶å›æ»š
- âœ… **AC 11**: Graphitiå†™å…¥å¤±è´¥æ—¶ï¼ŒCanvasæ“ä½œæˆåŠŸï¼Œä»…è®°å½•æ—¥å¿—
- âœ… **AC 12**: å¤šè½®å¯¹è¯å¯æ¢å¤Checkpointerçš„Stateï¼ŒåŒæ—¶æŸ¥è¯¢Graphitiçš„å†å²
- âœ… **AC 13**: è·¨CanvasæŸ¥è¯¢ä»…ä½¿ç”¨Graphitiï¼Œä¸è®¿é—®Checkpointer

### 6.2 æ€§èƒ½éªŒæ”¶

- âœ… **AC 14**: å•æ¬¡checkpointå†™å…¥ < 100msï¼ˆPostgresSaverï¼‰
- âœ… **AC 15**: æ‰¹é‡æ“ä½œï¼ˆ10ä¸ªèŠ‚ç‚¹ï¼‰æ€»è€—æ—¶ < 3ç§’
- âœ… **AC 16**: checkpointæ¸…ç†è„šæœ¬å¯æ­£å¸¸è¿è¡Œ
- âœ… **AC 17**: Graphitiå¼‚æ­¥å†™å…¥ä¸é˜»å¡Agentæ‰§è¡Œ
- âœ… **AC 18**: æ‰¹é‡æ“ä½œå‡å°‘90% checkpointå†™å…¥æ¬¡æ•°
- âœ… **AC 19**: L1ç¼“å­˜å‘½ä¸­ç‡ > 80%
- âœ… **AC 20**: Graphitiæ‰¹é‡å†™å…¥å»¶è¿Ÿ < 5ç§’

### 6.3 ä¸€è‡´æ€§éªŒæ”¶

- âœ… **AC 21**: Canvasæ–‡ä»¶ â†” Checkpointer State: å¼ºä¸€è‡´æ€§
- âœ… **AC 22**: Canvasæ–‡ä»¶ â†” Graphiti: æœ€ç»ˆä¸€è‡´æ€§ï¼ˆ<5ç§’åŒæ­¥å»¶è¿Ÿï¼‰
- âœ… **AC 23**: å›æ»šåCanvasæ–‡ä»¶ã€LangGraph Stateã€ä¸šåŠ¡è®°å¿†ä¸‰è€…ä¸€è‡´
- âœ… **AC 24**: ä¸€è‡´æ€§æ ¡éªŒè„šæœ¬å¯æ£€æµ‹å¹¶ä¿®å¤ä¸ä¸€è‡´

### 6.4 é›†æˆéªŒæ”¶ï¼ˆä¸Epic 12 Storiesé…åˆï¼‰

- âœ… **AC 25**: Story 12.1 checkpointeré…ç½®å¯è¢«Story 12.2è°ƒç”¨
- âœ… **AC 26**: Story 12.5å¯ä½¿ç”¨æœ¬ç« èŠ‚å®šä¹‰çš„configç»“æ„
- âœ… **AC 27**: Story 12.7æµ‹è¯•ç”¨ä¾‹è¦†ç›–checkpointeræ‰€æœ‰åœºæ™¯

---

## ğŸ“š ä¸ƒã€å®æ–½è·¯çº¿å›¾

### Phase 1: CheckpointeråŸºç¡€è®¾æ–½ (1å‘¨)

- [ ] PostgreSQLæ•°æ®åº“æ­å»ºå’Œschemaåˆ›å»º
- [ ] PostgresSaverå’ŒInMemorySaveré…ç½®
- [ ] thread_idç”Ÿæˆå’Œç®¡ç†
- [ ] configå‚æ•°ç»“æ„å®ç°
- [ ] åŸºç¡€StateGraphç¼–è¯‘å’Œæµ‹è¯•

### Phase 2: State Schemaå’ŒAgenté›†æˆ (1-2å‘¨)

- [ ] CanvasLearningStateå®Œæ•´å®šä¹‰
- [ ] 12ä¸ªAgent Nodeæ”¹é€ ï¼ˆè¿”å›æ–°Stateï¼‰
- [ ] å¼ºä¸€è‡´æ€§è·¯å¾„å®ç°ï¼ˆCanvas â†” Stateï¼‰
- [ ] å•å…ƒæµ‹è¯•è¦†ç›–

### Phase 3: ä¸šåŠ¡è®°å¿†ç³»ç»Ÿé›†æˆ (2å‘¨)

- [ ] Graphitiå¼‚æ­¥å­˜å‚¨å®ç°
- [ ] Temporal Memoryé›†æˆ
- [ ] Semantic Memoryé›†æˆ
- [ ] æœ€ç»ˆä¸€è‡´æ€§è·¯å¾„å®ç°
- [ ] æ‰¹é‡å†™å…¥é˜Ÿåˆ—

### Phase 4: æ€§èƒ½ä¼˜åŒ– (1å‘¨)

- [ ] åˆ†å±‚ç¼“å­˜å®ç°
- [ ] æ‰¹é‡æ“ä½œä¼˜åŒ–
- [ ] å¼‚æ­¥æŒä¹…åŒ–éªŒè¯
- [ ] æ€§èƒ½åŸºå‡†æµ‹è¯•

### Phase 5: å®¹é”™å’Œç›‘æ§ (1å‘¨)

- [ ] å†²çªæ£€æµ‹å’Œä¿®å¤è„šæœ¬
- [ ] å›æ»šæœºåˆ¶å®ç°
- [ ] ä¸€è‡´æ€§æ ¡éªŒå·¥å…·
- [ ] ç›‘æ§å’Œæ—¥å¿—ç³»ç»Ÿ

### Phase 6: é›†æˆæµ‹è¯•å’Œéƒ¨ç½² (1å‘¨)

- [ ] ç«¯åˆ°ç«¯é›†æˆæµ‹è¯•
- [ ] æ€§èƒ½å‹åŠ›æµ‹è¯•
- [ ] ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²
- [ ] æ–‡æ¡£å®Œå–„

**æ€»æ—¶é—´ä¼°ç®—**: 7-9å‘¨

---

## ğŸ”„ å…«ã€è‰¾å®¾æµ©æ–¯å¤ä¹ ç³»ç»Ÿ3å±‚è®°å¿†é›†æˆ (v1.1.6æ–°å¢)

### 8.1 é›†æˆèƒŒæ™¯

**é—®é¢˜æè¿°**:
- è‰¾å®¾æµ©æ–¯å¤ä¹ ç³»ç»ŸåŸè®¾è®¡ä»…ä¾èµ–Canvasè¯„åˆ†ï¼ˆâ‰¥60åˆ†ï¼‰ä½œä¸ºå”¯ä¸€æ•°æ®æº
- Py-FSRSä½¿ç”¨é»˜è®¤17å‚æ•°ï¼ˆç›¸å½“äºæ¨¡æ‹Ÿæ•°æ®ï¼Œéä¸ªæ€§åŒ–ï¼‰
- ç¼ºå°‘ä¸»åŠ¨å‘ç°éœ€è¦å¤ä¹ æ¦‚å¿µçš„æœºåˆ¶ï¼ˆå®Œå…¨è¢«åŠ¨ç­‰å¾…è¯„åˆ†è§¦å‘ï¼‰

**è§£å†³æ–¹æ¡ˆ**:
é€šè¿‡æ•´åˆ3å±‚è®°å¿†ç³»ç»Ÿï¼ˆTemporal + Graphiti + Semanticï¼‰ï¼Œå®ç°100%çœŸå®æ•°æ®é©±åŠ¨çš„è‰¾å®¾æµ©æ–¯å¤ä¹ ç³»ç»Ÿã€‚

---

### 8.2 3å±‚è®°å¿†ç³»ç»Ÿæ•°æ®æä¾›

#### 8.2.1 Temporal Memoryæä¾›çš„æ•°æ®

**èŒè´£**: å­¦ä¹ è¡Œä¸ºæ•°æ®æº

**æä¾›æ•°æ®**:
1. **é•¿æœŸæœªè®¿é—®çš„å·²æŒæ¡æ¦‚å¿µ** (è§¦å‘ç‚¹4 - æ¡ä»¶1)
   ```python
   query_temporal_learning_behavior(
       filter_type="inactive_mastered",
       days_threshold=7,
       min_mastery=0.6
   )
   # è¿”å›: [{"concept": "é€†å¦å‘½é¢˜", "last_access_days": 10, "mastery": 0.85, ...}]
   ```

2. **å†å²å¤ä¹ è®°å½•** (FSRSå‚æ•°ä¼˜åŒ–)
   ```python
   query_temporal_learning_behavior(
       filter_type="all_reviews",
       min_samples=100,
       fields=["review_time", "rating", "interval", "stability", "difficulty"]
   )
   # è¿”å›: [{"interval": 7, "rating": 3, "retention": 0.9, ...}, ...]
   ```

3. **æœ€è¿‘è¡Œä¸ºæ•°æ®** (ä¼˜å…ˆçº§è®¡ç®—)
   ```python
   query_temporal_learning_behavior(
       filter_type="recent_behavior",
       canvas_file="ç¬”è®°åº“/ç¦»æ•£æ•°å­¦.canvas",
       concept_ids=["node123", "node456"],
       metrics=["review_frequency", "avg_interval", "accuracy_trend"]
   )
   # è¿”å›: {"review_frequency_normalized": 0.3, "interval_deviation": 0.5, ...}
   ```

**æ•°æ®schema (Neo4j - Temporal Memory)**:
```cypher
// å­¦ä¹ äº‹ä»¶èŠ‚ç‚¹
CREATE (e:LearningEvent {
    event_id: randomUUID(),
    session_id: $session_id,
    canvas_file: $canvas_file,
    node_id: $node_id,
    concept: $concept,
    operation_type: $operation_type, // 'scoring', 'review', 'view', 'doc_interaction'
    timestamp: datetime(),
    score: $score,              // å¦‚æœoperation_type='scoring'
    rating: $rating,            // å¦‚æœoperation_type='review' (1-4)
    duration: $duration,        // å¦‚æœoperation_type='view' (ç§’)
    metadata: $metadata         // JSONå¯¹è±¡
})

// ç´¢å¼•
CREATE INDEX learning_event_timestamp IF NOT EXISTS FOR (e:LearningEvent) ON (e.timestamp);
CREATE INDEX learning_event_concept IF NOT EXISTS FOR (e:LearningEvent) ON (e.concept, e.timestamp);
```

---

#### 8.2.2 Graphitiæä¾›çš„æ•°æ®

**èŒè´£**: æ¦‚å¿µå…³ç³»ç½‘ç»œæ•°æ®æº

**æä¾›æ•°æ®**:
1. **çŸ¥è¯†æ–­å±‚æ£€æµ‹** (è§¦å‘ç‚¹4 - æ¡ä»¶2)
   ```python
   query_graphiti_concept_network(
       analysis_type="prerequisite_gap",
       min_prerequisite_mastery=0.8,
       gap_days_threshold=14
   )
   # è¿”å›: [{"concept": "è¡Œåˆ—å¼", "prerequisite_mastery": 0.9, "days_not_learned": 20, ...}]
   ```

2. **æ¦‚å¿µéš¾åº¦åˆ†å¸ƒ** (FSRSå‚æ•°ä¼˜åŒ–)
   ```python
   query_graphiti_concept_network(
       analysis_type="learning_difficulty_distribution",
       concepts=["é€†å¦å‘½é¢˜", "å¾·æ‘©æ ¹å®šå¾‹", ...]
   )
   # è¿”å›: {"é€†å¦å‘½é¢˜": {"difficulty": 0.6}, "å¾·æ‘©æ ¹å®šå¾‹": {"difficulty": 0.7}, ...}
   ```

3. **æ¦‚å¿µå…³ç³»æ•°æ®** (ä¼˜å…ˆçº§è®¡ç®—)
   ```python
   query_graphiti_concept_network(
       analysis_type="related_concepts",
       canvas_file="ç¬”è®°åº“/ç¦»æ•£æ•°å­¦.canvas",
       concept_ids=["node123", "node456"]
   )
   # è¿”å›: {"prerequisite_completeness": 0.8, "related_difficulty_normalized": 0.5, ...}
   ```

**CypheræŸ¥è¯¢ç¤ºä¾‹** (Neo4j):
```cypher
// æŸ¥è¯¢çŸ¥è¯†æ–­å±‚: å‰ç½®æ¦‚å¿µå·²æŒæ¡ä½†åç»­æ¦‚å¿µæœªå­¦ä¹ 
MATCH (pre:Concept)-[:PREREQUISITE_OF]->(post:Concept)
WHERE pre.mastery >= 0.8
  AND post.mastery < 0.3
  AND duration.inDays(post.last_learned, datetime()).days > 14
RETURN post.concept, pre.concept AS prerequisite, pre.mastery, post.last_learned
```

---

#### 8.2.3 Semantic Memoryæä¾›çš„æ•°æ®

**èŒè´£**: æ–‡æ¡£äº¤äº’æ•°æ®æº

**æä¾›æ•°æ®**:
1. **éšæ€§éœ€æ±‚æ£€æµ‹** (è§¦å‘ç‚¹4 - æ¡ä»¶3)
   ```python
   query_semantic_document_interactions(
       pattern="related_doc_frequent_but_concept_inactive",
       related_access_threshold=5,
       concept_inactive_days=7
   )
   # è¿”å›: [{"concept": "ç‰¹å¾å‘é‡", "related_doc_accesses": 8, "concept_inactive_days": 10, ...}]
   ```

2. **å¤ä¹ å‚ä¸åº¦ç›¸å…³æ€§** (FSRSå‚æ•°ä¼˜åŒ–)
   ```python
   query_semantic_document_interactions(
       pattern="review_engagement_correlation",
       concepts=["é€†å¦å‘½é¢˜", "å¾·æ‘©æ ¹å®šå¾‹", ...]
   )
   # è¿”å›: {"é€†å¦å‘½é¢˜": {"engagement": 0.7}, "å¾·æ‘©æ ¹å®šå¾‹": {"engagement": 0.5}, ...}
   ```

3. **è®¿é—®æŒ‡æ ‡** (ä¼˜å…ˆçº§è®¡ç®—)
   ```python
   query_semantic_document_interactions(
       pattern="access_metrics",
       canvas_file="ç¬”è®°åº“/ç¦»æ•£æ•°å­¦.canvas",
       concept_ids=["node123", "node456"]
   )
   # è¿”å›: {"recent_access_frequency": 0.3, "dwell_time_normalized": 0.4, ...}
   ```

**æ•°æ®schema (LanceDB + CUDA Metadata)**:
```python
# LanceDB Document Metadata (Pydantic Schema)
{
    "document_path": "Canvas/Math53/é€†å¦å‘½é¢˜-å£è¯­åŒ–è§£é‡Š-20250115.md",
    "concept": "é€†å¦å‘½é¢˜",
    "canvas_file": "Canvas/Math53/ç¦»æ•£æ•°å­¦.canvas",
    "node_id": "node123",
    "access_count": 12,
    "last_accessed": "2025-11-10T14:30:00Z",
    "avg_dwell_time": 180,  # ç§’
    "related_docs": ["å¾·æ‘©æ ¹å®šå¾‹-å¯¹æ¯”è¡¨.md", "å‘½é¢˜é€»è¾‘-å››å±‚æ¬¡ç­”æ¡ˆ.md"]
}
# 768ç»´å‘é‡åµŒå…¥ (sentence-transformers/all-MiniLM-L6-v2 + CUDAåŠ é€Ÿ)
```

---

### 8.3 è‰¾å®¾æµ©æ–¯ç³»ç»Ÿæ•°æ®æµå‘

```mermaid
graph TB
    subgraph "æ•°æ®é‡‡é›†å±‚"
        A1[CanvasèŠ‚ç‚¹è¯„åˆ†] -->|è¯„åˆ†â‰¥60| E[EbbinghausReviewSystem]
        A2[ç”¨æˆ·æ‰‹åŠ¨æ ‡è®°] --> E
        A3[æ‰¹é‡å¯¼å…¥å†å²] --> E
        A4[è¡Œä¸ºç›‘æ§è§¦å‘] --> E
    end

    subgraph "3å±‚è®°å¿†ç³»ç»ŸæŸ¥è¯¢"
        B1[Temporal Memory<br/>æœªè®¿é—®æ¦‚å¿µæŸ¥è¯¢] --> A4
        B2[Graphiti<br/>çŸ¥è¯†æ–­å±‚æ£€æµ‹] --> A4
        B3[Semantic Memory<br/>éšæ€§éœ€æ±‚æ£€æµ‹] --> A4
    end

    subgraph "å‚æ•°ä¼˜åŒ–"
        C1[Temporal<br/>å†å²å¤ä¹ è®°å½•] --> F[optimize_fsrs_parameters]
        C2[Graphiti<br/>æ¦‚å¿µéš¾åº¦åˆ†å¸ƒ] --> F
        C3[Semantic<br/>å¤ä¹ å‚ä¸åº¦] --> F
        F --> G[ä¼˜åŒ–åçš„17å‚æ•°]
        G --> E
    end

    subgraph "ä¼˜å…ˆçº§è®¡ç®—"
        D1[Temporal<br/>è¡Œä¸ºæ•°æ®] --> H[get_today_review_summary]
        D2[Graphiti<br/>æ¦‚å¿µå…³ç³»] --> H
        D3[Semantic<br/>æ–‡æ¡£äº¤äº’] --> H
        E --> H
        H --> I[ä¼˜å…ˆçº§æ’åºçš„Canvasåˆ—è¡¨]
    end

    style A4 fill:#ffe0b2
    style F fill:#c5e1a5
    style H fill:#b3e5fc
```

**æ•°æ®æµè¯´æ˜**:
1. **è§¦å‘ç‚¹1-3**: Canvasè¯„åˆ†ã€æ‰‹åŠ¨æ ‡è®°ã€æ‰¹é‡å¯¼å…¥ â†’ ç›´æ¥è°ƒç”¨EbbinghausReviewSystem
2. **è§¦å‘ç‚¹4**: æ¯æ—¥å‡Œæ™¨2:00å®šæ—¶ä»»åŠ¡ â†’ æŸ¥è¯¢3å±‚è®°å¿†ç³»ç»Ÿ â†’ æ‰¹é‡æ·»åŠ åˆ°EbbinghausReviewSystem
3. **å‚æ•°ä¼˜åŒ–**: ç´¯ç§¯100æ¬¡å¤ä¹ å â†’ ä»3å±‚è®°å¿†æå–æ•°æ® â†’ ä¼˜åŒ–FSRS 17å‚æ•°
4. **ä¼˜å…ˆçº§è®¡ç®—**: æ¯æ—¥æ˜¾ç¤ºå¤ä¹ é¢æ¿æ—¶ â†’ ä»3å±‚è®°å¿†èšåˆæ•°æ® â†’ å¤šç»´åº¦ä¼˜å…ˆçº§æ’åº

---

### 8.4 LangGraphé›†æˆç‚¹

#### 8.4.1 è¡Œä¸ºç›‘æ§åå°ä»»åŠ¡

**LangGraphå®ç°**: ä½¿ç”¨LangGraphçš„å‘¨æœŸæ€§ä»»åŠ¡èŠ‚ç‚¹

```python
from langgraph.graph import StateGraph
from datetime import datetime, time

class EbbinghausMonitoringState(TypedDict):
    last_check_time: datetime
    triggered_concepts: List[Dict]

def behavior_monitoring_node(state: EbbinghausMonitoringState):
    """
    æ¯æ—¥å‡Œæ™¨2:00æ‰§è¡Œçš„è¡Œä¸ºç›‘æ§èŠ‚ç‚¹
    """
    # æŸ¥è¯¢3å±‚è®°å¿†ç³»ç»Ÿ
    temporal_concepts = query_temporal_learning_behavior(
        filter_type="inactive_mastered",
        days_threshold=7,
        min_mastery=0.6
    )

    graphiti_concepts = query_graphiti_concept_network(
        analysis_type="prerequisite_gap",
        min_prerequisite_mastery=0.8,
        gap_days_threshold=14
    )

    semantic_concepts = query_semantic_document_interactions(
        pattern="related_doc_frequent_but_concept_inactive",
        related_access_threshold=5,
        concept_inactive_days=7
    )

    # åˆå¹¶å¹¶å»é‡
    all_concepts = merge_and_deduplicate(
        temporal_concepts, graphiti_concepts, semantic_concepts
    )

    # æ‰¹é‡æ·»åŠ åˆ°è‰¾å®¾æµ©æ–¯ç³»ç»Ÿ
    review_system = EbbinghausReviewSystem()
    for concept in all_concepts:
        review_system.add_concept_for_review(
            canvas_file=concept['canvas_file'],
            node_id=concept['node_id'],
            concept=concept['concept'],
            initial_mastery=concept.get('mastery', 0.6),
            trigger_source="behavior_monitoring"
        )

    return {
        "last_check_time": datetime.now(),
        "triggered_concepts": all_concepts
    }

# LangGraph StateGraphé…ç½®
monitoring_graph = StateGraph(EbbinghausMonitoringState)
monitoring_graph.add_node("behavior_monitoring", behavior_monitoring_node)
monitoring_graph.add_edge(START, "behavior_monitoring")
monitoring_graph.add_edge("behavior_monitoring", END)

# å®šæ—¶è°ƒåº¦ (ä½¿ç”¨APScheduler)
from apscheduler.schedulers.background import BackgroundScheduler

scheduler = BackgroundScheduler()
scheduler.add_job(
    func=lambda: monitoring_graph.compile().invoke({"last_check_time": None}),
    trigger="cron",
    hour=2,  # å‡Œæ™¨2ç‚¹
    minute=0
)
scheduler.start()
```

#### 8.4.2 è¯„åˆ†åè¡Œä¸ºè®°å½•

**é›†æˆç‚¹**: scoring-agentèŠ‚ç‚¹æ‰§è¡Œå

```python
@tool
def score_and_track(
    canvas_file: str,
    node_id: str,
    concept: str,
    score: int,
    config: RunnableConfig
) -> str:
    """
    è¯„åˆ†å¹¶è®°å½•åˆ°Temporal Memoryï¼ˆé›†æˆåˆ°scoring-agentï¼‰
    """
    # Step 1: æ›´æ–°CanvasèŠ‚ç‚¹é¢œè‰²
    write_to_canvas(canvas_file, {"id": node_id, "color": get_color_by_score(score)}, config)

    # Step 2: è®°å½•è¡Œä¸ºåˆ°Temporal Memory
    track_learning_behavior(
        session_id=config.configurable.get("session_id"),
        canvas_file=canvas_file,
        node_id=node_id,
        concept=concept,
        operation_type="scoring",
        behavior_data={"score": score},
        config=config
    )

    # Step 3: å¦‚æœè¯„åˆ†â‰¥60ï¼Œæ·»åŠ åˆ°è‰¾å®¾æµ©æ–¯ç³»ç»Ÿ
    if score >= 60:
        review_system = EbbinghausReviewSystem()
        review_system.add_concept_for_review(
            canvas_file=canvas_file,
            node_id=node_id,
            concept=concept,
            initial_mastery=score / 100.0,
            trigger_source="scoring"
        )

    return f"âœ… è¯„åˆ†å®Œæˆ: {score}åˆ†, å·²è®°å½•è¡Œä¸ºæ•°æ®"
```

---

### 8.5 ä¸€è‡´æ€§ä¿è¯æœºåˆ¶

#### 8.5.1 3å±‚è®°å¿†å†™å…¥å¤±è´¥å¤„ç†

**é—®é¢˜**: è¡Œä¸ºè®°å½•å†™å…¥å¤±è´¥æ—¶ï¼Œè‰¾å®¾æµ©æ–¯ç³»ç»Ÿå¯èƒ½ç¼ºå°‘æ•°æ®

**è§£å†³æ–¹æ¡ˆ**: æœ€ç»ˆä¸€è‡´æ€§ + é‡è¯•é˜Ÿåˆ—

```python
class MemoryWriteQueue:
    """3å±‚è®°å¿†ç³»ç»Ÿå†™å…¥é˜Ÿåˆ—ï¼ˆæœ€ç»ˆä¸€è‡´æ€§ä¿è¯ï¼‰"""

    def __init__(self):
        self.redis_client = redis.Redis(...)
        self.retry_max = 3

    def enqueue_temporal_write(self, event_data: Dict):
        """
        å°†Temporal Memoryå†™å…¥ä»»åŠ¡åŠ å…¥é˜Ÿåˆ—
        """
        task = {
            "type": "temporal_write",
            "data": event_data,
            "retry_count": 0,
            "created_at": datetime.now().isoformat()
        }
        self.redis_client.lpush("memory_write_queue", json.dumps(task))

    async def process_queue(self):
        """
        åå°workerå¤„ç†å†™å…¥é˜Ÿåˆ—
        """
        while True:
            task_json = self.redis_client.brpop("memory_write_queue", timeout=5)
            if not task_json:
                continue

            task = json.loads(task_json[1])

            try:
                if task["type"] == "temporal_write":
                    temporal_manager.store_learning_event(task["data"])
                elif task["type"] == "graphiti_write":
                    mcp__graphiti_memory__add_episode(content=task["data"])
                # å†™å…¥æˆåŠŸï¼Œä¸é‡æ–°å…¥é˜Ÿ
            except Exception as e:
                # å†™å…¥å¤±è´¥ï¼Œé‡è¯•
                task["retry_count"] += 1
                if task["retry_count"] < self.retry_max:
                    self.redis_client.lpush("memory_write_queue", json.dumps(task))
                else:
                    # è¶…è¿‡é‡è¯•æ¬¡æ•°ï¼Œè®°å½•åˆ°é”™è¯¯æ—¥å¿—
                    logger.error(f"Memory write failed after {self.retry_max} retries: {task}")
```

#### 8.5.2 è‰¾å®¾æµ©æ–¯ç³»ç»Ÿæ•°æ®ä¿®å¤

**é—®é¢˜**: å¦‚æœæŸäº›è¯„åˆ†äº‹ä»¶æœªæˆåŠŸè§¦å‘è‰¾å®¾æµ©æ–¯ç³»ç»Ÿï¼Œå¯¼è‡´å¤ä¹ åˆ—è¡¨ä¸å®Œæ•´

**è§£å†³æ–¹æ¡ˆ**: å®šæœŸä¸€è‡´æ€§æ‰«æ

```python
def sync_ebbinghaus_from_canvas():
    """
    ä»Canvasæ–‡ä»¶æ‰«ææ‰€æœ‰â‰¥60åˆ†èŠ‚ç‚¹ï¼Œç¡®ä¿éƒ½åœ¨è‰¾å®¾æµ©æ–¯ç³»ç»Ÿä¸­
    """
    # Step 1: æ‰«ææ‰€æœ‰Canvasæ–‡ä»¶ï¼Œæ‰¾åˆ°ç»¿è‰²+ç´«è‰²èŠ‚ç‚¹
    canvas_files = glob.glob("ç¬”è®°åº“/**/*.canvas", recursive=True)
    concepts_in_canvas = []

    for canvas_file in canvas_files:
        canvas_data = read_canvas(canvas_file)
        for node in canvas_data.get("nodes", []):
            if node.get("color") in ["2", "3"]:  # ç»¿è‰²æˆ–ç´«è‰²
                concepts_in_canvas.append({
                    "canvas_file": canvas_file,
                    "node_id": node["id"],
                    "concept": extract_concept_from_node(node)
                })

    # Step 2: æŸ¥è¯¢è‰¾å®¾æµ©æ–¯ç³»ç»Ÿç°æœ‰æ¦‚å¿µ
    review_system = EbbinghausReviewSystem()
    concepts_in_ebbinghaus = review_system.list_all_concepts()

    # Step 3: æ‰¾å‡ºç¼ºå¤±çš„æ¦‚å¿µ
    missing_concepts = [
        c for c in concepts_in_canvas
        if f"{c['canvas_file']}_{c['node_id']}" not in concepts_in_ebbinghaus
    ]

    # Step 4: æ‰¹é‡æ·»åŠ ç¼ºå¤±æ¦‚å¿µ
    for concept in missing_concepts:
        review_system.add_concept_for_review(
            canvas_file=concept['canvas_file'],
            node_id=concept['node_id'],
            concept=concept['concept'],
            initial_mastery=0.7,  # é»˜è®¤æŒæ¡åº¦
            trigger_source="consistency_sync"
        )

    logger.info(f"âœ… ä¸€è‡´æ€§æ‰«æå®Œæˆ: ä¿®å¤äº†{len(missing_concepts)}ä¸ªç¼ºå¤±æ¦‚å¿µ")

# å®šæœŸæ‰§è¡Œï¼ˆæ¯å‘¨æ—¥å‡Œæ™¨3ç‚¹ï¼‰
scheduler.add_job(
    func=sync_ebbinghaus_from_canvas,
    trigger="cron",
    day_of_week="sun",
    hour=3,
    minute=0
)
```

---

### 8.6 æ€§èƒ½ä¼˜åŒ–

#### 8.6.1 æ‰¹é‡æŸ¥è¯¢ä¼˜åŒ–

**é—®é¢˜**: ä¼˜å…ˆçº§è®¡ç®—æ—¶éœ€è¦ä¸ºæ¯ä¸ªCanvasæŸ¥è¯¢3å±‚è®°å¿†ç³»ç»Ÿï¼Œå¯èƒ½å¾ˆæ…¢

**ä¼˜åŒ–æ–¹æ¡ˆ**: æ‰¹é‡æŸ¥è¯¢ + ç¼“å­˜

```python
class MemoryBatchQuery:
    """3å±‚è®°å¿†ç³»ç»Ÿæ‰¹é‡æŸ¥è¯¢ä¼˜åŒ–"""

    def __init__(self):
        self.cache = TTLCache(maxsize=1000, ttl=300)  # 5åˆ†é’Ÿç¼“å­˜

    def batch_query_for_review_summary(self, canvas_files: List[str]) -> Dict:
        """
        æ‰¹é‡æŸ¥è¯¢å¤šä¸ªCanvasçš„3å±‚è®°å¿†æ•°æ®
        """
        # Step 1: æ£€æŸ¥ç¼“å­˜
        cached_results = {}
        uncached_files = []
        for canvas_file in canvas_files:
            cache_key = f"review_summary:{canvas_file}"
            if cache_key in self.cache:
                cached_results[canvas_file] = self.cache[cache_key]
            else:
                uncached_files.append(canvas_file)

        if not uncached_files:
            return cached_results

        # Step 2: æ‰¹é‡æŸ¥è¯¢Temporal Memoryï¼ˆå•æ¬¡æŸ¥è¯¢ï¼‰
        temporal_data = query_temporal_learning_behavior(
            filter_type="batch_recent_behavior",
            canvas_files=uncached_files,
            metrics=["review_frequency", "avg_interval", "accuracy_trend"]
        )

        # Step 3: æ‰¹é‡æŸ¥è¯¢Graphitiï¼ˆå•æ¬¡CypheræŸ¥è¯¢ï¼‰
        graphiti_data = query_graphiti_concept_network_batch(
            canvas_files=uncached_files,
            analysis=["prerequisite_mastery", "related_difficulty"]
        )

        # Step 4: æ‰¹é‡æŸ¥è¯¢Semantic Memoryï¼ˆå•æ¬¡å‘é‡æœç´¢ï¼‰
        semantic_data = query_semantic_document_interactions_batch(
            canvas_files=uncached_files,
            metrics=["access_frequency", "dwell_time"]
        )

        # Step 5: åˆå¹¶ç»“æœå¹¶ç¼“å­˜
        for canvas_file in uncached_files:
            result = {
                "temporal": temporal_data.get(canvas_file, {}),
                "graphiti": graphiti_data.get(canvas_file, {}),
                "semantic": semantic_data.get(canvas_file, {})
            }
            cache_key = f"review_summary:{canvas_file}"
            self.cache[cache_key] = result
            cached_results[canvas_file] = result

        return cached_results
```

#### 8.6.2 å¼‚æ­¥å¹¶å‘æŸ¥è¯¢

**ä½¿ç”¨LangGraphçš„å¹¶å‘èŠ‚ç‚¹**:

```python
from langgraph.graph import StateGraph

class MemoryQueryState(TypedDict):
    canvas_files: List[str]
    temporal_results: Dict
    graphiti_results: Dict
    semantic_results: Dict

async def query_temporal_parallel(state: MemoryQueryState):
    """å¹¶å‘æŸ¥è¯¢Temporal Memory"""
    results = await query_temporal_learning_behavior_async(
        filter_type="batch_recent_behavior",
        canvas_files=state["canvas_files"]
    )
    return {"temporal_results": results}

async def query_graphiti_parallel(state: MemoryQueryState):
    """å¹¶å‘æŸ¥è¯¢Graphiti"""
    results = await query_graphiti_concept_network_async_batch(
        canvas_files=state["canvas_files"]
    )
    return {"graphiti_results": results}

async def query_semantic_parallel(state: MemoryQueryState):
    """å¹¶å‘æŸ¥è¯¢Semantic Memory"""
    results = await query_semantic_document_interactions_async_batch(
        canvas_files=state["canvas_files"]
    )
    return {"semantic_results": results}

# æ„å»ºå¹¶å‘æŸ¥è¯¢å›¾
memory_query_graph = StateGraph(MemoryQueryState)
memory_query_graph.add_node("temporal", query_temporal_parallel)
memory_query_graph.add_node("graphiti", query_graphiti_parallel)
memory_query_graph.add_node("semantic", query_semantic_parallel)

# å¹¶å‘æ‰§è¡Œ3ä¸ªæŸ¥è¯¢
memory_query_graph.add_edge(START, "temporal")
memory_query_graph.add_edge(START, "graphiti")
memory_query_graph.add_edge(START, "semantic")
memory_query_graph.add_edge("temporal", END)
memory_query_graph.add_edge("graphiti", END)
memory_query_graph.add_edge("semantic", END)

# ä½¿ç”¨
compiled = memory_query_graph.compile()
result = await compiled.ainvoke({"canvas_files": ["ç¦»æ•£æ•°å­¦.canvas", "çº¿æ€§ä»£æ•°.canvas"]})
# resultåŒ…å«æ‰€æœ‰3å±‚è®°å¿†çš„æŸ¥è¯¢ç»“æœ
```

---

### 8.7 ç›‘æ§å’Œå‘Šè­¦

#### 8.7.1 å…³é”®æŒ‡æ ‡ç›‘æ§

```python
# PrometheusæŒ‡æ ‡å®šä¹‰
from prometheus_client import Counter, Histogram, Gauge

# è¡Œä¸ºç›‘æ§è§¦å‘ç»Ÿè®¡
behavior_monitoring_triggered = Counter(
    'ebbinghaus_behavior_monitoring_triggered_total',
    'Total behavior monitoring triggers',
    ['source']  # temporal/graphiti/semantic
)

# 3å±‚è®°å¿†æŸ¥è¯¢å»¶è¿Ÿ
memory_query_latency = Histogram(
    'memory_query_duration_seconds',
    'Memory query latency',
    ['memory_type']  # temporal/graphiti/semantic
)

# FSRSå‚æ•°ä¼˜åŒ–é¢‘ç‡
fsrs_optimization_count = Counter(
    'fsrs_parameter_optimization_total',
    'Total FSRS parameter optimizations'
)

# å½“å‰å¤ä¹ é˜Ÿåˆ—å¤§å°
review_queue_size = Gauge(
    'ebbinghaus_review_queue_size',
    'Current review queue size'
)
```

#### 8.7.2 å‘Šè­¦è§„åˆ™

```yaml
# AlertManagerå‘Šè­¦è§„åˆ™
groups:
  - name: ebbinghaus_system
    rules:
      - alert: MemoryQuerySlow
        expr: memory_query_duration_seconds{quantile="0.95"} > 1.0
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "3å±‚è®°å¿†ç³»ç»ŸæŸ¥è¯¢å˜æ…¢"
          description: "{{ $labels.memory_type }} æŸ¥è¯¢P95å»¶è¿Ÿ > 1ç§’"

      - alert: BehaviorMonitoringFailed
        expr: rate(behavior_monitoring_triggered_total[1h]) == 0
        for: 25h  # è¶…è¿‡1å¤©æœªè§¦å‘
        labels:
          severity: critical
        annotations:
          summary: "è¡Œä¸ºç›‘æ§æœªæ‰§è¡Œ"
          description: "è¿‡å»25å°æ—¶å†…æœªè§¦å‘è¡Œä¸ºç›‘æ§ï¼Œå¯èƒ½ç³»ç»Ÿæ•…éšœ"

      - alert: ReviewQueueTooLarge
        expr: review_queue_size > 500
        for: 1h
        labels:
          severity: warning
        annotations:
          summary: "å¤ä¹ é˜Ÿåˆ—è¿‡å¤§"
          description: "å½“å‰å¤ä¹ é˜Ÿåˆ—: {{ $value }} ä¸ªæ¦‚å¿µ"
```

---

### 8.8 æµ‹è¯•ç­–ç•¥

#### 8.8.1 é›†æˆæµ‹è¯•

```python
@pytest.mark.integration
async def test_ebbinghaus_3layer_memory_integration():
    """
    æµ‹è¯•è‰¾å®¾æµ©æ–¯ç³»ç»Ÿä¸3å±‚è®°å¿†ç³»ç»Ÿçš„å®Œæ•´é›†æˆ
    """
    # Setup: åˆ›å»ºæµ‹è¯•æ•°æ®
    canvas_file = "test_data/æµ‹è¯•-ç¦»æ•£æ•°å­¦.canvas"
    concept = "æµ‹è¯•-é€†å¦å‘½é¢˜"

    # Step 1: æ¨¡æ‹Ÿè¯„åˆ†è§¦å‘
    await score_and_track(
        canvas_file=canvas_file,
        node_id="test_node_123",
        concept=concept,
        score=85,
        config=test_config
    )

    # Step 2: éªŒè¯Temporal Memoryå·²è®°å½•
    temporal_events = query_temporal_learning_behavior(
        filter_type="all_reviews",
        min_samples=1
    )
    assert any(e["concept"] == concept for e in temporal_events)

    # Step 3: éªŒè¯è‰¾å®¾æµ©æ–¯ç³»ç»Ÿå·²æ·»åŠ 
    review_system = EbbinghausReviewSystem()
    concepts = review_system.list_all_concepts()
    assert f"{canvas_file}_test_node_123" in concepts

    # Step 4: æ¨¡æ‹Ÿ7å¤©åè¡Œä¸ºç›‘æ§è§¦å‘
    await simulate_time_advance(days=7)
    await behavior_monitoring_node({})

    # Step 5: éªŒè¯è§¦å‘äº†å¤ä¹ æ¨é€
    review_summary = get_today_review_summary()
    assert any(c["canvas_file"] == canvas_file for c in review_summary)
```

#### 8.8.2 æ€§èƒ½æµ‹è¯•

```python
@pytest.mark.performance
async def test_memory_query_performance():
    """
    æµ‹è¯•3å±‚è®°å¿†ç³»ç»ŸæŸ¥è¯¢æ€§èƒ½
    """
    canvas_files = [f"test_data/canvas_{i}.canvas" for i in range(50)]

    # æµ‹è¯•æ‰¹é‡æŸ¥è¯¢æ€§èƒ½
    start_time = time.time()
    batch_query = MemoryBatchQuery()
    results = batch_query.batch_query_for_review_summary(canvas_files)
    duration = time.time() - start_time

    # æ€§èƒ½è¦æ±‚: 50ä¸ªCanvasæ‰¹é‡æŸ¥è¯¢ < 2ç§’
    assert duration < 2.0
    assert len(results) == 50
```

---

## ğŸ” åã€LangGraph Agentic RAGæ£€ç´¢æ¶æ„

### 10.1 æ¦‚è¿°

**è®¾è®¡ç›®æ ‡**: å®ç°æ™ºèƒ½æ£€ç´¢3å±‚è®°å¿†ç³»ç»Ÿ(Graphiti + Temporal + Semantic)æ¥ç”Ÿæˆé«˜è´¨é‡ä¸ªæ€§åŒ–æ£€éªŒç™½æ¿ã€‚

**æ ¸å¿ƒæŒ‘æˆ˜**:
1. **å¤šæºæ£€ç´¢**: éœ€è¦ä»3ä¸ªå¼‚æ„æ•°æ®æº(Neo4jå›¾æ•°æ®åº“ + Neo4jæ—¶åºæ•°æ® + LanceDBå‘é‡åº“)æ£€ç´¢æ•°æ®
2. **æ£€ç´¢è´¨é‡è¯„ä¼°**: éœ€è¦è¯„ä¼°æ£€ç´¢ç»“æœçš„è´¨é‡,å†³å®šæ˜¯å¦éœ€è¦é‡å†™æŸ¥è¯¢
3. **æ™ºèƒ½å†³ç­–**: Agentéœ€è¦è‡ªä¸»å†³å®šè°ƒç”¨å“ªäº›å·¥å…·ã€å¦‚ä½•ç»„åˆç»“æœ

**è§£å†³æ–¹æ¡ˆ**: LangGraph Agentic RAGæ¨¡å¼

---

### 10.2 Agentic RAGæ¶æ„å…¨æ™¯

#### 10.2.1 æ¶æ„å›¾ï¼ˆå«GraphRAGæ™ºèƒ½è·¯ç”±ï¼‰

```mermaid
graph TB
    START([å¼€å§‹]) --> ROUTER[question_router]

    ROUTER -->|local: å…·ä½“æ¦‚å¿µæŸ¥è¯¢| A1[generate_query_local]
    ROUTER -->|global: æ•°æ®é›†çº§åˆ†æ| A2[generate_query_global]
    ROUTER -->|hybrid: å¤æ‚ç»¼åˆæŸ¥è¯¢| A3[generate_query_hybrid]

    A1 -->|éœ€è¦æ£€ç´¢| B1[local_search_tools]
    A2 -->|éœ€è¦æ£€ç´¢| B2[graphrag_global_tool]
    A3 -->|éœ€è¦æ£€ç´¢| B3[composite_search_tools]

    A1 -->|å¯ç›´æ¥å›ç­”| H[generate]
    A2 -->|å¯ç›´æ¥å›ç­”| H
    A3 -->|å¯ç›´æ¥å›ç­”| H

    B1 --> C[Graphiti Tool]
    B1 --> D[LanceDB Tool]
    B1 --> E[Temporal Tool]

    B2 --> F_global[GraphRAG Global Search]

    B3 --> C_composite[Graphiti Tool]
    B3 --> D_composite[LanceDB Tool]
    B3 --> E_composite[Temporal Tool]
    B3 --> F_composite[GraphRAG Tool]

    C --> F[grade_documents]
    D --> F
    E --> F

    F_global --> F

    C_composite --> F_fusion[fusion_rerank]
    D_composite --> F_fusion
    E_composite --> F_fusion
    F_composite --> F_fusion

    F_fusion --> F

    F -->|æ–‡æ¡£è´¨é‡ >= 0.5| H[generate]
    F -->|æ–‡æ¡£è´¨é‡ < 0.5| G[rewrite_query]

    G --> ROUTER

    H --> END([ç»“æŸ])

    style ROUTER fill:#ffeb3b
    style A1 fill:#e1f5fe
    style A2 fill:#e1f5fe
    style A3 fill:#e1f5fe
    style B1 fill:#fff3e0
    style B2 fill:#fff3e0
    style B3 fill:#fff3e0
    style F fill:#f3e5f5
    style F_fusion fill:#f3e5f5
    style G fill:#ffcdd2
    style H fill:#c8e6c9
    style F_global fill:#b2dfdb
```

**æ¶æ„å›¾è¯´æ˜**:

1. **Question Routerï¼ˆé»„è‰²ï¼‰**: æ™ºèƒ½é—®é¢˜åˆ†ç±»å™¨ï¼Œæ ¹æ®æŸ¥è¯¢ç±»å‹è·¯ç”±åˆ°ä¸åŒçš„æ£€ç´¢ç­–ç•¥
   - **local**: å…·ä½“æ¦‚å¿µæŸ¥è¯¢ï¼ˆå¦‚"ä»€ä¹ˆæ˜¯é€†å¦å‘½é¢˜ï¼Ÿ"ï¼‰â†’ Graphiti + LanceDB + Temporal
   - **global**: æ•°æ®é›†çº§åˆ†æï¼ˆå¦‚"ç¦»æ•£æ•°å­¦ä¸­å“ªäº›æ¦‚å¿µæœ€å®¹æ˜“æ··æ·†ï¼Ÿ"ï¼‰â†’ GraphRAG Global Search
   - **hybrid**: å¤æ‚ç»¼åˆæŸ¥è¯¢ï¼ˆå¦‚"é€»è¾‘æ¨ç†çš„æ ¸å¿ƒæ¦‚å¿µå’Œæˆ‘çš„è–„å¼±ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ"ï¼‰â†’ å››å±‚å¹¶è¡Œæ£€ç´¢

2. **ä¸‰ç§æ£€ç´¢è·¯å¾„**:
   - **Local Search Path (B1)**: ä¼ ç»Ÿä¸‰å±‚æ£€ç´¢ï¼ˆGraphiti + LanceDB + Temporalï¼‰
   - **Global Search Path (B2)**: ä»…GraphRAGå…¨å±€æœç´¢
   - **Composite Search Path (B3)**: å››å±‚å¹¶è¡Œæ£€ç´¢ + Fusion Reranking

3. **Fusion Rerankingï¼ˆç´«è‰²ï¼‰**: å¯¹å¹¶è¡Œæ£€ç´¢ç»“æœè¿›è¡Œèåˆé‡æ’åº
   - ä½¿ç”¨RRFï¼ˆReciprocal Rank Fusionï¼‰å¹³è¡¡å››ä¸ªæ•°æ®æº
   - å¯é€‰Cross-Encoderè¿›ä¸€æ­¥æå‡å‡†ç¡®æ€§

4. **Self-Correction Loop**: grade_documentsè¯„ä¼°è´¨é‡ä¸è¶³æ—¶ï¼Œè§¦å‘rewrite_queryå¹¶é‡æ–°è·¯ç”±
```

#### 10.2.2 æ ¸å¿ƒç»„ä»¶

| ç»„ä»¶ | ç±»å‹ | èŒè´£ | è¾“å…¥ | è¾“å‡º |
|------|------|------|------|------|
| **question_router** | Function Node | æ™ºèƒ½é—®é¢˜åˆ†ç±»ï¼šlocal/global/hybrid | Canvasä¸Šä¸‹æ–‡ + æŸ¥è¯¢ | è·¯ç”±å†³ç­–(query_type) |
| **generate_query_local** | Agent Node | ç”ŸæˆLocal SearchæŸ¥è¯¢ | Canvasä¸Šä¸‹æ–‡ + ç”¨æˆ·ç†è§£ | å†³ç­– + æŸ¥è¯¢è¯­å¥ |
| **generate_query_global** | Agent Node | ç”ŸæˆGlobal SearchæŸ¥è¯¢ | Canvasä¸Šä¸‹æ–‡ + åˆ†æéœ€æ±‚ | å†³ç­– + å…¨å±€æŸ¥è¯¢ |
| **generate_query_hybrid** | Agent Node | ç”ŸæˆHybrid SearchæŸ¥è¯¢ | Canvasä¸Šä¸‹æ–‡ + å¤æ‚éœ€æ±‚ | å†³ç­– + æ··åˆæŸ¥è¯¢ |
| **local_search_tools** | Tool Node | æ‰§è¡ŒGraphiti + LanceDB + Temporal | æŸ¥è¯¢è¯­å¥ | 3æºæ£€ç´¢ç»“æœ |
| **graphrag_global_tool** | Tool Node | æ‰§è¡ŒGraphRAGå…¨å±€æœç´¢ | å…¨å±€æŸ¥è¯¢ | ç¤¾åŒºæ‘˜è¦ + å…¨å±€ç­”æ¡ˆ |
| **composite_search_tools** | Tool Node | å¹¶è¡Œæ‰§è¡Œ4å±‚æ£€ç´¢ | æ··åˆæŸ¥è¯¢ | 4æºæ£€ç´¢ç»“æœ |
| **fusion_rerank** | Function Node | èåˆé‡æ’åº4æºç»“æœ | 4æºæ£€ç´¢ç»“æœ | èåˆåæ–‡æ¡£åˆ—è¡¨ |
| **grade_documents** | Function Node | è¯„ä¼°æ–‡æ¡£è´¨é‡ | æ£€ç´¢ç»“æœ | è´¨é‡è¯„åˆ†(0-1) |
| **rewrite_query** | Function Node | é‡å†™æŸ¥è¯¢ | ä½è´¨é‡æ–‡æ¡£ + åŸæŸ¥è¯¢ | æ–°æŸ¥è¯¢è¯­å¥ |
| **generate** | Function Node | ç”Ÿæˆæ£€éªŒé—®é¢˜ | é«˜è´¨é‡æ–‡æ¡£ | æ£€éªŒé—®é¢˜åˆ—è¡¨ |

**æ–°å¢ç»„ä»¶è¯¦è§£**:

1. **question_router**:
   - ä½¿ç”¨LLMåˆ†ææŸ¥è¯¢ç±»å‹ï¼ˆlocal/global/hybridï¼‰
   - å†³ç­–é€»è¾‘ï¼š
     - åŒ…å«"ä»€ä¹ˆæ˜¯"ã€"å®šä¹‰"ã€"å…·ä½“æ¦‚å¿µ" â†’ local
     - åŒ…å«"å“ªäº›"ã€"æ¯”è¾ƒ"ã€"æ•°æ®é›†çº§" â†’ global
     - åŒ…å«"æ ¸å¿ƒæ¦‚å¿µ+è–„å¼±ç‚¹"ã€"è·¯å¾„+å…³è”" â†’ hybrid

2. **fusion_rerank**:
   - ä½¿ç”¨RRFï¼ˆReciprocal Rank Fusionï¼‰èåˆGraphitiã€LanceDBã€Temporalã€GraphRAGå››ä¸ªæ•°æ®æº
   - å¯é€‰Cross-Encoderè¿›ä¸€æ­¥æå‡å‡†ç¡®æ€§
   - å»é‡ç­–ç•¥ï¼šåŸºäºconceptå­—æ®µåˆå¹¶ç›¸åŒæ¦‚å¿µçš„å¤šæºè¯æ®

3. **composite_search_tools**:
   - ä½¿ç”¨`asyncio.gather()`å¹¶è¡Œæ‰§è¡Œ4ä¸ªå·¥å…·
   - è¶…æ—¶æ§åˆ¶ï¼šå•ä¸ªå·¥å…·æœ€å¤š5ç§’ï¼Œæ€»è¶…æ—¶10ç§’
   - å¤±è´¥é™çº§ï¼šè‡³å°‘2ä¸ªå·¥å…·æˆåŠŸå³å¯ç»§ç»­

---

### 10.3 State Schemaå®šä¹‰

```python
# âœ… Verified from LangGraph Skill (references/llms-txt.md:8723-8825)
from typing import Annotated, Literal
from typing_extensions import TypedDict
from langgraph.graph.message import add_messages

class AgenticRAGState(TypedDict):
    """Agentic RAG State Schemaï¼ˆå«GraphRAGæ”¯æŒï¼‰"""

    # ========== è¾“å…¥å‚æ•° ==========
    canvas_file: str                  # Canvasæ–‡ä»¶è·¯å¾„
    canvas_context: dict              # Canvasä¸Šä¸‹æ–‡(çº¢/ç´«èŠ‚ç‚¹åˆ—è¡¨)
    user_understanding: str           # ç”¨æˆ·å·²æœ‰ç†è§£(é»„è‰²èŠ‚ç‚¹å†…å®¹)

    # ========== å¯¹è¯å†å² ==========
    messages: Annotated[list, add_messages]  # LangChainæ¶ˆæ¯ç´¯ç§¯

    # ========== è·¯ç”±å†³ç­– ==========
    query_type: Literal["local", "global", "hybrid"]  # æŸ¥è¯¢ç±»å‹è·¯ç”±
    routing_reasoning: str            # è·¯ç”±å†³ç­–ç†ç”±

    # ========== æ£€ç´¢ä¸­é—´ç»“æœ ==========
    current_query: str                # å½“å‰æŸ¥è¯¢è¯­å¥
    retrieval_attempts: int           # æ£€ç´¢å°è¯•æ¬¡æ•°
    retrieved_documents: list[dict]   # æ£€ç´¢åˆ°çš„æ–‡æ¡£åˆ—è¡¨

    # æ–‡æ¡£ç»“æ„ç¤ºä¾‹ï¼ˆå«GraphRAGï¼‰:
    # {
    #     "source": "graphrag",  # graphiti/lancedb/temporal/graphrag
    #     "content": "...",
    #     "metadata": {
    #         "concept": "é€»è¾‘æ¨ç†",
    #         "relevance_score": 0.85,
    #         "community_id": "c12",       # GraphRAGä¸“ç”¨
    #         "community_level": 2,        # GraphRAGä¸“ç”¨
    #         "last_error_date": "2025-11-10"
    #     }
    # }

    # ========== GraphRAGä¸“ç”¨å­—æ®µ ==========
    graphrag_results: dict | None     # GraphRAGå…¨å±€æœç´¢ç»“æœ
    # ç»“æ„:
    # {
    #     "answer": "ç»¼åˆç­”æ¡ˆ...",
    #     "communities": [{"id": "c1", "title": "é€»è¾‘æ¨ç†", "summary": "...", "weight": 0.8}],
    #     "sources": [{"entity_id": "e1", "entity_name": "é€†å¦å‘½é¢˜", "relevance": 0.9}],
    #     "confidence": 0.85
    # }

    # ========== èåˆé‡æ’åºå­—æ®µ ==========
    fusion_strategy: Literal["rrf", "cross_encoder", "linear"] | None  # èåˆç­–ç•¥
    fusion_weights: dict[str, float] | None  # æ•°æ®æºæƒé‡é…ç½®
    # ç¤ºä¾‹: {"graphiti": 0.3, "lancedb": 0.25, "temporal": 0.2, "graphrag": 0.25}

    # ========== æ–‡æ¡£è´¨é‡è¯„ä¼° ==========
    document_quality_score: float     # 0-1åˆ†æ•°
    quality_issues: list[str]         # è´¨é‡é—®é¢˜åˆ—è¡¨

    # ========== æœ€ç»ˆè¾“å‡º ==========
    verification_questions: list[dict]  # ç”Ÿæˆçš„æ£€éªŒé—®é¢˜

    # é—®é¢˜ç»“æ„ç¤ºä¾‹ï¼ˆå«GraphRAGæ¥æºï¼‰:
    # {
    #     "question": "ä»€ä¹ˆæ˜¯é€†å¦å‘½é¢˜?",
    #     "type": "åŸºç¡€å‹",  # çªç ´å‹/åŸºç¡€å‹/æ£€éªŒå‹/åº”ç”¨å‹
    #     "source_concept": "é€†å¦å‘½é¢˜",
    #     "difficulty": "ä¸­ç­‰",
    #     "data_sources": ["graphiti", "temporal", "graphrag"],  # å¯åŒ…å«graphrag
    #     "community_context": "é€»è¾‘æ¨ç†ç¤¾åŒº"  # GraphRAGæä¾›çš„ç¤¾åŒºèƒŒæ™¯
    # }

    # ========== æ‰§è¡ŒçŠ¶æ€ ==========
    workflow_stage: Literal[
        "routing",
        "query_generation",
        "tool_execution",
        "fusion_reranking",
        "document_grading",
        "query_rewriting",
        "question_generation",
        "completed"
    ]
    error_log: list[dict]             # é”™è¯¯æ—¥å¿—
```

---

### 10.4 Retrieval Toolså®šä¹‰

#### 10.4.1 Graphiti Hybrid Search Tool

```python
# âœ… Verified from Graphiti Skill
from langchain.tools import tool
from typing import List, Dict

@tool
async def graphiti_hybrid_search_tool(
    query: str,
    canvas_context: dict,
    max_results: int = 20,
    rerank_strategy: str = "rrf"
) -> List[Dict]:
    """
    Graphitiæ··åˆæ£€ç´¢å·¥å…· (Graph + Semantic + BM25)

    Args:
        query: æŸ¥è¯¢è¯­å¥
        canvas_context: Canvasä¸Šä¸‹æ–‡(åŒ…å«topic_node_id)
        max_results: æœ€å¤§ç»“æœæ•°
        rerank_strategy: é‡æ’åºç­–ç•¥(rrf/mmr/node_distance/cross_encoder/episode_mentions)

    Returns:
        æ£€ç´¢ç»“æœåˆ—è¡¨

    ä½¿ç”¨åœºæ™¯:
        - æ£€éªŒç™½æ¿ç”Ÿæˆ: æŸ¥è¯¢å†å²è–„å¼±ç‚¹å’Œç›¸å…³æ¦‚å¿µ
        - è‰¾å®¾æµ©æ–¯å¤ä¹ : æŸ¥è¯¢éœ€è¦å¤ä¹ çš„æ¦‚å¿µ
    """
    from app.core.graphiti_client import graphiti

    # Step 1: Graphitiæ··åˆæ£€ç´¢
    results = await graphiti.search(
        query=query,
        center_node_uuid=canvas_context.get("topic_node_id"),
        max_distance=2,              # å›¾éå†æœ€å¤§è·³æ•°
        num_results=max_results,
        rerank_strategy=rerank_strategy
    )

    # Step 2: æ ¼å¼åŒ–ç»“æœ
    formatted_results = []
    for result in results:
        formatted_results.append({
            "source": "graphiti",
            "content": result.get("text", ""),
            "metadata": {
                "concept": result.get("concept", ""),
                "node_id": result.get("uuid", ""),
                "relevance_score": result.get("score", 0.0),
                "graph_distance": result.get("distance", 0),
                "last_access": result.get("last_access", None),
                "mastery_level": result.get("mastery", 0.0)
            }
        })

    return formatted_results
```

**Rerankingç­–ç•¥é€‰æ‹©æŒ‡å—**:

| ç­–ç•¥ | é€‚ç”¨åœºæ™¯ | ä¼˜åŠ¿ | åŠ£åŠ¿ |
|------|---------|------|------|
| **rrf** (Reciprocal Rank Fusion) | æ£€éªŒç™½æ¿ç”Ÿæˆ(æ¨è) | å¹³è¡¡è¯­ä¹‰/å…³é”®è¯/å›¾å…³ç³» | æ— ç‰¹æ®Šåå¥½ |
| **mmr** (Maximal Marginal Relevance) | éœ€è¦å¤šæ ·æ€§çš„é—®é¢˜ç”Ÿæˆ | é¿å…é‡å¤æ¦‚å¿µ | å¯èƒ½é—æ¼é«˜ç›¸å…³å†…å®¹ |
| **node_distance** | å¼ºè°ƒæ¦‚å¿µå…³è”æ€§ | ä¼˜å…ˆè¿”å›å›¾ä¸­ç›¸è¿‘èŠ‚ç‚¹ | å¿½ç•¥è¯­ä¹‰ç›¸ä¼¼åº¦ |
| **cross_encoder** | æœ€é«˜å‡†ç¡®æ€§è¦æ±‚ | BERTäº¤å‰ç¼–ç å™¨,æœ€å‡†ç¡® | æœ€æ…¢(~500ms) |
| **episode_mentions** | è‰¾å®¾æµ©æ–¯å¤ä¹ æ¨è | ä¼˜å…ˆé«˜é¢‘é”™è¯¯æ¦‚å¿µ | å¿½ç•¥æ–°æ¦‚å¿µ |

---

#### 10.4.2 LanceDB Hybrid Search Tool

```python
# âœ… Verified from LanceDB Context7 (lancedb.search() + rerank pattern)
@tool
async def lancedb_hybrid_search_tool(
    query: str,
    n_results: int = 10,
    filter_metadata: dict = None,
    rerank_strategy: str = "rrf"
) -> List[Dict]:
    """
    LanceDBæ··åˆæ£€ç´¢å·¥å…· (Vector + BM25 Full-Text Search)

    Args:
        query: æŸ¥è¯¢è¯­å¥
        n_results: è¿”å›ç»“æœæ•°
        filter_metadata: å…ƒæ•°æ®è¿‡æ»¤(å¦‚document_type, canvas_file)
        rerank_strategy: é‡æ’åºç­–ç•¥
            - "rrf": Reciprocal Rank Fusion (é»˜è®¤,å¹³è¡¡å‘é‡å’Œå…³é”®è¯)
            - "linear": LinearCombinationReranker (å¯è°ƒæƒé‡)
            - "cross_encoder": CrossEncoderReranker (æœ€é«˜å‡†ç¡®æ€§,æœ€æ…¢)
            - "cohere": CohereReranker (éœ€è¦API key)

    Returns:
        ç›¸ä¼¼æ–‡æ¡£åˆ—è¡¨

    ä½¿ç”¨åœºæ™¯:
        - æ£€ç´¢å†å²è§£é‡Šæ–‡æ¡£(oral-explanation, clarification-pathç­‰)
        - æŸ¥æ‰¾ç›¸ä¼¼æ¦‚å¿µçš„å­¦ä¹ ææ–™
        - ç»“åˆè¯­ä¹‰ç›¸ä¼¼åº¦å’Œå…³é”®è¯åŒ¹é…çš„ç»¼åˆæ£€ç´¢

    æ€§èƒ½ç‰¹ç‚¹:
        - Vector search: è¯­ä¹‰ç†è§£,æ•æ‰æ¦‚å¿µç›¸ä¼¼æ€§
        - BM25 FTS: å…³é”®è¯ç²¾ç¡®åŒ¹é…,term frequencyä¼˜åŒ–
        - Hybrid: 100xæŸ¥è¯¢æ€§èƒ½æå‡ vs Parquet-based DBs
        - æŸ¥è¯¢å»¶è¿Ÿ: <100ms (1M vectors, p95)
    """
    from app.core.lancedb_client import lancedb_table
    from lancedb.rerankers import RRFReranker, LinearCombinationReranker, CrossEncoderReranker

    # Step 1: é€‰æ‹©Reranker
    rerankers = {
        "rrf": RRFReranker(),
        "linear": LinearCombinationReranker(weight=0.7),  # 0.7æƒé‡ç»™å‘é‡æ£€ç´¢
        "cross_encoder": CrossEncoderReranker(),
        # Cohere rerankeréœ€è¦API key,æŒ‰éœ€å¯ç”¨
        # "cohere": CohereReranker(api_key=os.getenv("COHERE_API_KEY"))
    }
    reranker = rerankers.get(rerank_strategy, RRFReranker())

    # Step 2: LanceDBæ··åˆæ£€ç´¢ (Vector + BM25)
    # âœ… Verified from LanceDB Context7: hybrid search pattern
    search_builder = lancedb_table.search(query, query_type="hybrid")

    # æ·»åŠ å…ƒæ•°æ®è¿‡æ»¤
    if filter_metadata:
        search_builder = search_builder.where(filter_metadata)

    # æ‰§è¡Œæ£€ç´¢å¹¶é‡æ’åº
    results = (
        search_builder
        .rerank(reranker=reranker)
        .limit(n_results)
        .to_list()
    )

    # Step 3: æ ¼å¼åŒ–ç»“æœ
    formatted_results = []
    for result in results:
        formatted_results.append({
            "source": "lancedb",
            "content": result.get("text", ""),
            "metadata": {
                "doc_path": result.get("doc_path", ""),
                "concept": result.get("concept", ""),
                "agent_type": result.get("agent_type", ""),
                "vector_score": result.get("_vector_score", 0.0),
                "fts_score": result.get("_fts_score", 0.0),
                "relevance_score": result.get("_relevance_score", 0.0),  # é‡æ’åºåç»¼åˆåˆ†æ•°
                "created_at": result.get("created_at", None)
            }
        })

    return formatted_results
```

**LanceDB Hybrid Searchä¼˜åŠ¿**:

| ç‰¹æ€§ | LanceDB Hybrid | ä¼ ç»ŸChromaDB | æå‡ |
|------|----------------|-------------|------|
| **æŸ¥è¯¢æ€§èƒ½** | <100ms (1M vectors) | ~150-200ms | **2x faster** |
| **æ£€ç´¢æ–¹å¼** | Vector + BM25 + Rerank | çº¯å‘é‡æ£€ç´¢ | **æ›´å…¨é¢** |
| **å¯æ‰©å±•æ€§** | 1B+ vectors | <500K optimal | **2000x scalability** |
| **å…³é”®è¯åŒ¹é…** | BM25å…¨æ–‡ç´¢å¼•(ç²¾ç¡®) | æ— (éœ€è¯­ä¹‰è¿‘ä¼¼) | **æ–°å¢èƒ½åŠ›** |
| **é‡æ’åºç­–ç•¥** | 5ç§(RRF/Linear/CrossEncoderç­‰) | æ—  | **æ›´æ™ºèƒ½** |

**Rerankeré€‰æ‹©æŒ‡å—**:

| Reranker | é€‚ç”¨åœºæ™¯ | æ€§èƒ½ | å‡†ç¡®æ€§ |
|----------|---------|------|--------|
| **RRF** | æ£€éªŒç™½æ¿ç”Ÿæˆ(æ¨è) | â­â­â­â­â­ (<50ms) | â­â­â­â­ |
| **LinearCombination** | éœ€è¦è°ƒæ•´å‘é‡/å…³é”®è¯æƒé‡ | â­â­â­â­â­ (<50ms) | â­â­â­â­ |
| **CrossEncoder** | æœ€é«˜å‡†ç¡®æ€§è¦æ±‚ | â­â­ (<500ms) | â­â­â­â­â­ |
| **Cohere** | å¤šè¯­è¨€,é«˜è´¨é‡å•†ä¸šåº”ç”¨ | â­â­â­ (APIè°ƒç”¨) | â­â­â­â­â­ |

**ä½¿ç”¨ç¤ºä¾‹**:

```python
# ç¤ºä¾‹1: æ£€ç´¢"é€†å¦å‘½é¢˜"ç›¸å…³çš„è§£é‡Šæ–‡æ¡£
results = await lancedb_hybrid_search_tool(
    query="é€†å¦å‘½é¢˜è§£é‡Š",
    n_results=10,
    filter_metadata={"document_type": "explanation"},
    rerank_strategy="rrf"
)

# ç¤ºä¾‹2: æŸ¥æ‰¾ç‰¹å®šCanvasçš„å­¦ä¹ ææ–™(ä½¿ç”¨CrossEncoderè·å¾—æœ€é«˜å‡†ç¡®æ€§)
results = await lancedb_hybrid_search_tool(
    query="ç¦»æ•£æ•°å­¦ é€»è¾‘æ¨ç†",
    n_results=20,
    filter_metadata={"canvas_file": "ç¦»æ•£æ•°å­¦.canvas"},
    rerank_strategy="cross_encoder"
)
```

---

#### 10.4.3 Temporal Behavior Query Tool

```python
@tool
async def temporal_behavior_query_tool(
    canvas_file: str,
    query_type: str = "weak_points",
    time_range_days: int = 30
) -> List[Dict]:
    """
    Temporal Memoryè¡Œä¸ºæŸ¥è¯¢å·¥å…·

    Args:
        canvas_file: Canvasæ–‡ä»¶è·¯å¾„
        query_type: æŸ¥è¯¢ç±»å‹
            - "weak_points": å†å²è–„å¼±ç‚¹
            - "inactive_mastered": é•¿æœŸæœªè®¿é—®çš„å·²æŒæ¡æ¦‚å¿µ
            - "recent_errors": æœ€è¿‘é”™è¯¯è®°å½•
        time_range_days: æ—¶é—´èŒƒå›´(å¤©æ•°)

    Returns:
        è¡Œä¸ºæ•°æ®åˆ—è¡¨

    ä½¿ç”¨åœºæ™¯:
        - æ£€éªŒç™½æ¿ç”Ÿæˆ: æŸ¥è¯¢å†å²è–„å¼±ç‚¹(70%æƒé‡)
        - è‰¾å®¾æµ©æ–¯å¤ä¹ : æŸ¥è¯¢éœ€è¦å¤ä¹ çš„æ¦‚å¿µ
    """
    from app.core.temporal_memory import temporal_memory_client
    from datetime import datetime, timedelta

    cutoff_date = datetime.now() - timedelta(days=time_range_days)

    if query_type == "weak_points":
        # æŸ¥è¯¢å†å²è¯„åˆ†<80çš„æ¦‚å¿µ
        cypher_query = f"""
        MATCH (c:Canvas {{path: "{canvas_file}"}})
        -[:CONTAINS]->(n:Node)
        -[:HAS_UNDERSTANDING_STATE]->(s:UnderstandingState)
        WHERE s.total_score < 80
          AND s.timestamp >= datetime('{cutoff_date.isoformat()}')
        RETURN n.id AS node_id,
               n.text AS concept,
               s.total_score AS score,
               s.timestamp AS error_date,
               s.accuracy AS accuracy,
               s.imagery AS imagery,
               s.completeness AS completeness,
               s.originality AS originality
        ORDER BY s.timestamp DESC, s.total_score ASC
        LIMIT 50
        """
    elif query_type == "inactive_mastered":
        # æŸ¥è¯¢>7å¤©æœªè®¿é—®ä½†è¯„åˆ†â‰¥80çš„æ¦‚å¿µ
        cypher_query = f"""
        MATCH (c:Canvas {{path: "{canvas_file}"}})
        -[:CONTAINS]->(n:Node)
        -[:HAS_UNDERSTANDING_STATE]->(s:UnderstandingState)
        WHERE s.total_score >= 80
          AND s.timestamp < datetime('{(datetime.now() - timedelta(days=7)).isoformat()}')
        RETURN n.id AS node_id,
               n.text AS concept,
               s.total_score AS score,
               s.timestamp AS last_access,
               duration.inDays(s.timestamp, datetime()).days AS days_inactive
        ORDER BY days_inactive DESC
        LIMIT 30
        """
    else:  # recent_errors
        cypher_query = f"""
        MATCH (c:Canvas {{path: "{canvas_file}"}})
        -[:CONTAINS]->(n:Node)
        -[r:HAS_UNDERSTANDING_STATE]->(s:UnderstandingState)
        WHERE s.color = '1'  // çº¢è‰²èŠ‚ç‚¹
          AND s.timestamp >= datetime('{cutoff_date.isoformat()}')
        RETURN n.id AS node_id,
               n.text AS concept,
               s.timestamp AS error_date,
               count(r) AS error_count
        ORDER BY error_count DESC, s.timestamp DESC
        LIMIT 20
        """

    # æ‰§è¡ŒæŸ¥è¯¢
    results = await temporal_memory_client.execute_cypher(cypher_query)

    # æ ¼å¼åŒ–ç»“æœ
    formatted_results = []
    for record in results:
        formatted_results.append({
            "source": "temporal",
            "content": record.get("concept", ""),
            "metadata": {
                "node_id": record.get("node_id", ""),
                "concept": record.get("concept", ""),
                "score": record.get("score", 0),
                "error_date": record.get("error_date", None),
                "days_inactive": record.get("days_inactive", 0),
                "error_count": record.get("error_count", 0),
                "accuracy": record.get("accuracy", 0),
                "imagery": record.get("imagery", 0),
                "completeness": record.get("completeness", 0),
                "originality": record.get("originality", 0)
            }
        })

    return formatted_results
```

---

#### 10.4.4 GraphRAG Global Search Tool

```python
# âœ… Verified from GraphRAG Integration Design (GRAPHRAG-INTEGRATION-DESIGN.md)
@tool
async def graphrag_global_search_tool(
    query: str,
    community_level: int = 2,
    max_data_tokens: int = 12000,
    temperature: float = 0.2
) -> Dict:
    """
    GraphRAGå…¨å±€æœç´¢å·¥å…· (Community-based Global Reasoning)

    Args:
        query: è‡ªç„¶è¯­è¨€æŸ¥è¯¢ï¼ˆæ•°æ®é›†çº§åˆ«çš„åˆ†ææ€§é—®é¢˜ï¼‰
        community_level: ç¤¾åŒºå±‚çº§ (0-3)
            - Level 0: æœ€ç»†ç²’åº¦ï¼ˆå•ä¸ªæ¦‚å¿µï¼‰
            - Level 1: æ¦‚å¿µç»„ï¼ˆæ¨èï¼Œå¦‚"é€»è¾‘æ¨ç†"ï¼‰
            - Level 2: ä¸»é¢˜åŸŸï¼ˆå¦‚"ç¦»æ•£æ•°å­¦åŸºç¡€"ï¼‰
            - Level 3: æ•°æ®é›†å…¨å±€ï¼ˆè·¨Canvasåˆ†æï¼‰
        max_data_tokens: æœ€å¤§ä¸Šä¸‹æ–‡tokenæ•°ï¼ˆé»˜è®¤12000ï¼‰
        temperature: LLMæ¸©åº¦ï¼ˆ0.0-1.0ï¼Œé»˜è®¤0.2ï¼‰

    Returns:
        å…¨å±€æœç´¢ç»“æœï¼ŒåŒ…å«ï¼š
        - answer: LLMç”Ÿæˆçš„ç»¼åˆç­”æ¡ˆ
        - communities: ç›¸å…³ç¤¾åŒºåˆ—è¡¨ï¼ˆå«æ‘˜è¦å’Œæƒé‡ï¼‰
        - sources: è¯æ®æ¥æºï¼ˆå®ä½“å’Œå…³ç³»ï¼‰
        - confidence: ç­”æ¡ˆç½®ä¿¡åº¦ï¼ˆ0.0-1.0ï¼‰

    ä½¿ç”¨åœºæ™¯:
        1. **æ•°æ®é›†çº§åˆ†æ**: "ç¦»æ•£æ•°å­¦ä¸­å“ªäº›æ¦‚å¿µæœ€å®¹æ˜“æ··æ·†ï¼Ÿ"
        2. **è·¨ä¸»é¢˜æ¯”è¾ƒ**: "é€»è¾‘æ¨ç†å’Œé›†åˆè®ºåœ¨å­¦ä¹ è·¯å¾„ä¸Šçš„ä¾èµ–å…³ç³»æ˜¯ä»€ä¹ˆï¼Ÿ"
        3. **å…¨å±€æ¨¡å¼è¯†åˆ«**: "æˆ‘åœ¨å“ªäº›ç±»å‹çš„é—®é¢˜ä¸Šæœ€å®¹æ˜“å‡ºé”™ï¼Ÿ"
        4. **å­¦ä¹ è·¯å¾„è§„åˆ’**: "ä»çº¿æ€§ä»£æ•°åˆ°æœºå™¨å­¦ä¹ çš„æœ€ä¼˜å­¦ä¹ è·¯å¾„æ˜¯ä»€ä¹ˆï¼Ÿ"

    æŠ€æœ¯ç‰¹ç‚¹:
        - **ç¤¾åŒºæ£€æµ‹**: Leidenç®—æ³•å±‚çº§èšç±»ï¼Œè‡ªåŠ¨è¯†åˆ«æ¦‚å¿µä¸»é¢˜
        - **å±‚çº§æ¨ç†**: æ”¯æŒ4å±‚ç¤¾åŒºå±‚çº§ï¼Œä»ç»†ç²’åº¦åˆ°å…¨å±€
        - **LLMé©±åŠ¨**: GPT-4oç»¼åˆå¤šä¸ªç¤¾åŒºæ‘˜è¦ï¼Œç”Ÿæˆå…¨å±€ç­”æ¡ˆ
        - **è¯æ®è¿½æº¯**: æ¯ä¸ªç­”æ¡ˆåŒ…å«æ¥æºç¤¾åŒºå’Œå®ä½“è¯æ®

    æ€§èƒ½ç‰¹ç‚¹:
        - æŸ¥è¯¢å»¶è¿Ÿ: 2-5ç§’ï¼ˆLevel 1-2ï¼‰ï¼Œ5-10ç§’ï¼ˆLevel 3ï¼‰
        - Tokenæ¶ˆè€—: 8K-15K tokens/queryï¼ˆå«ç¤¾åŒºæ‘˜è¦ä¸Šä¸‹æ–‡ï¼‰
        - é€‚ç”¨è§„æ¨¡: 100-10Kæ¦‚å¿µçš„æ•°æ®é›†

    ä¸Local Searchå¯¹æ¯”:
        | ç»´åº¦ | GraphRAG Global Search | Graphiti/LanceDB Local Search |
        |------|----------------------|-------------------------------|
        | æŸ¥è¯¢ç±»å‹ | åˆ†ææ€§ã€æ¯”è¾ƒæ€§ã€å…¨å±€æ€§ | æŸ¥æ‰¾æ€§ã€å®šä¹‰æ€§ã€å±€éƒ¨æ€§ |
        | ä¸Šä¸‹æ–‡èŒƒå›´ | æ•´ä¸ªæ•°æ®é›†æˆ–ä¸»é¢˜åŸŸ | å•ä¸ªæ¦‚å¿µæˆ–å…¶é‚»å±… |
        | ç­”æ¡ˆç”Ÿæˆ | LLMç»¼åˆå¤šç¤¾åŒºæ‘˜è¦ | ç›´æ¥è¿”å›æ£€ç´¢æ–‡æ¡£ |
        | å»¶è¿Ÿ | 2-10ç§’ | <200ms |
        | é€‚ç”¨è§„æ¨¡ | 100-10Kæ¦‚å¿µ | æ— é™åˆ¶ |
    """
    from graphrag.query.structured_search.global_search import GlobalSearch
    from graphrag.query.context_builder.community_context import CommunityContextBuilder
    from app.core.graphrag_client import (
        load_entities,
        load_communities,
        load_community_reports
    )

    # Step 1: åŠ è½½GraphRAGæ•°æ®ï¼ˆç¤¾åŒºç»“æ„ï¼‰
    entities = await load_entities()
    communities = await load_communities(level=community_level)
    community_reports = await load_community_reports(level=community_level)

    # Step 2: æ„å»ºç¤¾åŒºä¸Šä¸‹æ–‡
    context_builder = CommunityContextBuilder(
        entities=entities,
        communities=communities,
        community_reports=community_reports
    )

    # Step 3: åˆå§‹åŒ–GlobalSearch
    searcher = GlobalSearch(
        llm=ChatOpenAI(model="gpt-4o", temperature=temperature),
        context_builder=context_builder,
        max_data_tokens=max_data_tokens
    )

    # Step 4: æ‰§è¡Œå…¨å±€æœç´¢
    result = await searcher.asearch(query=query)

    # Step 5: è®¡ç®—ç­”æ¡ˆç½®ä¿¡åº¦
    confidence = calculate_confidence(
        result=result,
        num_communities=len(communities),
        num_sources=len(result.context_data.get("sources", []))
    )

    # Step 6: æ ¼å¼åŒ–è¿”å›ç»“æœ
    return {
        "source": "graphrag",
        "answer": result.response,
        "communities": [
            {
                "id": c.id,
                "title": c.title,
                "summary": c.summary,
                "weight": c.weight,
                "level": community_level
            }
            for c in result.context_data.get("communities", [])
        ],
        "sources": [
            {
                "entity_id": s.get("id"),
                "entity_name": s.get("name"),
                "entity_type": s.get("type"),
                "relevance": s.get("rank", 0.0)
            }
            for s in result.context_data.get("sources", [])
        ],
        "confidence": confidence,
        "metadata": {
            "query": query,
            "community_level": community_level,
            "num_communities": len(communities),
            "num_sources": len(result.context_data.get("sources", [])),
            "tokens_used": result.context_data.get("num_tokens", 0)
        }
    }


def calculate_confidence(result, num_communities: int, num_sources: int) -> float:
    """
    è®¡ç®—GraphRAGå…¨å±€æœç´¢çš„ç­”æ¡ˆç½®ä¿¡åº¦

    ç½®ä¿¡åº¦åŸºäºï¼š
    1. ç¤¾åŒºè¦†ç›–åº¦ï¼ˆæŸ¥è¯¢å‘½ä¸­å¤šå°‘ç¤¾åŒºï¼‰
    2. è¯æ®æ¥æºæ•°é‡ï¼ˆå¼•ç”¨å¤šå°‘å®ä½“ï¼‰
    3. LLMå“åº”é•¿åº¦ï¼ˆç­”æ¡ˆæ˜¯å¦è¯¦ç»†ï¼‰

    Returns:
        0.0-1.0ä¹‹é—´çš„ç½®ä¿¡åº¦åˆ†æ•°
    """
    # å› å­1: ç¤¾åŒºè¦†ç›–åº¦ (0.0-0.4)
    community_coverage = min(0.4, num_communities * 0.05)

    # å› å­2: è¯æ®æ¥æºå……åˆ†æ€§ (0.0-0.4)
    source_sufficiency = min(0.4, num_sources * 0.04)

    # å› å­3: ç­”æ¡ˆè¯¦ç»†ç¨‹åº¦ (0.0-0.2)
    response_length = len(result.response.split())
    detail_score = min(0.2, response_length / 500 * 0.2)

    return community_coverage + source_sufficiency + detail_score
```

**GraphRAGä½¿ç”¨åœºæ™¯æŒ‡å—**:

| æŸ¥è¯¢ç±»å‹ | ç¤ºä¾‹é—®é¢˜ | æ¨èLevel | é¢„æœŸå»¶è¿Ÿ |
|---------|---------|----------|---------|
| **ä¸»é¢˜å†…åˆ†æ** | "é€»è¾‘æ¨ç†ä¸­å“ªäº›æ¦‚å¿µæœ€å®¹æ˜“æ··æ·†ï¼Ÿ" | Level 1 | 2-3ç§’ |
| **ä¸»é¢˜é—´æ¯”è¾ƒ** | "çº¿æ€§ä»£æ•°å’Œæ¦‚ç‡è®ºçš„å…³è”æ˜¯ä»€ä¹ˆï¼Ÿ" | Level 2 | 3-5ç§’ |
| **æ•°æ®é›†å…¨å±€** | "æˆ‘çš„è–„å¼±ç¯èŠ‚ä¸»è¦é›†ä¸­åœ¨å“ªäº›é¢†åŸŸï¼Ÿ" | Level 3 | 5-10ç§’ |
| **å­¦ä¹ è·¯å¾„è§„åˆ’** | "ä»å¾®ç§¯åˆ†åˆ°æœºå™¨å­¦ä¹ çš„æœ€ä¼˜è·¯å¾„ï¼Ÿ" | Level 2 | 3-5ç§’ |

**ä¸Graphiti/LanceDBååŒç­–ç•¥**:

```python
# åœºæ™¯1: æ··åˆæ£€ç´¢ï¼ˆå…ˆGlobalåLocalï¼‰
# ç”¨æˆ·é—®é¢˜: "ç¦»æ•£æ•°å­¦ä¸­é€»è¾‘æ¨ç†çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä»€ä¹ˆï¼Ÿ"

# Step 1: GraphRAGè¯†åˆ«æ ¸å¿ƒä¸»é¢˜
global_result = await graphrag_global_search_tool(
    query="ç¦»æ•£æ•°å­¦é€»è¾‘æ¨ç†çš„æ ¸å¿ƒæ¦‚å¿µ",
    community_level=1
)
# è¿”å›: "é€»è¾‘æ¨ç†"ç¤¾åŒºï¼ŒåŒ…å«5ä¸ªæ ¸å¿ƒæ¦‚å¿µ

# Step 2: LanceDBæ£€ç´¢æ¯ä¸ªæ ¸å¿ƒæ¦‚å¿µçš„è¯¦ç»†è§£é‡Š
for concept in global_result["sources"]:
    details = await lancedb_hybrid_search_tool(
        query=concept["entity_name"],
        filter_metadata={"document_type": "explanation"}
    )
    concept["detailed_explanation"] = details

# Step 3: GraphitiæŸ¥è¯¢æ¦‚å¿µé—´çš„å…³ç³»
for concept in global_result["sources"]:
    relations = await graphiti_hybrid_search_tool(
        query=f"{concept['entity_name']} ç›¸å…³æ¦‚å¿µ",
        canvas_context={"topic_node_id": concept["entity_id"]}
    )
    concept["related_concepts"] = relations
```

**ä½¿ç”¨ç¤ºä¾‹**:

```python
# ç¤ºä¾‹1: å…¨å±€è–„å¼±ç‚¹åˆ†æï¼ˆæ£€éªŒç™½æ¿ç”Ÿæˆåœºæ™¯ï¼‰
result = await graphrag_global_search_tool(
    query="åœ¨ç¦»æ•£æ•°å­¦Canvasä¸­ï¼Œæˆ‘æœ€è–„å¼±çš„æ¦‚å¿µé›†ç¾¤æ˜¯ä»€ä¹ˆï¼Ÿå®ƒä»¬ä¹‹é—´æœ‰ä»€ä¹ˆå…±æ€§ï¼Ÿ",
    community_level=2
)
# è¿”å›:
# {
#   "answer": "æ‚¨çš„è–„å¼±æ¦‚å¿µä¸»è¦é›†ä¸­åœ¨'é€»è¾‘æ¨ç†'å’Œ'é›†åˆè®º'ä¸¤ä¸ªä¸»é¢˜...",
#   "communities": [
#     {"title": "é€»è¾‘æ¨ç†", "summary": "åŒ…å«å‘½é¢˜é€»è¾‘ã€é€†å¦å‘½é¢˜ç­‰6ä¸ªæ¦‚å¿µ", "weight": 0.8},
#     {"title": "é›†åˆè®º", "summary": "åŒ…å«é›†åˆè¿ç®—ã€å¹‚é›†ç­‰4ä¸ªæ¦‚å¿µ", "weight": 0.6}
#   ],
#   "confidence": 0.85
# }

# ç¤ºä¾‹2: å­¦ä¹ è·¯å¾„è§„åˆ’
result = await graphrag_global_search_tool(
    query="ä»çº¿æ€§ä»£æ•°åˆ°æœºå™¨å­¦ä¹ ï¼Œæˆ‘åº”è¯¥æŒ‰ä»€ä¹ˆé¡ºåºå­¦ä¹ ï¼Ÿå“ªäº›æ¦‚å¿µæ˜¯å¿…é¡»å…ˆæŒæ¡çš„ï¼Ÿ",
    community_level=2
)

# ç¤ºä¾‹3: è·¨Canvasæ¯”è¾ƒåˆ†æ
result = await graphrag_global_search_tool(
    query="æ¯”è¾ƒæˆ‘åœ¨'ç¦»æ•£æ•°å­¦'å’Œ'çº¿æ€§ä»£æ•°'ä¸¤ä¸ªCanvasä¸­çš„å­¦ä¹ æ¨¡å¼ï¼Œæœ‰ä»€ä¹ˆå…±åŒçš„è–„å¼±ç‚¹ï¼Ÿ",
    community_level=3
)
```

---

#### 10.4.5 æœ¬åœ°æ¨¡å‹é…ç½®ï¼ˆæˆæœ¬ä¼˜åŒ–ï¼‰

**èƒŒæ™¯**: åŸºäºADR-001å†³ç­–ï¼ŒGraphRAGé»˜è®¤ä½¿ç”¨**Qwen2.5-14B-Instructæœ¬åœ°æ¨¡å‹**ä½œä¸ºä¸»è¦LLMæä¾›å•†ï¼Œé€šè¿‡90% Qwen2.5 + 10% APIçš„æ··åˆç­–ç•¥ï¼Œå°†æœˆåº¦æˆæœ¬ä»$570é™ä½åˆ°$57ï¼ˆèŠ‚çœ90%ï¼‰ã€‚

**æ ¸å¿ƒé…ç½®**:

```python
# âœ… Verified from ADR-001 & Story GraphRAG.2 (æœ¬åœ°æ¨¡å‹é›†æˆ)
from langchain_community.llms import Ollama
from langchain_openai import ChatOpenAI
import random
from typing import Dict, Any

class HybridLLMProvider:
    """
    æ··åˆLLMæä¾›å•†ï¼ˆæœ¬åœ°æ¨¡å‹ä¼˜å…ˆç­–ç•¥ï¼‰

    ç­–ç•¥:
        - 90%è¯·æ±‚ä½¿ç”¨Qwen2.5-14Bï¼ˆæœ¬åœ°Ollamaï¼Œæˆæœ¬$0ï¼‰
        - 10%è¯·æ±‚ä½¿ç”¨gpt-4o-miniï¼ˆAPIé™çº§ï¼Œæˆæœ¬$0.02ï¼‰
        - æœ¬åœ°è¶…æ—¶/å¤±è´¥æ—¶è‡ªåŠ¨é™çº§åˆ°API

    æˆæœ¬ä¼˜åŒ–:
        - åŸè®¾è®¡: 100% gpt-4oï¼Œæœˆæˆæœ¬$570
        - ä¼˜åŒ–å: 90% Qwen2.5 + 10% gpt-4o-miniï¼Œæœˆæˆæœ¬$57
        - **èŠ‚çœ90%** ($513/æœˆ)

    è´¨é‡ä¿è¯:
        - Qwen2.5è´¨é‡: ~85%ï¼ˆä¸gpt-4oåŸºçº¿å¯¹æ¯”ï¼‰
        - gpt-4o-miniè´¨é‡: ~90%
        - æ··åˆç­–ç•¥ç»¼åˆè´¨é‡: ~86%
        - è´¨é‡ä¸‹é™: 14%ï¼ˆå¯æ¥å—èŒƒå›´ï¼Œæ¢å–90%æˆæœ¬èŠ‚çœï¼‰
    """

    def __init__(
        self,
        local_ratio: float = 0.9,
        local_timeout: int = 8,
        api_timeout: int = 5
    ):
        # æœ¬åœ°æ¨¡å‹ï¼ˆQwen2.5-14B via Ollamaï¼‰
        self.local_llm = Ollama(
            model="qwen2.5:14b-instruct",
            base_url="http://localhost:11434",
            temperature=0.2,
            timeout=local_timeout
        )

        # APIé™çº§æ¨¡å‹ï¼ˆgpt-4o-miniï¼‰
        self.api_llm = ChatOpenAI(
            model="gpt-4o-mini",
            temperature=0.2,
            timeout=api_timeout
        )

        self.local_ratio = local_ratio
        self.cost_tracker = CostTracker()

    async def ainvoke(self, prompt: str) -> str:
        """
        æ··åˆç­–ç•¥è°ƒç”¨ï¼ˆå¼‚æ­¥ï¼‰

        å†³ç­–æµç¨‹:
            1. éšæœºæ•°å†³å®šè·¯ç”±ï¼ˆ90%æœ¬åœ° vs 10%APIï¼‰
            2. å¦‚æœé€‰æ‹©æœ¬åœ°ï¼Œå…ˆå°è¯•æœ¬åœ°æ¨¡å‹
            3. æœ¬åœ°è¶…æ—¶/å¤±è´¥ â†’ è‡ªåŠ¨é™çº§åˆ°API
            4. è®°å½•æˆæœ¬å’Œé™çº§äº‹ä»¶
        """
        use_local = random.random() < self.local_ratio

        if use_local:
            try:
                # å°è¯•æœ¬åœ°æ¨¡å‹
                response = await self.local_llm.ainvoke(prompt)
                self.cost_tracker.record_local_call(model="qwen2.5:14b", cost=0.0)
                return response
            except TimeoutError:
                # è¶…æ—¶ â†’ é™çº§åˆ°API
                self.cost_tracker.record_degradation(reason="local_timeout")
                return await self._invoke_api(prompt)
            except Exception as e:
                # å¤±è´¥ â†’ é™çº§åˆ°API
                self.cost_tracker.record_degradation(reason="local_failure", error=str(e))
                return await self._invoke_api(prompt)
        else:
            # ç›´æ¥ä½¿ç”¨APIï¼ˆ10%æµé‡ï¼Œç”¨äºè´¨é‡å¯¹æ¯”ï¼‰
            return await self._invoke_api(prompt)

    async def _invoke_api(self, prompt: str) -> str:
        """APIè°ƒç”¨ï¼ˆgpt-4o-miniï¼‰"""
        response = await self.api_llm.ainvoke(prompt)

        # ä¼°ç®—tokenæ¶ˆè€—å’Œæˆæœ¬
        input_tokens = len(prompt) // 4  # ç²—ç•¥ä¼°ç®—
        output_tokens = len(response.content) // 4
        cost = self.cost_tracker.record_api_call(
            provider="openai",
            model="gpt-4o-mini",
            input_tokens=input_tokens,
            output_tokens=output_tokens
        )

        return response.content


# é›†æˆåˆ°GraphRAG Global Search
async def graphrag_global_search_with_local_model(
    query: str,
    community_level: int = 2,
    max_data_tokens: int = 12000
) -> Dict:
    """
    ä½¿ç”¨æœ¬åœ°æ¨¡å‹çš„GraphRAGå…¨å±€æœç´¢ï¼ˆæˆæœ¬ä¼˜åŒ–ç‰ˆæœ¬ï¼‰

    å˜æ›´:
        - LLM: gpt-4o â†’ HybridLLMProviderï¼ˆ90% Qwen2.5 + 10% gpt-4o-miniï¼‰
        - æœˆæˆæœ¬: $570 â†’ $57ï¼ˆèŠ‚çœ90%ï¼‰
        - è´¨é‡: 100% â†’ 86%ï¼ˆä¸‹é™14%ï¼‰
    """
    from graphrag.query.structured_search.global_search import GlobalSearch
    from graphrag.query.context_builder.community_context import CommunityContextBuilder

    # ä½¿ç”¨æ··åˆLLMæä¾›å•†ï¼ˆæœ¬åœ°ä¼˜å…ˆï¼‰
    hybrid_llm = HybridLLMProvider(
        local_ratio=0.9,
        local_timeout=8,
        api_timeout=5
    )

    # åŠ è½½ç¤¾åŒºæ•°æ®
    context_builder = CommunityContextBuilder(...)

    # åˆå§‹åŒ–GlobalSearchï¼ˆä½¿ç”¨æ··åˆLLMï¼‰
    searcher = GlobalSearch(
        llm=hybrid_llm,  # æ›¿æ¢åŸæ¥çš„ChatOpenAI(model="gpt-4o")
        context_builder=context_builder,
        max_data_tokens=max_data_tokens
    )

    # æ‰§è¡Œæœç´¢
    result = await searcher.asearch(query=query)

    return {
        "answer": result.response,
        "cost_info": {
            "monthly_cost_estimate": hybrid_llm.cost_tracker.get_monthly_cost(),
            "degradation_rate": hybrid_llm.cost_tracker.get_degradation_rate(),
            "local_success_rate": hybrid_llm.cost_tracker.get_local_success_rate()
        }
    }
```

**æˆæœ¬å¯¹æ¯”**:

| LLMé…ç½® | æˆæœ¬/æŸ¥è¯¢ | æœˆæˆæœ¬ï¼ˆ3000æŸ¥è¯¢ï¼‰ | è´¨é‡ | å»¶è¿Ÿï¼ˆP95ï¼‰ |
|---------|----------|------------------|------|------------|
| **åŸè®¾è®¡**: gpt-4oï¼ˆ100%ï¼‰ | $0.19 | $570 | 100% | 4.5ç§’ |
| **æ–¹æ¡ˆA**: gpt-4o-miniï¼ˆ100%ï¼‰ | $0.02 | $60 | 90% | 2.8ç§’ |
| **æ–¹æ¡ˆB**: Qwen2.5ï¼ˆ100%ï¼‰ | $0.00 | $0 | 85% | 5.2ç§’ |
| **æœ€ç»ˆæ–¹æ¡ˆ**: Qwen2.5ï¼ˆ90%ï¼‰+ gpt-4o-miniï¼ˆ10%ï¼‰ | $0.02 | **$57** | 86% | 5.0ç§’ |

**æ€§èƒ½ä¼˜åŒ–**:
- **æœ¬åœ°æ¨¡å‹**: RTX 4090 GPUæ¨ç†ï¼Œ~5ç§’/æŸ¥è¯¢ï¼ˆ14Bå‚æ•°ï¼‰
- **APIé™çº§**: ä»…åœ¨æœ¬åœ°è¶…æ—¶ï¼ˆ>8ç§’ï¼‰æˆ–å¤±è´¥æ—¶è§¦å‘
- **é™çº§ç‡ç›®æ ‡**: <5%ï¼ˆæœ¬åœ°æˆåŠŸç‡â‰¥95%ï¼‰

**æˆæœ¬ç›‘æ§**:
```python
# âœ… Verified from Story GraphRAG.5 (æˆæœ¬ç›‘æ§)
# æœˆåº¦æˆæœ¬å‘Šè­¦é˜ˆå€¼
COST_ALERT_THRESHOLDS = {
    "warning": 60.0,    # $60 (75%é¢„ç®—)ï¼Œå‘é€è­¦å‘Šé‚®ä»¶
    "critical": 80.0    # $80 (100%é¢„ç®—)ï¼Œè‡ªåŠ¨åˆ‡æ¢ä¸º100%æœ¬åœ°æ¨¡å¼
}

# å®æ—¶æˆæœ¬è¿½è¸ª
monthly_cost = hybrid_llm.cost_tracker.get_monthly_cost()
if monthly_cost >= COST_ALERT_THRESHOLDS["critical"]:
    # ç´§æ€¥é™çº§ï¼šåˆ‡æ¢ä¸º100%æœ¬åœ°æ¨¡å¼
    hybrid_llm.local_ratio = 1.0
    send_alert(f"æˆæœ¬è¶…æ ‡ï¼Œå·²åˆ‡æ¢ä¸º100%æœ¬åœ°æ¨¡å¼ï¼ˆæœˆæˆæœ¬: ${monthly_cost}ï¼‰")
elif monthly_cost >= COST_ALERT_THRESHOLDS["warning"]:
    # è­¦å‘Šï¼šå‘é€é‚®ä»¶æé†’
    send_alert(f"æˆæœ¬é¢„è­¦ï¼Œå½“å‰æœˆæˆæœ¬: ${monthly_cost}ï¼ˆç›®æ ‡<$80ï¼‰")
```

**è´¨é‡éªŒè¯**:
- **æµ‹è¯•æ•°æ®é›†**: 100ä¸ªCanvasæŸ¥è¯¢æ ·æœ¬
- **è¯„ä¼°æŒ‡æ ‡**: F1 Scoreï¼ˆå®ä½“æå–å‡†ç¡®æ€§ï¼‰
- **åŸºçº¿å¯¹æ¯”**: gpt-4oè´¨é‡ä½œä¸º100%åŸºçº¿
- **éªŒæ”¶æ ‡å‡†**: Qwen2.5è´¨é‡â‰¥85%ï¼ˆStory GraphRAG.2 AC 5ï¼‰

**éƒ¨ç½²è¦æ±‚**:
- **ç¡¬ä»¶**: RTX 4090 24GB VRAMï¼ˆæ¨èï¼‰æˆ–A100
- **è½¯ä»¶**: Ollama + Qwen2.5-14B-Instructæ¨¡å‹
- **ç½‘ç»œ**: æ— éœ€å¤–ç½‘ï¼ˆæœ¬åœ°æ¨ç†ï¼‰ï¼ŒAPIä½œä¸ºé™çº§æ–¹æ¡ˆéœ€è¦å¤–ç½‘
- **æˆæœ¬**: ä¸€æ¬¡æ€§ç¡¬ä»¶æŠ•èµ„$1600ï¼ˆRTX 4090ï¼‰ï¼Œ2.8ä¸ªæœˆå³å¯å›æœ¬

---

### 10.5 Agentic RAG Graphå®ç°

#### 10.5.1 å®Œæ•´Graphå®šä¹‰ï¼ˆå«GraphRAGæ™ºèƒ½è·¯ç”±ï¼‰

```python
# âœ… Verified from LangGraph Skill (references/llms-txt.md:8723-8825)
from langgraph.graph import StateGraph, START, END
from langgraph.prebuilt import ToolNode

# Step 1: å®šä¹‰toolsåˆ—è¡¨ï¼ˆæ‰©å±•ä¸º4ä¸ªå·¥å…·ï¼‰
tools = [
    graphiti_hybrid_search_tool,
    lancedb_hybrid_search_tool,
    temporal_behavior_query_tool,
    graphrag_global_search_tool  # æ–°å¢
]

# Step 2: åˆ›å»ºStateGraph
builder = StateGraph(AgenticRAGState)

# Step 3: æ·»åŠ èŠ‚ç‚¹
# 3.1 è·¯ç”±å’ŒæŸ¥è¯¢ç”ŸæˆèŠ‚ç‚¹
builder.add_node("question_router", question_router_node)  # æ–°å¢ï¼šæ™ºèƒ½è·¯ç”±
builder.add_node("generate_query_local", generate_query_or_respond_node)  # é‡å‘½å
builder.add_node("generate_query_global", generate_query_or_respond_node)  # å…¨å±€æŸ¥è¯¢ç”Ÿæˆ
builder.add_node("generate_query_hybrid", generate_query_or_respond_node)  # æ··åˆæŸ¥è¯¢ç”Ÿæˆ

# 3.2 æ£€ç´¢èŠ‚ç‚¹
builder.add_node("local_search_tools", ToolNode([
    graphiti_hybrid_search_tool,
    lancedb_hybrid_search_tool,
    temporal_behavior_query_tool
]))  # Localæ£€ç´¢ï¼š3ä¸ªå·¥å…·
builder.add_node("graphrag_global_search", graphrag_global_search_node)  # æ–°å¢ï¼šå…¨å±€æœç´¢
builder.add_node("composite_search", composite_search_node)  # æ–°å¢ï¼šç»„åˆæœç´¢

# 3.3 èåˆå’Œè¯„åˆ†èŠ‚ç‚¹
builder.add_node("fusion_rerank", fusion_rerank_node)  # æ–°å¢ï¼šèåˆé‡æ’åº
builder.add_node("grade_documents", grade_documents_node)  # æ–‡æ¡£è´¨é‡è¯„åˆ†

# 3.4 è‡ªæˆ‘ä¿®æ­£å’Œç”ŸæˆèŠ‚ç‚¹
builder.add_node("rewrite", rewrite_query_node)  # æŸ¥è¯¢é‡å†™
builder.add_node("generate", generate_questions_node)  # ç”Ÿæˆæ£€éªŒé—®é¢˜

# Step 4: æ·»åŠ è¾¹å’Œæ¡ä»¶è·¯ç”±
# 4.1 å…¥å£ï¼šä»STARTè¿›å…¥question_router
builder.add_edge(START, "question_router")

# 4.2 ä»question_routerè·¯ç”±åˆ°ä¸åŒçš„æŸ¥è¯¢ç”ŸæˆèŠ‚ç‚¹
builder.add_conditional_edges(
    "question_router",
    route_by_query_type,  # æ¡ä»¶å‡½æ•°ï¼šåŸºäºquery_typeè·¯ç”±
    {
        "local": "generate_query_local",
        "global": "generate_query_global",
        "hybrid": "generate_query_hybrid"
    }
)

# 4.3 ä»æŸ¥è¯¢ç”ŸæˆèŠ‚ç‚¹åˆ°æ£€ç´¢èŠ‚ç‚¹æˆ–ç›´æ¥ç”Ÿæˆ
# Localè·¯å¾„
builder.add_conditional_edges(
    "generate_query_local",
    route_query,  # æ¡ä»¶å‡½æ•°ï¼šæ˜¯å¦éœ€è¦æ£€ç´¢
    {
        "retrieve": "local_search_tools",
        "generate": "generate"
    }
)

# Globalè·¯å¾„
builder.add_conditional_edges(
    "generate_query_global",
    route_query,
    {
        "retrieve": "graphrag_global_search",
        "generate": "generate"
    }
)

# Hybridè·¯å¾„
builder.add_conditional_edges(
    "generate_query_hybrid",
    route_query,
    {
        "retrieve": "composite_search",
        "generate": "generate"
    }
)

# 4.4 æ£€ç´¢åå¤„ç†
# Localå’ŒGlobalæ£€ç´¢ç›´æ¥è¿›å…¥grade_documents
builder.add_edge("local_search_tools", "grade_documents")
builder.add_edge("graphrag_global_search", "grade_documents")

# Compositeæ£€ç´¢éœ€è¦å…ˆç»è¿‡fusion_rerank
builder.add_edge("composite_search", "fusion_rerank")
builder.add_edge("fusion_rerank", "grade_documents")

# 4.5 åŸºäºæ–‡æ¡£è´¨é‡å†³å®šä¸‹ä¸€æ­¥
builder.add_conditional_edges(
    "grade_documents",
    grade_documents_route,  # æ¡ä»¶å‡½æ•°ï¼šè´¨é‡è¯„åˆ†
    {
        "rewrite": "rewrite",
        "generate": "generate"
    }
)

# 4.6 æŸ¥è¯¢é‡å†™åé‡æ–°è·¯ç”±
builder.add_edge("rewrite", "question_router")  # é‡æ–°è¿›å…¥è·¯ç”±é€»è¾‘

# 4.7 ç”Ÿæˆå®Œæˆåç»“æŸ
builder.add_edge("generate", END)

# Step 5: ç¼–è¯‘graph
agentic_rag_graph = builder.compile()
```

**æ–°æ¶æ„çš„å…³é”®æ”¹è¿›**:

1. **æ™ºèƒ½è·¯ç”±å…¥å£**: æ‰€æœ‰æŸ¥è¯¢å…ˆç»è¿‡question_routeråˆ†ç±»
2. **ä¸‰ç§æ£€ç´¢è·¯å¾„**: Localï¼ˆ3å·¥å…·ï¼‰ã€Globalï¼ˆGraphRAGï¼‰ã€Hybridï¼ˆ4å·¥å…·å¹¶è¡Œï¼‰
3. **Fusion Reranking**: Hybridè·¯å¾„ä¸“äº«ï¼Œèåˆ4ä¸ªæ•°æ®æº
4. **Self-Correction Loop**: é‡å†™åé‡æ–°è·¯ç”±ï¼Œè€Œéå›åˆ°å›ºå®šèŠ‚ç‚¹
5. **çµæ´»æ‰©å±•**: å¯æ ¹æ®query_typeè½»æ¾æ·»åŠ æ–°çš„æ£€ç´¢ç­–ç•¥

**è·¯ç”±å†³ç­–ç¤ºä¾‹**:

```python
# ç¤ºä¾‹1ï¼šLocalè·¯ç”±
query = "ä»€ä¹ˆæ˜¯é€†å¦å‘½é¢˜ï¼Ÿ"
# question_router â†’ local â†’ generate_query_local â†’ local_search_tools â†’ grade_documents â†’ generate

# ç¤ºä¾‹2ï¼šGlobalè·¯ç”±
query = "ç¦»æ•£æ•°å­¦ä¸­å“ªäº›æ¦‚å¿µæœ€å®¹æ˜“æ··æ·†ï¼Ÿ"
# question_router â†’ global â†’ generate_query_global â†’ graphrag_global_search â†’ grade_documents â†’ generate

# ç¤ºä¾‹3ï¼šHybridè·¯ç”±
query = "é€»è¾‘æ¨ç†çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä»€ä¹ˆï¼Œæˆ‘åœ¨å“ªäº›æ–¹é¢æœ€è–„å¼±ï¼Ÿ"
# question_router â†’ hybrid â†’ generate_query_hybrid â†’ composite_search â†’ fusion_rerank â†’ grade_documents â†’ generate

# ç¤ºä¾‹4ï¼šSelf-Correction Loop
query = "ç¦»æ•£æ•°å­¦"  # æ¨¡ç³ŠæŸ¥è¯¢
# question_router â†’ local â†’ generate_query_local â†’ local_search_tools â†’ grade_documents (è´¨é‡ä½)
# â†’ rewrite â†’ question_router â†’ local â†’ ... (é‡è¯•)
```
```

---

#### 10.5.2 èŠ‚ç‚¹å®ç°ï¼ˆå«GraphRAGæ–°èŠ‚ç‚¹ï¼‰

**Node 0: question_router** (æ–°å¢)

```python
# âœ… Verified from GRAPHRAG-INTEGRATION-DESIGN.md (Question Router Pattern)
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
import json

async def question_router_node(state: AgenticRAGState) -> AgenticRAGState:
    """
    æ™ºèƒ½é—®é¢˜è·¯ç”±èŠ‚ç‚¹ï¼šåˆ†ç±»æŸ¥è¯¢ç±»å‹ï¼ˆlocal/global/hybridï¼‰

    è·¯ç”±å†³ç­–é€»è¾‘ï¼š
    - local: å…·ä½“æ¦‚å¿µæŸ¥è¯¢ã€å®šä¹‰æŸ¥æ‰¾ â†’ Graphiti + LanceDB + Temporal
    - global: æ•°æ®é›†çº§åˆ†æã€è·¨ä¸»é¢˜æ¯”è¾ƒ â†’ GraphRAG Global Search
    - hybrid: å¤æ‚ç»¼åˆæŸ¥è¯¢ï¼ˆå¦‚"æ ¸å¿ƒæ¦‚å¿µ+è–„å¼±ç‚¹"ï¼‰ â†’ å››å±‚å¹¶è¡Œæ£€ç´¢
    """

    router_prompt = ChatPromptTemplate.from_messages([
        ("system", """ä½ æ˜¯Canvaså­¦ä¹ ç³»ç»Ÿçš„æŸ¥è¯¢è·¯ç”±å™¨ã€‚
        åˆ†æç”¨æˆ·çš„æŸ¥è¯¢ï¼Œç¡®å®šæœ€ä½³æ£€ç´¢ç­–ç•¥ã€‚

        **è·¯ç”±è§„åˆ™**ï¼š

        1. **local** - å…·ä½“æ¦‚å¿µæŸ¥è¯¢ï¼ˆå±€éƒ¨æ£€ç´¢ï¼‰
           - è§¦å‘è¯ï¼šä»€ä¹ˆæ˜¯ã€å®šä¹‰ã€è§£é‡Šã€å¦‚ä½•ç†è§£
           - ç¤ºä¾‹ï¼š"ä»€ä¹ˆæ˜¯é€†å¦å‘½é¢˜ï¼Ÿ"ã€"è§£é‡Šé›†åˆè®ºçš„åŸºæœ¬æ¦‚å¿µ"
           - ç­–ç•¥ï¼šGraphitiå›¾éå† + LanceDBè¯­ä¹‰æ£€ç´¢ + Temporalå†å²æ•°æ®

        2. **global** - æ•°æ®é›†çº§åˆ†æï¼ˆå…¨å±€æ¨ç†ï¼‰
           - è§¦å‘è¯ï¼šå“ªäº›ã€æ¯”è¾ƒã€æ•´ä½“ã€æ•°æ®é›†çº§ã€è·¨ä¸»é¢˜
           - ç¤ºä¾‹ï¼š"ç¦»æ•£æ•°å­¦ä¸­å“ªäº›æ¦‚å¿µæœ€å®¹æ˜“æ··æ·†ï¼Ÿ"ã€"æˆ‘çš„è–„å¼±ç¯èŠ‚é›†ä¸­åœ¨å“ªäº›é¢†åŸŸï¼Ÿ"
           - ç­–ç•¥ï¼šGraphRAGç¤¾åŒºæ£€æµ‹ + å…¨å±€æ‘˜è¦ + LLMç»¼åˆåˆ†æ

        3. **hybrid** - å¤æ‚ç»¼åˆæŸ¥è¯¢ï¼ˆæ··åˆæ£€ç´¢ï¼‰
           - è§¦å‘è¯ï¼šåŒ…å«å¤šä¸ªç»´åº¦ï¼ˆå¦‚"æ ¸å¿ƒæ¦‚å¿µ+è–„å¼±ç‚¹"ã€"è·¯å¾„+å…³è”"ï¼‰
           - ç¤ºä¾‹ï¼š"é€»è¾‘æ¨ç†çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä»€ä¹ˆï¼Œæˆ‘åœ¨å“ªäº›æ–¹é¢æœ€è–„å¼±ï¼Ÿ"
           - ç­–ç•¥ï¼šGraphiti + LanceDB + Temporal + GraphRAGå››å±‚å¹¶è¡Œ

        è¿”å›JSONæ ¼å¼ï¼š
        {
            "query_type": "local|global|hybrid",
            "reasoning": "è·¯ç”±å†³ç­–ç†ç”±ï¼ˆ50å­—ä»¥å†…ï¼‰"
        }"""),
        ("user", """Canvasæ–‡ä»¶ï¼š{canvas_file}

æŸ¥è¯¢å†…å®¹ï¼š{query}

Canvasä¸Šä¸‹æ–‡ï¼ˆçº¢/ç´«èŠ‚ç‚¹æ•°é‡ï¼‰ï¼š
- çº¢è‰²èŠ‚ç‚¹ï¼ˆå®Œå…¨ä¸ç†è§£ï¼‰ï¼š{red_count}ä¸ª
- ç´«è‰²èŠ‚ç‚¹ï¼ˆä¼¼æ‡‚éæ‡‚ï¼‰ï¼š{purple_count}ä¸ª

è¯·åˆ†ææŸ¥è¯¢ç±»å‹å¹¶è·¯ç”±ã€‚""")
    ])

    llm = ChatOpenAI(model="gpt-4o", temperature=0)
    chain = router_prompt | llm

    # å‡†å¤‡è¾“å…¥æ•°æ®
    red_count = len([n for n in state["canvas_context"].get("nodes", []) if n.get("color") == "1"])
    purple_count = len([n for n in state["canvas_context"].get("nodes", []) if n.get("color") == "3"])

    result = await chain.ainvoke({
        "canvas_file": state["canvas_file"],
        "query": state.get("current_query", ""),
        "red_count": red_count,
        "purple_count": purple_count
    })

    # è§£æLLMå“åº”
    try:
        classification = json.loads(result.content)
    except json.JSONDecodeError:
        # é™çº§ç­–ç•¥ï¼šé»˜è®¤ä½¿ç”¨local
        classification = {
            "query_type": "local",
            "reasoning": "JSONè§£æå¤±è´¥ï¼Œé™çº§ä¸ºlocalæ£€ç´¢"
        }

    return {
        **state,
        "query_type": classification["query_type"],
        "routing_reasoning": classification["reasoning"],
        "workflow_stage": "routing",
        "messages": state["messages"] + [{"role": "system", "content": f"è·¯ç”±å†³ç­–ï¼š{classification['query_type']} | ç†ç”±ï¼š{classification['reasoning']}"}]
    }
```

**Node 1A: graphrag_global_search_node** (æ–°å¢)

```python
# âœ… Verified from GRAPHRAG-INTEGRATION-DESIGN.md
async def graphrag_global_search_node(state: AgenticRAGState) -> AgenticRAGState:
    """
    GraphRAGå…¨å±€æœç´¢èŠ‚ç‚¹ï¼šæ‰§è¡Œç¤¾åŒºçº§åˆ«çš„å…¨å±€æ¨ç†

    ä½¿ç”¨åœºæ™¯ï¼š
    - æ•°æ®é›†çº§åˆ†æï¼š"å“ªäº›æ¦‚å¿µæœ€å®¹æ˜“æ··æ·†ï¼Ÿ"
    - è·¨ä¸»é¢˜æ¯”è¾ƒï¼š"çº¿æ€§ä»£æ•°å’Œæ¦‚ç‡è®ºçš„å…³è”æ˜¯ä»€ä¹ˆï¼Ÿ"
    - å­¦ä¹ è·¯å¾„è§„åˆ’ï¼š"ä»å¾®ç§¯åˆ†åˆ°æœºå™¨å­¦ä¹ çš„æœ€ä¼˜è·¯å¾„ï¼Ÿ"
    """
    from graphrag.query.structured_search.global_search import GlobalSearch
    from graphrag.query.context_builder.community_context import CommunityContextBuilder
    from app.core.graphrag_client import (
        load_entities,
        load_communities,
        load_community_reports
    )

    # é»˜è®¤ä½¿ç”¨Level 2ï¼ˆä¸»é¢˜åŸŸçº§åˆ«ï¼‰
    community_level = state.get("graphrag_community_level", 2)

    # Step 1: åŠ è½½GraphRAGæ•°æ®
    entities = await load_entities()
    communities = await load_communities(level=community_level)
    community_reports = await load_community_reports(level=community_level)

    # Step 2: æ„å»ºç¤¾åŒºä¸Šä¸‹æ–‡
    context_builder = CommunityContextBuilder(
        entities=entities,
        communities=communities,
        community_reports=community_reports
    )

    # Step 3: æ‰§è¡Œå…¨å±€æœç´¢
    searcher = GlobalSearch(
        llm=ChatOpenAI(model="gpt-4o", temperature=0.2),
        context_builder=context_builder,
        max_data_tokens=12000
    )

    result = await searcher.asearch(query=state["current_query"])

    # Step 4: æ ¼å¼åŒ–ä¸ºç»Ÿä¸€æ–‡æ¡£æ ¼å¼
    formatted_docs = []

    # å°†GraphRAGå…¨å±€ç­”æ¡ˆè½¬æ¢ä¸ºæ–‡æ¡£
    formatted_docs.append({
        "source": "graphrag",
        "content": result.response,
        "metadata": {
            "concept": "å…¨å±€åˆ†æ",
            "relevance_score": 1.0,  # GraphRAGç­”æ¡ˆé»˜è®¤é«˜ç›¸å…³æ€§
            "community_level": community_level,
            "num_communities": len(result.context_data.get("communities", [])),
            "num_sources": len(result.context_data.get("sources", []))
        }
    })

    # å°†ç¤¾åŒºæ‘˜è¦ä¹Ÿä½œä¸ºæ–‡æ¡£
    for community in result.context_data.get("communities", []):
        formatted_docs.append({
            "source": "graphrag",
            "content": community.summary,
            "metadata": {
                "concept": community.title,
                "relevance_score": community.weight,
                "community_id": community.id,
                "community_level": community_level
            }
        })

    return {
        **state,
        "retrieved_documents": formatted_docs,
        "graphrag_results": {
            "answer": result.response,
            "communities": result.context_data.get("communities", []),
            "sources": result.context_data.get("sources", []),
            "confidence": calculate_graphrag_confidence(result, len(communities))
        },
        "workflow_stage": "tool_execution",
        "messages": state["messages"] + [{"role": "system", "content": f"GraphRAGå…¨å±€æœç´¢å®Œæˆï¼š{len(formatted_docs)}ä¸ªæ–‡æ¡£"}]
    }


def calculate_graphrag_confidence(result, num_communities: int) -> float:
    """è®¡ç®—GraphRAGç­”æ¡ˆç½®ä¿¡åº¦"""
    num_sources = len(result.context_data.get("sources", []))
    response_length = len(result.response.split())

    community_coverage = min(0.4, num_communities * 0.05)
    source_sufficiency = min(0.4, num_sources * 0.04)
    detail_score = min(0.2, response_length / 500 * 0.2)

    return community_coverage + source_sufficiency + detail_score
```

**Node 1B: composite_search_node** (æ–°å¢)

```python
# âœ… Verified from GRAPHRAG-INTEGRATION-DESIGN.md (Parallel Execution Pattern)
import asyncio

async def composite_search_node(state: AgenticRAGState) -> AgenticRAGState:
    """
    ç»„åˆæœç´¢èŠ‚ç‚¹ï¼šå¹¶è¡Œæ‰§è¡Œå››å±‚æ£€ç´¢ï¼ˆGraphiti + LanceDB + Temporal + GraphRAGï¼‰

    ä½¿ç”¨åœºæ™¯ï¼š
    - å¤æ‚ç»¼åˆæŸ¥è¯¢ï¼š"é€»è¾‘æ¨ç†çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä»€ä¹ˆï¼Œæˆ‘åœ¨å“ªäº›æ–¹é¢æœ€è–„å¼±ï¼Ÿ"
    - éœ€è¦å…¨æ–¹ä½ä¸Šä¸‹æ–‡çš„é—®é¢˜ç”Ÿæˆ
    """

    # å®šä¹‰å¹¶è¡Œä»»åŠ¡
    async def graphiti_task():
        result = await graphiti_hybrid_search_tool(
            query=state["current_query"],
            canvas_context=state["canvas_context"],
            max_results=20,
            rerank_strategy="rrf"
        )
        return result

    async def lancedb_task():
        result = await lancedb_hybrid_search_tool(
            query=state["current_query"],
            n_results=15,
            filter_metadata={"canvas_file": state["canvas_file"]},
            rerank_strategy="rrf"
        )
        return result

    async def temporal_task():
        result = await temporal_behavior_query_tool(
            canvas_file=state["canvas_file"],
            query_type="weak_points",
            time_range_days=30
        )
        return result

    async def graphrag_task():
        result = await graphrag_global_search_tool(
            query=state["current_query"],
            community_level=2,
            max_data_tokens=12000
        )
        return result

    # å¹¶è¡Œæ‰§è¡Œï¼ˆè¶…æ—¶æ§åˆ¶ï¼šå•ä¸ª5ç§’ï¼Œæ€»è®¡10ç§’ï¼‰
    try:
        results = await asyncio.wait_for(
            asyncio.gather(
                graphiti_task(),
                lancedb_task(),
                temporal_task(),
                graphrag_task(),
                return_exceptions=True
            ),
            timeout=10.0
        )
    except asyncio.TimeoutError:
        return {
            **state,
            "error_log": state.get("error_log", []) + [{
                "timestamp": datetime.now().isoformat(),
                "error": "Composite search timeout",
                "action": "Fallback to local search"
            }],
            "workflow_stage": "tool_execution"
        }

    # åˆå¹¶æ‰€æœ‰æˆåŠŸçš„ç»“æœ
    all_docs = []
    for i, result in enumerate(results):
        if isinstance(result, Exception):
            continue  # è·³è¿‡å¤±è´¥çš„ä»»åŠ¡
        all_docs.extend(result)

    # å¤±è´¥é™çº§ï¼šè‡³å°‘2ä¸ªå·¥å…·æˆåŠŸ
    successful_count = len([r for r in results if not isinstance(r, Exception)])
    if successful_count < 2:
        return {
            **state,
            "error_log": state.get("error_log", []) + [{
                "timestamp": datetime.now().isoformat(),
                "error": f"Only {successful_count}/4 tools succeeded",
                "action": "Quality may be degraded"
            }],
            "retrieved_documents": all_docs,
            "workflow_stage": "tool_execution"
        }

    return {
        **state,
        "retrieved_documents": all_docs,
        "workflow_stage": "fusion_reranking",
        "messages": state["messages"] + [{"role": "system", "content": f"ç»„åˆæœç´¢å®Œæˆï¼š{len(all_docs)}ä¸ªæ–‡æ¡£ï¼ˆ{successful_count}/4å·¥å…·æˆåŠŸï¼‰"}]
    }
```

**Node 1C: fusion_rerank_node** (æ–°å¢)

```python
# âœ… Verified from LanceDB Context7 (RRF Reranker Pattern)
from lancedb.rerankers import RRFReranker, CrossEncoderReranker
from collections import defaultdict

async def fusion_rerank_node(state: AgenticRAGState) -> AgenticRAGState:
    """
    èåˆé‡æ’åºèŠ‚ç‚¹ï¼šä½¿ç”¨RRFèåˆå››ä¸ªæ•°æ®æºçš„æ£€ç´¢ç»“æœ

    é‡æ’åºç­–ç•¥ï¼š
    - rrf (é»˜è®¤)ï¼šReciprocal Rank Fusionï¼Œå¹³è¡¡å„æ•°æ®æº
    - cross_encoderï¼šCrossEncoderæ¨¡å‹ï¼Œæœ€é«˜å‡†ç¡®æ€§ä½†æœ€æ…¢
    - linearï¼šçº¿æ€§åŠ æƒç»„åˆï¼ˆå¯é…ç½®æƒé‡ï¼‰
    """
    retrieved_docs = state.get("retrieved_documents", [])

    if not retrieved_docs:
        return {**state, "workflow_stage": "document_grading"}

    fusion_strategy = state.get("fusion_strategy", "rrf")

    if fusion_strategy == "rrf":
        # RRFèåˆï¼ˆæ¨èï¼‰
        reranked_docs = rrf_fusion(retrieved_docs)
    elif fusion_strategy == "cross_encoder":
        # CrossEncoderé‡æ’åºï¼ˆæœ€å‡†ç¡®ä½†æ…¢ï¼‰
        reranker = CrossEncoderReranker()
        reranked_docs = reranker.rerank(state["current_query"], retrieved_docs)
    elif fusion_strategy == "linear":
        # çº¿æ€§åŠ æƒèåˆ
        weights = state.get("fusion_weights", {
            "graphiti": 0.3,
            "lancedb": 0.25,
            "temporal": 0.2,
            "graphrag": 0.25
        })
        reranked_docs = linear_fusion(retrieved_docs, weights)
    else:
        reranked_docs = retrieved_docs

    # å»é‡ï¼šåŸºäºconceptå­—æ®µåˆå¹¶ç›¸åŒæ¦‚å¿µ
    deduplicated_docs = deduplicate_by_concept(reranked_docs)

    return {
        **state,
        "retrieved_documents": deduplicated_docs,
        "workflow_stage": "document_grading",
        "messages": state["messages"] + [{"role": "system", "content": f"èåˆé‡æ’åºå®Œæˆï¼š{len(reranked_docs)} â†’ {len(deduplicated_docs)}ä¸ªæ–‡æ¡£"}]
    }


def rrf_fusion(docs: list[dict], k: int = 60) -> list[dict]:
    """
    Reciprocal Rank Fusion (RRF)ç®—æ³•

    å…¬å¼ï¼šRRF_score(doc) = Î£ 1/(k + rank_source_i(doc))
    """
    # æŒ‰sourceåˆ†ç»„
    docs_by_source = defaultdict(list)
    for doc in docs:
        docs_by_source[doc["source"]].append(doc)

    # è®¡ç®—æ¯ä¸ªsourceå†…çš„rank
    for source, source_docs in docs_by_source.items():
        sorted_docs = sorted(source_docs, key=lambda d: d["metadata"].get("relevance_score", 0), reverse=True)
        for rank, doc in enumerate(sorted_docs, start=1):
            doc["rank"] = rank

    # è®¡ç®—RRFåˆ†æ•°
    rrf_scores = defaultdict(float)
    for doc in docs:
        doc_id = doc["metadata"].get("concept", "") + doc.get("content", "")[:50]
        rrf_scores[doc_id] += 1.0 / (k + doc.get("rank", 1))

    # é‡æ–°æ’åº
    for doc in docs:
        doc_id = doc["metadata"].get("concept", "") + doc.get("content", "")[:50]
        doc["rrf_score"] = rrf_scores[doc_id]

    return sorted(docs, key=lambda d: d.get("rrf_score", 0), reverse=True)


def linear_fusion(docs: list[dict], weights: dict[str, float]) -> list[dict]:
    """çº¿æ€§åŠ æƒèåˆ"""
    for doc in docs:
        source = doc["source"]
        weight = weights.get(source, 0.25)
        relevance = doc["metadata"].get("relevance_score", 0)
        doc["fusion_score"] = weight * relevance

    return sorted(docs, key=lambda d: d.get("fusion_score", 0), reverse=True)


def deduplicate_by_concept(docs: list[dict]) -> list[dict]:
    """åŸºäºconceptå­—æ®µå»é‡ï¼Œä¿ç•™æœ€é«˜åˆ†çš„æ–‡æ¡£"""
    concept_to_doc = {}
    for doc in docs:
        concept = doc["metadata"].get("concept", "")
        score = doc.get("rrf_score", doc.get("fusion_score", 0))

        if concept not in concept_to_doc or score > concept_to_doc[concept].get("rrf_score", 0):
            concept_to_doc[concept] = doc

    return list(concept_to_doc.values())
```

---

**Node 1: generate_query_or_respond** (åŸæœ‰èŠ‚ç‚¹ï¼Œéœ€æ›´æ–°)

```python
from langchain_anthropic import ChatAnthropic

async def generate_query_or_respond_node(state: AgenticRAGState) -> AgenticRAGState:
    """
    Agentå†³ç­–èŠ‚ç‚¹: æ˜¯å¦éœ€è¦æ£€ç´¢
    """
    # åˆå§‹åŒ–LLM
    llm = ChatAnthropic(model="claude-sonnet-4-5-20250929")
    llm_with_tools = llm.bind_tools(tools)

    # æ„å»ºç³»ç»Ÿæç¤º
    system_prompt = """ä½ æ˜¯Canvaså­¦ä¹ ç³»ç»Ÿçš„æ£€éªŒç™½æ¿ç”ŸæˆAgentã€‚

ä½ çš„ä»»åŠ¡: åŸºäºç”¨æˆ·çš„Canvaså­¦ä¹ æƒ…å†µ,ç”Ÿæˆä¸ªæ€§åŒ–æ£€éªŒé—®é¢˜ã€‚

ä¼˜å…ˆçº§ç­–ç•¥:
1. **70%æƒé‡**: å†å²è–„å¼±ç‚¹(è¯„åˆ†<80çš„æ¦‚å¿µ)
2. **30%æƒé‡**: å·²æŒæ¡æ¦‚å¿µ(è¯„åˆ†â‰¥80,ç”¨äºå·©å›º)

å¯ç”¨å·¥å…·:
- graphiti_hybrid_search_tool: æ£€ç´¢å†å²è–„å¼±ç‚¹å’Œç›¸å…³æ¦‚å¿µ(æ¨èä½¿ç”¨rerank_strategy='rrf')
- lancedb_hybrid_search_tool: æ··åˆæ£€ç´¢è§£é‡Šæ–‡æ¡£(Vector + BM25, æ¨èrerank_strategy='rrf')
- temporal_behavior_query_tool: æŸ¥è¯¢å­¦ä¹ è¡Œä¸ºæ•°æ®(æ¨èquery_type='weak_points')

å†³ç­–é€»è¾‘:
- å¦‚æœCanvasæœ‰çº¢è‰²/ç´«è‰²èŠ‚ç‚¹ â†’ è°ƒç”¨temporal_behavior_query_toolæŸ¥è¯¢å†å²è–„å¼±ç‚¹
- å¦‚æœéœ€è¦æ¦‚å¿µå…³è” â†’ è°ƒç”¨graphiti_hybrid_search_tool
- å¦‚æœéœ€è¦å‚è€ƒè§£é‡Šæ–‡æ¡£ â†’ è°ƒç”¨lancedb_hybrid_search_tool
- å¦‚æœå·²æœ‰è¶³å¤Ÿä¿¡æ¯ â†’ ç›´æ¥ç”Ÿæˆæ£€éªŒé—®é¢˜
"""

    # æ„å»ºç”¨æˆ·æ¶ˆæ¯
    user_message = f"""
Canvasæ–‡ä»¶: {state['canvas_file']}

Canvasä¸Šä¸‹æ–‡:
{json.dumps(state['canvas_context'], ensure_ascii=False, indent=2)}

ç”¨æˆ·å·²æœ‰ç†è§£:
{state['user_understanding']}

è¯·å†³ç­–: æ˜¯å¦éœ€è¦æ£€ç´¢3å±‚è®°å¿†ç³»ç»Ÿ? å¦‚æœéœ€è¦,è°ƒç”¨ç›¸åº”çš„å·¥å…·ã€‚
"""

    # è°ƒç”¨LLM
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_message}
    ]

    response = await llm_with_tools.ainvoke(messages)

    # æ›´æ–°State
    return {
        **state,
        "messages": state["messages"] + [response],
        "workflow_stage": "query_generation",
        "retrieval_attempts": state.get("retrieval_attempts", 0) + 1
    }
```

**Node 2: grade_documents**

```python
async def grade_documents_node(state: AgenticRAGState) -> AgenticRAGState:
    """
    æ–‡æ¡£è´¨é‡è¯„åˆ†èŠ‚ç‚¹
    """
    retrieved_docs = state.get("retrieved_documents", [])

    if not retrieved_docs:
        return {
            **state,
            "document_quality_score": 0.0,
            "quality_issues": ["No documents retrieved"],
            "workflow_stage": "document_grading"
        }

    # è¯„åˆ†æ ‡å‡†
    quality_score = 0.0
    quality_issues = []

    # 1. æ£€æŸ¥è–„å¼±ç‚¹è¦†ç›–ç‡ (40%æƒé‡)
    temporal_docs = [d for d in retrieved_docs if d["source"] == "temporal"]
    weak_point_coverage = len(temporal_docs) / max(len(retrieved_docs), 1)
    quality_score += weak_point_coverage * 0.4

    if weak_point_coverage < 0.5:
        quality_issues.append("è–„å¼±ç‚¹è¦†ç›–ç‡ä¸è¶³(<50%)")

    # 2. æ£€æŸ¥ç›¸å…³æ€§åˆ†æ•° (30%æƒé‡)
    avg_relevance = sum(d["metadata"].get("relevance_score", 0) for d in retrieved_docs) / len(retrieved_docs)
    quality_score += avg_relevance * 0.3

    if avg_relevance < 0.6:
        quality_issues.append(f"å¹³å‡ç›¸å…³æ€§åˆ†æ•°è¿‡ä½({avg_relevance:.2f})")

    # 3. æ£€æŸ¥æ•°æ®æºå¤šæ ·æ€§ (20%æƒé‡)
    sources = set(d["source"] for d in retrieved_docs)
    source_diversity = len(sources) / 3.0  # æœ€å¤š3ä¸ªæ•°æ®æº
    quality_score += source_diversity * 0.2

    if len(sources) < 2:
        quality_issues.append("æ•°æ®æºå•ä¸€(å»ºè®®è‡³å°‘2ä¸ª)")

    # 4. æ£€æŸ¥æ–‡æ¡£æ•°é‡ (10%æƒé‡)
    doc_count_score = min(len(retrieved_docs) / 20.0, 1.0)  # ç†æƒ³20ä¸ªæ–‡æ¡£
    quality_score += doc_count_score * 0.1

    if len(retrieved_docs) < 10:
        quality_issues.append(f"æ–‡æ¡£æ•°é‡ä¸è¶³({len(retrieved_docs)}<10)")

    return {
        **state,
        "document_quality_score": quality_score,
        "quality_issues": quality_issues,
        "workflow_stage": "document_grading"
    }
```

**Node 3: rewrite_query**

```python
async def rewrite_query_node(state: AgenticRAGState) -> AgenticRAGState:
    """
    æŸ¥è¯¢é‡å†™èŠ‚ç‚¹
    """
    llm = ChatAnthropic(model="claude-sonnet-4-5-20250929")

    system_prompt = """ä½ æ˜¯æŸ¥è¯¢ä¼˜åŒ–ä¸“å®¶ã€‚

å½“å‰æŸ¥è¯¢çš„é—®é¢˜:
{quality_issues}

è¯·é‡å†™æŸ¥è¯¢,æ”¹è¿›ç­–ç•¥:
1. å¦‚æœè–„å¼±ç‚¹è¦†ç›–ç‡ä¸è¶³ â†’ æ˜ç¡®è¦æ±‚temporal_behavior_query_toolä½¿ç”¨query_type='weak_points'
2. å¦‚æœç›¸å…³æ€§åˆ†æ•°ä½ â†’ ä¼˜åŒ–æŸ¥è¯¢è¯­å¥,ä½¿ç”¨æ›´ç²¾ç¡®çš„å…³é”®è¯
3. å¦‚æœæ•°æ®æºå•ä¸€ â†’ æ˜ç¡®è¦æ±‚è°ƒç”¨å¤šä¸ªå·¥å…·(è‡³å°‘2ä¸ª)
4. å¦‚æœæ–‡æ¡£æ•°é‡ä¸è¶³ â†’ å¢åŠ max_resultså‚æ•°

åŸæŸ¥è¯¢: {original_query}
"""

    response = await llm.ainvoke([{
        "role": "system",
        "content": system_prompt.format(
            quality_issues="\n".join(state["quality_issues"]),
            original_query=state.get("current_query", "")
        )
    }])

    return {
        **state,
        "current_query": response.content,
        "workflow_stage": "query_rewriting"
    }
```

**Node 4: generate_questions**

```python
async def generate_questions_node(state: AgenticRAGState) -> AgenticRAGState:
    """
    ç”Ÿæˆæ£€éªŒé—®é¢˜èŠ‚ç‚¹
    """
    llm = ChatAnthropic(model="claude-sonnet-4-5-20250929")

    # æ„å»ºæ–‡æ¡£ä¸Šä¸‹æ–‡
    docs_context = ""
    for doc in state.get("retrieved_documents", []):
        docs_context += f"""
æ¥æº: {doc['source']}
æ¦‚å¿µ: {doc['metadata'].get('concept', 'N/A')}
ç›¸å…³æ€§: {doc['metadata'].get('relevance_score', 0):.2f}
å†…å®¹: {doc['content'][:200]}...
---
"""

    system_prompt = """ä½ æ˜¯Canvaså­¦ä¹ ç³»ç»Ÿçš„æ£€éªŒé—®é¢˜ç”Ÿæˆä¸“å®¶ã€‚

åŸºäºæ£€ç´¢åˆ°çš„3å±‚è®°å¿†æ•°æ®,ç”Ÿæˆä¸ªæ€§åŒ–æ£€éªŒé—®é¢˜ã€‚

ç”Ÿæˆç­–ç•¥:
1. **70%é—®é¢˜**: æ¥è‡ªå†å²è–„å¼±ç‚¹(temporalæ•°æ®ä¸­score<80çš„æ¦‚å¿µ)
2. **30%é—®é¢˜**: æ¥è‡ªå·²æŒæ¡æ¦‚å¿µ(ç”¨äºå·©å›ºå¤ä¹ )
3. **é—®é¢˜ç±»å‹**: æ ¹æ®èŠ‚ç‚¹é¢œè‰²é€‰æ‹©
   - çº¢è‰²èŠ‚ç‚¹ â†’ çªç ´å‹/åŸºç¡€å‹é—®é¢˜(1-2ä¸ª)
   - ç´«è‰²èŠ‚ç‚¹ â†’ æ£€éªŒå‹/åº”ç”¨å‹é—®é¢˜(2-3ä¸ª)

é—®é¢˜æ ¼å¼:
{{
    "question": "é—®é¢˜æ–‡æœ¬",
    "type": "çªç ´å‹|åŸºç¡€å‹|æ£€éªŒå‹|åº”ç”¨å‹",
    "source_concept": "æ¦‚å¿µåç§°",
    "difficulty": "ç®€å•|ä¸­ç­‰|å›°éš¾",
    "data_sources": ["graphiti", "temporal"]
}}
"""

    user_message = f"""
Canvasä¸Šä¸‹æ–‡:
{json.dumps(state['canvas_context'], ensure_ascii=False, indent=2)}

æ£€ç´¢åˆ°çš„æ–‡æ¡£:
{docs_context}

è¯·ç”Ÿæˆ5-10ä¸ªæ£€éªŒé—®é¢˜,ç¡®ä¿70%æ¥è‡ªå†å²è–„å¼±ç‚¹ã€‚
"""

    response = await llm.ainvoke([
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_message}
    ])

    # è§£æJSONå“åº”
    import json
    questions = json.loads(response.content)

    return {
        **state,
        "verification_questions": questions,
        "workflow_stage": "completed"
    }
```

---

#### 10.5.3 Conditional Edge Functions

```python
def route_query(state: AgenticRAGState) -> str:
    """
    å†³ç­–: æ˜¯å¦éœ€è¦æ£€ç´¢
    """
    last_message = state["messages"][-1]

    # æ£€æŸ¥LLMæ˜¯å¦è°ƒç”¨äº†å·¥å…·
    if hasattr(last_message, "tool_calls") and last_message.tool_calls:
        return "retrieve"
    else:
        return "generate"

def grade_documents_route(state: AgenticRAGState) -> str:
    """
    å†³ç­–: åŸºäºæ–‡æ¡£è´¨é‡é€‰æ‹©ä¸‹ä¸€æ­¥
    """
    quality_score = state.get("document_quality_score", 0.0)
    retrieval_attempts = state.get("retrieval_attempts", 0)

    # è´¨é‡é˜ˆå€¼: 0.5
    if quality_score >= 0.5:
        return "generate"

    # æœ€å¤šé‡è¯•2æ¬¡
    if retrieval_attempts >= 3:
        return "generate"  # å¼ºåˆ¶ç”Ÿæˆ,é¿å…æ— é™å¾ªç¯

    return "rewrite"


def route_by_query_type(state: AgenticRAGState) -> str:
    """
    å†³ç­–: åŸºäºquery_typeè·¯ç”±åˆ°ä¸åŒçš„æŸ¥è¯¢ç”ŸæˆèŠ‚ç‚¹
    """
    query_type = state.get("query_type", "local")

    # éªŒè¯query_typeæœ‰æ•ˆæ€§
    if query_type not in ["local", "global", "hybrid"]:
        # é™çº§ç­–ç•¥
        return "local"

    return query_type
```

---

#### 10.5.4 å››å±‚æ£€ç´¢èåˆç­–ç•¥è®¾è®¡

**èåˆç›®æ ‡**: ç»¼åˆGraphitiã€LanceDBã€Temporalã€GraphRAGå››ä¸ªæ•°æ®æºçš„æ£€ç´¢ç»“æœï¼Œç”Ÿæˆé«˜è´¨é‡ã€å¤šæ ·åŒ–ã€ä¸Šä¸‹æ–‡ä¸°å¯Œçš„æ–‡æ¡£é›†åˆ

**æ ¸å¿ƒæŒ‘æˆ˜**:
1. **å¼‚æ„æ•°æ®æº**: æ¯ä¸ªæ•°æ®æºçš„relevance_scoreè®¡ç®—æ–¹å¼ä¸åŒ
2. **é‡å¤æ¦‚å¿µ**: åŒä¸€æ¦‚å¿µå¯èƒ½å‡ºç°åœ¨å¤šä¸ªæ•°æ®æºä¸­
3. **æƒé‡å¹³è¡¡**: å¦‚ä½•å¹³è¡¡å±€éƒ¨ç²¾ç¡®æ€§(Graphiti/LanceDB)å’Œå…¨å±€è§†è§’(GraphRAG)
4. **æ€§èƒ½çº¦æŸ**: èåˆç®—æ³•ä¸èƒ½æˆä¸ºæ€§èƒ½ç“¶é¢ˆ

**èåˆç­–ç•¥å¯¹æ¯”**:

| ç­–ç•¥ | ç®—æ³•å¤æ‚åº¦ | å‡†ç¡®æ€§ | é€Ÿåº¦ | é€‚ç”¨åœºæ™¯ |
|------|----------|--------|------|---------|
| **RRF** (Reciprocal Rank Fusion) | O(N log N) | â­â­â­â­ | â­â­â­â­â­ | æ£€éªŒç™½æ¿ç”Ÿæˆï¼ˆæ¨èï¼‰ |
| **Linear Combination** | O(N) | â­â­â­ | â­â­â­â­â­ | éœ€è¦è°ƒæ•´æƒé‡æ—¶ |
| **CrossEncoder** | O(NÂ²) | â­â­â­â­â­ | â­â­ | æœ€é«˜å‡†ç¡®æ€§è¦æ±‚ |
| **MMR** (Maximum Marginal Relevance) | O(NÂ²) | â­â­â­â­ | â­â­â­ | éœ€è¦å¤šæ ·æ€§æ—¶ |

**æ¨èç­–ç•¥: RRF (Reciprocal Rank Fusion)**

**RRFä¼˜åŠ¿**:
1. **ä¸åˆ†æ•°æ— å…³**: ä¸ä¾èµ–å„æ•°æ®æºçš„relevance_score scale
2. **è‡ªåŠ¨å¹³è¡¡**: è‡ªç„¶åœ°å¹³è¡¡å„æ•°æ®æºçš„è´¡çŒ®
3. **æ€§èƒ½ä¼˜å¼‚**: O(N log N)ï¼Œé€‚åˆå®æ—¶åœºæ™¯
4. **å®æˆ˜éªŒè¯**: LanceDBã€Graphitiå®˜æ–¹æ¨è

**RRFå…¬å¼**:

```
RRF_score(doc) = Î£_{source âˆˆ {Graphiti, LanceDB, Temporal, GraphRAG}} 1 / (k + rank_source(doc))

å…¶ä¸­ï¼š
- k: å¸¸æ•°ï¼Œé€šå¸¸å–60ï¼ˆæ¨èå€¼ï¼‰
- rank_source(doc): æ–‡æ¡£åœ¨è¯¥æ•°æ®æºä¸­çš„æ’åï¼ˆ1-indexedï¼‰
```

**RRFå®ç°ç»†èŠ‚**:

```python
def rrf_fusion_detailed(docs: list[dict], k: int = 60) -> list[dict]:
    """
    å››å±‚æ£€ç´¢èåˆçš„å®Œæ•´RRFå®ç°

    Step 1: æŒ‰sourceåˆ†ç»„
    Step 2: è®¡ç®—æ¯ä¸ªsourceå†…çš„æ’å
    Step 3: è®¡ç®—æ¯ä¸ªæ–‡æ¡£çš„RRFåˆ†æ•°
    Step 4: å»é‡ï¼ˆä¿ç•™æœ€é«˜åˆ†ï¼‰
    Step 5: é‡æ–°æ’åº
    """

    # Step 1: æŒ‰sourceåˆ†ç»„
    docs_by_source = {
        "graphiti": [d for d in docs if d["source"] == "graphiti"],
        "lancedb": [d for d in docs if d["source"] == "lancedb"],
        "temporal": [d for d in docs if d["source"] == "temporal"],
        "graphrag": [d for d in docs if d["source"] == "graphrag"]
    }

    # Step 2: ä¸ºæ¯ä¸ªsourceå†…çš„æ–‡æ¡£è®¡ç®—æ’å
    for source, source_docs in docs_by_source.items():
        # æŒ‰åŸå§‹relevance_scoreæ’åº
        sorted_docs = sorted(
            source_docs,
            key=lambda d: d["metadata"].get("relevance_score", 0),
            reverse=True
        )

        # åˆ†é…æ’å
        for rank, doc in enumerate(sorted_docs, start=1):
            doc[f"rank_{source}"] = rank

    # Step 3: è®¡ç®—RRFåˆ†æ•°
    rrf_scores = defaultdict(float)
    concept_to_docs = defaultdict(list)

    for doc in docs:
        concept = doc["metadata"].get("concept", "")
        source = doc["source"]

        # è·å–è¯¥æ–‡æ¡£åœ¨å…¶æºä¸­çš„æ’å
        rank = doc.get(f"rank_{source}", 999)

        # ç´¯åŠ RRFåˆ†æ•°
        doc_id = f"{concept}_{source}"
        rrf_scores[doc_id] = 1.0 / (k + rank)

        # ä¿å­˜æ–‡æ¡£å¼•ç”¨
        concept_to_docs[concept].append({
            "doc": doc,
            "rrf_score": rrf_scores[doc_id]
        })

    # Step 4: å»é‡ - å¯¹æ¯ä¸ªconceptï¼Œä¿ç•™RRFåˆ†æ•°æœ€é«˜çš„æ–‡æ¡£
    deduplicated_docs = []
    for concept, doc_entries in concept_to_docs.items():
        # æŒ‰RRFåˆ†æ•°æ’åº
        best_entry = max(doc_entries, key=lambda e: e["rrf_score"])
        best_doc = best_entry["doc"]
        best_doc["rrf_score"] = best_entry["rrf_score"]

        # åˆå¹¶æ‰€æœ‰æ¥æºçš„è¯æ®
        all_sources = [e["doc"]["source"] for e in doc_entries]
        best_doc["metadata"]["contributing_sources"] = all_sources
        best_doc["metadata"]["source_count"] = len(set(all_sources))

        deduplicated_docs.append(best_doc)

    # Step 5: æŒ‰RRFåˆ†æ•°é‡æ–°æ’åº
    final_docs = sorted(deduplicated_docs, key=lambda d: d["rrf_score"], reverse=True)

    return final_docs
```

**èåˆç­–ç•¥é€‰æ‹©å†³ç­–æ ‘**:

```
ç”¨æˆ·åœºæ™¯åˆ¤æ–­
    â”‚
    â”œâ”€ é»˜è®¤åœºæ™¯ï¼ˆæ£€éªŒç™½æ¿ç”Ÿæˆï¼‰
    â”‚   â””â”€ ä½¿ç”¨RRFï¼ˆk=60ï¼‰
    â”‚
    â”œâ”€ éœ€è¦è°ƒæ•´æ•°æ®æºæƒé‡
    â”‚   â””â”€ ä½¿ç”¨Linear Combination
    â”‚       - Graphiti: 0.3 (å›¾å…³ç³»)
    â”‚       - LanceDB: 0.25 (è¯­ä¹‰æ£€ç´¢)
    â”‚       - Temporal: 0.2 (å†å²è–„å¼±ç‚¹)
    â”‚       - GraphRAG: 0.25 (å…¨å±€è§†è§’)
    â”‚
    â”œâ”€ æœ€é«˜å‡†ç¡®æ€§è¦æ±‚ï¼ˆé‡è¦æ¦‚å¿µï¼‰
    â”‚   â””â”€ ä½¿ç”¨CrossEncoder
    â”‚       - æ¨¡å‹: cross-encoder/ms-marco-MiniLM-L-6-v2
    â”‚       - å»¶è¿Ÿ: ~500ms for 50 docs
    â”‚
    â””â”€ éœ€è¦å¤šæ ·æ€§ï¼ˆé¿å…é‡å¤ï¼‰
        â””â”€ ä½¿ç”¨MMR
            - Î»: 0.5ï¼ˆç›¸å…³æ€§å’Œå¤šæ ·æ€§å¹³è¡¡ï¼‰
```

**æ€§èƒ½ä¼˜åŒ–**:

1. **å¹¶è¡ŒåŒ–**:
   ```python
   # åœ¨composite_search_nodeä¸­å·²å®ç°
   results = await asyncio.gather(
       graphiti_task(), lancedb_task(), temporal_task(), graphrag_task()
   )
   ```

2. **Early Stopping**:
   ```python
   # å¦‚æœå‰20ä¸ªæ–‡æ¡£RRFåˆ†æ•°å·®è·<0.01ï¼Œåœæ­¢æ’åº
   if len(final_docs) > 20:
       if final_docs[19]["rrf_score"] - final_docs[20]["rrf_score"] < 0.01:
           return final_docs[:20]
   ```

3. **ç¼“å­˜**:
   ```python
   # ç¼“å­˜åŒä¸€æŸ¥è¯¢çš„èåˆç»“æœï¼ˆ5åˆ†é’Ÿæœ‰æ•ˆæœŸï¼‰
   from functools import lru_cache
   from hashlib import md5

   @lru_cache(maxsize=100)
   def cached_rrf_fusion(query_hash: str, docs_json: str):
       docs = json.loads(docs_json)
       return rrf_fusion(docs)
   ```

**è´¨é‡è¯„ä¼°æŒ‡æ ‡**:

```python
def evaluate_fusion_quality(fused_docs: list[dict], canvas_context: dict) -> dict:
    """
    è¯„ä¼°èåˆç»“æœçš„è´¨é‡

    è¿”å›:
    {
        "diversity_score": 0.0-1.0,  # æ¦‚å¿µå¤šæ ·æ€§
        "source_coverage": 0.0-1.0,  # æ•°æ®æºè¦†ç›–ç‡
        "relevance_avg": 0.0-1.0,    # å¹³å‡ç›¸å…³æ€§
        "weak_point_coverage": 0.0-1.0  # è–„å¼±ç‚¹è¦†ç›–ç‡
    }
    """

    # 1. å¤šæ ·æ€§è¯„åˆ†ï¼šå”¯ä¸€æ¦‚å¿µæ•° / æ€»æ–‡æ¡£æ•°
    unique_concepts = len(set(d["metadata"]["concept"] for d in fused_docs))
    diversity_score = unique_concepts / max(len(fused_docs), 1)

    # 2. æ•°æ®æºè¦†ç›–ç‡ï¼šä½¿ç”¨çš„æ•°æ®æºæ•° / 4
    sources_used = set(d["source"] for d in fused_docs)
    source_coverage = len(sources_used) / 4.0

    # 3. å¹³å‡ç›¸å…³æ€§
    relevance_avg = sum(d.get("rrf_score", 0) for d in fused_docs) / max(len(fused_docs), 1)

    # 4. è–„å¼±ç‚¹è¦†ç›–ç‡ï¼štemporalæ–‡æ¡£æ•° / çº¢ç´«èŠ‚ç‚¹æ•°
    temporal_count = len([d for d in fused_docs if d["source"] == "temporal"])
    red_purple_count = len(canvas_context.get("nodes", []))
    weak_point_coverage = min(1.0, temporal_count / max(red_purple_count, 1))

    return {
        "diversity_score": diversity_score,
        "source_coverage": source_coverage,
        "relevance_avg": relevance_avg,
        "weak_point_coverage": weak_point_coverage,
        "overall_quality": (diversity_score + source_coverage + relevance_avg + weak_point_coverage) / 4.0
    }
```

---

### 10.6 æ£€éªŒç™½æ¿ç”Ÿæˆå®Œæ•´å·¥ä½œæµ

#### 10.6.1 ç«¯åˆ°ç«¯æµç¨‹

```python
async def generate_review_canvas_with_agentic_rag(
    canvas_path: str,
    config: dict
) -> dict:
    """
    ä½¿ç”¨Agentic RAGç”Ÿæˆæ£€éªŒç™½æ¿

    Args:
        canvas_path: Canvasæ–‡ä»¶è·¯å¾„
        config: LangGraph config(thread_id, user_idç­‰)

    Returns:
        æ£€éªŒç™½æ¿ç”Ÿæˆç»“æœ
    """
    # Step 1: è¯»å–Canvas,æå–çº¢è‰²/ç´«è‰²èŠ‚ç‚¹
    canvas_data = read_canvas(canvas_path)
    red_purple_nodes = [
        node for node in canvas_data.get("nodes", [])
        if node.get("color") in ["1", "3"]  # çº¢è‰²æˆ–ç´«è‰²
    ]

    # Step 2: æå–ç”¨æˆ·ç†è§£(é»„è‰²èŠ‚ç‚¹)
    yellow_nodes = [
        node for node in canvas_data.get("nodes", [])
        if node.get("color") == "6"
    ]
    user_understanding = "\n\n".join([
        f"{node.get('text', '')}" for node in yellow_nodes
    ])

    # Step 3: æ„å»ºCanvasä¸Šä¸‹æ–‡
    canvas_context = {
        "canvas_file": canvas_path,
        "topic": Path(canvas_path).stem,
        "topic_node_id": canvas_data.get("topic_node_id", None),
        "red_nodes": [n for n in red_purple_nodes if n["color"] == "1"],
        "purple_nodes": [n for n in red_purple_nodes if n["color"] == "3"],
        "total_nodes": len(canvas_data.get("nodes", []))
    }

    # Step 4: åˆå§‹åŒ–Agentic RAG State
    initial_state = {
        "canvas_file": canvas_path,
        "canvas_context": canvas_context,
        "user_understanding": user_understanding,
        "messages": [],
        "current_query": "",
        "retrieval_attempts": 0,
        "retrieved_documents": [],
        "document_quality_score": 0.0,
        "quality_issues": [],
        "verification_questions": [],
        "workflow_stage": "query_generation",
        "error_log": []
    }

    # Step 5: è°ƒç”¨Agentic RAG Graph
    try:
        result = await agentic_rag_graph.ainvoke(
            initial_state,
            config=config
        )
    except Exception as e:
        logger.error(f"Agentic RAG execution failed: {e}")
        raise

    # Step 6: ç”Ÿæˆæ£€éªŒç™½æ¿Canvasæ–‡ä»¶
    review_canvas_path = generate_review_canvas_file(
        original_canvas_path=canvas_path,
        questions=result["verification_questions"],
        metadata={
            "document_quality_score": result["document_quality_score"],
            "retrieval_attempts": result["retrieval_attempts"],
            "data_sources": list(set(
                source for q in result["verification_questions"]
                for source in q.get("data_sources", [])
            ))
        }
    )

    return {
        "review_canvas_path": review_canvas_path,
        "questions_count": len(result["verification_questions"]),
        "document_quality_score": result["document_quality_score"],
        "retrieval_attempts": result["retrieval_attempts"],
        "workflow_stage": result["workflow_stage"]
    }
```

---

### 10.7 æ€§èƒ½æŒ‡æ ‡ä¸ä¼˜åŒ–

#### 10.7.1 æ€§èƒ½ç›®æ ‡ï¼ˆå«GraphRAGï¼‰

**æ ¸å¿ƒæŒ‡æ ‡**:

| æŒ‡æ ‡ | Localè·¯å¾„ | Globalè·¯å¾„ | Hybridè·¯å¾„ | æµ‹é‡æ–¹å¼ |
|------|----------|-----------|-----------|---------|
| **ç«¯åˆ°ç«¯å»¶è¿Ÿ** | <5ç§’ | <8ç§’ | <12ç§’ | ä»Canvasè¯»å–åˆ°æ£€éªŒç™½æ¿ç”Ÿæˆå®Œæˆ |
| **é—®é¢˜è·¯ç”±å»¶è¿Ÿ** | <500ms | <500ms | <500ms | question_routerèŠ‚ç‚¹æ‰§è¡Œæ—¶é—´ |
| **å·¥å…·è°ƒç”¨å»¶è¿Ÿ** | <1ç§’/å·¥å…· | 2-5ç§’ | <10ç§’ï¼ˆå¹¶è¡Œï¼‰ | retrieval toolsæ‰§è¡Œæ—¶é—´ |
| **èåˆé‡æ’åºå»¶è¿Ÿ** | N/A | N/A | <500ms | fusion_rerankèŠ‚ç‚¹æ‰§è¡Œæ—¶é—´ |
| **æ–‡æ¡£è¯„åˆ†å»¶è¿Ÿ** | <500ms | <500ms | <800ms | grade_documentsèŠ‚ç‚¹æ‰§è¡Œæ—¶é—´ |
| **æŸ¥è¯¢é‡å†™å»¶è¿Ÿ** | <2ç§’ | <2ç§’ | <2ç§’ | rewrite_queryèŠ‚ç‚¹æ‰§è¡Œæ—¶é—´ |
| **æ£€éªŒé—®é¢˜ç”Ÿæˆ** | <3ç§’ | <3ç§’ | <3ç§’ | generate_questionsèŠ‚ç‚¹æ‰§è¡Œæ—¶é—´ |

**åˆ†è·¯å¾„æ€§èƒ½åˆ†æ**:

**Localè·¯å¾„ (Graphiti + LanceDB + Temporal)**:
- æŸ¥è¯¢è·¯ç”±: ~300ms
- å·¥å…·å¹¶è¡Œè°ƒç”¨: ~1ç§’ï¼ˆ3ä¸ªå·¥å…·ï¼‰
  - Graphiti: ~400ms
  - LanceDB: ~300ms
  - Temporal: ~200ms
- æ–‡æ¡£è¯„åˆ†: ~300ms
- é—®é¢˜ç”Ÿæˆ: ~2ç§’
- **æ€»å»¶è¿Ÿ**: ~4ç§’

**Globalè·¯å¾„ (GraphRAG Global Search)**:
- æŸ¥è¯¢è·¯ç”±: ~300ms
- GraphRAGæœç´¢: 2-5ç§’ï¼ˆLevel 1-2ï¼‰æˆ–5-10ç§’ï¼ˆLevel 3ï¼‰
  - ç¤¾åŒºåŠ è½½: ~500ms
  - å…¨å±€æ¨ç†: 1.5-4ç§’ï¼ˆGPT-4oï¼‰
  - ç»“æœæ ¼å¼åŒ–: ~200ms
- æ–‡æ¡£è¯„åˆ†: ~300ms
- é—®é¢˜ç”Ÿæˆ: ~2ç§’
- **æ€»å»¶è¿Ÿ**: ~5-8ç§’ï¼ˆLevel 2ï¼‰ï¼Œ~8-13ç§’ï¼ˆLevel 3ï¼‰

**Hybridè·¯å¾„ (4å±‚å¹¶è¡Œæ£€ç´¢ + Fusion Reranking)**:
- æŸ¥è¯¢è·¯ç”±: ~300ms
- å·¥å…·å¹¶è¡Œè°ƒç”¨: ~5ç§’ï¼ˆ4ä¸ªå·¥å…·ï¼Œæœ€æ…¢å·¥å…·å†³å®šï¼‰
  - Graphiti: ~400ms
  - LanceDB: ~300ms
  - Temporal: ~200ms
  - GraphRAG: ~5ç§’ï¼ˆLevel 2ï¼‰
- Fusioné‡æ’åº: ~400msï¼ˆRRFç®—æ³•ï¼Œå¤„ç†50-80ä¸ªæ–‡æ¡£ï¼‰
- æ–‡æ¡£è¯„åˆ†: ~500ms
- é—®é¢˜ç”Ÿæˆ: ~2ç§’
- **æ€»å»¶è¿Ÿ**: ~8-12ç§’

**æ€§èƒ½ç“¶é¢ˆåˆ†æ**:

| ç»„ä»¶ | å»¶è¿Ÿå æ¯” | ä¼˜åŒ–ä¼˜å…ˆçº§ | ä¼˜åŒ–ç­–ç•¥ |
|------|---------|----------|---------|
| **GraphRAG Global Search** | 50-60% (Hybridè·¯å¾„) | ğŸ”´ é«˜ | 1. é™ä½community_levelï¼ˆ2â†’1ï¼‰<br>2. å‡å°‘max_data_tokensï¼ˆ12Kâ†’8Kï¼‰<br>3. åˆ‡æ¢åˆ°gpt-4o-mini |
| **é—®é¢˜ç”Ÿæˆ** | 20-25% | ğŸŸ¡ ä¸­ | 1. ä½¿ç”¨claude-3-haiku<br>2. å‡å°‘ç”Ÿæˆé—®é¢˜æ•°é‡ |
| **Fusion Reranking** | 3-5% | ğŸŸ¢ ä½ | å·²ä¼˜åŒ–ï¼ˆRRFç®—æ³•ï¼ŒO(N log N)ï¼‰ |
| **æ–‡æ¡£è¯„åˆ†** | 3-5% | ğŸŸ¢ ä½ | å·²ä¼˜åŒ– |

**GraphRAGæ€§èƒ½ä¼˜åŒ–å»ºè®®**:

```python
# æ–¹æ¡ˆ1: é™ä½community_levelï¼ˆæ¨èï¼‰
# Level 2 â†’ Level 1: 5ç§’ â†’ 2.5ç§’ï¼ˆ50%å‡å°‘ï¼‰
result = await graphrag_global_search_tool(
    query=query,
    community_level=1,  # ä»2é™åˆ°1
    max_data_tokens=12000
)

# æ–¹æ¡ˆ2: å‡å°‘ä¸Šä¸‹æ–‡tokenæ•°
# 12K â†’ 8K tokens: 5ç§’ â†’ 3.5ç§’ï¼ˆ30%å‡å°‘ï¼‰
result = await graphrag_global_search_tool(
    query=query,
    community_level=2,
    max_data_tokens=8000  # ä»12Ké™åˆ°8K
)

# æ–¹æ¡ˆ3: åˆ‡æ¢åˆ°æ›´å¿«çš„LLM
# gpt-4o â†’ gpt-4o-mini: 5ç§’ â†’ 2ç§’ï¼ˆ60%å‡å°‘ï¼Œä½†å‡†ç¡®æ€§ä¸‹é™~10%ï¼‰
searcher = GlobalSearch(
    llm=ChatOpenAI(model="gpt-4o-mini", temperature=0.2),  # æ›´å¿«ä½†ç¨å¼±
    context_builder=context_builder
)
```

**Tokenæ¶ˆè€—ä¼°ç®—** (æ›´æ–°ç‰ˆæœ¬ - å«æœ¬åœ°æ¨¡å‹æˆæœ¬ä¼˜åŒ–):

| è·¯å¾„ | Input Tokens | Output Tokens | æ€»Tokens | æˆæœ¬ï¼ˆåŸè®¾è®¡ gpt-4oï¼‰ | æˆæœ¬ï¼ˆä¼˜åŒ–å Qwen2.5 90% + gpt-4o-mini 10%ï¼‰ |
|------|-------------|--------------|---------|-------------------|--------------------------------|
| **Local** | ~1,500 | ~800 | ~2,300 | $0.02 | **$0.002** (â†“90%) |
| **Global (Level 1)** | ~5,000 | ~1,200 | ~6,200 | $0.06 | **$0.006** (â†“90%) |
| **Global (Level 2)** | ~10,000 | ~1,500 | ~11,500 | $0.11 | **$0.011** (â†“90%) |
| **Global (Level 3)** | ~18,000 | ~2,000 | ~20,000 | $0.19 | **$0.019** (â†“90%) |
| **Hybrid** | ~12,000 | ~1,500 | ~13,500 | $0.13 | **$0.013** (â†“90%) |

**æœˆåº¦æˆæœ¬ä¼°ç®—** (å‡è®¾3000æ¬¡æŸ¥è¯¢/æœˆ):

| æŸ¥è¯¢è·¯å¾„åˆ†å¸ƒ | åŸè®¾è®¡æœˆæˆæœ¬ | ä¼˜åŒ–åæœˆæˆæœ¬ | æˆæœ¬èŠ‚çœ |
|------------|------------|------------|---------|
| **50% Local + 30% Global(L2) + 20% Hybrid** | $570 | **$57** | $513 (90%) |
| **70% Local + 20% Global(L1) + 10% Global(L2)** | $330 | **$33** | $297 (90%) |
| **100% Global (Level 3)** | $570 | **$57** | $513 (90%) |

**æ¨èé…ç½®**ï¼ˆå¹³è¡¡æ€§èƒ½å’Œæˆæœ¬ï¼‰:
- **é»˜è®¤ç­–ç•¥**: æ··åˆè·¯å¾„ï¼ˆ50% Local + 30% Global L2 + 20% Hybridï¼‰ï¼Œæœˆæˆæœ¬**$57**
- **æ¿€è¿›ä¼˜åŒ–**: ä¼˜å…ˆLocalè·¯å¾„ï¼ˆ70% Localï¼‰ï¼Œæœˆæˆæœ¬**$33**
- **LLMé…ç½®**: 90% Qwen2.5-14Bï¼ˆæœ¬åœ°ï¼‰+ 10% gpt-4o-miniï¼ˆAPIï¼‰
- **æˆæœ¬ä¸Šé™**: æœˆé¢„ç®—$80ï¼Œè¶…è¿‡è‡ªåŠ¨åˆ‡æ¢ä¸º100%æœ¬åœ°æ¨¡å¼
- **è´¨é‡ä¿è¯**: Qwen2.5è´¨é‡â‰¥85%ï¼Œæ··åˆç­–ç•¥ç»¼åˆè´¨é‡86%

**æˆæœ¬ä¼˜åŒ–ROIåˆ†æ**:
- **ç¡¬ä»¶æŠ•èµ„**: RTX 4090 24GB VRAMï¼ˆ$1600ï¼‰
- **æœˆåº¦èŠ‚çœ**: $513/æœˆï¼ˆåŸ$570 â†’ ä¼˜åŒ–$57ï¼‰
- **å›æœ¬å‘¨æœŸ**: 3.1ä¸ªæœˆï¼ˆ$1600 Ã· $513ï¼‰
- **å¹´åº¦èŠ‚çœ**: $6156ï¼ˆç¬¬ä¸€å¹´æ‰£é™¤ç¡¬ä»¶æˆæœ¬å‡€èŠ‚çœ$4556ï¼‰

#### 10.7.2 æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

**1. å¹¶å‘å·¥å…·è°ƒç”¨**

```python
# âŒ ä½æ•ˆ: ä¸²è¡Œè°ƒç”¨3ä¸ªå·¥å…·
graphiti_results = await graphiti_hybrid_search_tool(query, canvas_context)
lancedb_results = await lancedb_hybrid_search_tool(query)
temporal_results = await temporal_behavior_query_tool(canvas_file)

# âœ… é«˜æ•ˆ: å¹¶å‘è°ƒç”¨3ä¸ªå·¥å…·
import asyncio

results = await asyncio.gather(
    graphiti_hybrid_search_tool(query, canvas_context),
    lancedb_hybrid_search_tool(query),
    temporal_behavior_query_tool(canvas_file)
)

graphiti_results, lancedb_results, temporal_results = results
```

**æ€§èƒ½æå‡**: 3ç§’ â†’ 1ç§’ (3å€æå‡)

**2. ç»“æœç¼“å­˜**

```python
from functools import lru_cache
from cachetools import TTLCache
import hashlib

# L1ç¼“å­˜: å†…å­˜ç¼“å­˜(5åˆ†é’ŸTTL)
retrieval_cache = TTLCache(maxsize=100, ttl=300)

async def cached_graphiti_search(query: str, canvas_context: dict):
    """å¸¦ç¼“å­˜çš„Graphitiæ£€ç´¢"""
    cache_key = hashlib.md5(
        f"{query}_{canvas_context['canvas_file']}".encode()
    ).hexdigest()

    if cache_key in retrieval_cache:
        logger.debug(f"Cache hit: {cache_key}")
        return retrieval_cache[cache_key]

    results = await graphiti_hybrid_search_tool(query, canvas_context)
    retrieval_cache[cache_key] = results
    return results
```

**æ€§èƒ½æå‡**: é‡å¤æŸ¥è¯¢ 1ç§’ â†’ <10ms (100å€æå‡)

**3. æŸ¥è¯¢é‡å†™é™åˆ¶**

```python
# é™åˆ¶æœ€å¤§é‡è¯•æ¬¡æ•°
MAX_RETRIEVAL_ATTEMPTS = 3

def grade_documents_route(state: AgenticRAGState) -> str:
    quality_score = state.get("document_quality_score", 0.0)
    attempts = state.get("retrieval_attempts", 0)

    # å¼ºåˆ¶ç»ˆæ­¢æ¡ä»¶
    if attempts >= MAX_RETRIEVAL_ATTEMPTS:
        logger.warning(f"Max retrieval attempts reached: {attempts}")
        return "generate"

    if quality_score >= 0.5:
        return "generate"

    return "rewrite"
```

**é¿å…**: æ— é™å¾ªç¯å¯¼è‡´è¶…æ—¶

---

#### 10.7.3 æœ¬åœ°æ¨¡å‹éƒ¨ç½²æŒ‡å—

**ç›®æ ‡**: éƒ¨ç½²Qwen2.5-14B-Instructæœ¬åœ°æ¨¡å‹ï¼Œå®ç°90%æˆæœ¬èŠ‚çœï¼ˆ$570 â†’ $57/æœˆï¼‰

**ç¡¬ä»¶è¦æ±‚**:

| ç»„ä»¶ | æ¨èé…ç½® | æœ€ä½é…ç½® | è¯´æ˜ |
|------|---------|---------|------|
| **GPU** | RTX 4090 24GB VRAM | RTX 3090 24GB | 14Bæ¨¡å‹éœ€è¦~18GB VRAMï¼ˆFP16ï¼‰ |
| **CPU** | Intel i7-12700K / AMD Ryzen 7 5800X | ä»»æ„8æ ¸+ | æ— GPUæ—¶ä½¿ç”¨CPUæ¨ç†ï¼ˆæ…¢10å€ï¼‰ |
| **å†…å­˜** | 32GB DDR4 | 16GB | OllamaæœåŠ¡ + æ“ä½œç³»ç»Ÿ + ç¼“å­˜ |
| **å­˜å‚¨** | 100GB SSD | 50GB HDD | Qwen2.5-14Bæ¨¡å‹æ–‡ä»¶~8GB + ç´¢å¼•æ•°æ® |
| **ç½‘ç»œ** | æ— éœ€å¤–ç½‘ï¼ˆæœ¬åœ°æ¨ç†ï¼‰ | å¯é€‰å¤–ç½‘ | APIé™çº§éœ€è¦å¤–ç½‘è¿æ¥OpenAI |

**è½¯ä»¶ä¾èµ–**:

```bash
# 1. å®‰è£…Ollamaï¼ˆè·¨å¹³å°ï¼‰
# Linux/Mac:
curl -fsSL https://ollama.com/install.sh | sh

# Windows:
# ä¸‹è½½å¹¶å®‰è£… https://ollama.com/download/windows

# 2. éªŒè¯Ollamaå®‰è£…
ollama --version
# é¢„æœŸè¾“å‡º: ollama version 0.1.x

# 3. æ‹‰å–Qwen2.5-14B-Instructæ¨¡å‹
ollama pull qwen2.5:14b-instruct
# ä¸‹è½½å¤§å°: ~8GB
# é¢„æœŸæ—¶é—´: 10-30åˆ†é’Ÿï¼ˆå–å†³äºç½‘é€Ÿï¼‰

# 4. éªŒè¯æ¨¡å‹å¯ç”¨
ollama run qwen2.5:14b-instruct "Hello, how are you?"
# é¢„æœŸè¾“å‡º: æ¨¡å‹ç”Ÿæˆçš„è‡ªç„¶è¯­è¨€å“åº”
```

**OllamaæœåŠ¡é…ç½®**:

```bash
# é…ç½®ç¯å¢ƒå˜é‡ï¼ˆLinux/Mac: ~/.bashrc æˆ– ~/.zshrcï¼‰
export OLLAMA_HOST=0.0.0.0:11434           # å…è®¸å±€åŸŸç½‘è®¿é—®
export OLLAMA_MODELS=/data/ollama_models  # è‡ªå®šä¹‰æ¨¡å‹å­˜å‚¨è·¯å¾„ï¼ˆå¯é€‰ï¼‰
export OLLAMA_NUM_PARALLEL=3               # å¹¶å‘è¯·æ±‚æ•°ï¼ˆé»˜è®¤1ï¼‰
export OLLAMA_MAX_LOADED_MODELS=2          # åŒæ—¶åŠ è½½çš„æ¨¡å‹æ•°ï¼ˆé»˜è®¤1ï¼‰

# Windows PowerShell:
$env:OLLAMA_HOST = "0.0.0.0:11434"

# å¯åŠ¨OllamaæœåŠ¡
ollama serve
# æœåŠ¡åœ°å€: http://localhost:11434
```

**æ€§èƒ½åŸºå‡†æµ‹è¯•**:

```python
# âœ… Verified from Story GraphRAG.2 (æ€§èƒ½æµ‹è¯•)
import time
from langchain_community.llms import Ollama

# åˆå§‹åŒ–æœ¬åœ°æ¨¡å‹
local_llm = Ollama(
    model="qwen2.5:14b-instruct",
    base_url="http://localhost:11434",
    temperature=0.2
)

# æµ‹è¯•1: å•æ¬¡æ¨ç†å»¶è¿Ÿ
test_prompt = """
ä»ä»¥ä¸‹Canvaså­¦ä¹ å†…å®¹ä¸­æå–å…³é”®å®ä½“å’Œå…³ç³»ï¼š
æ¦‚å¿µï¼šé€†å¦å‘½é¢˜ (Contrapositive)
å®šä¹‰ï¼šå‘½é¢˜"è‹¥påˆ™q"çš„é€†å¦å‘½é¢˜æ˜¯"è‹¥éqåˆ™ép"
æ€§è´¨ï¼šåŸå‘½é¢˜ä¸é€†å¦å‘½é¢˜ç­‰ä»·
"""

start = time.time()
response = local_llm.invoke(test_prompt)
latency = time.time() - start

print(f"æ¨ç†å»¶è¿Ÿ: {latency:.2f}ç§’")
# RTX 4090é¢„æœŸ: 3-5ç§’
# RTX 3090é¢„æœŸ: 5-8ç§’
# CPUæ¨ç†é¢„æœŸ: 30-60ç§’

# æµ‹è¯•2: æ‰¹é‡æ¨ç†ï¼ˆ10æ¬¡ï¼‰
latencies = []
for i in range(10):
    start = time.time()
    response = local_llm.invoke(test_prompt)
    latencies.append(time.time() - start)

p50 = sorted(latencies)[int(len(latencies) * 0.5)]
p95 = sorted(latencies)[int(len(latencies) * 0.95)]

print(f"P50å»¶è¿Ÿ: {p50:.2f}ç§’, P95å»¶è¿Ÿ: {p95:.2f}ç§’")
# RTX 4090é¢„æœŸ: P50=3.5ç§’, P95=5.2ç§’
```

**è´¨é‡éªŒè¯**:

```python
# âœ… Verified from Story GraphRAG.2 AC 5ï¼ˆè´¨é‡â‰¥85%ï¼‰
from graphrag.evaluation.entity_extraction_evaluator import EntityExtractionEvaluator

# Step 1: å‡†å¤‡æµ‹è¯•æ•°æ®é›†ï¼ˆ100ä¸ªCanvaså†…å®¹æ ·æœ¬ï¼‰
test_dataset = load_canvas_samples(count=100)

# Step 2: ä½¿ç”¨Qwen2.5æå–å®ä½“
qwen_results = []
for sample in test_dataset:
    entities = await extract_entities_with_qwen(sample)
    qwen_results.append(entities)

# Step 3: ä½¿ç”¨gpt-4oæå–å®ä½“ï¼ˆä½œä¸ºåŸºçº¿ï¼‰
gpt4o_results = []
for sample in test_dataset:
    entities = await extract_entities_with_gpt4o(sample)
    gpt4o_results.append(entities)

# Step 4: è®¡ç®—F1 Score
evaluator = EntityExtractionEvaluator()
f1_score = evaluator.compare(
    predictions=qwen_results,
    ground_truth=gpt4o_results
)

print(f"Qwen2.5è´¨é‡: {f1_score * 100:.1f}%ï¼ˆåŸºçº¿: gpt-4o 100%ï¼‰")
# éªŒæ”¶æ ‡å‡†: F1 Score â‰¥ 0.85 (85%)

# Step 5: åˆ†æè´¨é‡å·®è·
if f1_score < 0.85:
    quality_report = evaluator.generate_error_analysis()
    print(f"è´¨é‡é—®é¢˜åˆ†æï¼š\n{quality_report}")
    # å¸¸è§é—®é¢˜ï¼š
    # - å®ä½“è¾¹ç•Œè¯†åˆ«ä¸å‡†ç¡®ï¼ˆ+Few-shot exampleså¯æ”¹å–„ï¼‰
    # - å…³ç³»ç±»å‹åˆ†ç±»é”™è¯¯ï¼ˆ+Fine-tuningå¯æ”¹å–„ï¼‰
    # - ä¸­æ–‡ä¸“ä¸šæœ¯è¯­ç†è§£åå·®ï¼ˆ+é¢†åŸŸè¯å…¸å¯æ”¹å–„ï¼‰
```

**æˆæœ¬ç›‘æ§é›†æˆ**:

```python
# âœ… Verified from config/graphrag_cost_control.json
import json
from graphrag.cost_tracker import CostTracker

# åŠ è½½æˆæœ¬æ§åˆ¶é…ç½®
with open("config/graphrag_cost_control.json", "r") as f:
    config = json.load(f)

# åˆå§‹åŒ–æˆæœ¬è¿½è¸ªå™¨
cost_tracker = CostTracker(
    budget_limit=config["cost_monitoring"]["budget"]["monthly_limit_usd"],
    warning_threshold=config["cost_monitoring"]["budget"]["warning_threshold_percent"],
    critical_threshold=config["cost_monitoring"]["budget"]["critical_threshold_percent"]
)

# é›†æˆåˆ°HybridLLMProvider
hybrid_llm = HybridLLMProvider(
    local_ratio=config["llm_providers"]["hybrid"]["local_ratio"],
    cost_tracker=cost_tracker
)

# å®šæœŸæ£€æŸ¥æˆæœ¬çŠ¶æ€
monthly_cost = cost_tracker.get_monthly_cost()
if monthly_cost >= config["cost_monitoring"]["alerts"]["critical"]["threshold_usd"]:
    # æˆæœ¬è¶…æ ‡ï¼šåˆ‡æ¢ä¸º100%æœ¬åœ°æ¨¡å¼
    hybrid_llm.local_ratio = 1.0
    send_alert(
        recipients=config["cost_monitoring"]["alerts"]["critical"]["recipients"],
        message=f"GraphRAGæˆæœ¬è¶…æ ‡ï¼ˆ${monthly_cost}ï¼‰ï¼Œå·²åˆ‡æ¢ä¸º100%æœ¬åœ°æ¨¡å¼"
    )
```

**æ•…éšœæ’æŸ¥**:

| é—®é¢˜ | ç—‡çŠ¶ | è§£å†³æ–¹æ¡ˆ |
|------|------|---------|
| **VRAMä¸è¶³** | `CUDA out of memory` | 1. é™ä½`OLLAMA_NUM_PARALLEL`<br>2. ä½¿ç”¨é‡åŒ–æ¨¡å‹ï¼ˆqwen2.5:14b-instruct-q4_0ï¼‰<br>3. å¢åŠ GPUæ˜¾å­˜æˆ–ä½¿ç”¨CPUæ¨ç† |
| **æ¨ç†é€Ÿåº¦æ…¢** | P95å»¶è¿Ÿ>10ç§’ | 1. æ£€æŸ¥GPUåˆ©ç”¨ç‡ï¼ˆnvidia-smiï¼‰<br>2. å‡å°‘å¹¶å‘è¯·æ±‚æ•°<br>3. ä¼˜åŒ–prompté•¿åº¦ï¼ˆ<2000 tokensï¼‰ |
| **è´¨é‡ä¸è¾¾æ ‡** | F1 Score < 85% | 1. æ·»åŠ Few-shot examplesåˆ°prompt<br>2. å¾®è°ƒQwen2.5æ¨¡å‹ï¼ˆéœ€è¦è®­ç»ƒæ•°æ®ï¼‰<br>3. æé«˜APIæ¯”ä¾‹ï¼ˆ10%â†’30%ï¼‰ |
| **APIé™çº§ç‡é«˜** | é™çº§ç‡>10% | 1. æ£€æŸ¥OllamaæœåŠ¡çŠ¶æ€ï¼ˆollama psï¼‰<br>2. å¢åŠ local_timeoutï¼ˆ8ç§’â†’12ç§’ï¼‰<br>3. é‡å¯OllamaæœåŠ¡ |
| **æˆæœ¬è¶…æ ‡** | æœˆæˆæœ¬>$80 | 1. æ£€æŸ¥APIè°ƒç”¨æ—¥å¿—ï¼ˆcost_tracker.get_api_log()ï¼‰<br>2. é™ä½APIæ¯”ä¾‹ï¼ˆ10%â†’5%ï¼‰<br>3. åˆ‡æ¢ä¸º100%æœ¬åœ°æ¨¡å¼ |

**ç”Ÿäº§éƒ¨ç½²æ£€æŸ¥æ¸…å•**:

- [ ] **ç¡¬ä»¶**: RTX 4090æˆ–åŒç­‰GPUå·²å®‰è£…å¹¶æ­£å¸¸å·¥ä½œ
- [ ] **Ollama**: æœåŠ¡å·²å¯åŠ¨ï¼ˆollama psæ˜¾ç¤ºqwen2.5:14b-instructï¼‰
- [ ] **è´¨é‡éªŒè¯**: F1 Score â‰¥ 85%ï¼ˆ100ä¸ªæ ·æœ¬æµ‹è¯•ï¼‰
- [ ] **æ€§èƒ½éªŒè¯**: P95å»¶è¿Ÿ â‰¤ 8ç§’ï¼ˆ10æ¬¡æ¨ç†æµ‹è¯•ï¼‰
- [ ] **æˆæœ¬ç›‘æ§**: CostTrackerå·²é›†æˆï¼Œå‘Šè­¦é‚®ä»¶é…ç½®æ­£ç¡®
- [ ] **é™çº§æœºåˆ¶**: APIé™çº§ç‡ < 5%ï¼ˆ7å¤©ç›‘æ§ï¼‰
- [ ] **é…ç½®æ–‡ä»¶**: `config/graphrag_cost_control.json`å·²éƒ¨ç½²
- [ ] **å¥åº·æ£€æŸ¥**: OllamaæœåŠ¡ç›‘æ§å·²å¯ç”¨ï¼ˆæ¯åˆ†é’Ÿæ£€æŸ¥ï¼‰
- [ ] **å¤‡ä»½æ–¹æ¡ˆ**: APIå¯†é’¥å·²é…ç½®ï¼Œé™çº§å¯ç”¨
- [ ] **æ–‡æ¡£**: è¿ç»´æ–‡æ¡£å·²åˆ›å»ºï¼ˆ`docs/operations/graphrag-local-model-deployment.md`ï¼‰

**å‚è€ƒèµ„æ–™**:
- **Ollamaå®˜æ–¹æ–‡æ¡£**: https://ollama.com/docs
- **Qwen2.5æ¨¡å‹å¡**: https://ollama.com/library/qwen2.5:14b-instruct
- **ADR-001**: æœ¬åœ°æ¨¡å‹vs APIæŠ€æœ¯å†³ç­–ï¼ˆ`docs/architecture/ADR-001-local-model-vs-api.md`ï¼‰
- **Story GraphRAG.2**: æœ¬åœ°æ¨¡å‹é›†æˆï¼ˆ`docs/stories/graphrag-2-local-model-integration.story.md`ï¼‰
- **Story GraphRAG.5**: æ€§èƒ½ä¼˜åŒ–ä¸æˆæœ¬ç›‘æ§ï¼ˆ`docs/stories/graphrag-5-performance-cost.story.md`ï¼‰

---

### 10.8 é›†æˆæµ‹è¯•

#### 10.8.1 ç«¯åˆ°ç«¯æµ‹è¯•

```python
@pytest.mark.asyncio
async def test_agentic_rag_end_to_end():
    """
    æµ‹è¯•Agentic RAGå®Œæ•´æµç¨‹
    """
    # Setup
    canvas_path = "test_data/ç¦»æ•£æ•°å­¦-æµ‹è¯•.canvas"
    config = create_langgraph_config(
        canvas_path=canvas_path,
        user_id="test_user",
        session_id=str(uuid.uuid4())
    )

    # Execute
    result = await generate_review_canvas_with_agentic_rag(
        canvas_path=canvas_path,
        config=config
    )

    # Assertions
    assert result["questions_count"] >= 5, "è‡³å°‘ç”Ÿæˆ5ä¸ªæ£€éªŒé—®é¢˜"
    assert result["questions_count"] <= 10, "æœ€å¤šç”Ÿæˆ10ä¸ªæ£€éªŒé—®é¢˜"
    assert result["document_quality_score"] >= 0.5, "æ–‡æ¡£è´¨é‡åº”â‰¥0.5"
    assert os.path.exists(result["review_canvas_path"]), "æ£€éªŒç™½æ¿æ–‡ä»¶åº”å­˜åœ¨"

    # éªŒè¯é—®é¢˜æƒé‡
    questions = read_verification_questions_from_canvas(result["review_canvas_path"])
    temporal_questions = [
        q for q in questions
        if "temporal" in q.get("data_sources", [])
    ]
    assert len(temporal_questions) / len(questions) >= 0.6, "è‡³å°‘60%é—®é¢˜æ¥è‡ªtemporal(å†å²è–„å¼±ç‚¹)"
```

#### 10.8.2 æ€§èƒ½åŸºå‡†æµ‹è¯•

```python
@pytest.mark.performance
async def test_agentic_rag_performance():
    """
    æµ‹è¯•Agentic RAGæ€§èƒ½æŒ‡æ ‡
    """
    canvas_path = "test_data/ç¦»æ•£æ•°å­¦-æµ‹è¯•.canvas"
    config = create_langgraph_config(canvas_path, "test_user", str(uuid.uuid4()))

    # æµ‹è¯•ç«¯åˆ°ç«¯å»¶è¿Ÿ
    start_time = time.time()
    result = await generate_review_canvas_with_agentic_rag(canvas_path, config)
    end_to_end_latency = time.time() - start_time

    assert end_to_end_latency < 5.0, f"ç«¯åˆ°ç«¯å»¶è¿Ÿåº”<5ç§’, å®é™…: {end_to_end_latency:.2f}ç§’"

    # æµ‹è¯•å·¥å…·è°ƒç”¨å»¶è¿Ÿ
    tool_latencies = extract_tool_latencies_from_state(result)
    for tool_name, latency in tool_latencies.items():
        assert latency < 1.0, f"{tool_name}å»¶è¿Ÿåº”<1ç§’, å®é™…: {latency:.2f}ç§’"
```

---

### 10.9 æ•…éšœå¤„ç†ä¸é™çº§ç­–ç•¥

#### 10.9.1 å·¥å…·è°ƒç”¨å¤±è´¥å¤„ç†

```python
async def graphiti_hybrid_search_tool_with_fallback(
    query: str,
    canvas_context: dict
) -> List[Dict]:
    """
    å¸¦é™çº§ç­–ç•¥çš„Graphitiæ£€ç´¢
    """
    try:
        # å°è¯•hybrid_search
        return await graphiti_hybrid_search_tool(query, canvas_context)
    except Exception as e:
        logger.error(f"Graphiti hybrid search failed: {e}")

        # é™çº§ç­–ç•¥1: ä½¿ç”¨çº¯è¯­ä¹‰æœç´¢(semantic_onlyæ¨¡å¼)
        try:
            return await graphiti.search(
                query=query,
                search_mode="semantic_only",
                num_results=20
            )
        except Exception as e2:
            logger.error(f"Graphiti semantic search failed: {e2}")

            # é™çº§ç­–ç•¥2: è¿”å›ç©ºç»“æœ,è®©å…¶ä»–å·¥å…·è¡¥å¿
            return []
```

#### 10.9.2 æ–‡æ¡£è´¨é‡ä¸è¶³å¤„ç†

```python
def grade_documents_route(state: AgenticRAGState) -> str:
    quality_score = state.get("document_quality_score", 0.0)
    attempts = state.get("retrieval_attempts", 0)

    # å¦‚æœé‡è¯•3æ¬¡ä»ç„¶è´¨é‡ä¸è¶³,ä½¿ç”¨é™çº§ç­–ç•¥
    if attempts >= 3 and quality_score < 0.5:
        logger.warning(
            f"Document quality still low after {attempts} attempts: {quality_score:.2f}"
        )

        # é™çº§ç­–ç•¥: ä½¿ç”¨Canvasæœ¬èº«çš„çº¢/ç´«èŠ‚ç‚¹æ–‡æœ¬ä½œä¸ºfallback
        if not state.get("retrieved_documents"):
            state["retrieved_documents"] = generate_fallback_documents_from_canvas(
                state["canvas_context"]
            )

        return "generate"

    if quality_score >= 0.5:
        return "generate"

    return "rewrite"
```

---

### 10.10 ç›‘æ§ä¸æ—¥å¿—

#### 10.10.1 å…³é”®æŒ‡æ ‡ç›‘æ§

```python
from prometheus_client import Histogram, Counter, Gauge

# Agentic RAGæ€§èƒ½æŒ‡æ ‡
agentic_rag_latency = Histogram(
    'agentic_rag_latency_seconds',
    'Agentic RAG end-to-end latency',
    ['canvas_file']
)

# å·¥å…·è°ƒç”¨ç»Ÿè®¡
tool_invocation_count = Counter(
    'agentic_rag_tool_invocations_total',
    'Total tool invocations',
    ['tool_name', 'status']  # status: success/failure
)

# æ–‡æ¡£è´¨é‡åˆ†å¸ƒ
document_quality_distribution = Histogram(
    'agentic_rag_document_quality_score',
    'Document quality score distribution',
    buckets=[0.0, 0.3, 0.5, 0.7, 0.9, 1.0]
)

# æŸ¥è¯¢é‡å†™æ¬¡æ•°
query_rewrite_count = Counter(
    'agentic_rag_query_rewrites_total',
    'Total query rewrites',
    ['canvas_file']
)
```

#### 10.10.2 ç»“æ„åŒ–æ—¥å¿—

```python
import structlog

logger = structlog.get_logger()

# è®°å½•Agentic RAGæ‰§è¡Œ
logger.info(
    "agentic_rag_started",
    canvas_file=canvas_path,
    session_id=config["configurable"]["session_id"],
    red_nodes_count=len(canvas_context["red_nodes"]),
    purple_nodes_count=len(canvas_context["purple_nodes"])
)

# è®°å½•å·¥å…·è°ƒç”¨
logger.info(
    "tool_invoked",
    tool_name="graphiti_hybrid_search_tool",
    query=query[:100],
    max_results=20,
    rerank_strategy="rrf"
)

# è®°å½•æ–‡æ¡£è´¨é‡è¯„åˆ†
logger.info(
    "documents_graded",
    quality_score=quality_score,
    quality_issues=quality_issues,
    retrieved_docs_count=len(retrieved_docs)
)

# è®°å½•æŸ¥è¯¢é‡å†™
logger.warning(
    "query_rewritten",
    original_query=original_query[:100],
    new_query=new_query[:100],
    attempt=retrieval_attempts
)
```

---

### 10.11 éªŒæ”¶æ ‡å‡†

- âœ… **AC 1**: Agentic RAG graphå¯æˆåŠŸç¼–è¯‘å¹¶æ‰§è¡Œ
- âœ… **AC 2**: 3ä¸ªretrieval toolså¯æ­£å¸¸è°ƒç”¨å¹¶è¿”å›ç»“æœ
- âœ… **AC 3**: æ–‡æ¡£è´¨é‡è¯„åˆ†<0.5æ—¶è‡ªåŠ¨è§¦å‘æŸ¥è¯¢é‡å†™
- âœ… **AC 4**: ç«¯åˆ°ç«¯æ£€éªŒç™½æ¿ç”Ÿæˆ<5ç§’
- âœ… **AC 5**: é›†æˆæµ‹è¯•è¦†ç›–ç‡â‰¥90%
- âœ… **AC 6**: ç”Ÿæˆçš„æ£€éªŒé—®é¢˜ä¸­â‰¥60%æ¥è‡ªå†å²è–„å¼±ç‚¹(temporalæ•°æ®)
- âœ… **AC 7**: å·¥å…·è°ƒç”¨å¤±è´¥æ—¶æœ‰é™çº§ç­–ç•¥,ä¸é˜»å¡ä¸»æµç¨‹
- âœ… **AC 8**: æŸ¥è¯¢é‡å†™æ¬¡æ•°â‰¤3,é¿å…æ— é™å¾ªç¯

---

## ğŸ“– ä¹ã€å‚è€ƒèµ„æ–™

### LangGraphå®˜æ–¹æ–‡æ¡£

- LangGraph Checkpointer: `@langgraph` Skill
- StateGraph API: references/llms-full.md
- PostgresSaveré…ç½®: references/checkpointer.md

### ç›¸å…³æ¶æ„æ–‡æ¡£

- **Graphitié›†æˆæ¶æ„**: `GRAPHITI-KNOWLEDGE-GRAPH-INTEGRATION-ARCHITECTURE.md` Section 8.6
- **å¤šAgentå¹¶å‘æ¶æ„**: `MULTI-AGENT-CONCURRENT-ANALYSIS-SYSTEM-ARCHITECTURE.md` Section 7.3
- **PRD**: `CANVAS-LEARNING-SYSTEM-OBSIDIAN-NATIVE-MIGRATION-PRD.md` Section 3.6

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0
**æœ€åæ›´æ–°**: 2025-11-11
**ä½œè€…**: Claude Code
**çŠ¶æ€**: æŠ€æœ¯æ–¹æ¡ˆè®¾è®¡å®Œæˆ
