# MULTI-AGENT-CONCURRENT-ANALYSIS-SYSTEM-ARCHITECTURE - Part 1

**Source**: `MULTI-AGENT-CONCURRENT-ANALYSIS-SYSTEM-ARCHITECTURE.md`
**Sections**: ğŸ“‹ æ‰§è¡Œæ‘˜è¦, ğŸ” ä¸€ã€é—®é¢˜åˆ†æä¸æ ¹å› å®šä½, ğŸ—ï¸ äºŒã€å¹¶å‘æ¶æ„è®¾è®¡, ğŸ¯ ä¸‰ã€ä»»åŠ¡è°ƒåº¦ä¸èµ„æºç®¡ç†, ğŸ”„ å››ã€ç»“æœèåˆä¸å†…å®¹å®Œæ•´æ€§, âš¡ äº”ã€æ€§èƒ½ä¼˜åŒ–ç­–ç•¥, ğŸ›¡ï¸ å…­ã€é”™è¯¯å¤„ç†ä¸æ¢å¤

---

---
document_type: "Architecture"
version: "1.0.0"
last_modified: "2025-11-19"
status: "approved"
iteration: 1

authors:
  - name: "Architect Agent"
    role: "Solution Architect"

reviewers:
  - name: "PO Agent"
    role: "Product Owner"
    approved: true

compatible_with:
  prd: "v1.0"
  api_spec: "v1.0"

api_spec_hash: "0dc1d3610d28bf99"

changes_from_previous:
  - "Initial Architecture with frontmatter metadata"

git:
  commit_sha: ""
  tag: ""

metadata:
  components_count: 0
  external_services: []
  technology_stack:
    frontend: []
    backend: ["Python 3.11", "asyncio"]
    database: []
    infrastructure: []
---

# Canvaså­¦ä¹ ç³»ç»Ÿ v2.0 - å¤šAgentå¹¶å‘åˆ†æç³»ç»ŸæŠ€æœ¯æ¶æ„

**æ–‡æ¡£ç‰ˆæœ¬**: v2.1 (LangGraph StateGraphé›†æˆç‰ˆ)
**åˆ›å»ºæ—¥æœŸ**: 2025-10-18
**æœ€åæ›´æ–°**: 2025-11-11 (**NEW**: Section 7.3 LangGraph StateGraphé…ç½®)
**ä½œè€…**: Claude (Architect Agent)
**é¡¹ç›®**: Canvaså­¦ä¹ ç³»ç»ŸæŠ€æœ¯å‡çº§Epic
**ç±»å‹**: Brownfieldå‡çº§æ¶æ„è®¾è®¡

---


## ğŸ“‹ æ‰§è¡Œæ‘˜è¦

æœ¬æ–‡æ¡£è®¾è®¡äº†Canvaså­¦ä¹ ç³»ç»Ÿv2.0çš„å¤šAgentå¹¶å‘åˆ†æç³»ç»Ÿï¼Œæ—¨åœ¨è§£å†³ç”¨æˆ·åé¦ˆçš„**é€Ÿåº¦è¿‡æ…¢**å’Œ**å†…å®¹å¤åˆ¶å¤±è´¥**é—®é¢˜ï¼Œå®ç°å¤šAgentå¹¶è¡Œå¤„ç†ï¼Œå°†ç³»ç»Ÿååé‡æå‡3-5å€ï¼ŒåŒæ—¶ç¡®ä¿å†…å®¹å®Œæ•´æ€§å’Œç³»ç»Ÿç¨³å®šæ€§ã€‚

### ğŸ¯ æ ¸å¿ƒç›®æ ‡

1. **æ€§èƒ½æå‡**: å¹¶å‘å¤„ç†å¤šä¸ªAgentï¼Œæ€»è€—æ—¶ä»ä¸²è¡Œç´¯åŠ å˜ä¸ºå¹¶è¡Œæœ€å¤§å€¼
2. **å†…å®¹å®Œæ•´æ€§**: è§£å†³å¤åˆ¶å¤±è´¥å’Œçœç•¥å·é—®é¢˜ï¼Œç¡®ä¿Agentç”Ÿæˆçš„å®Œæ•´å†…å®¹æ­£ç¡®å¤åˆ¶åˆ°é»„è‰²èŠ‚ç‚¹
3. **æ™ºèƒ½è°ƒåº¦**: åŸºäºAgentç‰¹æ€§å’Œä»»åŠ¡ç±»å‹çš„æ™ºèƒ½ä»»åŠ¡åˆ†é…
4. **ç»“æœèåˆ**: å¤šAgentè¾“å‡ºç»“æœçš„æ™ºèƒ½åˆå¹¶å’Œå»é‡
5. **å®¹é”™æœºåˆ¶**: å¹¶å‘ç¯å¢ƒä¸‹çš„é”™è¯¯å¤„ç†å’Œæ¢å¤

---


## ğŸ” ä¸€ã€é—®é¢˜åˆ†æä¸æ ¹å› å®šä½

### 1.1 å½“å‰ç³»ç»Ÿç“¶é¢ˆåˆ†æ

#### æ€§èƒ½ç“¶é¢ˆæ ¹å› 
```yaml
ä¸²è¡Œæ‰§è¡Œç“¶é¢ˆ:
  é—®é¢˜: Agenté€ä¸ªæ‰§è¡Œï¼Œæ€»è€—æ—¶ = Î£(å•ä¸ªAgentè€—æ—¶)
  ç¤ºä¾‹: 5ä¸ªAgent Ã— 8ç§’ = 40ç§’
  æ ¹å› : ç¼ºä¹å¹¶å‘æ‰§è¡Œæœºåˆ¶

é‡å¤I/Oæ“ä½œ:
  é—®é¢˜: æ¯ä¸ªAgentè°ƒç”¨éƒ½æ¶‰åŠCanvasæ–‡ä»¶è¯»å†™
  ç¤ºä¾‹: 5æ¬¡æ–‡ä»¶è¯»å– + 5æ¬¡æ–‡ä»¶å†™å…¥ = 10æ¬¡I/O
  æ ¹å› : ç¼ºä¹ç¼“å­˜å’Œæ‰¹é‡æ“ä½œæœºåˆ¶

Agentè°ƒç”¨å¼€é”€:
  é—®é¢˜: æ¯æ¬¡è°ƒç”¨éƒ½éœ€è¦åˆå§‹åŒ–å’Œä¸Šä¸‹æ–‡åˆ‡æ¢
  ç¤ºä¾‹: 5æ¬¡ä¸Šä¸‹æ–‡åˆ‡æ¢ = 2-3ç§’é¢å¤–å¼€é”€
  æ ¹å› : ç¼ºä¹è¿æ¥æ± å’Œä¼šè¯å¤ç”¨
```

#### å†…å®¹å¤åˆ¶å¤±è´¥æ ¹å› 
```yaml
å­—ç¬¦æˆªæ–­é—®é¢˜:
  å¯èƒ½åŸå› :
    - JSONåºåˆ—åŒ–/ååºåˆ—åŒ–è¿‡ç¨‹ä¸­çš„å­—ç¬¦é™åˆ¶
    - CanvasèŠ‚ç‚¹æ–‡æœ¬å­—æ®µé•¿åº¦é™åˆ¶ï¼ˆObsidiané™åˆ¶ï¼‰
    - ä¸­æ–‡å­—ç¬¦ç¼–ç å¤„ç†é—®é¢˜
  è¡¨ç°: å†…å®¹è¢«çœç•¥å·(...)æˆªæ–­

å†…å­˜ç®¡ç†é—®é¢˜:
  å¯èƒ½åŸå› :
    - å¤§æ–‡æœ¬å¯¹è±¡çš„å†…å­˜åˆ†é…å¤±è´¥
    - å­—ç¬¦ä¸²æ‹¼æ¥è¿‡ç¨‹ä¸­çš„ç¼“å†²åŒºæº¢å‡º
    - å¤šçº¿ç¨‹ç¯å¢ƒä¸‹çš„å†…å­˜ç«äº‰
  è¡¨ç°: éƒ¨åˆ†å†…å®¹ä¸¢å¤±

æ–‡ä»¶å†™å…¥æ—¶æœºé—®é¢˜:
  å¯èƒ½åŸå› :
    - å¼‚æ­¥å†™å…¥æœªå®Œæˆå°±è¿”å›
    - æ–‡ä»¶é”ç«äº‰å¯¼è‡´å†™å…¥ä¸å®Œæ•´
    - ä¸´æ—¶æ–‡ä»¶å’Œæ­£å¼æ–‡ä»¶çš„åŒæ­¥é—®é¢˜
  è¡¨ç°: å†™å…¥çš„å†…å®¹ä¸å®Œæ•´
```

### 1.2 Sub-agentæ€§èƒ½ç‰¹å¾åˆ†æ

åŸºäºç°æœ‰11ä¸ªSub-agentsçš„ç‰¹å¾åˆ†æï¼š

```yaml
è®¡ç®—å¯†é›†å‹Agents:
  oral-explanation: 6-8ç§’ (800-1200è¯ç”Ÿæˆ)
  clarification-path: 8-10ç§’ (1500+è¯æ·±åº¦è§£é‡Š)
  four-level-explanation: 7-9ç§’ (4å±‚æ¬¡è§£é‡Š)
  ç‰¹ç‚¹: CPUå¯†é›†ï¼Œé€‚åˆå¤šè¿›ç¨‹å¹¶è¡Œ

I/Oå¯†é›†å‹Agents:
  comparison-table: 3-4ç§’ (ç»“æ„åŒ–è¡¨æ ¼ç”Ÿæˆ)
  memory-anchor: 2-3ç§’ (ç±»æ¯”å’Œè®°å¿†ç‚¹ç”Ÿæˆ)
  basic-decomposition: 2-3ç§’ (3-7ä¸ªé—®é¢˜ç”Ÿæˆ)
  ç‰¹ç‚¹: I/Oå¯†é›†ï¼Œé€‚åˆå¼‚æ­¥å¹¶å‘

è½»é‡çº§Agents:
  scoring-agent: 1-2ç§’ (4ç»´è¯„åˆ†)
  verification-question-agent: 3-4ç§’ (é—®é¢˜ç”Ÿæˆ)
  deep-decomposition: 3-5ç§’ (æ·±åº¦é—®é¢˜ç”Ÿæˆ)
  ç‰¹ç‚¹: å¿«é€Ÿå“åº”ï¼Œé€‚åˆé«˜é¢‘è°ƒç”¨
```

---


## ğŸ—ï¸ äºŒã€å¹¶å‘æ¶æ„è®¾è®¡

### 2.1 æ•´ä½“æ¶æ„å›¾

```mermaid
graph TB
    subgraph "ç”¨æˆ·æ¥å£å±‚"
        UI[Claude Code Interface]
        CMD[Command Parser]
    end

    subgraph "å¹¶å‘æ§åˆ¶å±‚"
        TC[Task Coordinator<br/>ä»»åŠ¡åè°ƒå™¨]
        TM[Task Manager<br/>ä»»åŠ¡ç®¡ç†å™¨]
        SM[Scheduler Manager<br/>è°ƒåº¦ç®¡ç†å™¨]
    end

    subgraph "æ‰§è¡Œå¼•æ“å±‚"
        PE[Process Pool<br/>è¿›ç¨‹æ± ]
        AE[Async Executor<br/>å¼‚æ­¥æ‰§è¡Œå™¨]
        CC[Connection Cache<br/>è¿æ¥ç¼“å­˜]
    end

    subgraph "Agentå±‚"
        A1[oral-explanation]
        A2[clarification-path]
        A3[comparison-table]
        A4[memory-anchor]
        A5[scoring-agent]
        A6[basic-decomposition]
        A7[deep-decomposition]
        A8[four-level-explanation]
        A9[verification-question-agent]
        A10[example-teaching]
    end

    subgraph "ç»“æœå¤„ç†å±‚"
        RM[Result Merger<br/>ç»“æœèåˆå™¨]
        VD[Content Validator<br/>å†…å®¹éªŒè¯å™¨]
        CW[Canvas Writer<br/>Canvaså†™å…¥å™¨]
    end

    subgraph "æ•°æ®å±‚"
        Cache[Memory Cache]
        Canvas[Canvas Files]
        Backup[Backup System]
    end

    UI --> CMD
    CMD --> TC
    TC --> TM
    TC --> SM
    TM --> PE
    TM --> AE
    SM --> CC
    PE --> A1
    PE --> A2
    PE --> A3
    AE --> A4
    AE --> A5
    AE --> A6
    CC --> A7
    CC --> A8
    CC --> A9
    CC --> A10
    A1 --> RM
    A2 --> RM
    A3 --> RM
    A4 --> RM
    A5 --> RM
    A6 --> RM
    A7 --> RM
    A8 --> RM
    A9 --> RM
    A10 --> RM
    RM --> VD
    VD --> CW
    CW --> Canvas
    CW --> Backup
    Cache --> TM
    Cache --> RM
```

### 2.2 æ ¸å¿ƒç»„ä»¶è®¾è®¡

#### 2.2.1 Task Coordinator (ä»»åŠ¡åè°ƒå™¨)

```python
class TaskCoordinator:
    """å¤šAgentå¹¶å‘ä»»åŠ¡åè°ƒå™¨

    è´Ÿè´£ä»»åŠ¡åˆ†è§£ã€ä¾èµ–ç®¡ç†ã€èµ„æºåˆ†é…å’Œæ‰§è¡Œåè°ƒ
    """

    def __init__(self, max_workers: int = 4, max_concurrent_agents: int = 3):
        self.max_workers = max_workers
        self.max_concurrent_agents = max_concurrent_agents
        self.task_queue = asyncio.Queue()
        self.result_queue = asyncio.Queue()
        self.task_tracker = TaskTracker()

    async def coordinate_concurrent_analysis(
        self,
        canvas_path: str,
        yellow_nodes: List[Dict],
        selected_agents: List[str],
        analysis_mode: str = "parallel"
    ) -> Dict[str, Any]:
        """åè°ƒå¤šAgentå¹¶å‘åˆ†æ

        Args:
            canvas_path: Canvasæ–‡ä»¶è·¯å¾„
            yellow_nodes: å¾…åˆ†æçš„é»„è‰²èŠ‚ç‚¹åˆ—è¡¨
            selected_agents: é€‰æ‹©çš„Agentåˆ—è¡¨
            analysis_mode: åˆ†ææ¨¡å¼ (parallel/sequential/hybrid)

        Returns:
            Dict: å¹¶å‘åˆ†æç»“æœ
        """
```

#### 2.2.2 Agent Task (ä»£ç†ä»»åŠ¡)

```python
@dataclass
class AgentTask:
    """Agentä»»åŠ¡å®šä¹‰"""
    task_id: str
    agent_name: str
    input_data: Dict[str, Any]
    priority: int = 1
    estimated_duration: float = 5.0
    dependencies: List[str] = field(default_factory=list)
    resource_requirements: Dict[str, Any] = field(default_factory=dict)

@dataclass
class TaskResult:
    """ä»»åŠ¡æ‰§è¡Œç»“æœ"""
    task_id: str
    agent_name: str
    status: str  # "success", "failed", "timeout"
    result_data: Optional[Dict[str, Any]] = None
    error_message: Optional[str] = None
    execution_time: float = 0.0
    content_length: int = 0
```

#### 2.2.3 Process Pool Manager (è¿›ç¨‹æ± ç®¡ç†å™¨)

```python
import aiomultiprocess
import asyncio
from concurrent.futures import ProcessPoolExecutor

class ProcessPoolManager:
    """AIå¢å¼ºçš„è¿›ç¨‹æ± ç®¡ç†å™¨

    ä½¿ç”¨aiomultiprocesså®ç°çœŸæ­£çš„å¹¶è¡Œå¤„ç†ï¼Œçªç ´Python GILé™åˆ¶
    """

    def __init__(self, max_workers: int = None):
        self.max_workers = max_workers or min(4, os.cpu_count())
        self.process_pool = None
        self.async_pool = None

    async def initialize(self):
        """åˆå§‹åŒ–è¿›ç¨‹æ± å’Œå¼‚æ­¥æ± """
        # è¿›ç¨‹æ± ç”¨äºCPUå¯†é›†å‹ä»»åŠ¡
        self.process_pool = ProcessPoolExecutor(
            max_workers=self.max_workers
        )

        # å¼‚æ­¥æ± ç”¨äºI/Oå¯†é›†å‹ä»»åŠ¡
        self.async_pool = aiomultiprocess.Pool(
            processes=self.max_workers
        )

    async def execute_computation_intensive(
        self,
        agent_task: AgentTask
    ) -> TaskResult:
        """æ‰§è¡Œè®¡ç®—å¯†é›†å‹ä»»åŠ¡ (oral-explanation, clarification-pathç­‰)"""

    async def execute_io_intensive(
        self,
        agent_task: AgentTask
    ) -> TaskResult:
        """æ‰§è¡ŒI/Oå¯†é›†å‹ä»»åŠ¡ (comparison-table, memory-anchorç­‰)"""

    async def execute_lightweight(
        self,
        agent_task: AgentTask
    ) -> TaskResult:
        """æ‰§è¡Œè½»é‡çº§ä»»åŠ¡ (scoring-agentç­‰)"""
```

---


## ğŸ¯ ä¸‰ã€ä»»åŠ¡è°ƒåº¦ä¸èµ„æºç®¡ç†

### 3.1 æ™ºèƒ½ä»»åŠ¡è°ƒåº¦ç­–ç•¥

#### 3.1.1 Agentåˆ†ç±»è°ƒåº¦

```python
class AgentClassifier:
    """Agentåˆ†ç±»å™¨ï¼ŒåŸºäºæ€§èƒ½ç‰¹å¾åˆ†ç±»"""

    AGENT_CATEGORIES = {
        "computation_intensive": {
            "agents": ["oral-explanation", "clarification-path", "four-level-explanation"],
            "executor": "process_pool",
            "max_concurrent": 2,
            "timeout": 30.0
        },
        "io_intensive": {
            "agents": ["comparison-table", "memory-anchor", "basic-decomposition"],
            "executor": "async_pool",
            "max_concurrent": 4,
            "timeout": 15.0
        },
        "lightweight": {
            "agents": ["scoring-agent", "verification-question-agent"],
            "executor": "direct",
            "max_concurrent": 6,
            "timeout": 10.0
        }
    }

    def classify_agent(self, agent_name: str) -> Dict[str, Any]:
        """åˆ†ç±»Agentå¹¶è¿”å›æ‰§è¡Œé…ç½®"""
        for category, config in self.AGENT_CATEGORIES.items():
            if agent_name in config["agents"]:
                return config
        return self.AGENT_CATEGORIES["lightweight"]
```

#### 3.1.2 åŠ¨æ€èµ„æºåˆ†é…

```python
class ResourceManager:
    """åŠ¨æ€èµ„æºç®¡ç†å™¨"""

    def __init__(self, total_memory_mb: int = 4096):
        self.total_memory = total_memory_mb
        self.allocated_memory = 0
        self.active_tasks = {}

    def allocate_resources(self, task: AgentTask) -> bool:
        """ä¸ºä»»åŠ¡åˆ†é…èµ„æº"""
        required_memory = self._estimate_memory_requirement(task)

        if self.allocated_memory + required_memory <= self.total_memory:
            self.allocated_memory += required_memory
            self.active_tasks[task.task_id] = {
                "memory": required_memory,
                "start_time": time.time()
            }
            return True
        return False

    def release_resources(self, task_id: str):
        """é‡Šæ”¾ä»»åŠ¡èµ„æº"""
        if task_id in self.active_tasks:
            self.allocated_memory -= self.active_tasks[task_id]["memory"]
            del self.active_tasks[task_id]

    def _estimate_memory_requirement(self, task: AgentTask) -> int:
        """ä¼°ç®—ä»»åŠ¡å†…å­˜éœ€æ±‚"""
        base_memory = 100  # MB
        agent_multiplier = {
            "oral-explanation": 3.0,
            "clarification-path": 3.5,
            "four-level-explanation": 3.2,
            "comparison-table": 1.5,
            "memory-anchor": 1.2
        }
        multiplier = agent_multiplier.get(task.agent_name, 1.0)
        return int(base_memory * multiplier)
```

### 3.2 ä»»åŠ¡ä¾èµ–ç®¡ç†

```python
class TaskDependencyManager:
    """ä»»åŠ¡ä¾èµ–ç®¡ç†å™¨"""

    def __init__(self):
        self.dependency_graph = {}
        self.execution_order = []

    def add_dependency(self, task_id: str, depends_on: str):
        """æ·»åŠ ä»»åŠ¡ä¾èµ–"""
        if task_id not in self.dependency_graph:
            self.dependency_graph[task_id] = []
        self.dependency_graph[task_id].append(depends_on)

    def get_execution_order(self, tasks: List[AgentTask]) -> List[List[str]]:
        """è·å–ä»»åŠ¡æ‰§è¡Œæ‰¹æ¬¡

        è¿”å›: æŒ‰ä¾èµ–å…³ç³»åˆ†æ‰¹çš„ä»»åŠ¡åˆ—è¡¨
        """
        # ä½¿ç”¨æ‹“æ‰‘æ’åºç¡®å®šæ‰§è¡Œé¡ºåº
        # å®ç°ç•¥...

    def can_execute(self, task_id: str, completed_tasks: Set[str]) -> bool:
        """æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å¯ä»¥æ‰§è¡Œ"""
        if task_id not in self.dependency_graph:
            return True
        return all(dep in completed_tasks
                  for dep in self.dependency_graph[task_id])
```

---


## ğŸ”„ å››ã€ç»“æœèåˆä¸å†…å®¹å®Œæ•´æ€§

### 4.1 ç»“æœèåˆç­–ç•¥

#### 4.1.1 Result Merger (ç»“æœèåˆå™¨)

```python
class ResultMerger:
    """å¤šAgentç»“æœèåˆå™¨"""

    def __init__(self):
        self.fusion_strategies = {
            "complementary": self._merge_complementary,
            "supplementary": self._merge_supplementary,
            "hierarchical": self._merge_hierarchical,
            "voting": self._merge_voting
        }

    async def merge_results(
        self,
        results: List[TaskResult],
        fusion_strategy: str = "complementary"
    ) -> Dict[str, Any]:
        """èåˆå¤šä¸ªAgentçš„æ‰§è¡Œç»“æœ

        Args:
            results: Agentæ‰§è¡Œç»“æœåˆ—è¡¨
            fusion_strategy: èåˆç­–ç•¥

        Returns:
            Dict: èåˆåçš„ç»“æœ
        """

        # 1. éªŒè¯ç»“æœå®Œæ•´æ€§
        validated_results = await self._validate_results(results)

        # 2. é€‰æ‹©èåˆç­–ç•¥
        strategy = self.fusion_strategies.get(fusion_strategy,
                                            self._merge_complementary)

        # 3. æ‰§è¡Œèåˆ
        merged_result = await strategy(validated_results)

        # 4. åå¤„ç†å’Œä¼˜åŒ–
        final_result = await self._post_process_result(merged_result)

        return final_result

    async def _merge_complementary(
        self,
        results: List[TaskResult]
    ) -> Dict[str, Any]:
        """äº’è¡¥èåˆ - ä¸åŒè§’åº¦çš„è§£é‡Šåˆå¹¶"""

        merged_content = {
            "sections": [],
            "cross_references": [],
            "summary_points": []
        }

        # æŒ‰Agentç±»å‹åˆ†ç»„
        results_by_type = self._group_results_by_agent(results)

        # æ„å»ºäº’è¡¥å†…å®¹ç»“æ„
        if "oral-explanation" in results_by_type:
            merged_content["sections"].append({
                "type": "oral_explanation",
                "title": "ğŸ—£ï¸ æ•™æˆå¼è®²è§£",
                "content": results_by_type["oral-explanation"]["content"]
            })

        if "clarification-path" in results_by_type:
            merged_content["sections"].append({
                "type": "clarification",
                "title": "ğŸ” æ·±åº¦æ¾„æ¸…",
                "content": results_by_type["clarification-path"]["content"]
            })

        if "comparison-table" in results_by_type:
            merged_content["sections"].append({
                "type": "comparison",
                "title": "ğŸ“Š æ¦‚å¿µå¯¹æ¯”",
                "content": results_by_type["comparison-table"]["content"]
            })

        # ç”Ÿæˆäº¤å‰å¼•ç”¨
        merged_content["cross_references"] = self._generate_cross_references(
            merged_content["sections"]
        )

        # ç”Ÿæˆè¦ç‚¹æ€»ç»“
        merged_content["summary_points"] = self._extract_summary_points(
            merged_content["sections"]
        )

        return merged_content
```

#### 4.1.2 å†…å®¹å®Œæ•´æ€§ä¿éšœ

```python
class ContentValidator:
    """å†…å®¹å®Œæ•´æ€§éªŒè¯å™¨"""

    def __init__(self):
        self.validation_rules = {
            "length_check": self._validate_content_length,
            "encoding_check": self._validate_encoding,
            "structure_check": self._validate_structure,
            "completeness_check": self._validate_completeness
        }

    async def validate_content(
        self,
        content: str,
        source_agent: str
    ) -> ValidationResult:
        """éªŒè¯å†…å®¹å®Œæ•´æ€§

        Args:
            content: å¾…éªŒè¯çš„å†…å®¹
            source_agent: æ¥æºAgent

        Returns:
            ValidationResult: éªŒè¯ç»“æœ
        """

        result = ValidationResult(is_valid=True)

        # æ‰§è¡Œå„é¡¹éªŒè¯
        for rule_name, rule_func in self.validation_rules.items():
            try:
                validation_result = await rule_func(content, source_agent)
                result.merge(validation_result)
            except Exception as e:
                result.add_error(f"{rule_name}: {str(e)}")

        return result

    async def _validate_content_length(
        self,
        content: str,
        source_agent: str
    ) -> ValidationResult:
        """éªŒè¯å†…å®¹é•¿åº¦"""

        result = ValidationResult()

        # æ£€æŸ¥æ˜¯å¦è¢«æˆªæ–­
        if "..." in content and content.count("...") > 3:
            result.add_warning("æ£€æµ‹åˆ°è¿‡å¤šçœç•¥å·ï¼Œå¯èƒ½å­˜åœ¨æˆªæ–­")

        # æ£€æŸ¥é¢„æœŸé•¿åº¦
        expected_lengths = {
            "oral-explanation": (800, 1200),
            "clarification-path": (1500, 2500),
            "four-level-explanation": (1200, 1600)
        }

        if source_agent in expected_lengths:
            min_len, max_len = expected_lengths[source_agent]
            actual_len = len(content)

            if actual_len < min_len * 0.8:
                result.add_error(
                    f"å†…å®¹è¿‡çŸ­: {actual_len}å­—ç¬¦ (é¢„æœŸ: {min_len}-{max_len})"
                )
            elif actual_len > max_len * 1.2:
                result.add_warning(
                    f"å†…å®¹è¿‡é•¿: {actual_len}å­—ç¬¦ (é¢„æœŸ: {min_len}-{max_len})"
                )

        return result

    async def _validate_encoding(
        self,
        content: str,
        source_agent: str
    ) -> ValidationResult:
        """éªŒè¯å­—ç¬¦ç¼–ç """

        result = ValidationResult()

        try:
            # æµ‹è¯•UTF-8ç¼–ç /è§£ç 
            encoded = content.encode('utf-8')
            decoded = encoded.decode('utf-8')

            if decoded != content:
                result.add_error("UTF-8ç¼–ç éªŒè¯å¤±è´¥")

        except UnicodeEncodeError as e:
            result.add_error(f"ç¼–ç é”™è¯¯: {str(e)}")

        # æ£€æŸ¥ç‰¹æ®Šå­—ç¬¦
        problematic_chars = ['ï¿½', '\ufffd', '\x00']
        for char in problematic_chars:
            if char in content:
                result.add_error(f"æ£€æµ‹åˆ°é—®é¢˜å­—ç¬¦: {repr(char)}")

        return result
```

### 4.2 Canvaså†™å…¥ä¼˜åŒ–

```python
class OptimizedCanvasWriter:
    """ä¼˜åŒ–çš„Canvaså†™å…¥å™¨"""

    def __init__(self, canvas_path: str):
        self.canvas_path = canvas_path
        self.write_buffer = {}
        self.lock = asyncio.Lock()

    async def write_merged_content(
        self,
        yellow_node_id: str,
        merged_content: Dict[str, Any],
        backup_enabled: bool = True
    ) -> bool:
        """å†™å…¥èåˆåçš„å†…å®¹åˆ°é»„è‰²èŠ‚ç‚¹

        Args:
            yellow_node_id: é»„è‰²èŠ‚ç‚¹ID
            merged_content: èåˆåçš„å†…å®¹
            backup_enabled: æ˜¯å¦å¯ç”¨å¤‡ä»½

        Returns:
            bool: å†™å…¥æ˜¯å¦æˆåŠŸ
        """

        async with self.lock:
            try:
                # 1. åˆ›å»ºå¤‡ä»½
                if backup_enabled:
                    await self._create_backup()

                # 2. è¯»å–Canvasæ•°æ®
                canvas_data = await self._read_canvas_safe()

                # 3. æ ¼å¼åŒ–å†…å®¹
                formatted_content = await self._format_content_for_canvas(
                    merged_content
                )

                # 4. åˆ†æ®µå†™å…¥ï¼ˆé¿å…å•æ¬¡å†™å…¥è¿‡å¤§ï¼‰
                await self._write_content_in_chunks(
                    canvas_data,
                    yellow_node_id,
                    formatted_content
                )

                # 5. éªŒè¯å†™å…¥ç»“æœ
                success = await self._verify_write_result(
                    yellow_node_id,
                    formatted_content
                )

                if success:
                    # 6. ä¿å­˜Canvasæ–‡ä»¶
                    await self._save_canvas_safe(canvas_data)

                return success

            except Exception as e:
                # å‘ç”Ÿé”™è¯¯æ—¶æ¢å¤å¤‡ä»½
                await self._restore_from_backup()
                raise CanvasWriteError(f"å†™å…¥å¤±è´¥: {str(e)}")

    async def _format_content_for_canvas(
        self,
        merged_content: Dict[str, Any]
    ) -> str:
        """ä¸ºCanvasæ ¼å¼åŒ–å†…å®¹"""

        formatted_parts = []

        # æ·»åŠ æ ‡é¢˜
        formatted_parts.append("## ğŸ¤– å¤šAgentæ™ºèƒ½åˆ†æç»“æœ\n")

        # æ·»åŠ å„ä¸ªsection
        for section in merged_content.get("sections", []):
            formatted_parts.append(f"### {section['title']}\n")
            formatted_parts.append(f"{section['content']}\n\n")

        # æ·»åŠ äº¤å‰å¼•ç”¨
        if merged_content.get("cross_references"):
            formatted_parts.append("### ğŸ”— å…³è”å‚è€ƒ\n")
            for ref in merged_content["cross_references"]:
                formatted_parts.append(f"- {ref}\n")
            formatted_parts.append("\n")

        # æ·»åŠ è¦ç‚¹æ€»ç»“
        if merged_content.get("summary_points"):
            formatted_parts.append("### ğŸ’¡ æ ¸å¿ƒè¦ç‚¹\n")
            for point in merged_content["summary_points"]:
                formatted_parts.append(f"- {point}\n")

        # æ·»åŠ å…ƒæ•°æ®
        formatted_parts.append("\n---\n")
        formatted_parts.append(f"*ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\n")
        formatted_parts.append("*å¤„ç†æ–¹å¼: å¤šAgentå¹¶å‘åˆ†æ + æ™ºèƒ½èåˆ*\n")

        return "".join(formatted_parts)

    async def _write_content_in_chunks(
        self,
        canvas_data: Dict,
        node_id: str,
        content: str,
        chunk_size: int = 1000
    ):
        """åˆ†å—å†™å…¥å†…å®¹ï¼Œé¿å…å•æ¬¡æ“ä½œè¿‡å¤§"""

        # æ‰¾åˆ°ç›®æ ‡èŠ‚ç‚¹
        target_node = None
        for node in canvas_data.get("nodes", []):
            if node.get("id") == node_id:
                target_node = node
                break

        if not target_node:
            raise ValueError(f"èŠ‚ç‚¹ {node_id} ä¸å­˜åœ¨")

        # å¦‚æœå†…å®¹å¾ˆå¤§ï¼Œåˆ†æ‰¹å¤„ç†
        if len(content) > chunk_size:
            # ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶å¤„ç†å¤§å†…å®¹
            temp_file = await self._write_to_temp_file(content)
            target_node["text"] = f"{{FILE:{temp_file}}}"
        else:
            # ç›´æ¥å†™å…¥å°å†…å®¹
            target_node["text"] = content

    async def _verify_write_result(
        self,
        node_id: str,
        expected_content: str
    ) -> bool:
        """éªŒè¯å†™å…¥ç»“æœ"""

        try:
            # é‡æ–°è¯»å–Canvas
            canvas_data = await self._read_canvas_safe()

            # æ‰¾åˆ°èŠ‚ç‚¹å¹¶éªŒè¯å†…å®¹
            for node in canvas_data.get("nodes", []):
                if node.get("id") == node_id:
                    actual_content = node.get("text", "")

                    # æ£€æŸ¥å†…å®¹é•¿åº¦
                    if len(actual_content) < len(expected_content) * 0.95:
                        return False

                    # æ£€æŸ¥å…³é”®å­—æ®µ
                    if "å¤šAgentæ™ºèƒ½åˆ†æç»“æœ" not in actual_content:
                        return False

                    return True

            return False

        except Exception:
            return False
```

---


## âš¡ äº”ã€æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

### 5.1 ç¼“å­˜æœºåˆ¶

```python
class AgentCache:
    """Agentç»“æœç¼“å­˜ç³»ç»Ÿ"""

    def __init__(self, cache_dir: str = ".cache/agents"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
        self.memory_cache = {}
        self.cache_ttl = 3600  # 1å°æ—¶

    async def get_cached_result(
        self,
        agent_name: str,
        input_hash: str
    ) -> Optional[TaskResult]:
        """è·å–ç¼“å­˜ç»“æœ"""

        # 1. æ£€æŸ¥å†…å­˜ç¼“å­˜
        memory_key = f"{agent_name}:{input_hash}"
        if memory_key in self.memory_cache:
            cached_item = self.memory_cache[memory_key]
            if time.time() - cached_item["timestamp"] < self.cache_ttl:
                return cached_item["result"]

        # 2. æ£€æŸ¥æ–‡ä»¶ç¼“å­˜
        cache_file = self.cache_dir / f"{agent_name}_{input_hash}.json"
        if cache_file.exists():
            try:
                with open(cache_file, 'r', encoding='utf-8') as f:
                    cached_data = json.load(f)

                if time.time() - cached_data["timestamp"] < self.cache_ttl:
                    result = TaskResult(**cached_data["result"])
                    # åŠ è½½åˆ°å†…å­˜ç¼“å­˜
                    self.memory_cache[memory_key] = {
                        "result": result,
                        "timestamp": time.time()
                    }
                    return result
            except Exception:
                pass

        return None

    async def cache_result(
        self,
        agent_name: str,
        input_hash: str,
        result: TaskResult
    ):
        """ç¼“å­˜Agentç»“æœ"""

        cache_data = {
            "timestamp": time.time(),
            "result": asdict(result)
        }

        # å†…å­˜ç¼“å­˜
        memory_key = f"{agent_name}:{input_hash}"
        self.memory_cache[memory_key] = cache_data

        # æ–‡ä»¶ç¼“å­˜
        cache_file = self.cache_dir / f"{agent_name}_{input_hash}.json"
        try:
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"ç¼“å­˜å†™å…¥å¤±è´¥: {e}")
```

### 5.2 è¿æ¥æ± ç®¡ç†

```python
class AgentConnectionPool:
    """Agentè¿æ¥æ± """

    def __init__(self, pool_size: int = 5):
        self.pool_size = pool_size
        self.connections = asyncio.Queue(maxsize=pool_size)
        self.active_connections = set()
        self.connection_timeout = 30.0

    async def initialize(self):
        """åˆå§‹åŒ–è¿æ¥æ± """
        for i in range(self.pool_size):
            connection = AgentConnection(f"conn_{i}")
            await connection.initialize()
            await self.connections.put(connection)

    async def get_connection(self) -> AgentConnection:
        """è·å–è¿æ¥"""
        try:
            connection = await asyncio.wait_for(
                self.connections.get(),
                timeout=self.connection_timeout
            )
            self.active_connections.add(connection)
            return connection
        except asyncio.TimeoutError:
            raise ConnectionPoolError("è¿æ¥æ± è·å–è¶…æ—¶")

    async def release_connection(self, connection: AgentConnection):
        """é‡Šæ”¾è¿æ¥"""
        if connection in self.active_connections:
            self.active_connections.remove(connection)
            await self.connections.put(connection)

    async def execute_with_connection(
        self,
        agent_name: str,
        input_data: Dict[str, Any]
    ) -> TaskResult:
        """ä½¿ç”¨è¿æ¥æ‰§è¡ŒAgentè°ƒç”¨"""

        connection = None
        try:
            connection = await self.get_connection()
            result = await connection.execute_agent(agent_name, input_data)
            return result
        finally:
            if connection:
                await self.release_connection(connection)
```

---


## ğŸ›¡ï¸ å…­ã€é”™è¯¯å¤„ç†ä¸æ¢å¤

### 6.1 é”™è¯¯åˆ†ç±»ä¸å¤„ç†

```python
class ConcurrentErrorHandler:
    """å¹¶å‘ç¯å¢ƒé”™è¯¯å¤„ç†å™¨"""

    ERROR_TYPES = {
        "agent_timeout": {
            "handler": "_handle_timeout",
            "retry": True,
            "max_retries": 2,
            "backoff_factor": 2.0
        },
        "memory_error": {
            "handler": "_handle_memory_error",
            "retry": True,
            "max_retries": 1,
            "backoff_factor": 1.5
        },
        "content_truncation": {
            "handler": "_handle_truncation",
            "retry": True,
            "max_retries": 3,
            "backoff_factor": 1.0
        },
        "canvas_write_error": {
            "handler": "_handle_write_error",
            "retry": True,
            "max_retries": 3,
            "backoff_factor": 1.5
        },
        "agent_failure": {
            "handler": "_handle_agent_failure",
            "retry": False,
            "max_retries": 0,
            "backoff_factor": 1.0
        }
    }

    async def handle_error(
        self,
        error: Exception,
        task: AgentTask,
        retry_count: int = 0
    ) -> ErrorHandlingResult:
        """å¤„ç†å¹¶å‘æ‰§è¡Œé”™è¯¯"""

        error_type = self._classify_error(error)
        error_config = self.ERROR_TYPES.get(error_type, {})

        # è·å–å¤„ç†å‡½æ•°
        handler_name = error_config.get("handler", "_handle_generic_error")
        handler = getattr(self, handler_name)

        # æ‰§è¡Œé”™è¯¯å¤„ç†
        result = await handler(error, task, retry_count)

        # å†³å®šæ˜¯å¦é‡è¯•
        should_retry = (
            error_config.get("retry", False) and
            retry_count < error_config.get("max_retries", 0)
        )

        if should_retry:
            # è®¡ç®—é€€é¿å»¶è¿Ÿ
            backoff_factor = error_config.get("backoff_factor", 1.0)
            delay = min(300, (backoff_factor ** retry_count))  # æœ€å¤§5åˆ†é’Ÿ
            result.retry_delay = delay

        return result

    async def _handle_truncation(
        self,
        error: Exception,
        task: AgentTask,
        retry_count: int
    ) -> ErrorHandlingResult:
        """å¤„ç†å†…å®¹æˆªæ–­é”™è¯¯"""

        result = ErrorHandlingResult()

        if retry_count == 0:
            # ç¬¬ä¸€æ¬¡é‡è¯•ï¼šå¢åŠ ç¼“å†²åŒºå¤§å°
            result.modifications = {
                "buffer_size": task.resource_requirements.get("buffer_size", 8192) * 2,
                "chunk_size": max(500, task.resource_requirements.get("chunk_size", 1000) // 2)
            }
            result.message = "æ£€æµ‹åˆ°å†…å®¹æˆªæ–­ï¼Œå¢åŠ ç¼“å†²åŒºå¤§å°é‡è¯•"

        elif retry_count == 1:
            # ç¬¬äºŒæ¬¡é‡è¯•ï¼šåˆ†æ®µå¤„ç†
            result.modifications = {
                "processing_mode": "segmented",
                "max_segment_length": 2000
            }
            result.message = "åˆ‡æ¢åˆ°åˆ†æ®µå¤„ç†æ¨¡å¼"

        else:
            # ç¬¬ä¸‰æ¬¡é‡è¯•ï¼šä½¿ç”¨å¤‡ç”¨Agent
            backup_agent = self._get_backup_agent(task.agent_name)
            if backup_agent:
                result.modifications = {
                    "agent_name": backup_agent,
                    "fallback_mode": True
                }
                result.message = f"ä½¿ç”¨å¤‡ç”¨Agent: {backup_agent}"

        return result

    async def _handle_memory_error(
        self,
        error: Exception,
        task: AgentTask,
        retry_count: int
    ) -> ErrorHandlingResult:
        """å¤„ç†å†…å­˜é”™è¯¯"""

        result = ErrorHandlingResult()

        # é‡Šæ”¾å…¶ä»–ä»»åŠ¡èµ„æº
        await self._release_low_priority_tasks()

        # å‡å°‘å†…å­˜éœ€æ±‚
        reduced_memory = task.resource_requirements.get("memory", 512) // 2
        result.modifications = {
            "memory": reduced_memory,
            "processing_mode": "streaming"
        }
        result.message = f"å†…å­˜ä¸è¶³ï¼Œå‡å°‘å†…å­˜åˆ†é…è‡³{reduced_memory}MB"

        return result
```

### 6.2 çŠ¶æ€æ¢å¤æœºåˆ¶

```python
class StateRecoveryManager:
    """çŠ¶æ€æ¢å¤ç®¡ç†å™¨"""

    def __init__(self, recovery_dir: str = ".recovery"):
        self.recovery_dir = Path(recovery_dir)
        self.recovery_dir.mkdir(exist_ok=True)
        self.checkpoints = {}

    async def create_checkpoint(
        self,
        session_id: str,
        tasks: List[AgentTask],
        completed_results: List[TaskResult]
    ):
        """åˆ›å»ºæ£€æŸ¥ç‚¹"""

        checkpoint = {
            "session_id": session_id,
            "timestamp": time.time(),
            "pending_tasks": [asdict(task) for task in tasks],
            "completed_results": [asdict(result) for result in completed_results],
            "system_state": await self._capture_system_state()
        }

        checkpoint_file = self.recovery_dir / f"{session_id}_{int(time.time())}.json"
        with open(checkpoint_file, 'w', encoding='utf-8') as f:
            json.dump(checkpoint, f, ensure_ascii=False, indent=2)

        self.checkpoints[session_id] = checkpoint_file

    async def recover_from_checkpoint(
        self,
        session_id: str
    ) -> Optional[RecoveryState]:
        """ä»æ£€æŸ¥ç‚¹æ¢å¤"""

        if session_id not in self.checkpoints:
            # æŸ¥æ‰¾æœ€æ–°çš„æ£€æŸ¥ç‚¹
            checkpoint_files = list(self.recovery_dir.glob(f"{session_id}_*.json"))
            if not checkpoint_files:
                return None

            checkpoint_file = max(checkpoint_files, key=lambda x: x.stat().st_mtime)
        else:
            checkpoint_file = self.checkpoints[session_id]

        try:
            with open(checkpoint_file, 'r', encoding='utf-8') as f:
                checkpoint_data = json.load(f)

            recovery_state = RecoveryState(
                session_id=checkpoint_data["session_id"],
                pending_tasks=[
                    AgentTask(**task_data)
                    for task_data in checkpoint_data["pending_tasks"]
                ],
                completed_results=[
                    TaskResult(**result_data)
                    for result_data in checkpoint_data["completed_results"]
                ],
                system_state=checkpoint_data["system_state"]
            )

            await self._restore_system_state(recovery_state.system_state)

            return recovery_state

        except Exception as e:
            print(f"æ¢å¤å¤±è´¥: {e}")
            return None
```

---
