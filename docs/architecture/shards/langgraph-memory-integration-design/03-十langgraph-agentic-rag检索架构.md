# LANGGRAPH-MEMORY-INTEGRATION-DESIGN - Part 3

**Source**: `LANGGRAPH-MEMORY-INTEGRATION-DESIGN.md`
**Sections**: ğŸ” åã€LangGraph Agentic RAGæ£€ç´¢æ¶æ„

---

## ğŸ” åã€LangGraph Agentic RAGæ£€ç´¢æ¶æ„

### 10.1 æ¦‚è¿°

**è®¾è®¡ç›®æ ‡**: å®ç°æ™ºèƒ½æ£€ç´¢3å±‚è®°å¿†ç³»ç»Ÿ(Graphiti + Temporal + Semantic)æ¥ç”Ÿæˆé«˜è´¨é‡ä¸ªæ€§åŒ–æ£€éªŒç™½æ¿ã€‚

**æ ¸å¿ƒæŒ‘æˆ˜**:
1. **å¤šæºæ£€ç´¢**: éœ€è¦ä»3ä¸ªå¼‚æ„æ•°æ®æº(Neo4jå›¾æ•°æ®åº“ + Neo4jæ—¶åºæ•°æ® + LanceDBå‘é‡åº“)æ£€ç´¢æ•°æ®
2. **æ£€ç´¢è´¨é‡è¯„ä¼°**: éœ€è¦è¯„ä¼°æ£€ç´¢ç»“æœçš„è´¨é‡,å†³å®šæ˜¯å¦éœ€è¦é‡å†™æŸ¥è¯¢
3. **æ™ºèƒ½å†³ç­–**: Agentéœ€è¦è‡ªä¸»å†³å®šè°ƒç”¨å“ªäº›å·¥å…·ã€å¦‚ä½•ç»„åˆç»“æœ

**è§£å†³æ–¹æ¡ˆ**: LangGraph Agentic RAGæ¨¡å¼

---

### 10.2 Agentic RAGæ¶æ„å…¨æ™¯

#### 10.2.1 æ¶æ„å›¾ï¼ˆå«GraphRAGæ™ºèƒ½è·¯ç”±ï¼‰

```mermaid
graph TB
    START([å¼€å§‹]) --> ROUTER[question_router]

    ROUTER -->|local: å…·ä½“æ¦‚å¿µæŸ¥è¯¢| A1[generate_query_local]
    ROUTER -->|global: æ•°æ®é›†çº§åˆ†æ| A2[generate_query_global]
    ROUTER -->|hybrid: å¤æ‚ç»¼åˆæŸ¥è¯¢| A3[generate_query_hybrid]

    A1 -->|éœ€è¦æ£€ç´¢| B1[local_search_tools]
    A2 -->|éœ€è¦æ£€ç´¢| B2[graphrag_global_tool]
    A3 -->|éœ€è¦æ£€ç´¢| B3[composite_search_tools]

    A1 -->|å¯ç›´æ¥å›ç­”| H[generate]
    A2 -->|å¯ç›´æ¥å›ç­”| H
    A3 -->|å¯ç›´æ¥å›ç­”| H

    B1 --> C[Graphiti Tool]
    B1 --> D[LanceDB Tool]
    B1 --> E[Temporal Tool]

    B2 --> F_global[GraphRAG Global Search]

    B3 --> C_composite[Graphiti Tool]
    B3 --> D_composite[LanceDB Tool]
    B3 --> E_composite[Temporal Tool]
    B3 --> F_composite[GraphRAG Tool]

    C --> F[grade_documents]
    D --> F
    E --> F

    F_global --> F

    C_composite --> F_fusion[fusion_rerank]
    D_composite --> F_fusion
    E_composite --> F_fusion
    F_composite --> F_fusion

    F_fusion --> F

    F -->|æ–‡æ¡£è´¨é‡ >= 0.5| H[generate]
    F -->|æ–‡æ¡£è´¨é‡ < 0.5| G[rewrite_query]

    G --> ROUTER

    H --> END([ç»“æŸ])

    style ROUTER fill:#ffeb3b
    style A1 fill:#e1f5fe
    style A2 fill:#e1f5fe
    style A3 fill:#e1f5fe
    style B1 fill:#fff3e0
    style B2 fill:#fff3e0
    style B3 fill:#fff3e0
    style F fill:#f3e5f5
    style F_fusion fill:#f3e5f5
    style G fill:#ffcdd2
    style H fill:#c8e6c9
    style F_global fill:#b2dfdb
```

**æ¶æ„å›¾è¯´æ˜**:

1. **Question Routerï¼ˆé»„è‰²ï¼‰**: æ™ºèƒ½é—®é¢˜åˆ†ç±»å™¨ï¼Œæ ¹æ®æŸ¥è¯¢ç±»å‹è·¯ç”±åˆ°ä¸åŒçš„æ£€ç´¢ç­–ç•¥
   - **local**: å…·ä½“æ¦‚å¿µæŸ¥è¯¢ï¼ˆå¦‚"ä»€ä¹ˆæ˜¯é€†å¦å‘½é¢˜ï¼Ÿ"ï¼‰â†’ Graphiti + LanceDB + Temporal
   - **global**: æ•°æ®é›†çº§åˆ†æï¼ˆå¦‚"ç¦»æ•£æ•°å­¦ä¸­å“ªäº›æ¦‚å¿µæœ€å®¹æ˜“æ··æ·†ï¼Ÿ"ï¼‰â†’ GraphRAG Global Search
   - **hybrid**: å¤æ‚ç»¼åˆæŸ¥è¯¢ï¼ˆå¦‚"é€»è¾‘æ¨ç†çš„æ ¸å¿ƒæ¦‚å¿µå’Œæˆ‘çš„è–„å¼±ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ"ï¼‰â†’ å››å±‚å¹¶è¡Œæ£€ç´¢

2. **ä¸‰ç§æ£€ç´¢è·¯å¾„**:
   - **Local Search Path (B1)**: ä¼ ç»Ÿä¸‰å±‚æ£€ç´¢ï¼ˆGraphiti + LanceDB + Temporalï¼‰
   - **Global Search Path (B2)**: ä»…GraphRAGå…¨å±€æœç´¢
   - **Composite Search Path (B3)**: å››å±‚å¹¶è¡Œæ£€ç´¢ + Fusion Reranking

3. **Fusion Rerankingï¼ˆç´«è‰²ï¼‰**: å¯¹å¹¶è¡Œæ£€ç´¢ç»“æœè¿›è¡Œèåˆé‡æ’åº
   - ä½¿ç”¨RRFï¼ˆReciprocal Rank Fusionï¼‰å¹³è¡¡å››ä¸ªæ•°æ®æº
   - å¯é€‰Cross-Encoderè¿›ä¸€æ­¥æå‡å‡†ç¡®æ€§

4. **Self-Correction Loop**: grade_documentsè¯„ä¼°è´¨é‡ä¸è¶³æ—¶ï¼Œè§¦å‘rewrite_queryå¹¶é‡æ–°è·¯ç”±
```

#### 10.2.2 æ ¸å¿ƒç»„ä»¶

| ç»„ä»¶ | ç±»å‹ | èŒè´£ | è¾“å…¥ | è¾“å‡º |
|------|------|------|------|------|
| **question_router** | Function Node | æ™ºèƒ½é—®é¢˜åˆ†ç±»ï¼šlocal/global/hybrid | Canvasä¸Šä¸‹æ–‡ + æŸ¥è¯¢ | è·¯ç”±å†³ç­–(query_type) |
| **generate_query_local** | Agent Node | ç”ŸæˆLocal SearchæŸ¥è¯¢ | Canvasä¸Šä¸‹æ–‡ + ç”¨æˆ·ç†è§£ | å†³ç­– + æŸ¥è¯¢è¯­å¥ |
| **generate_query_global** | Agent Node | ç”ŸæˆGlobal SearchæŸ¥è¯¢ | Canvasä¸Šä¸‹æ–‡ + åˆ†æéœ€æ±‚ | å†³ç­– + å…¨å±€æŸ¥è¯¢ |
| **generate_query_hybrid** | Agent Node | ç”ŸæˆHybrid SearchæŸ¥è¯¢ | Canvasä¸Šä¸‹æ–‡ + å¤æ‚éœ€æ±‚ | å†³ç­– + æ··åˆæŸ¥è¯¢ |
| **local_search_tools** | Tool Node | æ‰§è¡ŒGraphiti + LanceDB + Temporal | æŸ¥è¯¢è¯­å¥ | 3æºæ£€ç´¢ç»“æœ |
| **graphrag_global_tool** | Tool Node | æ‰§è¡ŒGraphRAGå…¨å±€æœç´¢ | å…¨å±€æŸ¥è¯¢ | ç¤¾åŒºæ‘˜è¦ + å…¨å±€ç­”æ¡ˆ |
| **composite_search_tools** | Tool Node | å¹¶è¡Œæ‰§è¡Œ4å±‚æ£€ç´¢ | æ··åˆæŸ¥è¯¢ | 4æºæ£€ç´¢ç»“æœ |
| **fusion_rerank** | Function Node | èåˆé‡æ’åº4æºç»“æœ | 4æºæ£€ç´¢ç»“æœ | èåˆåæ–‡æ¡£åˆ—è¡¨ |
| **grade_documents** | Function Node | è¯„ä¼°æ–‡æ¡£è´¨é‡ | æ£€ç´¢ç»“æœ | è´¨é‡è¯„åˆ†(0-1) |
| **rewrite_query** | Function Node | é‡å†™æŸ¥è¯¢ | ä½è´¨é‡æ–‡æ¡£ + åŸæŸ¥è¯¢ | æ–°æŸ¥è¯¢è¯­å¥ |
| **generate** | Function Node | ç”Ÿæˆæ£€éªŒé—®é¢˜ | é«˜è´¨é‡æ–‡æ¡£ | æ£€éªŒé—®é¢˜åˆ—è¡¨ |

**æ–°å¢ç»„ä»¶è¯¦è§£**:

1. **question_router**:
   - ä½¿ç”¨LLMåˆ†ææŸ¥è¯¢ç±»å‹ï¼ˆlocal/global/hybridï¼‰
   - å†³ç­–é€»è¾‘ï¼š
     - åŒ…å«"ä»€ä¹ˆæ˜¯"ã€"å®šä¹‰"ã€"å…·ä½“æ¦‚å¿µ" â†’ local
     - åŒ…å«"å“ªäº›"ã€"æ¯”è¾ƒ"ã€"æ•°æ®é›†çº§" â†’ global
     - åŒ…å«"æ ¸å¿ƒæ¦‚å¿µ+è–„å¼±ç‚¹"ã€"è·¯å¾„+å…³è”" â†’ hybrid

2. **fusion_rerank**:
   - ä½¿ç”¨RRFï¼ˆReciprocal Rank Fusionï¼‰èåˆGraphitiã€LanceDBã€Temporalã€GraphRAGå››ä¸ªæ•°æ®æº
   - å¯é€‰Cross-Encoderè¿›ä¸€æ­¥æå‡å‡†ç¡®æ€§
   - å»é‡ç­–ç•¥ï¼šåŸºäºconceptå­—æ®µåˆå¹¶ç›¸åŒæ¦‚å¿µçš„å¤šæºè¯æ®

3. **composite_search_tools**:
   - ä½¿ç”¨`asyncio.gather()`å¹¶è¡Œæ‰§è¡Œ4ä¸ªå·¥å…·
   - è¶…æ—¶æ§åˆ¶ï¼šå•ä¸ªå·¥å…·æœ€å¤š5ç§’ï¼Œæ€»è¶…æ—¶10ç§’
   - å¤±è´¥é™çº§ï¼šè‡³å°‘2ä¸ªå·¥å…·æˆåŠŸå³å¯ç»§ç»­

---

### 10.3 State Schemaå®šä¹‰

```python
# âœ… Verified from LangGraph Skill (references/llms-txt.md:8723-8825)
from typing import Annotated, Literal
from typing_extensions import TypedDict
from langgraph.graph.message import add_messages

class AgenticRAGState(TypedDict):
    """Agentic RAG State Schemaï¼ˆå«GraphRAGæ”¯æŒï¼‰"""

    # ========== è¾“å…¥å‚æ•° ==========
    canvas_file: str                  # Canvasæ–‡ä»¶è·¯å¾„
    canvas_context: dict              # Canvasä¸Šä¸‹æ–‡(çº¢/ç´«èŠ‚ç‚¹åˆ—è¡¨)
    user_understanding: str           # ç”¨æˆ·å·²æœ‰ç†è§£(é»„è‰²èŠ‚ç‚¹å†…å®¹)

    # ========== å¯¹è¯å†å² ==========
    messages: Annotated[list, add_messages]  # LangChainæ¶ˆæ¯ç´¯ç§¯

    # ========== è·¯ç”±å†³ç­– ==========
    query_type: Literal["local", "global", "hybrid"]  # æŸ¥è¯¢ç±»å‹è·¯ç”±
    routing_reasoning: str            # è·¯ç”±å†³ç­–ç†ç”±

    # ========== æ£€ç´¢ä¸­é—´ç»“æœ ==========
    current_query: str                # å½“å‰æŸ¥è¯¢è¯­å¥
    retrieval_attempts: int           # æ£€ç´¢å°è¯•æ¬¡æ•°
    retrieved_documents: list[dict]   # æ£€ç´¢åˆ°çš„æ–‡æ¡£åˆ—è¡¨

    # æ–‡æ¡£ç»“æ„ç¤ºä¾‹ï¼ˆå«GraphRAGï¼‰:
    # {
    #     "source": "graphrag",  # graphiti/lancedb/temporal/graphrag
    #     "content": "...",
    #     "metadata": {
    #         "concept": "é€»è¾‘æ¨ç†",
    #         "relevance_score": 0.85,
    #         "community_id": "c12",       # GraphRAGä¸“ç”¨
    #         "community_level": 2,        # GraphRAGä¸“ç”¨
    #         "last_error_date": "2025-11-10"
    #     }
    # }

    # ========== GraphRAGä¸“ç”¨å­—æ®µ ==========
    graphrag_results: dict | None     # GraphRAGå…¨å±€æœç´¢ç»“æœ
    # ç»“æ„:
    # {
    #     "answer": "ç»¼åˆç­”æ¡ˆ...",
    #     "communities": [{"id": "c1", "title": "é€»è¾‘æ¨ç†", "summary": "...", "weight": 0.8}],
    #     "sources": [{"entity_id": "e1", "entity_name": "é€†å¦å‘½é¢˜", "relevance": 0.9}],
    #     "confidence": 0.85
    # }

    # ========== èåˆé‡æ’åºå­—æ®µ ==========
    fusion_strategy: Literal["rrf", "cross_encoder", "linear"] | None  # èåˆç­–ç•¥
    fusion_weights: dict[str, float] | None  # æ•°æ®æºæƒé‡é…ç½®
    # ç¤ºä¾‹: {"graphiti": 0.3, "lancedb": 0.25, "temporal": 0.2, "graphrag": 0.25}

    # ========== æ–‡æ¡£è´¨é‡è¯„ä¼° ==========
    document_quality_score: float     # 0-1åˆ†æ•°
    quality_issues: list[str]         # è´¨é‡é—®é¢˜åˆ—è¡¨

    # ========== æœ€ç»ˆè¾“å‡º ==========
    verification_questions: list[dict]  # ç”Ÿæˆçš„æ£€éªŒé—®é¢˜

    # é—®é¢˜ç»“æ„ç¤ºä¾‹ï¼ˆå«GraphRAGæ¥æºï¼‰:
    # {
    #     "question": "ä»€ä¹ˆæ˜¯é€†å¦å‘½é¢˜?",
    #     "type": "åŸºç¡€å‹",  # çªç ´å‹/åŸºç¡€å‹/æ£€éªŒå‹/åº”ç”¨å‹
    #     "source_concept": "é€†å¦å‘½é¢˜",
    #     "difficulty": "ä¸­ç­‰",
    #     "data_sources": ["graphiti", "temporal", "graphrag"],  # å¯åŒ…å«graphrag
    #     "community_context": "é€»è¾‘æ¨ç†ç¤¾åŒº"  # GraphRAGæä¾›çš„ç¤¾åŒºèƒŒæ™¯
    # }

    # ========== æ‰§è¡ŒçŠ¶æ€ ==========
    workflow_stage: Literal[
        "routing",
        "query_generation",
        "tool_execution",
        "fusion_reranking",
        "document_grading",
        "query_rewriting",
        "question_generation",
        "completed"
    ]
    error_log: list[dict]             # é”™è¯¯æ—¥å¿—
```

---

### 10.4 Retrieval Toolså®šä¹‰

#### 10.4.1 Graphiti Hybrid Search Tool

```python
# âœ… Verified from Graphiti Skill
from langchain.tools import tool
from typing import List, Dict

@tool
async def graphiti_hybrid_search_tool(
    query: str,
    canvas_context: dict,
    max_results: int = 20,
    rerank_strategy: str = "rrf"
) -> List[Dict]:
    """
    Graphitiæ··åˆæ£€ç´¢å·¥å…· (Graph + Semantic + BM25)

    Args:
        query: æŸ¥è¯¢è¯­å¥
        canvas_context: Canvasä¸Šä¸‹æ–‡(åŒ…å«topic_node_id)
        max_results: æœ€å¤§ç»“æœæ•°
        rerank_strategy: é‡æ’åºç­–ç•¥(rrf/mmr/node_distance/cross_encoder/episode_mentions)

    Returns:
        æ£€ç´¢ç»“æœåˆ—è¡¨

    ä½¿ç”¨åœºæ™¯:
        - æ£€éªŒç™½æ¿ç”Ÿæˆ: æŸ¥è¯¢å†å²è–„å¼±ç‚¹å’Œç›¸å…³æ¦‚å¿µ
        - è‰¾å®¾æµ©æ–¯å¤ä¹ : æŸ¥è¯¢éœ€è¦å¤ä¹ çš„æ¦‚å¿µ
    """
    from app.core.graphiti_client import graphiti

    # Step 1: Graphitiæ··åˆæ£€ç´¢
    results = await graphiti.search(
        query=query,
        center_node_uuid=canvas_context.get("topic_node_id"),
        max_distance=2,              # å›¾éå†æœ€å¤§è·³æ•°
        num_results=max_results,
        rerank_strategy=rerank_strategy
    )

    # Step 2: æ ¼å¼åŒ–ç»“æœ
    formatted_results = []
    for result in results:
        formatted_results.append({
            "source": "graphiti",
            "content": result.get("text", ""),
            "metadata": {
                "concept": result.get("concept", ""),
                "node_id": result.get("uuid", ""),
                "relevance_score": result.get("score", 0.0),
                "graph_distance": result.get("distance", 0),
                "last_access": result.get("last_access", None),
                "mastery_level": result.get("mastery", 0.0)
            }
        })

    return formatted_results
```

**Rerankingç­–ç•¥é€‰æ‹©æŒ‡å—**:

| ç­–ç•¥ | é€‚ç”¨åœºæ™¯ | ä¼˜åŠ¿ | åŠ£åŠ¿ |
|------|---------|------|------|
| **rrf** (Reciprocal Rank Fusion) | æ£€éªŒç™½æ¿ç”Ÿæˆ(æ¨è) | å¹³è¡¡è¯­ä¹‰/å…³é”®è¯/å›¾å…³ç³» | æ— ç‰¹æ®Šåå¥½ |
| **mmr** (Maximal Marginal Relevance) | éœ€è¦å¤šæ ·æ€§çš„é—®é¢˜ç”Ÿæˆ | é¿å…é‡å¤æ¦‚å¿µ | å¯èƒ½é—æ¼é«˜ç›¸å…³å†…å®¹ |
| **node_distance** | å¼ºè°ƒæ¦‚å¿µå…³è”æ€§ | ä¼˜å…ˆè¿”å›å›¾ä¸­ç›¸è¿‘èŠ‚ç‚¹ | å¿½ç•¥è¯­ä¹‰ç›¸ä¼¼åº¦ |
| **cross_encoder** | æœ€é«˜å‡†ç¡®æ€§è¦æ±‚ | BERTäº¤å‰ç¼–ç å™¨,æœ€å‡†ç¡® | æœ€æ…¢(~500ms) |
| **episode_mentions** | è‰¾å®¾æµ©æ–¯å¤ä¹ æ¨è | ä¼˜å…ˆé«˜é¢‘é”™è¯¯æ¦‚å¿µ | å¿½ç•¥æ–°æ¦‚å¿µ |

---

#### 10.4.2 LanceDB Hybrid Search Tool

```python
# âœ… Verified from LanceDB Context7 (lancedb.search() + rerank pattern)
@tool
async def lancedb_hybrid_search_tool(
    query: str,
    n_results: int = 10,
    filter_metadata: dict = None,
    rerank_strategy: str = "rrf"
) -> List[Dict]:
    """
    LanceDBæ··åˆæ£€ç´¢å·¥å…· (Vector + BM25 Full-Text Search)

    Args:
        query: æŸ¥è¯¢è¯­å¥
        n_results: è¿”å›ç»“æœæ•°
        filter_metadata: å…ƒæ•°æ®è¿‡æ»¤(å¦‚document_type, canvas_file)
        rerank_strategy: é‡æ’åºç­–ç•¥
            - "rrf": Reciprocal Rank Fusion (é»˜è®¤,å¹³è¡¡å‘é‡å’Œå…³é”®è¯)
            - "linear": LinearCombinationReranker (å¯è°ƒæƒé‡)
            - "cross_encoder": CrossEncoderReranker (æœ€é«˜å‡†ç¡®æ€§,æœ€æ…¢)
            - "cohere": CohereReranker (éœ€è¦API key)

    Returns:
        ç›¸ä¼¼æ–‡æ¡£åˆ—è¡¨

    ä½¿ç”¨åœºæ™¯:
        - æ£€ç´¢å†å²è§£é‡Šæ–‡æ¡£(oral-explanation, clarification-pathç­‰)
        - æŸ¥æ‰¾ç›¸ä¼¼æ¦‚å¿µçš„å­¦ä¹ ææ–™
        - ç»“åˆè¯­ä¹‰ç›¸ä¼¼åº¦å’Œå…³é”®è¯åŒ¹é…çš„ç»¼åˆæ£€ç´¢

    æ€§èƒ½ç‰¹ç‚¹:
        - Vector search: è¯­ä¹‰ç†è§£,æ•æ‰æ¦‚å¿µç›¸ä¼¼æ€§
        - BM25 FTS: å…³é”®è¯ç²¾ç¡®åŒ¹é…,term frequencyä¼˜åŒ–
        - Hybrid: 100xæŸ¥è¯¢æ€§èƒ½æå‡ vs Parquet-based DBs
        - æŸ¥è¯¢å»¶è¿Ÿ: <100ms (1M vectors, p95)
    """
    from app.core.lancedb_client import lancedb_table
    from lancedb.rerankers import RRFReranker, LinearCombinationReranker, CrossEncoderReranker

    # Step 1: é€‰æ‹©Reranker
    rerankers = {
        "rrf": RRFReranker(),
        "linear": LinearCombinationReranker(weight=0.7),  # 0.7æƒé‡ç»™å‘é‡æ£€ç´¢
        "cross_encoder": CrossEncoderReranker(),
        # Cohere rerankeréœ€è¦API key,æŒ‰éœ€å¯ç”¨
        # "cohere": CohereReranker(api_key=os.getenv("COHERE_API_KEY"))
    }
    reranker = rerankers.get(rerank_strategy, RRFReranker())

    # Step 2: LanceDBæ··åˆæ£€ç´¢ (Vector + BM25)
    # âœ… Verified from LanceDB Context7: hybrid search pattern
    search_builder = lancedb_table.search(query, query_type="hybrid")

    # æ·»åŠ å…ƒæ•°æ®è¿‡æ»¤
    if filter_metadata:
        search_builder = search_builder.where(filter_metadata)

    # æ‰§è¡Œæ£€ç´¢å¹¶é‡æ’åº
    results = (
        search_builder
        .rerank(reranker=reranker)
        .limit(n_results)
        .to_list()
    )

    # Step 3: æ ¼å¼åŒ–ç»“æœ
    formatted_results = []
    for result in results:
        formatted_results.append({
            "source": "lancedb",
            "content": result.get("text", ""),
            "metadata": {
                "doc_path": result.get("doc_path", ""),
                "concept": result.get("concept", ""),
                "agent_type": result.get("agent_type", ""),
                "vector_score": result.get("_vector_score", 0.0),
                "fts_score": result.get("_fts_score", 0.0),
                "relevance_score": result.get("_relevance_score", 0.0),  # é‡æ’åºåç»¼åˆåˆ†æ•°
                "created_at": result.get("created_at", None)
            }
        })

    return formatted_results
```

**LanceDB Hybrid Searchä¼˜åŠ¿**:

| ç‰¹æ€§ | LanceDB Hybrid | ä¼ ç»ŸChromaDB | æå‡ |
|------|----------------|-------------|------|
| **æŸ¥è¯¢æ€§èƒ½** | <100ms (1M vectors) | ~150-200ms | **2x faster** |
| **æ£€ç´¢æ–¹å¼** | Vector + BM25 + Rerank | çº¯å‘é‡æ£€ç´¢ | **æ›´å…¨é¢** |
| **å¯æ‰©å±•æ€§** | 1B+ vectors | <500K optimal | **2000x scalability** |
| **å…³é”®è¯åŒ¹é…** | BM25å…¨æ–‡ç´¢å¼•(ç²¾ç¡®) | æ— (éœ€è¯­ä¹‰è¿‘ä¼¼) | **æ–°å¢èƒ½åŠ›** |
| **é‡æ’åºç­–ç•¥** | 5ç§(RRF/Linear/CrossEncoderç­‰) | æ—  | **æ›´æ™ºèƒ½** |

**Rerankeré€‰æ‹©æŒ‡å—**:

| Reranker | é€‚ç”¨åœºæ™¯ | æ€§èƒ½ | å‡†ç¡®æ€§ |
|----------|---------|------|--------|
| **RRF** | æ£€éªŒç™½æ¿ç”Ÿæˆ(æ¨è) | â­â­â­â­â­ (<50ms) | â­â­â­â­ |
| **LinearCombination** | éœ€è¦è°ƒæ•´å‘é‡/å…³é”®è¯æƒé‡ | â­â­â­â­â­ (<50ms) | â­â­â­â­ |
| **CrossEncoder** | æœ€é«˜å‡†ç¡®æ€§è¦æ±‚ | â­â­ (<500ms) | â­â­â­â­â­ |
| **Cohere** | å¤šè¯­è¨€,é«˜è´¨é‡å•†ä¸šåº”ç”¨ | â­â­â­ (APIè°ƒç”¨) | â­â­â­â­â­ |

**ä½¿ç”¨ç¤ºä¾‹**:

```python
# ç¤ºä¾‹1: æ£€ç´¢"é€†å¦å‘½é¢˜"ç›¸å…³çš„è§£é‡Šæ–‡æ¡£
results = await lancedb_hybrid_search_tool(
    query="é€†å¦å‘½é¢˜è§£é‡Š",
    n_results=10,
    filter_metadata={"document_type": "explanation"},
    rerank_strategy="rrf"
)

# ç¤ºä¾‹2: æŸ¥æ‰¾ç‰¹å®šCanvasçš„å­¦ä¹ ææ–™(ä½¿ç”¨CrossEncoderè·å¾—æœ€é«˜å‡†ç¡®æ€§)
results = await lancedb_hybrid_search_tool(
    query="ç¦»æ•£æ•°å­¦ é€»è¾‘æ¨ç†",
    n_results=20,
    filter_metadata={"canvas_file": "ç¦»æ•£æ•°å­¦.canvas"},
    rerank_strategy="cross_encoder"
)
```

---

#### 10.4.3 Temporal Behavior Query Tool

```python
@tool
async def temporal_behavior_query_tool(
    canvas_file: str,
    query_type: str = "weak_points",
    time_range_days: int = 30
) -> List[Dict]:
    """
    Temporal Memoryè¡Œä¸ºæŸ¥è¯¢å·¥å…·

    Args:
        canvas_file: Canvasæ–‡ä»¶è·¯å¾„
        query_type: æŸ¥è¯¢ç±»å‹
            - "weak_points": å†å²è–„å¼±ç‚¹
            - "inactive_mastered": é•¿æœŸæœªè®¿é—®çš„å·²æŒæ¡æ¦‚å¿µ
            - "recent_errors": æœ€è¿‘é”™è¯¯è®°å½•
        time_range_days: æ—¶é—´èŒƒå›´(å¤©æ•°)

    Returns:
        è¡Œä¸ºæ•°æ®åˆ—è¡¨

    ä½¿ç”¨åœºæ™¯:
        - æ£€éªŒç™½æ¿ç”Ÿæˆ: æŸ¥è¯¢å†å²è–„å¼±ç‚¹(70%æƒé‡)
        - è‰¾å®¾æµ©æ–¯å¤ä¹ : æŸ¥è¯¢éœ€è¦å¤ä¹ çš„æ¦‚å¿µ
    """
    from app.core.temporal_memory import temporal_memory_client
    from datetime import datetime, timedelta

    cutoff_date = datetime.now() - timedelta(days=time_range_days)

    if query_type == "weak_points":
        # æŸ¥è¯¢å†å²è¯„åˆ†<80çš„æ¦‚å¿µ
        cypher_query = f"""
        MATCH (c:Canvas {{path: "{canvas_file}"}})
        -[:CONTAINS]->(n:Node)
        -[:HAS_UNDERSTANDING_STATE]->(s:UnderstandingState)
        WHERE s.total_score < 80
          AND s.timestamp >= datetime('{cutoff_date.isoformat()}')
        RETURN n.id AS node_id,
               n.text AS concept,
               s.total_score AS score,
               s.timestamp AS error_date,
               s.accuracy AS accuracy,
               s.imagery AS imagery,
               s.completeness AS completeness,
               s.originality AS originality
        ORDER BY s.timestamp DESC, s.total_score ASC
        LIMIT 50
        """
    elif query_type == "inactive_mastered":
        # æŸ¥è¯¢>7å¤©æœªè®¿é—®ä½†è¯„åˆ†â‰¥80çš„æ¦‚å¿µ
        cypher_query = f"""
        MATCH (c:Canvas {{path: "{canvas_file}"}})
        -[:CONTAINS]->(n:Node)
        -[:HAS_UNDERSTANDING_STATE]->(s:UnderstandingState)
        WHERE s.total_score >= 80
          AND s.timestamp < datetime('{(datetime.now() - timedelta(days=7)).isoformat()}')
        RETURN n.id AS node_id,
               n.text AS concept,
               s.total_score AS score,
               s.timestamp AS last_access,
               duration.inDays(s.timestamp, datetime()).days AS days_inactive
        ORDER BY days_inactive DESC
        LIMIT 30
        """
    else:  # recent_errors
        cypher_query = f"""
        MATCH (c:Canvas {{path: "{canvas_file}"}})
        -[:CONTAINS]->(n:Node)
        -[r:HAS_UNDERSTANDING_STATE]->(s:UnderstandingState)
        WHERE s.color = '1'  // çº¢è‰²èŠ‚ç‚¹
          AND s.timestamp >= datetime('{cutoff_date.isoformat()}')
        RETURN n.id AS node_id,
               n.text AS concept,
               s.timestamp AS error_date,
               count(r) AS error_count
        ORDER BY error_count DESC, s.timestamp DESC
        LIMIT 20
        """

    # æ‰§è¡ŒæŸ¥è¯¢
    results = await temporal_memory_client.execute_cypher(cypher_query)

    # æ ¼å¼åŒ–ç»“æœ
    formatted_results = []
    for record in results:
        formatted_results.append({
            "source": "temporal",
            "content": record.get("concept", ""),
            "metadata": {
                "node_id": record.get("node_id", ""),
                "concept": record.get("concept", ""),
                "score": record.get("score", 0),
                "error_date": record.get("error_date", None),
                "days_inactive": record.get("days_inactive", 0),
                "error_count": record.get("error_count", 0),
                "accuracy": record.get("accuracy", 0),
                "imagery": record.get("imagery", 0),
                "completeness": record.get("completeness", 0),
                "originality": record.get("originality", 0)
            }
        })

    return formatted_results
```

---

#### 10.4.4 GraphRAG Global Search Tool

```python
# âœ… Verified from GraphRAG Integration Design (GRAPHRAG-INTEGRATION-DESIGN.md)
@tool
async def graphrag_global_search_tool(
    query: str,
    community_level: int = 2,
    max_data_tokens: int = 12000,
    temperature: float = 0.2
) -> Dict:
    """
    GraphRAGå…¨å±€æœç´¢å·¥å…· (Community-based Global Reasoning)

    Args:
        query: è‡ªç„¶è¯­è¨€æŸ¥è¯¢ï¼ˆæ•°æ®é›†çº§åˆ«çš„åˆ†ææ€§é—®é¢˜ï¼‰
        community_level: ç¤¾åŒºå±‚çº§ (0-3)
            - Level 0: æœ€ç»†ç²’åº¦ï¼ˆå•ä¸ªæ¦‚å¿µï¼‰
            - Level 1: æ¦‚å¿µç»„ï¼ˆæ¨èï¼Œå¦‚"é€»è¾‘æ¨ç†"ï¼‰
            - Level 2: ä¸»é¢˜åŸŸï¼ˆå¦‚"ç¦»æ•£æ•°å­¦åŸºç¡€"ï¼‰
            - Level 3: æ•°æ®é›†å…¨å±€ï¼ˆè·¨Canvasåˆ†æï¼‰
        max_data_tokens: æœ€å¤§ä¸Šä¸‹æ–‡tokenæ•°ï¼ˆé»˜è®¤12000ï¼‰
        temperature: LLMæ¸©åº¦ï¼ˆ0.0-1.0ï¼Œé»˜è®¤0.2ï¼‰

    Returns:
        å…¨å±€æœç´¢ç»“æœï¼ŒåŒ…å«ï¼š
        - answer: LLMç”Ÿæˆçš„ç»¼åˆç­”æ¡ˆ
        - communities: ç›¸å…³ç¤¾åŒºåˆ—è¡¨ï¼ˆå«æ‘˜è¦å’Œæƒé‡ï¼‰
        - sources: è¯æ®æ¥æºï¼ˆå®ä½“å’Œå…³ç³»ï¼‰
        - confidence: ç­”æ¡ˆç½®ä¿¡åº¦ï¼ˆ0.0-1.0ï¼‰

    ä½¿ç”¨åœºæ™¯:
        1. **æ•°æ®é›†çº§åˆ†æ**: "ç¦»æ•£æ•°å­¦ä¸­å“ªäº›æ¦‚å¿µæœ€å®¹æ˜“æ··æ·†ï¼Ÿ"
        2. **è·¨ä¸»é¢˜æ¯”è¾ƒ**: "é€»è¾‘æ¨ç†å’Œé›†åˆè®ºåœ¨å­¦ä¹ è·¯å¾„ä¸Šçš„ä¾èµ–å…³ç³»æ˜¯ä»€ä¹ˆï¼Ÿ"
        3. **å…¨å±€æ¨¡å¼è¯†åˆ«**: "æˆ‘åœ¨å“ªäº›ç±»å‹çš„é—®é¢˜ä¸Šæœ€å®¹æ˜“å‡ºé”™ï¼Ÿ"
        4. **å­¦ä¹ è·¯å¾„è§„åˆ’**: "ä»çº¿æ€§ä»£æ•°åˆ°æœºå™¨å­¦ä¹ çš„æœ€ä¼˜å­¦ä¹ è·¯å¾„æ˜¯ä»€ä¹ˆï¼Ÿ"

    æŠ€æœ¯ç‰¹ç‚¹:
        - **ç¤¾åŒºæ£€æµ‹**: Leidenç®—æ³•å±‚çº§èšç±»ï¼Œè‡ªåŠ¨è¯†åˆ«æ¦‚å¿µä¸»é¢˜
        - **å±‚çº§æ¨ç†**: æ”¯æŒ4å±‚ç¤¾åŒºå±‚çº§ï¼Œä»ç»†ç²’åº¦åˆ°å…¨å±€
        - **LLMé©±åŠ¨**: GPT-4oç»¼åˆå¤šä¸ªç¤¾åŒºæ‘˜è¦ï¼Œç”Ÿæˆå…¨å±€ç­”æ¡ˆ
        - **è¯æ®è¿½æº¯**: æ¯ä¸ªç­”æ¡ˆåŒ…å«æ¥æºç¤¾åŒºå’Œå®ä½“è¯æ®

    æ€§èƒ½ç‰¹ç‚¹:
        - æŸ¥è¯¢å»¶è¿Ÿ: 2-5ç§’ï¼ˆLevel 1-2ï¼‰ï¼Œ5-10ç§’ï¼ˆLevel 3ï¼‰
        - Tokenæ¶ˆè€—: 8K-15K tokens/queryï¼ˆå«ç¤¾åŒºæ‘˜è¦ä¸Šä¸‹æ–‡ï¼‰
        - é€‚ç”¨è§„æ¨¡: 100-10Kæ¦‚å¿µçš„æ•°æ®é›†

    ä¸Local Searchå¯¹æ¯”:
        | ç»´åº¦ | GraphRAG Global Search | Graphiti/LanceDB Local Search |
        |------|----------------------|-------------------------------|
        | æŸ¥è¯¢ç±»å‹ | åˆ†ææ€§ã€æ¯”è¾ƒæ€§ã€å…¨å±€æ€§ | æŸ¥æ‰¾æ€§ã€å®šä¹‰æ€§ã€å±€éƒ¨æ€§ |
        | ä¸Šä¸‹æ–‡èŒƒå›´ | æ•´ä¸ªæ•°æ®é›†æˆ–ä¸»é¢˜åŸŸ | å•ä¸ªæ¦‚å¿µæˆ–å…¶é‚»å±… |
        | ç­”æ¡ˆç”Ÿæˆ | LLMç»¼åˆå¤šç¤¾åŒºæ‘˜è¦ | ç›´æ¥è¿”å›æ£€ç´¢æ–‡æ¡£ |
        | å»¶è¿Ÿ | 2-10ç§’ | <200ms |
        | é€‚ç”¨è§„æ¨¡ | 100-10Kæ¦‚å¿µ | æ— é™åˆ¶ |
    """
    from graphrag.query.structured_search.global_search import GlobalSearch
    from graphrag.query.context_builder.community_context import CommunityContextBuilder
    from app.core.graphrag_client import (
        load_entities,
        load_communities,
        load_community_reports
    )

    # Step 1: åŠ è½½GraphRAGæ•°æ®ï¼ˆç¤¾åŒºç»“æ„ï¼‰
    entities = await load_entities()
    communities = await load_communities(level=community_level)
    community_reports = await load_community_reports(level=community_level)

    # Step 2: æ„å»ºç¤¾åŒºä¸Šä¸‹æ–‡
    context_builder = CommunityContextBuilder(
        entities=entities,
        communities=communities,
        community_reports=community_reports
    )

    # Step 3: åˆå§‹åŒ–GlobalSearch
    searcher = GlobalSearch(
        llm=ChatOpenAI(model="gpt-4o", temperature=temperature),
        context_builder=context_builder,
        max_data_tokens=max_data_tokens
    )

    # Step 4: æ‰§è¡Œå…¨å±€æœç´¢
    result = await searcher.asearch(query=query)

    # Step 5: è®¡ç®—ç­”æ¡ˆç½®ä¿¡åº¦
    confidence = calculate_confidence(
        result=result,
        num_communities=len(communities),
        num_sources=len(result.context_data.get("sources", []))
    )

    # Step 6: æ ¼å¼åŒ–è¿”å›ç»“æœ
    return {
        "source": "graphrag",
        "answer": result.response,
        "communities": [
            {
                "id": c.id,
                "title": c.title,
                "summary": c.summary,
                "weight": c.weight,
                "level": community_level
            }
            for c in result.context_data.get("communities", [])
        ],
        "sources": [
            {
                "entity_id": s.get("id"),
                "entity_name": s.get("name"),
                "entity_type": s.get("type"),
                "relevance": s.get("rank", 0.0)
            }
            for s in result.context_data.get("sources", [])
        ],
        "confidence": confidence,
        "metadata": {
            "query": query,
            "community_level": community_level,
            "num_communities": len(communities),
            "num_sources": len(result.context_data.get("sources", [])),
            "tokens_used": result.context_data.get("num_tokens", 0)
        }
    }


def calculate_confidence(result, num_communities: int, num_sources: int) -> float:
    """
    è®¡ç®—GraphRAGå…¨å±€æœç´¢çš„ç­”æ¡ˆç½®ä¿¡åº¦

    ç½®ä¿¡åº¦åŸºäºï¼š
    1. ç¤¾åŒºè¦†ç›–åº¦ï¼ˆæŸ¥è¯¢å‘½ä¸­å¤šå°‘ç¤¾åŒºï¼‰
    2. è¯æ®æ¥æºæ•°é‡ï¼ˆå¼•ç”¨å¤šå°‘å®ä½“ï¼‰
    3. LLMå“åº”é•¿åº¦ï¼ˆç­”æ¡ˆæ˜¯å¦è¯¦ç»†ï¼‰

    Returns:
        0.0-1.0ä¹‹é—´çš„ç½®ä¿¡åº¦åˆ†æ•°
    """
    # å› å­1: ç¤¾åŒºè¦†ç›–åº¦ (0.0-0.4)
    community_coverage = min(0.4, num_communities * 0.05)

    # å› å­2: è¯æ®æ¥æºå……åˆ†æ€§ (0.0-0.4)
    source_sufficiency = min(0.4, num_sources * 0.04)

    # å› å­3: ç­”æ¡ˆè¯¦ç»†ç¨‹åº¦ (0.0-0.2)
    response_length = len(result.response.split())
    detail_score = min(0.2, response_length / 500 * 0.2)

    return community_coverage + source_sufficiency + detail_score
```

**GraphRAGä½¿ç”¨åœºæ™¯æŒ‡å—**:

| æŸ¥è¯¢ç±»å‹ | ç¤ºä¾‹é—®é¢˜ | æ¨èLevel | é¢„æœŸå»¶è¿Ÿ |
|---------|---------|----------|---------|
| **ä¸»é¢˜å†…åˆ†æ** | "é€»è¾‘æ¨ç†ä¸­å“ªäº›æ¦‚å¿µæœ€å®¹æ˜“æ··æ·†ï¼Ÿ" | Level 1 | 2-3ç§’ |
| **ä¸»é¢˜é—´æ¯”è¾ƒ** | "çº¿æ€§ä»£æ•°å’Œæ¦‚ç‡è®ºçš„å…³è”æ˜¯ä»€ä¹ˆï¼Ÿ" | Level 2 | 3-5ç§’ |
| **æ•°æ®é›†å…¨å±€** | "æˆ‘çš„è–„å¼±ç¯èŠ‚ä¸»è¦é›†ä¸­åœ¨å“ªäº›é¢†åŸŸï¼Ÿ" | Level 3 | 5-10ç§’ |
| **å­¦ä¹ è·¯å¾„è§„åˆ’** | "ä»å¾®ç§¯åˆ†åˆ°æœºå™¨å­¦ä¹ çš„æœ€ä¼˜è·¯å¾„ï¼Ÿ" | Level 2 | 3-5ç§’ |

**ä¸Graphiti/LanceDBååŒç­–ç•¥**:

```python
# åœºæ™¯1: æ··åˆæ£€ç´¢ï¼ˆå…ˆGlobalåLocalï¼‰
# ç”¨æˆ·é—®é¢˜: "ç¦»æ•£æ•°å­¦ä¸­é€»è¾‘æ¨ç†çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä»€ä¹ˆï¼Ÿ"

# Step 1: GraphRAGè¯†åˆ«æ ¸å¿ƒä¸»é¢˜
global_result = await graphrag_global_search_tool(
    query="ç¦»æ•£æ•°å­¦é€»è¾‘æ¨ç†çš„æ ¸å¿ƒæ¦‚å¿µ",
    community_level=1
)
# è¿”å›: "é€»è¾‘æ¨ç†"ç¤¾åŒºï¼ŒåŒ…å«5ä¸ªæ ¸å¿ƒæ¦‚å¿µ

# Step 2: LanceDBæ£€ç´¢æ¯ä¸ªæ ¸å¿ƒæ¦‚å¿µçš„è¯¦ç»†è§£é‡Š
for concept in global_result["sources"]:
    details = await lancedb_hybrid_search_tool(
        query=concept["entity_name"],
        filter_metadata={"document_type": "explanation"}
    )
    concept["detailed_explanation"] = details

# Step 3: GraphitiæŸ¥è¯¢æ¦‚å¿µé—´çš„å…³ç³»
for concept in global_result["sources"]:
    relations = await graphiti_hybrid_search_tool(
        query=f"{concept['entity_name']} ç›¸å…³æ¦‚å¿µ",
        canvas_context={"topic_node_id": concept["entity_id"]}
    )
    concept["related_concepts"] = relations
```

**ä½¿ç”¨ç¤ºä¾‹**:

```python
# ç¤ºä¾‹1: å…¨å±€è–„å¼±ç‚¹åˆ†æï¼ˆæ£€éªŒç™½æ¿ç”Ÿæˆåœºæ™¯ï¼‰
result = await graphrag_global_search_tool(
    query="åœ¨ç¦»æ•£æ•°å­¦Canvasä¸­ï¼Œæˆ‘æœ€è–„å¼±çš„æ¦‚å¿µé›†ç¾¤æ˜¯ä»€ä¹ˆï¼Ÿå®ƒä»¬ä¹‹é—´æœ‰ä»€ä¹ˆå…±æ€§ï¼Ÿ",
    community_level=2
)
# è¿”å›:
# {
#   "answer": "æ‚¨çš„è–„å¼±æ¦‚å¿µä¸»è¦é›†ä¸­åœ¨'é€»è¾‘æ¨ç†'å’Œ'é›†åˆè®º'ä¸¤ä¸ªä¸»é¢˜...",
#   "communities": [
#     {"title": "é€»è¾‘æ¨ç†", "summary": "åŒ…å«å‘½é¢˜é€»è¾‘ã€é€†å¦å‘½é¢˜ç­‰6ä¸ªæ¦‚å¿µ", "weight": 0.8},
#     {"title": "é›†åˆè®º", "summary": "åŒ…å«é›†åˆè¿ç®—ã€å¹‚é›†ç­‰4ä¸ªæ¦‚å¿µ", "weight": 0.6}
#   ],
#   "confidence": 0.85
# }

# ç¤ºä¾‹2: å­¦ä¹ è·¯å¾„è§„åˆ’
result = await graphrag_global_search_tool(
    query="ä»çº¿æ€§ä»£æ•°åˆ°æœºå™¨å­¦ä¹ ï¼Œæˆ‘åº”è¯¥æŒ‰ä»€ä¹ˆé¡ºåºå­¦ä¹ ï¼Ÿå“ªäº›æ¦‚å¿µæ˜¯å¿…é¡»å…ˆæŒæ¡çš„ï¼Ÿ",
    community_level=2
)

# ç¤ºä¾‹3: è·¨Canvasæ¯”è¾ƒåˆ†æ
result = await graphrag_global_search_tool(
    query="æ¯”è¾ƒæˆ‘åœ¨'ç¦»æ•£æ•°å­¦'å’Œ'çº¿æ€§ä»£æ•°'ä¸¤ä¸ªCanvasä¸­çš„å­¦ä¹ æ¨¡å¼ï¼Œæœ‰ä»€ä¹ˆå…±åŒçš„è–„å¼±ç‚¹ï¼Ÿ",
    community_level=3
)
```

---

#### 10.4.5 æœ¬åœ°æ¨¡å‹é…ç½®ï¼ˆæˆæœ¬ä¼˜åŒ–ï¼‰

**èƒŒæ™¯**: åŸºäºADR-001å†³ç­–ï¼ŒGraphRAGé»˜è®¤ä½¿ç”¨**Qwen2.5-14B-Instructæœ¬åœ°æ¨¡å‹**ä½œä¸ºä¸»è¦LLMæä¾›å•†ï¼Œé€šè¿‡90% Qwen2.5 + 10% APIçš„æ··åˆç­–ç•¥ï¼Œå°†æœˆåº¦æˆæœ¬ä»$570é™ä½åˆ°$57ï¼ˆèŠ‚çœ90%ï¼‰ã€‚

**æ ¸å¿ƒé…ç½®**:

```python
# âœ… Verified from ADR-001 & Story GraphRAG.2 (æœ¬åœ°æ¨¡å‹é›†æˆ)
from langchain_community.llms import Ollama
from langchain_openai import ChatOpenAI
import random
from typing import Dict, Any

class HybridLLMProvider:
    """
    æ··åˆLLMæä¾›å•†ï¼ˆæœ¬åœ°æ¨¡å‹ä¼˜å…ˆç­–ç•¥ï¼‰

    ç­–ç•¥:
        - 90%è¯·æ±‚ä½¿ç”¨Qwen2.5-14Bï¼ˆæœ¬åœ°Ollamaï¼Œæˆæœ¬$0ï¼‰
        - 10%è¯·æ±‚ä½¿ç”¨gpt-4o-miniï¼ˆAPIé™çº§ï¼Œæˆæœ¬$0.02ï¼‰
        - æœ¬åœ°è¶…æ—¶/å¤±è´¥æ—¶è‡ªåŠ¨é™çº§åˆ°API

    æˆæœ¬ä¼˜åŒ–:
        - åŸè®¾è®¡: 100% gpt-4oï¼Œæœˆæˆæœ¬$570
        - ä¼˜åŒ–å: 90% Qwen2.5 + 10% gpt-4o-miniï¼Œæœˆæˆæœ¬$57
        - **èŠ‚çœ90%** ($513/æœˆ)

    è´¨é‡ä¿è¯:
        - Qwen2.5è´¨é‡: ~85%ï¼ˆä¸gpt-4oåŸºçº¿å¯¹æ¯”ï¼‰
        - gpt-4o-miniè´¨é‡: ~90%
        - æ··åˆç­–ç•¥ç»¼åˆè´¨é‡: ~86%
        - è´¨é‡ä¸‹é™: 14%ï¼ˆå¯æ¥å—èŒƒå›´ï¼Œæ¢å–90%æˆæœ¬èŠ‚çœï¼‰
    """

    def __init__(
        self,
        local_ratio: float = 0.9,
        local_timeout: int = 8,
        api_timeout: int = 5
    ):
        # æœ¬åœ°æ¨¡å‹ï¼ˆQwen2.5-14B via Ollamaï¼‰
        self.local_llm = Ollama(
            model="qwen2.5:14b-instruct",
            base_url="http://localhost:11434",
            temperature=0.2,
            timeout=local_timeout
        )

        # APIé™çº§æ¨¡å‹ï¼ˆgpt-4o-miniï¼‰
        self.api_llm = ChatOpenAI(
            model="gpt-4o-mini",
            temperature=0.2,
            timeout=api_timeout
        )

        self.local_ratio = local_ratio
        self.cost_tracker = CostTracker()

    async def ainvoke(self, prompt: str) -> str:
        """
        æ··åˆç­–ç•¥è°ƒç”¨ï¼ˆå¼‚æ­¥ï¼‰

        å†³ç­–æµç¨‹:
            1. éšæœºæ•°å†³å®šè·¯ç”±ï¼ˆ90%æœ¬åœ° vs 10%APIï¼‰
            2. å¦‚æœé€‰æ‹©æœ¬åœ°ï¼Œå…ˆå°è¯•æœ¬åœ°æ¨¡å‹
            3. æœ¬åœ°è¶…æ—¶/å¤±è´¥ â†’ è‡ªåŠ¨é™çº§åˆ°API
            4. è®°å½•æˆæœ¬å’Œé™çº§äº‹ä»¶
        """
        use_local = random.random() < self.local_ratio

        if use_local:
            try:
                # å°è¯•æœ¬åœ°æ¨¡å‹
                response = await self.local_llm.ainvoke(prompt)
                self.cost_tracker.record_local_call(model="qwen2.5:14b", cost=0.0)
                return response
            except TimeoutError:
                # è¶…æ—¶ â†’ é™çº§åˆ°API
                self.cost_tracker.record_degradation(reason="local_timeout")
                return await self._invoke_api(prompt)
            except Exception as e:
                # å¤±è´¥ â†’ é™çº§åˆ°API
                self.cost_tracker.record_degradation(reason="local_failure", error=str(e))
                return await self._invoke_api(prompt)
        else:
            # ç›´æ¥ä½¿ç”¨APIï¼ˆ10%æµé‡ï¼Œç”¨äºè´¨é‡å¯¹æ¯”ï¼‰
            return await self._invoke_api(prompt)

    async def _invoke_api(self, prompt: str) -> str:
        """APIè°ƒç”¨ï¼ˆgpt-4o-miniï¼‰"""
        response = await self.api_llm.ainvoke(prompt)

        # ä¼°ç®—tokenæ¶ˆè€—å’Œæˆæœ¬
        input_tokens = len(prompt) // 4  # ç²—ç•¥ä¼°ç®—
        output_tokens = len(response.content) // 4
        cost = self.cost_tracker.record_api_call(
            provider="openai",
            model="gpt-4o-mini",
            input_tokens=input_tokens,
            output_tokens=output_tokens
        )

        return response.content


# é›†æˆåˆ°GraphRAG Global Search
async def graphrag_global_search_with_local_model(
    query: str,
    community_level: int = 2,
    max_data_tokens: int = 12000
) -> Dict:
    """
    ä½¿ç”¨æœ¬åœ°æ¨¡å‹çš„GraphRAGå…¨å±€æœç´¢ï¼ˆæˆæœ¬ä¼˜åŒ–ç‰ˆæœ¬ï¼‰

    å˜æ›´:
        - LLM: gpt-4o â†’ HybridLLMProviderï¼ˆ90% Qwen2.5 + 10% gpt-4o-miniï¼‰
        - æœˆæˆæœ¬: $570 â†’ $57ï¼ˆèŠ‚çœ90%ï¼‰
        - è´¨é‡: 100% â†’ 86%ï¼ˆä¸‹é™14%ï¼‰
    """
    from graphrag.query.structured_search.global_search import GlobalSearch
    from graphrag.query.context_builder.community_context import CommunityContextBuilder

    # ä½¿ç”¨æ··åˆLLMæä¾›å•†ï¼ˆæœ¬åœ°ä¼˜å…ˆï¼‰
    hybrid_llm = HybridLLMProvider(
        local_ratio=0.9,
        local_timeout=8,
        api_timeout=5
    )

    # åŠ è½½ç¤¾åŒºæ•°æ®
    context_builder = CommunityContextBuilder(...)

    # åˆå§‹åŒ–GlobalSearchï¼ˆä½¿ç”¨æ··åˆLLMï¼‰
    searcher = GlobalSearch(
        llm=hybrid_llm,  # æ›¿æ¢åŸæ¥çš„ChatOpenAI(model="gpt-4o")
        context_builder=context_builder,
        max_data_tokens=max_data_tokens
    )

    # æ‰§è¡Œæœç´¢
    result = await searcher.asearch(query=query)

    return {
        "answer": result.response,
        "cost_info": {
            "monthly_cost_estimate": hybrid_llm.cost_tracker.get_monthly_cost(),
            "degradation_rate": hybrid_llm.cost_tracker.get_degradation_rate(),
            "local_success_rate": hybrid_llm.cost_tracker.get_local_success_rate()
        }
    }
```

**æˆæœ¬å¯¹æ¯”**:

| LLMé…ç½® | æˆæœ¬/æŸ¥è¯¢ | æœˆæˆæœ¬ï¼ˆ3000æŸ¥è¯¢ï¼‰ | è´¨é‡ | å»¶è¿Ÿï¼ˆP95ï¼‰ |
|---------|----------|------------------|------|------------|
| **åŸè®¾è®¡**: gpt-4oï¼ˆ100%ï¼‰ | $0.19 | $570 | 100% | 4.5ç§’ |
| **æ–¹æ¡ˆA**: gpt-4o-miniï¼ˆ100%ï¼‰ | $0.02 | $60 | 90% | 2.8ç§’ |
| **æ–¹æ¡ˆB**: Qwen2.5ï¼ˆ100%ï¼‰ | $0.00 | $0 | 85% | 5.2ç§’ |
| **æœ€ç»ˆæ–¹æ¡ˆ**: Qwen2.5ï¼ˆ90%ï¼‰+ gpt-4o-miniï¼ˆ10%ï¼‰ | $0.02 | **$57** | 86% | 5.0ç§’ |

**æ€§èƒ½ä¼˜åŒ–**:
- **æœ¬åœ°æ¨¡å‹**: RTX 4090 GPUæ¨ç†ï¼Œ~5ç§’/æŸ¥è¯¢ï¼ˆ14Bå‚æ•°ï¼‰
- **APIé™çº§**: ä»…åœ¨æœ¬åœ°è¶…æ—¶ï¼ˆ>8ç§’ï¼‰æˆ–å¤±è´¥æ—¶è§¦å‘
- **é™çº§ç‡ç›®æ ‡**: <5%ï¼ˆæœ¬åœ°æˆåŠŸç‡â‰¥95%ï¼‰

**æˆæœ¬ç›‘æ§**:
```python
# âœ… Verified from Story GraphRAG.5 (æˆæœ¬ç›‘æ§)
# æœˆåº¦æˆæœ¬å‘Šè­¦é˜ˆå€¼
COST_ALERT_THRESHOLDS = {
    "warning": 60.0,    # $60 (75%é¢„ç®—)ï¼Œå‘é€è­¦å‘Šé‚®ä»¶
    "critical": 80.0    # $80 (100%é¢„ç®—)ï¼Œè‡ªåŠ¨åˆ‡æ¢ä¸º100%æœ¬åœ°æ¨¡å¼
}

# å®æ—¶æˆæœ¬è¿½è¸ª
monthly_cost = hybrid_llm.cost_tracker.get_monthly_cost()
if monthly_cost >= COST_ALERT_THRESHOLDS["critical"]:
    # ç´§æ€¥é™çº§ï¼šåˆ‡æ¢ä¸º100%æœ¬åœ°æ¨¡å¼
    hybrid_llm.local_ratio = 1.0
    send_alert(f"æˆæœ¬è¶…æ ‡ï¼Œå·²åˆ‡æ¢ä¸º100%æœ¬åœ°æ¨¡å¼ï¼ˆæœˆæˆæœ¬: ${monthly_cost}ï¼‰")
elif monthly_cost >= COST_ALERT_THRESHOLDS["warning"]:
    # è­¦å‘Šï¼šå‘é€é‚®ä»¶æé†’
    send_alert(f"æˆæœ¬é¢„è­¦ï¼Œå½“å‰æœˆæˆæœ¬: ${monthly_cost}ï¼ˆç›®æ ‡<$80ï¼‰")
```

**è´¨é‡éªŒè¯**:
- **æµ‹è¯•æ•°æ®é›†**: 100ä¸ªCanvasæŸ¥è¯¢æ ·æœ¬
- **è¯„ä¼°æŒ‡æ ‡**: F1 Scoreï¼ˆå®ä½“æå–å‡†ç¡®æ€§ï¼‰
- **åŸºçº¿å¯¹æ¯”**: gpt-4oè´¨é‡ä½œä¸º100%åŸºçº¿
- **éªŒæ”¶æ ‡å‡†**: Qwen2.5è´¨é‡â‰¥85%ï¼ˆStory GraphRAG.2 AC 5ï¼‰

**éƒ¨ç½²è¦æ±‚**:
- **ç¡¬ä»¶**: RTX 4090 24GB VRAMï¼ˆæ¨èï¼‰æˆ–A100
- **è½¯ä»¶**: Ollama + Qwen2.5-14B-Instructæ¨¡å‹
- **ç½‘ç»œ**: æ— éœ€å¤–ç½‘ï¼ˆæœ¬åœ°æ¨ç†ï¼‰ï¼ŒAPIä½œä¸ºé™çº§æ–¹æ¡ˆéœ€è¦å¤–ç½‘
- **æˆæœ¬**: ä¸€æ¬¡æ€§ç¡¬ä»¶æŠ•èµ„$1600ï¼ˆRTX 4090ï¼‰ï¼Œ2.8ä¸ªæœˆå³å¯å›æœ¬

---

### 10.5 Agentic RAG Graphå®ç°

#### 10.5.1 å®Œæ•´Graphå®šä¹‰ï¼ˆå«GraphRAGæ™ºèƒ½è·¯ç”±ï¼‰

```python
# âœ… Verified from LangGraph Skill (references/llms-txt.md:8723-8825)
from langgraph.graph import StateGraph, START, END
from langgraph.prebuilt import ToolNode

# Step 1: å®šä¹‰toolsåˆ—è¡¨ï¼ˆæ‰©å±•ä¸º4ä¸ªå·¥å…·ï¼‰
tools = [
    graphiti_hybrid_search_tool,
    lancedb_hybrid_search_tool,
    temporal_behavior_query_tool,
    graphrag_global_search_tool  # æ–°å¢
]

# Step 2: åˆ›å»ºStateGraph
builder = StateGraph(AgenticRAGState)

# Step 3: æ·»åŠ èŠ‚ç‚¹
# 3.1 è·¯ç”±å’ŒæŸ¥è¯¢ç”ŸæˆèŠ‚ç‚¹
builder.add_node("question_router", question_router_node)  # æ–°å¢ï¼šæ™ºèƒ½è·¯ç”±
builder.add_node("generate_query_local", generate_query_or_respond_node)  # é‡å‘½å
builder.add_node("generate_query_global", generate_query_or_respond_node)  # å…¨å±€æŸ¥è¯¢ç”Ÿæˆ
builder.add_node("generate_query_hybrid", generate_query_or_respond_node)  # æ··åˆæŸ¥è¯¢ç”Ÿæˆ

# 3.2 æ£€ç´¢èŠ‚ç‚¹
builder.add_node("local_search_tools", ToolNode([
    graphiti_hybrid_search_tool,
    lancedb_hybrid_search_tool,
    temporal_behavior_query_tool
]))  # Localæ£€ç´¢ï¼š3ä¸ªå·¥å…·
builder.add_node("graphrag_global_search", graphrag_global_search_node)  # æ–°å¢ï¼šå…¨å±€æœç´¢
builder.add_node("composite_search", composite_search_node)  # æ–°å¢ï¼šç»„åˆæœç´¢

# 3.3 èåˆå’Œè¯„åˆ†èŠ‚ç‚¹
builder.add_node("fusion_rerank", fusion_rerank_node)  # æ–°å¢ï¼šèåˆé‡æ’åº
builder.add_node("grade_documents", grade_documents_node)  # æ–‡æ¡£è´¨é‡è¯„åˆ†

# 3.4 è‡ªæˆ‘ä¿®æ­£å’Œç”ŸæˆèŠ‚ç‚¹
builder.add_node("rewrite", rewrite_query_node)  # æŸ¥è¯¢é‡å†™
builder.add_node("generate", generate_questions_node)  # ç”Ÿæˆæ£€éªŒé—®é¢˜

# Step 4: æ·»åŠ è¾¹å’Œæ¡ä»¶è·¯ç”±
# 4.1 å…¥å£ï¼šä»STARTè¿›å…¥question_router
builder.add_edge(START, "question_router")

# 4.2 ä»question_routerè·¯ç”±åˆ°ä¸åŒçš„æŸ¥è¯¢ç”ŸæˆèŠ‚ç‚¹
builder.add_conditional_edges(
    "question_router",
    route_by_query_type,  # æ¡ä»¶å‡½æ•°ï¼šåŸºäºquery_typeè·¯ç”±
    {
        "local": "generate_query_local",
        "global": "generate_query_global",
        "hybrid": "generate_query_hybrid"
    }
)

# 4.3 ä»æŸ¥è¯¢ç”ŸæˆèŠ‚ç‚¹åˆ°æ£€ç´¢èŠ‚ç‚¹æˆ–ç›´æ¥ç”Ÿæˆ
# Localè·¯å¾„
builder.add_conditional_edges(
    "generate_query_local",
    route_query,  # æ¡ä»¶å‡½æ•°ï¼šæ˜¯å¦éœ€è¦æ£€ç´¢
    {
        "retrieve": "local_search_tools",
        "generate": "generate"
    }
)

# Globalè·¯å¾„
builder.add_conditional_edges(
    "generate_query_global",
    route_query,
    {
        "retrieve": "graphrag_global_search",
        "generate": "generate"
    }
)

# Hybridè·¯å¾„
builder.add_conditional_edges(
    "generate_query_hybrid",
    route_query,
    {
        "retrieve": "composite_search",
        "generate": "generate"
    }
)

# 4.4 æ£€ç´¢åå¤„ç†
# Localå’ŒGlobalæ£€ç´¢ç›´æ¥è¿›å…¥grade_documents
builder.add_edge("local_search_tools", "grade_documents")
builder.add_edge("graphrag_global_search", "grade_documents")

# Compositeæ£€ç´¢éœ€è¦å…ˆç»è¿‡fusion_rerank
builder.add_edge("composite_search", "fusion_rerank")
builder.add_edge("fusion_rerank", "grade_documents")

# 4.5 åŸºäºæ–‡æ¡£è´¨é‡å†³å®šä¸‹ä¸€æ­¥
builder.add_conditional_edges(
    "grade_documents",
    grade_documents_route,  # æ¡ä»¶å‡½æ•°ï¼šè´¨é‡è¯„åˆ†
    {
        "rewrite": "rewrite",
        "generate": "generate"
    }
)

# 4.6 æŸ¥è¯¢é‡å†™åé‡æ–°è·¯ç”±
builder.add_edge("rewrite", "question_router")  # é‡æ–°è¿›å…¥è·¯ç”±é€»è¾‘

# 4.7 ç”Ÿæˆå®Œæˆåç»“æŸ
builder.add_edge("generate", END)

# Step 5: ç¼–è¯‘graph
agentic_rag_graph = builder.compile()
```

**æ–°æ¶æ„çš„å…³é”®æ”¹è¿›**:

1. **æ™ºèƒ½è·¯ç”±å…¥å£**: æ‰€æœ‰æŸ¥è¯¢å…ˆç»è¿‡question_routeråˆ†ç±»
2. **ä¸‰ç§æ£€ç´¢è·¯å¾„**: Localï¼ˆ3å·¥å…·ï¼‰ã€Globalï¼ˆGraphRAGï¼‰ã€Hybridï¼ˆ4å·¥å…·å¹¶è¡Œï¼‰
3. **Fusion Reranking**: Hybridè·¯å¾„ä¸“äº«ï¼Œèåˆ4ä¸ªæ•°æ®æº
4. **Self-Correction Loop**: é‡å†™åé‡æ–°è·¯ç”±ï¼Œè€Œéå›åˆ°å›ºå®šèŠ‚ç‚¹
5. **çµæ´»æ‰©å±•**: å¯æ ¹æ®query_typeè½»æ¾æ·»åŠ æ–°çš„æ£€ç´¢ç­–ç•¥

**è·¯ç”±å†³ç­–ç¤ºä¾‹**:

```python
# ç¤ºä¾‹1ï¼šLocalè·¯ç”±
query = "ä»€ä¹ˆæ˜¯é€†å¦å‘½é¢˜ï¼Ÿ"
# question_router â†’ local â†’ generate_query_local â†’ local_search_tools â†’ grade_documents â†’ generate

# ç¤ºä¾‹2ï¼šGlobalè·¯ç”±
query = "ç¦»æ•£æ•°å­¦ä¸­å“ªäº›æ¦‚å¿µæœ€å®¹æ˜“æ··æ·†ï¼Ÿ"
# question_router â†’ global â†’ generate_query_global â†’ graphrag_global_search â†’ grade_documents â†’ generate

# ç¤ºä¾‹3ï¼šHybridè·¯ç”±
query = "é€»è¾‘æ¨ç†çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä»€ä¹ˆï¼Œæˆ‘åœ¨å“ªäº›æ–¹é¢æœ€è–„å¼±ï¼Ÿ"
# question_router â†’ hybrid â†’ generate_query_hybrid â†’ composite_search â†’ fusion_rerank â†’ grade_documents â†’ generate

# ç¤ºä¾‹4ï¼šSelf-Correction Loop
query = "ç¦»æ•£æ•°å­¦"  # æ¨¡ç³ŠæŸ¥è¯¢
# question_router â†’ local â†’ generate_query_local â†’ local_search_tools â†’ grade_documents (è´¨é‡ä½)
# â†’ rewrite â†’ question_router â†’ local â†’ ... (é‡è¯•)
```
```

---

#### 10.5.2 èŠ‚ç‚¹å®ç°ï¼ˆå«GraphRAGæ–°èŠ‚ç‚¹ï¼‰

**Node 0: question_router** (æ–°å¢)

```python
# âœ… Verified from GRAPHRAG-INTEGRATION-DESIGN.md (Question Router Pattern)
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
import json

async def question_router_node(state: AgenticRAGState) -> AgenticRAGState:
    """
    æ™ºèƒ½é—®é¢˜è·¯ç”±èŠ‚ç‚¹ï¼šåˆ†ç±»æŸ¥è¯¢ç±»å‹ï¼ˆlocal/global/hybridï¼‰

    è·¯ç”±å†³ç­–é€»è¾‘ï¼š
    - local: å…·ä½“æ¦‚å¿µæŸ¥è¯¢ã€å®šä¹‰æŸ¥æ‰¾ â†’ Graphiti + LanceDB + Temporal
    - global: æ•°æ®é›†çº§åˆ†æã€è·¨ä¸»é¢˜æ¯”è¾ƒ â†’ GraphRAG Global Search
    - hybrid: å¤æ‚ç»¼åˆæŸ¥è¯¢ï¼ˆå¦‚"æ ¸å¿ƒæ¦‚å¿µ+è–„å¼±ç‚¹"ï¼‰ â†’ å››å±‚å¹¶è¡Œæ£€ç´¢
    """

    router_prompt = ChatPromptTemplate.from_messages([
        ("system", """ä½ æ˜¯Canvaså­¦ä¹ ç³»ç»Ÿçš„æŸ¥è¯¢è·¯ç”±å™¨ã€‚
        åˆ†æç”¨æˆ·çš„æŸ¥è¯¢ï¼Œç¡®å®šæœ€ä½³æ£€ç´¢ç­–ç•¥ã€‚

        **è·¯ç”±è§„åˆ™**ï¼š

        1. **local** - å…·ä½“æ¦‚å¿µæŸ¥è¯¢ï¼ˆå±€éƒ¨æ£€ç´¢ï¼‰
           - è§¦å‘è¯ï¼šä»€ä¹ˆæ˜¯ã€å®šä¹‰ã€è§£é‡Šã€å¦‚ä½•ç†è§£
           - ç¤ºä¾‹ï¼š"ä»€ä¹ˆæ˜¯é€†å¦å‘½é¢˜ï¼Ÿ"ã€"è§£é‡Šé›†åˆè®ºçš„åŸºæœ¬æ¦‚å¿µ"
           - ç­–ç•¥ï¼šGraphitiå›¾éå† + LanceDBè¯­ä¹‰æ£€ç´¢ + Temporalå†å²æ•°æ®

        2. **global** - æ•°æ®é›†çº§åˆ†æï¼ˆå…¨å±€æ¨ç†ï¼‰
           - è§¦å‘è¯ï¼šå“ªäº›ã€æ¯”è¾ƒã€æ•´ä½“ã€æ•°æ®é›†çº§ã€è·¨ä¸»é¢˜
           - ç¤ºä¾‹ï¼š"ç¦»æ•£æ•°å­¦ä¸­å“ªäº›æ¦‚å¿µæœ€å®¹æ˜“æ··æ·†ï¼Ÿ"ã€"æˆ‘çš„è–„å¼±ç¯èŠ‚é›†ä¸­åœ¨å“ªäº›é¢†åŸŸï¼Ÿ"
           - ç­–ç•¥ï¼šGraphRAGç¤¾åŒºæ£€æµ‹ + å…¨å±€æ‘˜è¦ + LLMç»¼åˆåˆ†æ

        3. **hybrid** - å¤æ‚ç»¼åˆæŸ¥è¯¢ï¼ˆæ··åˆæ£€ç´¢ï¼‰
           - è§¦å‘è¯ï¼šåŒ…å«å¤šä¸ªç»´åº¦ï¼ˆå¦‚"æ ¸å¿ƒæ¦‚å¿µ+è–„å¼±ç‚¹"ã€"è·¯å¾„+å…³è”"ï¼‰
           - ç¤ºä¾‹ï¼š"é€»è¾‘æ¨ç†çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä»€ä¹ˆï¼Œæˆ‘åœ¨å“ªäº›æ–¹é¢æœ€è–„å¼±ï¼Ÿ"
           - ç­–ç•¥ï¼šGraphiti + LanceDB + Temporal + GraphRAGå››å±‚å¹¶è¡Œ

        è¿”å›JSONæ ¼å¼ï¼š
        {
            "query_type": "local|global|hybrid",
            "reasoning": "è·¯ç”±å†³ç­–ç†ç”±ï¼ˆ50å­—ä»¥å†…ï¼‰"
        }"""),
        ("user", """Canvasæ–‡ä»¶ï¼š{canvas_file}

æŸ¥è¯¢å†…å®¹ï¼š{query}

Canvasä¸Šä¸‹æ–‡ï¼ˆçº¢/ç´«èŠ‚ç‚¹æ•°é‡ï¼‰ï¼š
- çº¢è‰²èŠ‚ç‚¹ï¼ˆå®Œå…¨ä¸ç†è§£ï¼‰ï¼š{red_count}ä¸ª
- ç´«è‰²èŠ‚ç‚¹ï¼ˆä¼¼æ‡‚éæ‡‚ï¼‰ï¼š{purple_count}ä¸ª

è¯·åˆ†ææŸ¥è¯¢ç±»å‹å¹¶è·¯ç”±ã€‚""")
    ])

    llm = ChatOpenAI(model="gpt-4o", temperature=0)
    chain = router_prompt | llm

    # å‡†å¤‡è¾“å…¥æ•°æ®
    red_count = len([n for n in state["canvas_context"].get("nodes", []) if n.get("color") == "1"])
    purple_count = len([n for n in state["canvas_context"].get("nodes", []) if n.get("color") == "3"])

    result = await chain.ainvoke({
        "canvas_file": state["canvas_file"],
        "query": state.get("current_query", ""),
        "red_count": red_count,
        "purple_count": purple_count
    })

    # è§£æLLMå“åº”
    try:
        classification = json.loads(result.content)
    except json.JSONDecodeError:
        # é™çº§ç­–ç•¥ï¼šé»˜è®¤ä½¿ç”¨local
        classification = {
            "query_type": "local",
            "reasoning": "JSONè§£æå¤±è´¥ï¼Œé™çº§ä¸ºlocalæ£€ç´¢"
        }

    return {
        **state,
        "query_type": classification["query_type"],
        "routing_reasoning": classification["reasoning"],
        "workflow_stage": "routing",
        "messages": state["messages"] + [{"role": "system", "content": f"è·¯ç”±å†³ç­–ï¼š{classification['query_type']} | ç†ç”±ï¼š{classification['reasoning']}"}]
    }
```

**Node 1A: graphrag_global_search_node** (æ–°å¢)

```python
# âœ… Verified from GRAPHRAG-INTEGRATION-DESIGN.md
async def graphrag_global_search_node(state: AgenticRAGState) -> AgenticRAGState:
    """
    GraphRAGå…¨å±€æœç´¢èŠ‚ç‚¹ï¼šæ‰§è¡Œç¤¾åŒºçº§åˆ«çš„å…¨å±€æ¨ç†

    ä½¿ç”¨åœºæ™¯ï¼š
    - æ•°æ®é›†çº§åˆ†æï¼š"å“ªäº›æ¦‚å¿µæœ€å®¹æ˜“æ··æ·†ï¼Ÿ"
    - è·¨ä¸»é¢˜æ¯”è¾ƒï¼š"çº¿æ€§ä»£æ•°å’Œæ¦‚ç‡è®ºçš„å…³è”æ˜¯ä»€ä¹ˆï¼Ÿ"
    - å­¦ä¹ è·¯å¾„è§„åˆ’ï¼š"ä»å¾®ç§¯åˆ†åˆ°æœºå™¨å­¦ä¹ çš„æœ€ä¼˜è·¯å¾„ï¼Ÿ"
    """
    from graphrag.query.structured_search.global_search import GlobalSearch
    from graphrag.query.context_builder.community_context import CommunityContextBuilder
    from app.core.graphrag_client import (
        load_entities,
        load_communities,
        load_community_reports
    )

    # é»˜è®¤ä½¿ç”¨Level 2ï¼ˆä¸»é¢˜åŸŸçº§åˆ«ï¼‰
    community_level = state.get("graphrag_community_level", 2)

    # Step 1: åŠ è½½GraphRAGæ•°æ®
    entities = await load_entities()
    communities = await load_communities(level=community_level)
    community_reports = await load_community_reports(level=community_level)

    # Step 2: æ„å»ºç¤¾åŒºä¸Šä¸‹æ–‡
    context_builder = CommunityContextBuilder(
        entities=entities,
        communities=communities,
        community_reports=community_reports
    )

    # Step 3: æ‰§è¡Œå…¨å±€æœç´¢
    searcher = GlobalSearch(
        llm=ChatOpenAI(model="gpt-4o", temperature=0.2),
        context_builder=context_builder,
        max_data_tokens=12000
    )

    result = await searcher.asearch(query=state["current_query"])

    # Step 4: æ ¼å¼åŒ–ä¸ºç»Ÿä¸€æ–‡æ¡£æ ¼å¼
    formatted_docs = []

    # å°†GraphRAGå…¨å±€ç­”æ¡ˆè½¬æ¢ä¸ºæ–‡æ¡£
    formatted_docs.append({
        "source": "graphrag",
        "content": result.response,
        "metadata": {
            "concept": "å…¨å±€åˆ†æ",
            "relevance_score": 1.0,  # GraphRAGç­”æ¡ˆé»˜è®¤é«˜ç›¸å…³æ€§
            "community_level": community_level,
            "num_communities": len(result.context_data.get("communities", [])),
            "num_sources": len(result.context_data.get("sources", []))
        }
    })

    # å°†ç¤¾åŒºæ‘˜è¦ä¹Ÿä½œä¸ºæ–‡æ¡£
    for community in result.context_data.get("communities", []):
        formatted_docs.append({
            "source": "graphrag",
            "content": community.summary,
            "metadata": {
                "concept": community.title,
                "relevance_score": community.weight,
                "community_id": community.id,
                "community_level": community_level
            }
        })

    return {
        **state,
        "retrieved_documents": formatted_docs,
        "graphrag_results": {
            "answer": result.response,
            "communities": result.context_data.get("communities", []),
            "sources": result.context_data.get("sources", []),
            "confidence": calculate_graphrag_confidence(result, len(communities))
        },
        "workflow_stage": "tool_execution",
        "messages": state["messages"] + [{"role": "system", "content": f"GraphRAGå…¨å±€æœç´¢å®Œæˆï¼š{len(formatted_docs)}ä¸ªæ–‡æ¡£"}]
    }


def calculate_graphrag_confidence(result, num_communities: int) -> float:
    """è®¡ç®—GraphRAGç­”æ¡ˆç½®ä¿¡åº¦"""
    num_sources = len(result.context_data.get("sources", []))
    response_length = len(result.response.split())

    community_coverage = min(0.4, num_communities * 0.05)
    source_sufficiency = min(0.4, num_sources * 0.04)
    detail_score = min(0.2, response_length / 500 * 0.2)

    return community_coverage + source_sufficiency + detail_score
```

**Node 1B: composite_search_node** (æ–°å¢)

```python
# âœ… Verified from GRAPHRAG-INTEGRATION-DESIGN.md (Parallel Execution Pattern)
import asyncio

async def composite_search_node(state: AgenticRAGState) -> AgenticRAGState:
    """
    ç»„åˆæœç´¢èŠ‚ç‚¹ï¼šå¹¶è¡Œæ‰§è¡Œå››å±‚æ£€ç´¢ï¼ˆGraphiti + LanceDB + Temporal + GraphRAGï¼‰

    ä½¿ç”¨åœºæ™¯ï¼š
    - å¤æ‚ç»¼åˆæŸ¥è¯¢ï¼š"é€»è¾‘æ¨ç†çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä»€ä¹ˆï¼Œæˆ‘åœ¨å“ªäº›æ–¹é¢æœ€è–„å¼±ï¼Ÿ"
    - éœ€è¦å…¨æ–¹ä½ä¸Šä¸‹æ–‡çš„é—®é¢˜ç”Ÿæˆ
    """

    # å®šä¹‰å¹¶è¡Œä»»åŠ¡
    async def graphiti_task():
        result = await graphiti_hybrid_search_tool(
            query=state["current_query"],
            canvas_context=state["canvas_context"],
            max_results=20,
            rerank_strategy="rrf"
        )
        return result

    async def lancedb_task():
        result = await lancedb_hybrid_search_tool(
            query=state["current_query"],
            n_results=15,
            filter_metadata={"canvas_file": state["canvas_file"]},
            rerank_strategy="rrf"
        )
        return result

    async def temporal_task():
        result = await temporal_behavior_query_tool(
            canvas_file=state["canvas_file"],
            query_type="weak_points",
            time_range_days=30
        )
        return result

    async def graphrag_task():
        result = await graphrag_global_search_tool(
            query=state["current_query"],
            community_level=2,
            max_data_tokens=12000
        )
        return result

    # å¹¶è¡Œæ‰§è¡Œï¼ˆè¶…æ—¶æ§åˆ¶ï¼šå•ä¸ª5ç§’ï¼Œæ€»è®¡10ç§’ï¼‰
    try:
        results = await asyncio.wait_for(
            asyncio.gather(
                graphiti_task(),
                lancedb_task(),
                temporal_task(),
                graphrag_task(),
                return_exceptions=True
            ),
            timeout=10.0
        )
    except asyncio.TimeoutError:
        return {
            **state,
            "error_log": state.get("error_log", []) + [{
                "timestamp": datetime.now().isoformat(),
                "error": "Composite search timeout",
                "action": "Fallback to local search"
            }],
            "workflow_stage": "tool_execution"
        }

    # åˆå¹¶æ‰€æœ‰æˆåŠŸçš„ç»“æœ
    all_docs = []
    for i, result in enumerate(results):
        if isinstance(result, Exception):
            continue  # è·³è¿‡å¤±è´¥çš„ä»»åŠ¡
        all_docs.extend(result)

    # å¤±è´¥é™çº§ï¼šè‡³å°‘2ä¸ªå·¥å…·æˆåŠŸ
    successful_count = len([r for r in results if not isinstance(r, Exception)])
    if successful_count < 2:
        return {
            **state,
            "error_log": state.get("error_log", []) + [{
                "timestamp": datetime.now().isoformat(),
                "error": f"Only {successful_count}/4 tools succeeded",
                "action": "Quality may be degraded"
            }],
            "retrieved_documents": all_docs,
            "workflow_stage": "tool_execution"
        }

    return {
        **state,
        "retrieved_documents": all_docs,
        "workflow_stage": "fusion_reranking",
        "messages": state["messages"] + [{"role": "system", "content": f"ç»„åˆæœç´¢å®Œæˆï¼š{len(all_docs)}ä¸ªæ–‡æ¡£ï¼ˆ{successful_count}/4å·¥å…·æˆåŠŸï¼‰"}]
    }
```

**Node 1C: fusion_rerank_node** (æ–°å¢)

```python
# âœ… Verified from LanceDB Context7 (RRF Reranker Pattern)
from lancedb.rerankers import RRFReranker, CrossEncoderReranker
from collections import defaultdict

async def fusion_rerank_node(state: AgenticRAGState) -> AgenticRAGState:
    """
    èåˆé‡æ’åºèŠ‚ç‚¹ï¼šä½¿ç”¨RRFèåˆå››ä¸ªæ•°æ®æºçš„æ£€ç´¢ç»“æœ

    é‡æ’åºç­–ç•¥ï¼š
    - rrf (é»˜è®¤)ï¼šReciprocal Rank Fusionï¼Œå¹³è¡¡å„æ•°æ®æº
    - cross_encoderï¼šCrossEncoderæ¨¡å‹ï¼Œæœ€é«˜å‡†ç¡®æ€§ä½†æœ€æ…¢
    - linearï¼šçº¿æ€§åŠ æƒç»„åˆï¼ˆå¯é…ç½®æƒé‡ï¼‰
    """
    retrieved_docs = state.get("retrieved_documents", [])

    if not retrieved_docs:
        return {**state, "workflow_stage": "document_grading"}

    fusion_strategy = state.get("fusion_strategy", "rrf")

    if fusion_strategy == "rrf":
        # RRFèåˆï¼ˆæ¨èï¼‰
        reranked_docs = rrf_fusion(retrieved_docs)
    elif fusion_strategy == "cross_encoder":
        # CrossEncoderé‡æ’åºï¼ˆæœ€å‡†ç¡®ä½†æ…¢ï¼‰
        reranker = CrossEncoderReranker()
        reranked_docs = reranker.rerank(state["current_query"], retrieved_docs)
    elif fusion_strategy == "linear":
        # çº¿æ€§åŠ æƒèåˆ
        weights = state.get("fusion_weights", {
            "graphiti": 0.3,
            "lancedb": 0.25,
            "temporal": 0.2,
            "graphrag": 0.25
        })
        reranked_docs = linear_fusion(retrieved_docs, weights)
    else:
        reranked_docs = retrieved_docs

    # å»é‡ï¼šåŸºäºconceptå­—æ®µåˆå¹¶ç›¸åŒæ¦‚å¿µ
    deduplicated_docs = deduplicate_by_concept(reranked_docs)

    return {
        **state,
        "retrieved_documents": deduplicated_docs,
        "workflow_stage": "document_grading",
        "messages": state["messages"] + [{"role": "system", "content": f"èåˆé‡æ’åºå®Œæˆï¼š{len(reranked_docs)} â†’ {len(deduplicated_docs)}ä¸ªæ–‡æ¡£"}]
    }


def rrf_fusion(docs: list[dict], k: int = 60) -> list[dict]:
    """
    Reciprocal Rank Fusion (RRF)ç®—æ³•

    å…¬å¼ï¼šRRF_score(doc) = Î£ 1/(k + rank_source_i(doc))
    """
    # æŒ‰sourceåˆ†ç»„
    docs_by_source = defaultdict(list)
    for doc in docs:
        docs_by_source[doc["source"]].append(doc)

    # è®¡ç®—æ¯ä¸ªsourceå†…çš„rank
    for source, source_docs in docs_by_source.items():
        sorted_docs = sorted(source_docs, key=lambda d: d["metadata"].get("relevance_score", 0), reverse=True)
        for rank, doc in enumerate(sorted_docs, start=1):
            doc["rank"] = rank

    # è®¡ç®—RRFåˆ†æ•°
    rrf_scores = defaultdict(float)
    for doc in docs:
        doc_id = doc["metadata"].get("concept", "") + doc.get("content", "")[:50]
        rrf_scores[doc_id] += 1.0 / (k + doc.get("rank", 1))

    # é‡æ–°æ’åº
    for doc in docs:
        doc_id = doc["metadata"].get("concept", "") + doc.get("content", "")[:50]
        doc["rrf_score"] = rrf_scores[doc_id]

    return sorted(docs, key=lambda d: d.get("rrf_score", 0), reverse=True)


def linear_fusion(docs: list[dict], weights: dict[str, float]) -> list[dict]:
    """çº¿æ€§åŠ æƒèåˆ"""
    for doc in docs:
        source = doc["source"]
        weight = weights.get(source, 0.25)
        relevance = doc["metadata"].get("relevance_score", 0)
        doc["fusion_score"] = weight * relevance

    return sorted(docs, key=lambda d: d.get("fusion_score", 0), reverse=True)


def deduplicate_by_concept(docs: list[dict]) -> list[dict]:
    """åŸºäºconceptå­—æ®µå»é‡ï¼Œä¿ç•™æœ€é«˜åˆ†çš„æ–‡æ¡£"""
    concept_to_doc = {}
    for doc in docs:
        concept = doc["metadata"].get("concept", "")
        score = doc.get("rrf_score", doc.get("fusion_score", 0))

        if concept not in concept_to_doc or score > concept_to_doc[concept].get("rrf_score", 0):
            concept_to_doc[concept] = doc

    return list(concept_to_doc.values())
```

---

**Node 1: generate_query_or_respond** (åŸæœ‰èŠ‚ç‚¹ï¼Œéœ€æ›´æ–°)

```python
from langchain_anthropic import ChatAnthropic

async def generate_query_or_respond_node(state: AgenticRAGState) -> AgenticRAGState:
    """
    Agentå†³ç­–èŠ‚ç‚¹: æ˜¯å¦éœ€è¦æ£€ç´¢
    """
    # åˆå§‹åŒ–LLM
    llm = ChatAnthropic(model="claude-sonnet-4-5-20250929")
    llm_with_tools = llm.bind_tools(tools)

    # æ„å»ºç³»ç»Ÿæç¤º
    system_prompt = """ä½ æ˜¯Canvaså­¦ä¹ ç³»ç»Ÿçš„æ£€éªŒç™½æ¿ç”ŸæˆAgentã€‚

ä½ çš„ä»»åŠ¡: åŸºäºç”¨æˆ·çš„Canvaså­¦ä¹ æƒ…å†µ,ç”Ÿæˆä¸ªæ€§åŒ–æ£€éªŒé—®é¢˜ã€‚

ä¼˜å…ˆçº§ç­–ç•¥:
1. **70%æƒé‡**: å†å²è–„å¼±ç‚¹(è¯„åˆ†<80çš„æ¦‚å¿µ)
2. **30%æƒé‡**: å·²æŒæ¡æ¦‚å¿µ(è¯„åˆ†â‰¥80,ç”¨äºå·©å›º)

å¯ç”¨å·¥å…·:
- graphiti_hybrid_search_tool: æ£€ç´¢å†å²è–„å¼±ç‚¹å’Œç›¸å…³æ¦‚å¿µ(æ¨èä½¿ç”¨rerank_strategy='rrf')
- lancedb_hybrid_search_tool: æ··åˆæ£€ç´¢è§£é‡Šæ–‡æ¡£(Vector + BM25, æ¨èrerank_strategy='rrf')
- temporal_behavior_query_tool: æŸ¥è¯¢å­¦ä¹ è¡Œä¸ºæ•°æ®(æ¨èquery_type='weak_points')

å†³ç­–é€»è¾‘:
- å¦‚æœCanvasæœ‰çº¢è‰²/ç´«è‰²èŠ‚ç‚¹ â†’ è°ƒç”¨temporal_behavior_query_toolæŸ¥è¯¢å†å²è–„å¼±ç‚¹
- å¦‚æœéœ€è¦æ¦‚å¿µå…³è” â†’ è°ƒç”¨graphiti_hybrid_search_tool
- å¦‚æœéœ€è¦å‚è€ƒè§£é‡Šæ–‡æ¡£ â†’ è°ƒç”¨lancedb_hybrid_search_tool
- å¦‚æœå·²æœ‰è¶³å¤Ÿä¿¡æ¯ â†’ ç›´æ¥ç”Ÿæˆæ£€éªŒé—®é¢˜
"""

    # æ„å»ºç”¨æˆ·æ¶ˆæ¯
    user_message = f"""
Canvasæ–‡ä»¶: {state['canvas_file']}

Canvasä¸Šä¸‹æ–‡:
{json.dumps(state['canvas_context'], ensure_ascii=False, indent=2)}

ç”¨æˆ·å·²æœ‰ç†è§£:
{state['user_understanding']}

è¯·å†³ç­–: æ˜¯å¦éœ€è¦æ£€ç´¢3å±‚è®°å¿†ç³»ç»Ÿ? å¦‚æœéœ€è¦,è°ƒç”¨ç›¸åº”çš„å·¥å…·ã€‚
"""

    # è°ƒç”¨LLM
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_message}
    ]

    response = await llm_with_tools.ainvoke(messages)

    # æ›´æ–°State
    return {
        **state,
        "messages": state["messages"] + [response],
        "workflow_stage": "query_generation",
        "retrieval_attempts": state.get("retrieval_attempts", 0) + 1
    }
```

**Node 2: grade_documents**

```python
async def grade_documents_node(state: AgenticRAGState) -> AgenticRAGState:
    """
    æ–‡æ¡£è´¨é‡è¯„åˆ†èŠ‚ç‚¹
    """
    retrieved_docs = state.get("retrieved_documents", [])

    if not retrieved_docs:
        return {
            **state,
            "document_quality_score": 0.0,
            "quality_issues": ["No documents retrieved"],
            "workflow_stage": "document_grading"
        }

    # è¯„åˆ†æ ‡å‡†
    quality_score = 0.0
    quality_issues = []

    # 1. æ£€æŸ¥è–„å¼±ç‚¹è¦†ç›–ç‡ (40%æƒé‡)
    temporal_docs = [d for d in retrieved_docs if d["source"] == "temporal"]
    weak_point_coverage = len(temporal_docs) / max(len(retrieved_docs), 1)
    quality_score += weak_point_coverage * 0.4

    if weak_point_coverage < 0.5:
        quality_issues.append("è–„å¼±ç‚¹è¦†ç›–ç‡ä¸è¶³(<50%)")

    # 2. æ£€æŸ¥ç›¸å…³æ€§åˆ†æ•° (30%æƒé‡)
    avg_relevance = sum(d["metadata"].get("relevance_score", 0) for d in retrieved_docs) / len(retrieved_docs)
    quality_score += avg_relevance * 0.3

    if avg_relevance < 0.6:
        quality_issues.append(f"å¹³å‡ç›¸å…³æ€§åˆ†æ•°è¿‡ä½({avg_relevance:.2f})")

    # 3. æ£€æŸ¥æ•°æ®æºå¤šæ ·æ€§ (20%æƒé‡)
    sources = set(d["source"] for d in retrieved_docs)
    source_diversity = len(sources) / 3.0  # æœ€å¤š3ä¸ªæ•°æ®æº
    quality_score += source_diversity * 0.2

    if len(sources) < 2:
        quality_issues.append("æ•°æ®æºå•ä¸€(å»ºè®®è‡³å°‘2ä¸ª)")

    # 4. æ£€æŸ¥æ–‡æ¡£æ•°é‡ (10%æƒé‡)
    doc_count_score = min(len(retrieved_docs) / 20.0, 1.0)  # ç†æƒ³20ä¸ªæ–‡æ¡£
    quality_score += doc_count_score * 0.1

    if len(retrieved_docs) < 10:
        quality_issues.append(f"æ–‡æ¡£æ•°é‡ä¸è¶³({len(retrieved_docs)}<10)")

    return {
        **state,
        "document_quality_score": quality_score,
        "quality_issues": quality_issues,
        "workflow_stage": "document_grading"
    }
```

**Node 3: rewrite_query**

```python
async def rewrite_query_node(state: AgenticRAGState) -> AgenticRAGState:
    """
    æŸ¥è¯¢é‡å†™èŠ‚ç‚¹
    """
    llm = ChatAnthropic(model="claude-sonnet-4-5-20250929")

    system_prompt = """ä½ æ˜¯æŸ¥è¯¢ä¼˜åŒ–ä¸“å®¶ã€‚

å½“å‰æŸ¥è¯¢çš„é—®é¢˜:
{quality_issues}

è¯·é‡å†™æŸ¥è¯¢,æ”¹è¿›ç­–ç•¥:
1. å¦‚æœè–„å¼±ç‚¹è¦†ç›–ç‡ä¸è¶³ â†’ æ˜ç¡®è¦æ±‚temporal_behavior_query_toolä½¿ç”¨query_type='weak_points'
2. å¦‚æœç›¸å…³æ€§åˆ†æ•°ä½ â†’ ä¼˜åŒ–æŸ¥è¯¢è¯­å¥,ä½¿ç”¨æ›´ç²¾ç¡®çš„å…³é”®è¯
3. å¦‚æœæ•°æ®æºå•ä¸€ â†’ æ˜ç¡®è¦æ±‚è°ƒç”¨å¤šä¸ªå·¥å…·(è‡³å°‘2ä¸ª)
4. å¦‚æœæ–‡æ¡£æ•°é‡ä¸è¶³ â†’ å¢åŠ max_resultså‚æ•°

åŸæŸ¥è¯¢: {original_query}
"""

    response = await llm.ainvoke([{
        "role": "system",
        "content": system_prompt.format(
            quality_issues="\n".join(state["quality_issues"]),
            original_query=state.get("current_query", "")
        )
    }])

    return {
        **state,
        "current_query": response.content,
        "workflow_stage": "query_rewriting"
    }
```

**Node 4: generate_questions**

```python
async def generate_questions_node(state: AgenticRAGState) -> AgenticRAGState:
    """
    ç”Ÿæˆæ£€éªŒé—®é¢˜èŠ‚ç‚¹
    """
    llm = ChatAnthropic(model="claude-sonnet-4-5-20250929")

    # æ„å»ºæ–‡æ¡£ä¸Šä¸‹æ–‡
    docs_context = ""
    for doc in state.get("retrieved_documents", []):
        docs_context += f"""
æ¥æº: {doc['source']}
æ¦‚å¿µ: {doc['metadata'].get('concept', 'N/A')}
ç›¸å…³æ€§: {doc['metadata'].get('relevance_score', 0):.2f}
å†…å®¹: {doc['content'][:200]}...
---
"""

    system_prompt = """ä½ æ˜¯Canvaså­¦ä¹ ç³»ç»Ÿçš„æ£€éªŒé—®é¢˜ç”Ÿæˆä¸“å®¶ã€‚

åŸºäºæ£€ç´¢åˆ°çš„3å±‚è®°å¿†æ•°æ®,ç”Ÿæˆä¸ªæ€§åŒ–æ£€éªŒé—®é¢˜ã€‚

ç”Ÿæˆç­–ç•¥:
1. **70%é—®é¢˜**: æ¥è‡ªå†å²è–„å¼±ç‚¹(temporalæ•°æ®ä¸­score<80çš„æ¦‚å¿µ)
2. **30%é—®é¢˜**: æ¥è‡ªå·²æŒæ¡æ¦‚å¿µ(ç”¨äºå·©å›ºå¤ä¹ )
3. **é—®é¢˜ç±»å‹**: æ ¹æ®èŠ‚ç‚¹é¢œè‰²é€‰æ‹©
   - çº¢è‰²èŠ‚ç‚¹ â†’ çªç ´å‹/åŸºç¡€å‹é—®é¢˜(1-2ä¸ª)
   - ç´«è‰²èŠ‚ç‚¹ â†’ æ£€éªŒå‹/åº”ç”¨å‹é—®é¢˜(2-3ä¸ª)

é—®é¢˜æ ¼å¼:
{{
    "question": "é—®é¢˜æ–‡æœ¬",
    "type": "çªç ´å‹|åŸºç¡€å‹|æ£€éªŒå‹|åº”ç”¨å‹",
    "source_concept": "æ¦‚å¿µåç§°",
    "difficulty": "ç®€å•|ä¸­ç­‰|å›°éš¾",
    "data_sources": ["graphiti", "temporal"]
}}
"""

    user_message = f"""
Canvasä¸Šä¸‹æ–‡:
{json.dumps(state['canvas_context'], ensure_ascii=False, indent=2)}

æ£€ç´¢åˆ°çš„æ–‡æ¡£:
{docs_context}

è¯·ç”Ÿæˆ5-10ä¸ªæ£€éªŒé—®é¢˜,ç¡®ä¿70%æ¥è‡ªå†å²è–„å¼±ç‚¹ã€‚
"""

    response = await llm.ainvoke([
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_message}
    ])

    # è§£æJSONå“åº”
    import json
    questions = json.loads(response.content)

    return {
        **state,
        "verification_questions": questions,
        "workflow_stage": "completed"
    }
```

---

#### 10.5.3 Conditional Edge Functions

```python
def route_query(state: AgenticRAGState) -> str:
    """
    å†³ç­–: æ˜¯å¦éœ€è¦æ£€ç´¢
    """
    last_message = state["messages"][-1]

    # æ£€æŸ¥LLMæ˜¯å¦è°ƒç”¨äº†å·¥å…·
    if hasattr(last_message, "tool_calls") and last_message.tool_calls:
        return "retrieve"
    else:
        return "generate"

def grade_documents_route(state: AgenticRAGState) -> str:
    """
    å†³ç­–: åŸºäºæ–‡æ¡£è´¨é‡é€‰æ‹©ä¸‹ä¸€æ­¥
    """
    quality_score = state.get("document_quality_score", 0.0)
    retrieval_attempts = state.get("retrieval_attempts", 0)

    # è´¨é‡é˜ˆå€¼: 0.5
    if quality_score >= 0.5:
        return "generate"

    # æœ€å¤šé‡è¯•2æ¬¡
    if retrieval_attempts >= 3:
        return "generate"  # å¼ºåˆ¶ç”Ÿæˆ,é¿å…æ— é™å¾ªç¯

    return "rewrite"


def route_by_query_type(state: AgenticRAGState) -> str:
    """
    å†³ç­–: åŸºäºquery_typeè·¯ç”±åˆ°ä¸åŒçš„æŸ¥è¯¢ç”ŸæˆèŠ‚ç‚¹
    """
    query_type = state.get("query_type", "local")

    # éªŒè¯query_typeæœ‰æ•ˆæ€§
    if query_type not in ["local", "global", "hybrid"]:
        # é™çº§ç­–ç•¥
        return "local"

    return query_type
```

---

#### 10.5.4 å››å±‚æ£€ç´¢èåˆç­–ç•¥è®¾è®¡

**èåˆç›®æ ‡**: ç»¼åˆGraphitiã€LanceDBã€Temporalã€GraphRAGå››ä¸ªæ•°æ®æºçš„æ£€ç´¢ç»“æœï¼Œç”Ÿæˆé«˜è´¨é‡ã€å¤šæ ·åŒ–ã€ä¸Šä¸‹æ–‡ä¸°å¯Œçš„æ–‡æ¡£é›†åˆ

**æ ¸å¿ƒæŒ‘æˆ˜**:
1. **å¼‚æ„æ•°æ®æº**: æ¯ä¸ªæ•°æ®æºçš„relevance_scoreè®¡ç®—æ–¹å¼ä¸åŒ
2. **é‡å¤æ¦‚å¿µ**: åŒä¸€æ¦‚å¿µå¯èƒ½å‡ºç°åœ¨å¤šä¸ªæ•°æ®æºä¸­
3. **æƒé‡å¹³è¡¡**: å¦‚ä½•å¹³è¡¡å±€éƒ¨ç²¾ç¡®æ€§(Graphiti/LanceDB)å’Œå…¨å±€è§†è§’(GraphRAG)
4. **æ€§èƒ½çº¦æŸ**: èåˆç®—æ³•ä¸èƒ½æˆä¸ºæ€§èƒ½ç“¶é¢ˆ

**èåˆç­–ç•¥å¯¹æ¯”**:

| ç­–ç•¥ | ç®—æ³•å¤æ‚åº¦ | å‡†ç¡®æ€§ | é€Ÿåº¦ | é€‚ç”¨åœºæ™¯ |
|------|----------|--------|------|---------|
| **RRF** (Reciprocal Rank Fusion) | O(N log N) | â­â­â­â­ | â­â­â­â­â­ | æ£€éªŒç™½æ¿ç”Ÿæˆï¼ˆæ¨èï¼‰ |
| **Linear Combination** | O(N) | â­â­â­ | â­â­â­â­â­ | éœ€è¦è°ƒæ•´æƒé‡æ—¶ |
| **CrossEncoder** | O(NÂ²) | â­â­â­â­â­ | â­â­ | æœ€é«˜å‡†ç¡®æ€§è¦æ±‚ |
| **MMR** (Maximum Marginal Relevance) | O(NÂ²) | â­â­â­â­ | â­â­â­ | éœ€è¦å¤šæ ·æ€§æ—¶ |

**æ¨èç­–ç•¥: RRF (Reciprocal Rank Fusion)**

**RRFä¼˜åŠ¿**:
1. **ä¸åˆ†æ•°æ— å…³**: ä¸ä¾èµ–å„æ•°æ®æºçš„relevance_score scale
2. **è‡ªåŠ¨å¹³è¡¡**: è‡ªç„¶åœ°å¹³è¡¡å„æ•°æ®æºçš„è´¡çŒ®
3. **æ€§èƒ½ä¼˜å¼‚**: O(N log N)ï¼Œé€‚åˆå®æ—¶åœºæ™¯
4. **å®æˆ˜éªŒè¯**: LanceDBã€Graphitiå®˜æ–¹æ¨è

**RRFå…¬å¼**:

```
RRF_score(doc) = Î£_{source âˆˆ {Graphiti, LanceDB, Temporal, GraphRAG}} 1 / (k + rank_source(doc))

å…¶ä¸­ï¼š
- k: å¸¸æ•°ï¼Œé€šå¸¸å–60ï¼ˆæ¨èå€¼ï¼‰
- rank_source(doc): æ–‡æ¡£åœ¨è¯¥æ•°æ®æºä¸­çš„æ’åï¼ˆ1-indexedï¼‰
```

**RRFå®ç°ç»†èŠ‚**:

```python
def rrf_fusion_detailed(docs: list[dict], k: int = 60) -> list[dict]:
    """
    å››å±‚æ£€ç´¢èåˆçš„å®Œæ•´RRFå®ç°

    Step 1: æŒ‰sourceåˆ†ç»„
    Step 2: è®¡ç®—æ¯ä¸ªsourceå†…çš„æ’å
    Step 3: è®¡ç®—æ¯ä¸ªæ–‡æ¡£çš„RRFåˆ†æ•°
    Step 4: å»é‡ï¼ˆä¿ç•™æœ€é«˜åˆ†ï¼‰
    Step 5: é‡æ–°æ’åº
    """

    # Step 1: æŒ‰sourceåˆ†ç»„
    docs_by_source = {
        "graphiti": [d for d in docs if d["source"] == "graphiti"],
        "lancedb": [d for d in docs if d["source"] == "lancedb"],
        "temporal": [d for d in docs if d["source"] == "temporal"],
        "graphrag": [d for d in docs if d["source"] == "graphrag"]
    }

    # Step 2: ä¸ºæ¯ä¸ªsourceå†…çš„æ–‡æ¡£è®¡ç®—æ’å
    for source, source_docs in docs_by_source.items():
        # æŒ‰åŸå§‹relevance_scoreæ’åº
        sorted_docs = sorted(
            source_docs,
            key=lambda d: d["metadata"].get("relevance_score", 0),
            reverse=True
        )

        # åˆ†é…æ’å
        for rank, doc in enumerate(sorted_docs, start=1):
            doc[f"rank_{source}"] = rank

    # Step 3: è®¡ç®—RRFåˆ†æ•°
    rrf_scores = defaultdict(float)
    concept_to_docs = defaultdict(list)

    for doc in docs:
        concept = doc["metadata"].get("concept", "")
        source = doc["source"]

        # è·å–è¯¥æ–‡æ¡£åœ¨å…¶æºä¸­çš„æ’å
        rank = doc.get(f"rank_{source}", 999)

        # ç´¯åŠ RRFåˆ†æ•°
        doc_id = f"{concept}_{source}"
        rrf_scores[doc_id] = 1.0 / (k + rank)

        # ä¿å­˜æ–‡æ¡£å¼•ç”¨
        concept_to_docs[concept].append({
            "doc": doc,
            "rrf_score": rrf_scores[doc_id]
        })

    # Step 4: å»é‡ - å¯¹æ¯ä¸ªconceptï¼Œä¿ç•™RRFåˆ†æ•°æœ€é«˜çš„æ–‡æ¡£
    deduplicated_docs = []
    for concept, doc_entries in concept_to_docs.items():
        # æŒ‰RRFåˆ†æ•°æ’åº
        best_entry = max(doc_entries, key=lambda e: e["rrf_score"])
        best_doc = best_entry["doc"]
        best_doc["rrf_score"] = best_entry["rrf_score"]

        # åˆå¹¶æ‰€æœ‰æ¥æºçš„è¯æ®
        all_sources = [e["doc"]["source"] for e in doc_entries]
        best_doc["metadata"]["contributing_sources"] = all_sources
        best_doc["metadata"]["source_count"] = len(set(all_sources))

        deduplicated_docs.append(best_doc)

    # Step 5: æŒ‰RRFåˆ†æ•°é‡æ–°æ’åº
    final_docs = sorted(deduplicated_docs, key=lambda d: d["rrf_score"], reverse=True)

    return final_docs
```

**èåˆç­–ç•¥é€‰æ‹©å†³ç­–æ ‘**:

```
ç”¨æˆ·åœºæ™¯åˆ¤æ–­
    â”‚
    â”œâ”€ é»˜è®¤åœºæ™¯ï¼ˆæ£€éªŒç™½æ¿ç”Ÿæˆï¼‰
    â”‚   â””â”€ ä½¿ç”¨RRFï¼ˆk=60ï¼‰
    â”‚
    â”œâ”€ éœ€è¦è°ƒæ•´æ•°æ®æºæƒé‡
    â”‚   â””â”€ ä½¿ç”¨Linear Combination
    â”‚       - Graphiti: 0.3 (å›¾å…³ç³»)
    â”‚       - LanceDB: 0.25 (è¯­ä¹‰æ£€ç´¢)
    â”‚       - Temporal: 0.2 (å†å²è–„å¼±ç‚¹)
    â”‚       - GraphRAG: 0.25 (å…¨å±€è§†è§’)
    â”‚
    â”œâ”€ æœ€é«˜å‡†ç¡®æ€§è¦æ±‚ï¼ˆé‡è¦æ¦‚å¿µï¼‰
    â”‚   â””â”€ ä½¿ç”¨CrossEncoder
    â”‚       - æ¨¡å‹: cross-encoder/ms-marco-MiniLM-L-6-v2
    â”‚       - å»¶è¿Ÿ: ~500ms for 50 docs
    â”‚
    â””â”€ éœ€è¦å¤šæ ·æ€§ï¼ˆé¿å…é‡å¤ï¼‰
        â””â”€ ä½¿ç”¨MMR
            - Î»: 0.5ï¼ˆç›¸å…³æ€§å’Œå¤šæ ·æ€§å¹³è¡¡ï¼‰
```

**æ€§èƒ½ä¼˜åŒ–**:

1. **å¹¶è¡ŒåŒ–**:
   ```python
   # åœ¨composite_search_nodeä¸­å·²å®ç°
   results = await asyncio.gather(
       graphiti_task(), lancedb_task(), temporal_task(), graphrag_task()
   )
   ```

2. **Early Stopping**:
   ```python
   # å¦‚æœå‰20ä¸ªæ–‡æ¡£RRFåˆ†æ•°å·®è·<0.01ï¼Œåœæ­¢æ’åº
   if len(final_docs) > 20:
       if final_docs[19]["rrf_score"] - final_docs[20]["rrf_score"] < 0.01:
           return final_docs[:20]
   ```

3. **ç¼“å­˜**:
   ```python
   # ç¼“å­˜åŒä¸€æŸ¥è¯¢çš„èåˆç»“æœï¼ˆ5åˆ†é’Ÿæœ‰æ•ˆæœŸï¼‰
   from functools import lru_cache
   from hashlib import md5

   @lru_cache(maxsize=100)
   def cached_rrf_fusion(query_hash: str, docs_json: str):
       docs = json.loads(docs_json)
       return rrf_fusion(docs)
   ```

**è´¨é‡è¯„ä¼°æŒ‡æ ‡**:

```python
def evaluate_fusion_quality(fused_docs: list[dict], canvas_context: dict) -> dict:
    """
    è¯„ä¼°èåˆç»“æœçš„è´¨é‡

    è¿”å›:
    {
        "diversity_score": 0.0-1.0,  # æ¦‚å¿µå¤šæ ·æ€§
        "source_coverage": 0.0-1.0,  # æ•°æ®æºè¦†ç›–ç‡
        "relevance_avg": 0.0-1.0,    # å¹³å‡ç›¸å…³æ€§
        "weak_point_coverage": 0.0-1.0  # è–„å¼±ç‚¹è¦†ç›–ç‡
    }
    """

    # 1. å¤šæ ·æ€§è¯„åˆ†ï¼šå”¯ä¸€æ¦‚å¿µæ•° / æ€»æ–‡æ¡£æ•°
    unique_concepts = len(set(d["metadata"]["concept"] for d in fused_docs))
    diversity_score = unique_concepts / max(len(fused_docs), 1)

    # 2. æ•°æ®æºè¦†ç›–ç‡ï¼šä½¿ç”¨çš„æ•°æ®æºæ•° / 4
    sources_used = set(d["source"] for d in fused_docs)
    source_coverage = len(sources_used) / 4.0

    # 3. å¹³å‡ç›¸å…³æ€§
    relevance_avg = sum(d.get("rrf_score", 0) for d in fused_docs) / max(len(fused_docs), 1)

    # 4. è–„å¼±ç‚¹è¦†ç›–ç‡ï¼štemporalæ–‡æ¡£æ•° / çº¢ç´«èŠ‚ç‚¹æ•°
    temporal_count = len([d for d in fused_docs if d["source"] == "temporal"])
    red_purple_count = len(canvas_context.get("nodes", []))
    weak_point_coverage = min(1.0, temporal_count / max(red_purple_count, 1))

    return {
        "diversity_score": diversity_score,
        "source_coverage": source_coverage,
        "relevance_avg": relevance_avg,
        "weak_point_coverage": weak_point_coverage,
        "overall_quality": (diversity_score + source_coverage + relevance_avg + weak_point_coverage) / 4.0
    }
```

---

### 10.6 æ£€éªŒç™½æ¿ç”Ÿæˆå®Œæ•´å·¥ä½œæµ

#### 10.6.1 ç«¯åˆ°ç«¯æµç¨‹

```python
async def generate_review_canvas_with_agentic_rag(
    canvas_path: str,
    config: dict
) -> dict:
    """
    ä½¿ç”¨Agentic RAGç”Ÿæˆæ£€éªŒç™½æ¿

    Args:
        canvas_path: Canvasæ–‡ä»¶è·¯å¾„
        config: LangGraph config(thread_id, user_idç­‰)

    Returns:
        æ£€éªŒç™½æ¿ç”Ÿæˆç»“æœ
    """
    # Step 1: è¯»å–Canvas,æå–çº¢è‰²/ç´«è‰²èŠ‚ç‚¹
    canvas_data = read_canvas(canvas_path)
    red_purple_nodes = [
        node for node in canvas_data.get("nodes", [])
        if node.get("color") in ["1", "3"]  # çº¢è‰²æˆ–ç´«è‰²
    ]

    # Step 2: æå–ç”¨æˆ·ç†è§£(é»„è‰²èŠ‚ç‚¹)
    yellow_nodes = [
        node for node in canvas_data.get("nodes", [])
        if node.get("color") == "6"
    ]
    user_understanding = "\n\n".join([
        f"{node.get('text', '')}" for node in yellow_nodes
    ])

    # Step 3: æ„å»ºCanvasä¸Šä¸‹æ–‡
    canvas_context = {
        "canvas_file": canvas_path,
        "topic": Path(canvas_path).stem,
        "topic_node_id": canvas_data.get("topic_node_id", None),
        "red_nodes": [n for n in red_purple_nodes if n["color"] == "1"],
        "purple_nodes": [n for n in red_purple_nodes if n["color"] == "3"],
        "total_nodes": len(canvas_data.get("nodes", []))
    }

    # Step 4: åˆå§‹åŒ–Agentic RAG State
    initial_state = {
        "canvas_file": canvas_path,
        "canvas_context": canvas_context,
        "user_understanding": user_understanding,
        "messages": [],
        "current_query": "",
        "retrieval_attempts": 0,
        "retrieved_documents": [],
        "document_quality_score": 0.0,
        "quality_issues": [],
        "verification_questions": [],
        "workflow_stage": "query_generation",
        "error_log": []
    }

    # Step 5: è°ƒç”¨Agentic RAG Graph
    try:
        result = await agentic_rag_graph.ainvoke(
            initial_state,
            config=config
        )
    except Exception as e:
        logger.error(f"Agentic RAG execution failed: {e}")
        raise

    # Step 6: ç”Ÿæˆæ£€éªŒç™½æ¿Canvasæ–‡ä»¶
    review_canvas_path = generate_review_canvas_file(
        original_canvas_path=canvas_path,
        questions=result["verification_questions"],
        metadata={
            "document_quality_score": result["document_quality_score"],
            "retrieval_attempts": result["retrieval_attempts"],
            "data_sources": list(set(
                source for q in result["verification_questions"]
                for source in q.get("data_sources", [])
            ))
        }
    )

    return {
        "review_canvas_path": review_canvas_path,
        "questions_count": len(result["verification_questions"]),
        "document_quality_score": result["document_quality_score"],
        "retrieval_attempts": result["retrieval_attempts"],
        "workflow_stage": result["workflow_stage"]
    }
```

---

### 10.7 æ€§èƒ½æŒ‡æ ‡ä¸ä¼˜åŒ–

#### 10.7.1 æ€§èƒ½ç›®æ ‡ï¼ˆå«GraphRAGï¼‰

**æ ¸å¿ƒæŒ‡æ ‡**:

| æŒ‡æ ‡ | Localè·¯å¾„ | Globalè·¯å¾„ | Hybridè·¯å¾„ | æµ‹é‡æ–¹å¼ |
|------|----------|-----------|-----------|---------|
| **ç«¯åˆ°ç«¯å»¶è¿Ÿ** | <5ç§’ | <8ç§’ | <12ç§’ | ä»Canvasè¯»å–åˆ°æ£€éªŒç™½æ¿ç”Ÿæˆå®Œæˆ |
| **é—®é¢˜è·¯ç”±å»¶è¿Ÿ** | <500ms | <500ms | <500ms | question_routerèŠ‚ç‚¹æ‰§è¡Œæ—¶é—´ |
| **å·¥å…·è°ƒç”¨å»¶è¿Ÿ** | <1ç§’/å·¥å…· | 2-5ç§’ | <10ç§’ï¼ˆå¹¶è¡Œï¼‰ | retrieval toolsæ‰§è¡Œæ—¶é—´ |
| **èåˆé‡æ’åºå»¶è¿Ÿ** | N/A | N/A | <500ms | fusion_rerankèŠ‚ç‚¹æ‰§è¡Œæ—¶é—´ |
| **æ–‡æ¡£è¯„åˆ†å»¶è¿Ÿ** | <500ms | <500ms | <800ms | grade_documentsèŠ‚ç‚¹æ‰§è¡Œæ—¶é—´ |
| **æŸ¥è¯¢é‡å†™å»¶è¿Ÿ** | <2ç§’ | <2ç§’ | <2ç§’ | rewrite_queryèŠ‚ç‚¹æ‰§è¡Œæ—¶é—´ |
| **æ£€éªŒé—®é¢˜ç”Ÿæˆ** | <3ç§’ | <3ç§’ | <3ç§’ | generate_questionsèŠ‚ç‚¹æ‰§è¡Œæ—¶é—´ |

**åˆ†è·¯å¾„æ€§èƒ½åˆ†æ**:

**Localè·¯å¾„ (Graphiti + LanceDB + Temporal)**:
- æŸ¥è¯¢è·¯ç”±: ~300ms
- å·¥å…·å¹¶è¡Œè°ƒç”¨: ~1ç§’ï¼ˆ3ä¸ªå·¥å…·ï¼‰
  - Graphiti: ~400ms
  - LanceDB: ~300ms
  - Temporal: ~200ms
- æ–‡æ¡£è¯„åˆ†: ~300ms
- é—®é¢˜ç”Ÿæˆ: ~2ç§’
- **æ€»å»¶è¿Ÿ**: ~4ç§’

**Globalè·¯å¾„ (GraphRAG Global Search)**:
- æŸ¥è¯¢è·¯ç”±: ~300ms
- GraphRAGæœç´¢: 2-5ç§’ï¼ˆLevel 1-2ï¼‰æˆ–5-10ç§’ï¼ˆLevel 3ï¼‰
  - ç¤¾åŒºåŠ è½½: ~500ms
  - å…¨å±€æ¨ç†: 1.5-4ç§’ï¼ˆGPT-4oï¼‰
  - ç»“æœæ ¼å¼åŒ–: ~200ms
- æ–‡æ¡£è¯„åˆ†: ~300ms
- é—®é¢˜ç”Ÿæˆ: ~2ç§’
- **æ€»å»¶è¿Ÿ**: ~5-8ç§’ï¼ˆLevel 2ï¼‰ï¼Œ~8-13ç§’ï¼ˆLevel 3ï¼‰

**Hybridè·¯å¾„ (4å±‚å¹¶è¡Œæ£€ç´¢ + Fusion Reranking)**:
- æŸ¥è¯¢è·¯ç”±: ~300ms
- å·¥å…·å¹¶è¡Œè°ƒç”¨: ~5ç§’ï¼ˆ4ä¸ªå·¥å…·ï¼Œæœ€æ…¢å·¥å…·å†³å®šï¼‰
  - Graphiti: ~400ms
  - LanceDB: ~300ms
  - Temporal: ~200ms
  - GraphRAG: ~5ç§’ï¼ˆLevel 2ï¼‰
- Fusioné‡æ’åº: ~400msï¼ˆRRFç®—æ³•ï¼Œå¤„ç†50-80ä¸ªæ–‡æ¡£ï¼‰
- æ–‡æ¡£è¯„åˆ†: ~500ms
- é—®é¢˜ç”Ÿæˆ: ~2ç§’
- **æ€»å»¶è¿Ÿ**: ~8-12ç§’

**æ€§èƒ½ç“¶é¢ˆåˆ†æ**:

| ç»„ä»¶ | å»¶è¿Ÿå æ¯” | ä¼˜åŒ–ä¼˜å…ˆçº§ | ä¼˜åŒ–ç­–ç•¥ |
|------|---------|----------|---------|
| **GraphRAG Global Search** | 50-60% (Hybridè·¯å¾„) | ğŸ”´ é«˜ | 1. é™ä½community_levelï¼ˆ2â†’1ï¼‰<br>2. å‡å°‘max_data_tokensï¼ˆ12Kâ†’8Kï¼‰<br>3. åˆ‡æ¢åˆ°gpt-4o-mini |
| **é—®é¢˜ç”Ÿæˆ** | 20-25% | ğŸŸ¡ ä¸­ | 1. ä½¿ç”¨claude-3-haiku<br>2. å‡å°‘ç”Ÿæˆé—®é¢˜æ•°é‡ |
| **Fusion Reranking** | 3-5% | ğŸŸ¢ ä½ | å·²ä¼˜åŒ–ï¼ˆRRFç®—æ³•ï¼ŒO(N log N)ï¼‰ |
| **æ–‡æ¡£è¯„åˆ†** | 3-5% | ğŸŸ¢ ä½ | å·²ä¼˜åŒ– |

**GraphRAGæ€§èƒ½ä¼˜åŒ–å»ºè®®**:

```python
# æ–¹æ¡ˆ1: é™ä½community_levelï¼ˆæ¨èï¼‰
# Level 2 â†’ Level 1: 5ç§’ â†’ 2.5ç§’ï¼ˆ50%å‡å°‘ï¼‰
result = await graphrag_global_search_tool(
    query=query,
    community_level=1,  # ä»2é™åˆ°1
    max_data_tokens=12000
)

# æ–¹æ¡ˆ2: å‡å°‘ä¸Šä¸‹æ–‡tokenæ•°
# 12K â†’ 8K tokens: 5ç§’ â†’ 3.5ç§’ï¼ˆ30%å‡å°‘ï¼‰
result = await graphrag_global_search_tool(
    query=query,
    community_level=2,
    max_data_tokens=8000  # ä»12Ké™åˆ°8K
)

# æ–¹æ¡ˆ3: åˆ‡æ¢åˆ°æ›´å¿«çš„LLM
# gpt-4o â†’ gpt-4o-mini: 5ç§’ â†’ 2ç§’ï¼ˆ60%å‡å°‘ï¼Œä½†å‡†ç¡®æ€§ä¸‹é™~10%ï¼‰
searcher = GlobalSearch(
    llm=ChatOpenAI(model="gpt-4o-mini", temperature=0.2),  # æ›´å¿«ä½†ç¨å¼±
    context_builder=context_builder
)
```

**Tokenæ¶ˆè€—ä¼°ç®—** (æ›´æ–°ç‰ˆæœ¬ - å«æœ¬åœ°æ¨¡å‹æˆæœ¬ä¼˜åŒ–):

| è·¯å¾„ | Input Tokens | Output Tokens | æ€»Tokens | æˆæœ¬ï¼ˆåŸè®¾è®¡ gpt-4oï¼‰ | æˆæœ¬ï¼ˆä¼˜åŒ–å Qwen2.5 90% + gpt-4o-mini 10%ï¼‰ |
|------|-------------|--------------|---------|-------------------|--------------------------------|
| **Local** | ~1,500 | ~800 | ~2,300 | $0.02 | **$0.002** (â†“90%) |
| **Global (Level 1)** | ~5,000 | ~1,200 | ~6,200 | $0.06 | **$0.006** (â†“90%) |
| **Global (Level 2)** | ~10,000 | ~1,500 | ~11,500 | $0.11 | **$0.011** (â†“90%) |
| **Global (Level 3)** | ~18,000 | ~2,000 | ~20,000 | $0.19 | **$0.019** (â†“90%) |
| **Hybrid** | ~12,000 | ~1,500 | ~13,500 | $0.13 | **$0.013** (â†“90%) |

**æœˆåº¦æˆæœ¬ä¼°ç®—** (å‡è®¾3000æ¬¡æŸ¥è¯¢/æœˆ):

| æŸ¥è¯¢è·¯å¾„åˆ†å¸ƒ | åŸè®¾è®¡æœˆæˆæœ¬ | ä¼˜åŒ–åæœˆæˆæœ¬ | æˆæœ¬èŠ‚çœ |
|------------|------------|------------|---------|
| **50% Local + 30% Global(L2) + 20% Hybrid** | $570 | **$57** | $513 (90%) |
| **70% Local + 20% Global(L1) + 10% Global(L2)** | $330 | **$33** | $297 (90%) |
| **100% Global (Level 3)** | $570 | **$57** | $513 (90%) |

**æ¨èé…ç½®**ï¼ˆå¹³è¡¡æ€§èƒ½å’Œæˆæœ¬ï¼‰:
- **é»˜è®¤ç­–ç•¥**: æ··åˆè·¯å¾„ï¼ˆ50% Local + 30% Global L2 + 20% Hybridï¼‰ï¼Œæœˆæˆæœ¬**$57**
- **æ¿€è¿›ä¼˜åŒ–**: ä¼˜å…ˆLocalè·¯å¾„ï¼ˆ70% Localï¼‰ï¼Œæœˆæˆæœ¬**$33**
- **LLMé…ç½®**: 90% Qwen2.5-14Bï¼ˆæœ¬åœ°ï¼‰+ 10% gpt-4o-miniï¼ˆAPIï¼‰
- **æˆæœ¬ä¸Šé™**: æœˆé¢„ç®—$80ï¼Œè¶…è¿‡è‡ªåŠ¨åˆ‡æ¢ä¸º100%æœ¬åœ°æ¨¡å¼
- **è´¨é‡ä¿è¯**: Qwen2.5è´¨é‡â‰¥85%ï¼Œæ··åˆç­–ç•¥ç»¼åˆè´¨é‡86%

**æˆæœ¬ä¼˜åŒ–ROIåˆ†æ**:
- **ç¡¬ä»¶æŠ•èµ„**: RTX 4090 24GB VRAMï¼ˆ$1600ï¼‰
- **æœˆåº¦èŠ‚çœ**: $513/æœˆï¼ˆåŸ$570 â†’ ä¼˜åŒ–$57ï¼‰
- **å›æœ¬å‘¨æœŸ**: 3.1ä¸ªæœˆï¼ˆ$1600 Ã· $513ï¼‰
- **å¹´åº¦èŠ‚çœ**: $6156ï¼ˆç¬¬ä¸€å¹´æ‰£é™¤ç¡¬ä»¶æˆæœ¬å‡€èŠ‚çœ$4556ï¼‰

#### 10.7.2 æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

**1. å¹¶å‘å·¥å…·è°ƒç”¨**

```python
# âŒ ä½æ•ˆ: ä¸²è¡Œè°ƒç”¨3ä¸ªå·¥å…·
graphiti_results = await graphiti_hybrid_search_tool(query, canvas_context)
lancedb_results = await lancedb_hybrid_search_tool(query)
temporal_results = await temporal_behavior_query_tool(canvas_file)

# âœ… é«˜æ•ˆ: å¹¶å‘è°ƒç”¨3ä¸ªå·¥å…·
import asyncio

results = await asyncio.gather(
    graphiti_hybrid_search_tool(query, canvas_context),
    lancedb_hybrid_search_tool(query),
    temporal_behavior_query_tool(canvas_file)
)

graphiti_results, lancedb_results, temporal_results = results
```

**æ€§èƒ½æå‡**: 3ç§’ â†’ 1ç§’ (3å€æå‡)

**2. ç»“æœç¼“å­˜**

```python
from functools import lru_cache
from cachetools import TTLCache
import hashlib

# L1ç¼“å­˜: å†…å­˜ç¼“å­˜(5åˆ†é’ŸTTL)
retrieval_cache = TTLCache(maxsize=100, ttl=300)

async def cached_graphiti_search(query: str, canvas_context: dict):
    """å¸¦ç¼“å­˜çš„Graphitiæ£€ç´¢"""
    cache_key = hashlib.md5(
        f"{query}_{canvas_context['canvas_file']}".encode()
    ).hexdigest()

    if cache_key in retrieval_cache:
        logger.debug(f"Cache hit: {cache_key}")
        return retrieval_cache[cache_key]

    results = await graphiti_hybrid_search_tool(query, canvas_context)
    retrieval_cache[cache_key] = results
    return results
```

**æ€§èƒ½æå‡**: é‡å¤æŸ¥è¯¢ 1ç§’ â†’ <10ms (100å€æå‡)

**3. æŸ¥è¯¢é‡å†™é™åˆ¶**

```python
# é™åˆ¶æœ€å¤§é‡è¯•æ¬¡æ•°
MAX_RETRIEVAL_ATTEMPTS = 3

def grade_documents_route(state: AgenticRAGState) -> str:
    quality_score = state.get("document_quality_score", 0.0)
    attempts = state.get("retrieval_attempts", 0)

    # å¼ºåˆ¶ç»ˆæ­¢æ¡ä»¶
    if attempts >= MAX_RETRIEVAL_ATTEMPTS:
        logger.warning(f"Max retrieval attempts reached: {attempts}")
        return "generate"

    if quality_score >= 0.5:
        return "generate"

    return "rewrite"
```

**é¿å…**: æ— é™å¾ªç¯å¯¼è‡´è¶…æ—¶

---

#### 10.7.3 æœ¬åœ°æ¨¡å‹éƒ¨ç½²æŒ‡å—

**ç›®æ ‡**: éƒ¨ç½²Qwen2.5-14B-Instructæœ¬åœ°æ¨¡å‹ï¼Œå®ç°90%æˆæœ¬èŠ‚çœï¼ˆ$570 â†’ $57/æœˆï¼‰

**ç¡¬ä»¶è¦æ±‚**:

| ç»„ä»¶ | æ¨èé…ç½® | æœ€ä½é…ç½® | è¯´æ˜ |
|------|---------|---------|------|
| **GPU** | RTX 4090 24GB VRAM | RTX 3090 24GB | 14Bæ¨¡å‹éœ€è¦~18GB VRAMï¼ˆFP16ï¼‰ |
| **CPU** | Intel i7-12700K / AMD Ryzen 7 5800X | ä»»æ„8æ ¸+ | æ— GPUæ—¶ä½¿ç”¨CPUæ¨ç†ï¼ˆæ…¢10å€ï¼‰ |
| **å†…å­˜** | 32GB DDR4 | 16GB | OllamaæœåŠ¡ + æ“ä½œç³»ç»Ÿ + ç¼“å­˜ |
| **å­˜å‚¨** | 100GB SSD | 50GB HDD | Qwen2.5-14Bæ¨¡å‹æ–‡ä»¶~8GB + ç´¢å¼•æ•°æ® |
| **ç½‘ç»œ** | æ— éœ€å¤–ç½‘ï¼ˆæœ¬åœ°æ¨ç†ï¼‰ | å¯é€‰å¤–ç½‘ | APIé™çº§éœ€è¦å¤–ç½‘è¿æ¥OpenAI |

**è½¯ä»¶ä¾èµ–**:

```bash
# 1. å®‰è£…Ollamaï¼ˆè·¨å¹³å°ï¼‰
# Linux/Mac:
curl -fsSL https://ollama.com/install.sh | sh

# Windows:
# ä¸‹è½½å¹¶å®‰è£… https://ollama.com/download/windows

# 2. éªŒè¯Ollamaå®‰è£…
ollama --version
# é¢„æœŸè¾“å‡º: ollama version 0.1.x

# 3. æ‹‰å–Qwen2.5-14B-Instructæ¨¡å‹
ollama pull qwen2.5:14b-instruct
# ä¸‹è½½å¤§å°: ~8GB
# é¢„æœŸæ—¶é—´: 10-30åˆ†é’Ÿï¼ˆå–å†³äºç½‘é€Ÿï¼‰

# 4. éªŒè¯æ¨¡å‹å¯ç”¨
ollama run qwen2.5:14b-instruct "Hello, how are you?"
# é¢„æœŸè¾“å‡º: æ¨¡å‹ç”Ÿæˆçš„è‡ªç„¶è¯­è¨€å“åº”
```

**OllamaæœåŠ¡é…ç½®**:

```bash
# é…ç½®ç¯å¢ƒå˜é‡ï¼ˆLinux/Mac: ~/.bashrc æˆ– ~/.zshrcï¼‰
export OLLAMA_HOST=0.0.0.0:11434           # å…è®¸å±€åŸŸç½‘è®¿é—®
export OLLAMA_MODELS=/data/ollama_models  # è‡ªå®šä¹‰æ¨¡å‹å­˜å‚¨è·¯å¾„ï¼ˆå¯é€‰ï¼‰
export OLLAMA_NUM_PARALLEL=3               # å¹¶å‘è¯·æ±‚æ•°ï¼ˆé»˜è®¤1ï¼‰
export OLLAMA_MAX_LOADED_MODELS=2          # åŒæ—¶åŠ è½½çš„æ¨¡å‹æ•°ï¼ˆé»˜è®¤1ï¼‰

# Windows PowerShell:
$env:OLLAMA_HOST = "0.0.0.0:11434"

# å¯åŠ¨OllamaæœåŠ¡
ollama serve
# æœåŠ¡åœ°å€: http://localhost:11434
```

**æ€§èƒ½åŸºå‡†æµ‹è¯•**:

```python
# âœ… Verified from Story GraphRAG.2 (æ€§èƒ½æµ‹è¯•)
import time
from langchain_community.llms import Ollama

# åˆå§‹åŒ–æœ¬åœ°æ¨¡å‹
local_llm = Ollama(
    model="qwen2.5:14b-instruct",
    base_url="http://localhost:11434",
    temperature=0.2
)

# æµ‹è¯•1: å•æ¬¡æ¨ç†å»¶è¿Ÿ
test_prompt = """
ä»ä»¥ä¸‹Canvaså­¦ä¹ å†…å®¹ä¸­æå–å…³é”®å®ä½“å’Œå…³ç³»ï¼š
æ¦‚å¿µï¼šé€†å¦å‘½é¢˜ (Contrapositive)
å®šä¹‰ï¼šå‘½é¢˜"è‹¥påˆ™q"çš„é€†å¦å‘½é¢˜æ˜¯"è‹¥éqåˆ™ép"
æ€§è´¨ï¼šåŸå‘½é¢˜ä¸é€†å¦å‘½é¢˜ç­‰ä»·
"""

start = time.time()
response = local_llm.invoke(test_prompt)
latency = time.time() - start

print(f"æ¨ç†å»¶è¿Ÿ: {latency:.2f}ç§’")
# RTX 4090é¢„æœŸ: 3-5ç§’
# RTX 3090é¢„æœŸ: 5-8ç§’
# CPUæ¨ç†é¢„æœŸ: 30-60ç§’

# æµ‹è¯•2: æ‰¹é‡æ¨ç†ï¼ˆ10æ¬¡ï¼‰
latencies = []
for i in range(10):
    start = time.time()
    response = local_llm.invoke(test_prompt)
    latencies.append(time.time() - start)

p50 = sorted(latencies)[int(len(latencies) * 0.5)]
p95 = sorted(latencies)[int(len(latencies) * 0.95)]

print(f"P50å»¶è¿Ÿ: {p50:.2f}ç§’, P95å»¶è¿Ÿ: {p95:.2f}ç§’")
# RTX 4090é¢„æœŸ: P50=3.5ç§’, P95=5.2ç§’
```

**è´¨é‡éªŒè¯**:

```python
# âœ… Verified from Story GraphRAG.2 AC 5ï¼ˆè´¨é‡â‰¥85%ï¼‰
from graphrag.evaluation.entity_extraction_evaluator import EntityExtractionEvaluator

# Step 1: å‡†å¤‡æµ‹è¯•æ•°æ®é›†ï¼ˆ100ä¸ªCanvaså†…å®¹æ ·æœ¬ï¼‰
test_dataset = load_canvas_samples(count=100)

# Step 2: ä½¿ç”¨Qwen2.5æå–å®ä½“
qwen_results = []
for sample in test_dataset:
    entities = await extract_entities_with_qwen(sample)
    qwen_results.append(entities)

# Step 3: ä½¿ç”¨gpt-4oæå–å®ä½“ï¼ˆä½œä¸ºåŸºçº¿ï¼‰
gpt4o_results = []
for sample in test_dataset:
    entities = await extract_entities_with_gpt4o(sample)
    gpt4o_results.append(entities)

# Step 4: è®¡ç®—F1 Score
evaluator = EntityExtractionEvaluator()
f1_score = evaluator.compare(
    predictions=qwen_results,
    ground_truth=gpt4o_results
)

print(f"Qwen2.5è´¨é‡: {f1_score * 100:.1f}%ï¼ˆåŸºçº¿: gpt-4o 100%ï¼‰")
# éªŒæ”¶æ ‡å‡†: F1 Score â‰¥ 0.85 (85%)

# Step 5: åˆ†æè´¨é‡å·®è·
if f1_score < 0.85:
    quality_report = evaluator.generate_error_analysis()
    print(f"è´¨é‡é—®é¢˜åˆ†æï¼š\n{quality_report}")
    # å¸¸è§é—®é¢˜ï¼š
    # - å®ä½“è¾¹ç•Œè¯†åˆ«ä¸å‡†ç¡®ï¼ˆ+Few-shot exampleså¯æ”¹å–„ï¼‰
    # - å…³ç³»ç±»å‹åˆ†ç±»é”™è¯¯ï¼ˆ+Fine-tuningå¯æ”¹å–„ï¼‰
    # - ä¸­æ–‡ä¸“ä¸šæœ¯è¯­ç†è§£åå·®ï¼ˆ+é¢†åŸŸè¯å…¸å¯æ”¹å–„ï¼‰
```

**æˆæœ¬ç›‘æ§é›†æˆ**:

```python
# âœ… Verified from config/graphrag_cost_control.json
import json
from graphrag.cost_tracker import CostTracker

# åŠ è½½æˆæœ¬æ§åˆ¶é…ç½®
with open("config/graphrag_cost_control.json", "r") as f:
    config = json.load(f)

# åˆå§‹åŒ–æˆæœ¬è¿½è¸ªå™¨
cost_tracker = CostTracker(
    budget_limit=config["cost_monitoring"]["budget"]["monthly_limit_usd"],
    warning_threshold=config["cost_monitoring"]["budget"]["warning_threshold_percent"],
    critical_threshold=config["cost_monitoring"]["budget"]["critical_threshold_percent"]
)

# é›†æˆåˆ°HybridLLMProvider
hybrid_llm = HybridLLMProvider(
    local_ratio=config["llm_providers"]["hybrid"]["local_ratio"],
    cost_tracker=cost_tracker
)

# å®šæœŸæ£€æŸ¥æˆæœ¬çŠ¶æ€
monthly_cost = cost_tracker.get_monthly_cost()
if monthly_cost >= config["cost_monitoring"]["alerts"]["critical"]["threshold_usd"]:
    # æˆæœ¬è¶…æ ‡ï¼šåˆ‡æ¢ä¸º100%æœ¬åœ°æ¨¡å¼
    hybrid_llm.local_ratio = 1.0
    send_alert(
        recipients=config["cost_monitoring"]["alerts"]["critical"]["recipients"],
        message=f"GraphRAGæˆæœ¬è¶…æ ‡ï¼ˆ${monthly_cost}ï¼‰ï¼Œå·²åˆ‡æ¢ä¸º100%æœ¬åœ°æ¨¡å¼"
    )
```

**æ•…éšœæ’æŸ¥**:

| é—®é¢˜ | ç—‡çŠ¶ | è§£å†³æ–¹æ¡ˆ |
|------|------|---------|
| **VRAMä¸è¶³** | `CUDA out of memory` | 1. é™ä½`OLLAMA_NUM_PARALLEL`<br>2. ä½¿ç”¨é‡åŒ–æ¨¡å‹ï¼ˆqwen2.5:14b-instruct-q4_0ï¼‰<br>3. å¢åŠ GPUæ˜¾å­˜æˆ–ä½¿ç”¨CPUæ¨ç† |
| **æ¨ç†é€Ÿåº¦æ…¢** | P95å»¶è¿Ÿ>10ç§’ | 1. æ£€æŸ¥GPUåˆ©ç”¨ç‡ï¼ˆnvidia-smiï¼‰<br>2. å‡å°‘å¹¶å‘è¯·æ±‚æ•°<br>3. ä¼˜åŒ–prompté•¿åº¦ï¼ˆ<2000 tokensï¼‰ |
| **è´¨é‡ä¸è¾¾æ ‡** | F1 Score < 85% | 1. æ·»åŠ Few-shot examplesåˆ°prompt<br>2. å¾®è°ƒQwen2.5æ¨¡å‹ï¼ˆéœ€è¦è®­ç»ƒæ•°æ®ï¼‰<br>3. æé«˜APIæ¯”ä¾‹ï¼ˆ10%â†’30%ï¼‰ |
| **APIé™çº§ç‡é«˜** | é™çº§ç‡>10% | 1. æ£€æŸ¥OllamaæœåŠ¡çŠ¶æ€ï¼ˆollama psï¼‰<br>2. å¢åŠ local_timeoutï¼ˆ8ç§’â†’12ç§’ï¼‰<br>3. é‡å¯OllamaæœåŠ¡ |
| **æˆæœ¬è¶…æ ‡** | æœˆæˆæœ¬>$80 | 1. æ£€æŸ¥APIè°ƒç”¨æ—¥å¿—ï¼ˆcost_tracker.get_api_log()ï¼‰<br>2. é™ä½APIæ¯”ä¾‹ï¼ˆ10%â†’5%ï¼‰<br>3. åˆ‡æ¢ä¸º100%æœ¬åœ°æ¨¡å¼ |

**ç”Ÿäº§éƒ¨ç½²æ£€æŸ¥æ¸…å•**:

- [ ] **ç¡¬ä»¶**: RTX 4090æˆ–åŒç­‰GPUå·²å®‰è£…å¹¶æ­£å¸¸å·¥ä½œ
- [ ] **Ollama**: æœåŠ¡å·²å¯åŠ¨ï¼ˆollama psæ˜¾ç¤ºqwen2.5:14b-instructï¼‰
- [ ] **è´¨é‡éªŒè¯**: F1 Score â‰¥ 85%ï¼ˆ100ä¸ªæ ·æœ¬æµ‹è¯•ï¼‰
- [ ] **æ€§èƒ½éªŒè¯**: P95å»¶è¿Ÿ â‰¤ 8ç§’ï¼ˆ10æ¬¡æ¨ç†æµ‹è¯•ï¼‰
- [ ] **æˆæœ¬ç›‘æ§**: CostTrackerå·²é›†æˆï¼Œå‘Šè­¦é‚®ä»¶é…ç½®æ­£ç¡®
- [ ] **é™çº§æœºåˆ¶**: APIé™çº§ç‡ < 5%ï¼ˆ7å¤©ç›‘æ§ï¼‰
- [ ] **é…ç½®æ–‡ä»¶**: `config/graphrag_cost_control.json`å·²éƒ¨ç½²
- [ ] **å¥åº·æ£€æŸ¥**: OllamaæœåŠ¡ç›‘æ§å·²å¯ç”¨ï¼ˆæ¯åˆ†é’Ÿæ£€æŸ¥ï¼‰
- [ ] **å¤‡ä»½æ–¹æ¡ˆ**: APIå¯†é’¥å·²é…ç½®ï¼Œé™çº§å¯ç”¨
- [ ] **æ–‡æ¡£**: è¿ç»´æ–‡æ¡£å·²åˆ›å»ºï¼ˆ`docs/operations/graphrag-local-model-deployment.md`ï¼‰

**å‚è€ƒèµ„æ–™**:
- **Ollamaå®˜æ–¹æ–‡æ¡£**: https://ollama.com/docs
- **Qwen2.5æ¨¡å‹å¡**: https://ollama.com/library/qwen2.5:14b-instruct
- **ADR-001**: æœ¬åœ°æ¨¡å‹vs APIæŠ€æœ¯å†³ç­–ï¼ˆ`docs/architecture/ADR-001-local-model-vs-api.md`ï¼‰
- **Story GraphRAG.2**: æœ¬åœ°æ¨¡å‹é›†æˆï¼ˆ`docs/stories/graphrag-2-local-model-integration.story.md`ï¼‰
- **Story GraphRAG.5**: æ€§èƒ½ä¼˜åŒ–ä¸æˆæœ¬ç›‘æ§ï¼ˆ`docs/stories/graphrag-5-performance-cost.story.md`ï¼‰

---

### 10.8 é›†æˆæµ‹è¯•

#### 10.8.1 ç«¯åˆ°ç«¯æµ‹è¯•

```python
@pytest.mark.asyncio
async def test_agentic_rag_end_to_end():
    """
    æµ‹è¯•Agentic RAGå®Œæ•´æµç¨‹
    """
    # Setup
    canvas_path = "test_data/ç¦»æ•£æ•°å­¦-æµ‹è¯•.canvas"
    config = create_langgraph_config(
        canvas_path=canvas_path,
        user_id="test_user",
        session_id=str(uuid.uuid4())
    )

    # Execute
    result = await generate_review_canvas_with_agentic_rag(
        canvas_path=canvas_path,
        config=config
    )

    # Assertions
    assert result["questions_count"] >= 5, "è‡³å°‘ç”Ÿæˆ5ä¸ªæ£€éªŒé—®é¢˜"
    assert result["questions_count"] <= 10, "æœ€å¤šç”Ÿæˆ10ä¸ªæ£€éªŒé—®é¢˜"
    assert result["document_quality_score"] >= 0.5, "æ–‡æ¡£è´¨é‡åº”â‰¥0.5"
    assert os.path.exists(result["review_canvas_path"]), "æ£€éªŒç™½æ¿æ–‡ä»¶åº”å­˜åœ¨"

    # éªŒè¯é—®é¢˜æƒé‡
    questions = read_verification_questions_from_canvas(result["review_canvas_path"])
    temporal_questions = [
        q for q in questions
        if "temporal" in q.get("data_sources", [])
    ]
    assert len(temporal_questions) / len(questions) >= 0.6, "è‡³å°‘60%é—®é¢˜æ¥è‡ªtemporal(å†å²è–„å¼±ç‚¹)"
```

#### 10.8.2 æ€§èƒ½åŸºå‡†æµ‹è¯•

```python
@pytest.mark.performance
async def test_agentic_rag_performance():
    """
    æµ‹è¯•Agentic RAGæ€§èƒ½æŒ‡æ ‡
    """
    canvas_path = "test_data/ç¦»æ•£æ•°å­¦-æµ‹è¯•.canvas"
    config = create_langgraph_config(canvas_path, "test_user", str(uuid.uuid4()))

    # æµ‹è¯•ç«¯åˆ°ç«¯å»¶è¿Ÿ
    start_time = time.time()
    result = await generate_review_canvas_with_agentic_rag(canvas_path, config)
    end_to_end_latency = time.time() - start_time

    assert end_to_end_latency < 5.0, f"ç«¯åˆ°ç«¯å»¶è¿Ÿåº”<5ç§’, å®é™…: {end_to_end_latency:.2f}ç§’"

    # æµ‹è¯•å·¥å…·è°ƒç”¨å»¶è¿Ÿ
    tool_latencies = extract_tool_latencies_from_state(result)
    for tool_name, latency in tool_latencies.items():
        assert latency < 1.0, f"{tool_name}å»¶è¿Ÿåº”<1ç§’, å®é™…: {latency:.2f}ç§’"
```

---

### 10.9 æ•…éšœå¤„ç†ä¸é™çº§ç­–ç•¥

#### 10.9.1 å·¥å…·è°ƒç”¨å¤±è´¥å¤„ç†

```python
async def graphiti_hybrid_search_tool_with_fallback(
    query: str,
    canvas_context: dict
) -> List[Dict]:
    """
    å¸¦é™çº§ç­–ç•¥çš„Graphitiæ£€ç´¢
    """
    try:
        # å°è¯•hybrid_search
        return await graphiti_hybrid_search_tool(query, canvas_context)
    except Exception as e:
        logger.error(f"Graphiti hybrid search failed: {e}")

        # é™çº§ç­–ç•¥1: ä½¿ç”¨çº¯è¯­ä¹‰æœç´¢(semantic_onlyæ¨¡å¼)
        try:
            return await graphiti.search(
                query=query,
                search_mode="semantic_only",
                num_results=20
            )
        except Exception as e2:
            logger.error(f"Graphiti semantic search failed: {e2}")

            # é™çº§ç­–ç•¥2: è¿”å›ç©ºç»“æœ,è®©å…¶ä»–å·¥å…·è¡¥å¿
            return []
```

#### 10.9.2 æ–‡æ¡£è´¨é‡ä¸è¶³å¤„ç†

```python
def grade_documents_route(state: AgenticRAGState) -> str:
    quality_score = state.get("document_quality_score", 0.0)
    attempts = state.get("retrieval_attempts", 0)

    # å¦‚æœé‡è¯•3æ¬¡ä»ç„¶è´¨é‡ä¸è¶³,ä½¿ç”¨é™çº§ç­–ç•¥
    if attempts >= 3 and quality_score < 0.5:
        logger.warning(
            f"Document quality still low after {attempts} attempts: {quality_score:.2f}"
        )

        # é™çº§ç­–ç•¥: ä½¿ç”¨Canvasæœ¬èº«çš„çº¢/ç´«èŠ‚ç‚¹æ–‡æœ¬ä½œä¸ºfallback
        if not state.get("retrieved_documents"):
            state["retrieved_documents"] = generate_fallback_documents_from_canvas(
                state["canvas_context"]
            )

        return "generate"

    if quality_score >= 0.5:
        return "generate"

    return "rewrite"
```

---

### 10.10 ç›‘æ§ä¸æ—¥å¿—

#### 10.10.1 å…³é”®æŒ‡æ ‡ç›‘æ§

```python
from prometheus_client import Histogram, Counter, Gauge

# Agentic RAGæ€§èƒ½æŒ‡æ ‡
agentic_rag_latency = Histogram(
    'agentic_rag_latency_seconds',
    'Agentic RAG end-to-end latency',
    ['canvas_file']
)

# å·¥å…·è°ƒç”¨ç»Ÿè®¡
tool_invocation_count = Counter(
    'agentic_rag_tool_invocations_total',
    'Total tool invocations',
    ['tool_name', 'status']  # status: success/failure
)

# æ–‡æ¡£è´¨é‡åˆ†å¸ƒ
document_quality_distribution = Histogram(
    'agentic_rag_document_quality_score',
    'Document quality score distribution',
    buckets=[0.0, 0.3, 0.5, 0.7, 0.9, 1.0]
)

# æŸ¥è¯¢é‡å†™æ¬¡æ•°
query_rewrite_count = Counter(
    'agentic_rag_query_rewrites_total',
    'Total query rewrites',
    ['canvas_file']
)
```

#### 10.10.2 ç»“æ„åŒ–æ—¥å¿—

```python
import structlog

logger = structlog.get_logger()

# è®°å½•Agentic RAGæ‰§è¡Œ
logger.info(
    "agentic_rag_started",
    canvas_file=canvas_path,
    session_id=config["configurable"]["session_id"],
    red_nodes_count=len(canvas_context["red_nodes"]),
    purple_nodes_count=len(canvas_context["purple_nodes"])
)

# è®°å½•å·¥å…·è°ƒç”¨
logger.info(
    "tool_invoked",
    tool_name="graphiti_hybrid_search_tool",
    query=query[:100],
    max_results=20,
    rerank_strategy="rrf"
)

# è®°å½•æ–‡æ¡£è´¨é‡è¯„åˆ†
logger.info(
    "documents_graded",
    quality_score=quality_score,
    quality_issues=quality_issues,
    retrieved_docs_count=len(retrieved_docs)
)

# è®°å½•æŸ¥è¯¢é‡å†™
logger.warning(
    "query_rewritten",
    original_query=original_query[:100],
    new_query=new_query[:100],
    attempt=retrieval_attempts
)
```

---

### 10.11 éªŒæ”¶æ ‡å‡†

- âœ… **AC 1**: Agentic RAG graphå¯æˆåŠŸç¼–è¯‘å¹¶æ‰§è¡Œ
- âœ… **AC 2**: 3ä¸ªretrieval toolså¯æ­£å¸¸è°ƒç”¨å¹¶è¿”å›ç»“æœ
- âœ… **AC 3**: æ–‡æ¡£è´¨é‡è¯„åˆ†<0.5æ—¶è‡ªåŠ¨è§¦å‘æŸ¥è¯¢é‡å†™
- âœ… **AC 4**: ç«¯åˆ°ç«¯æ£€éªŒç™½æ¿ç”Ÿæˆ<5ç§’
- âœ… **AC 5**: é›†æˆæµ‹è¯•è¦†ç›–ç‡â‰¥90%
- âœ… **AC 6**: ç”Ÿæˆçš„æ£€éªŒé—®é¢˜ä¸­â‰¥60%æ¥è‡ªå†å²è–„å¼±ç‚¹(temporalæ•°æ®)
- âœ… **AC 7**: å·¥å…·è°ƒç”¨å¤±è´¥æ—¶æœ‰é™çº§ç­–ç•¥,ä¸é˜»å¡ä¸»æµç¨‹
- âœ… **AC 8**: æŸ¥è¯¢é‡å†™æ¬¡æ•°â‰¤3,é¿å…æ— é™å¾ªç¯

---
