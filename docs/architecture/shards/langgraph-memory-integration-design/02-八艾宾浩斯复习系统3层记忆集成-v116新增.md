# LANGGRAPH-MEMORY-INTEGRATION-DESIGN - Part 2

**Source**: `LANGGRAPH-MEMORY-INTEGRATION-DESIGN.md`
**Sections**: ğŸ”„ å…«ã€è‰¾å®¾æµ©æ–¯å¤ä¹ ç³»ç»Ÿ3å±‚è®°å¿†é›†æˆ (v1.1.6æ–°å¢)

---

## ğŸ”„ å…«ã€è‰¾å®¾æµ©æ–¯å¤ä¹ ç³»ç»Ÿ3å±‚è®°å¿†é›†æˆ (v1.1.6æ–°å¢)

### 8.1 é›†æˆèƒŒæ™¯

**é—®é¢˜æè¿°**:
- è‰¾å®¾æµ©æ–¯å¤ä¹ ç³»ç»ŸåŸè®¾è®¡ä»…ä¾èµ–Canvasè¯„åˆ†ï¼ˆâ‰¥60åˆ†ï¼‰ä½œä¸ºå”¯ä¸€æ•°æ®æº
- Py-FSRSä½¿ç”¨é»˜è®¤17å‚æ•°ï¼ˆç›¸å½“äºæ¨¡æ‹Ÿæ•°æ®ï¼Œéä¸ªæ€§åŒ–ï¼‰
- ç¼ºå°‘ä¸»åŠ¨å‘ç°éœ€è¦å¤ä¹ æ¦‚å¿µçš„æœºåˆ¶ï¼ˆå®Œå…¨è¢«åŠ¨ç­‰å¾…è¯„åˆ†è§¦å‘ï¼‰

**è§£å†³æ–¹æ¡ˆ**:
é€šè¿‡æ•´åˆ3å±‚è®°å¿†ç³»ç»Ÿï¼ˆTemporal + Graphiti + Semanticï¼‰ï¼Œå®ç°100%çœŸå®æ•°æ®é©±åŠ¨çš„è‰¾å®¾æµ©æ–¯å¤ä¹ ç³»ç»Ÿã€‚

---

### 8.2 3å±‚è®°å¿†ç³»ç»Ÿæ•°æ®æä¾›

#### 8.2.1 Temporal Memoryæä¾›çš„æ•°æ®

**èŒè´£**: å­¦ä¹ è¡Œä¸ºæ•°æ®æº

**æä¾›æ•°æ®**:
1. **é•¿æœŸæœªè®¿é—®çš„å·²æŒæ¡æ¦‚å¿µ** (è§¦å‘ç‚¹4 - æ¡ä»¶1)
   ```python
   query_temporal_learning_behavior(
       filter_type="inactive_mastered",
       days_threshold=7,
       min_mastery=0.6
   )
   # è¿”å›: [{"concept": "é€†å¦å‘½é¢˜", "last_access_days": 10, "mastery": 0.85, ...}]
   ```

2. **å†å²å¤ä¹ è®°å½•** (FSRSå‚æ•°ä¼˜åŒ–)
   ```python
   query_temporal_learning_behavior(
       filter_type="all_reviews",
       min_samples=100,
       fields=["review_time", "rating", "interval", "stability", "difficulty"]
   )
   # è¿”å›: [{"interval": 7, "rating": 3, "retention": 0.9, ...}, ...]
   ```

3. **æœ€è¿‘è¡Œä¸ºæ•°æ®** (ä¼˜å…ˆçº§è®¡ç®—)
   ```python
   query_temporal_learning_behavior(
       filter_type="recent_behavior",
       canvas_file="ç¬”è®°åº“/ç¦»æ•£æ•°å­¦.canvas",
       concept_ids=["node123", "node456"],
       metrics=["review_frequency", "avg_interval", "accuracy_trend"]
   )
   # è¿”å›: {"review_frequency_normalized": 0.3, "interval_deviation": 0.5, ...}
   ```

**æ•°æ®schema (Neo4j - Temporal Memory)**:
```cypher
// å­¦ä¹ äº‹ä»¶èŠ‚ç‚¹
CREATE (e:LearningEvent {
    event_id: randomUUID(),
    session_id: $session_id,
    canvas_file: $canvas_file,
    node_id: $node_id,
    concept: $concept,
    operation_type: $operation_type, // 'scoring', 'review', 'view', 'doc_interaction'
    timestamp: datetime(),
    score: $score,              // å¦‚æœoperation_type='scoring'
    rating: $rating,            // å¦‚æœoperation_type='review' (1-4)
    duration: $duration,        // å¦‚æœoperation_type='view' (ç§’)
    metadata: $metadata         // JSONå¯¹è±¡
})

// ç´¢å¼•
CREATE INDEX learning_event_timestamp IF NOT EXISTS FOR (e:LearningEvent) ON (e.timestamp);
CREATE INDEX learning_event_concept IF NOT EXISTS FOR (e:LearningEvent) ON (e.concept, e.timestamp);
```

---

#### 8.2.2 Graphitiæä¾›çš„æ•°æ®

**èŒè´£**: æ¦‚å¿µå…³ç³»ç½‘ç»œæ•°æ®æº

**æä¾›æ•°æ®**:
1. **çŸ¥è¯†æ–­å±‚æ£€æµ‹** (è§¦å‘ç‚¹4 - æ¡ä»¶2)
   ```python
   query_graphiti_concept_network(
       analysis_type="prerequisite_gap",
       min_prerequisite_mastery=0.8,
       gap_days_threshold=14
   )
   # è¿”å›: [{"concept": "è¡Œåˆ—å¼", "prerequisite_mastery": 0.9, "days_not_learned": 20, ...}]
   ```

2. **æ¦‚å¿µéš¾åº¦åˆ†å¸ƒ** (FSRSå‚æ•°ä¼˜åŒ–)
   ```python
   query_graphiti_concept_network(
       analysis_type="learning_difficulty_distribution",
       concepts=["é€†å¦å‘½é¢˜", "å¾·æ‘©æ ¹å®šå¾‹", ...]
   )
   # è¿”å›: {"é€†å¦å‘½é¢˜": {"difficulty": 0.6}, "å¾·æ‘©æ ¹å®šå¾‹": {"difficulty": 0.7}, ...}
   ```

3. **æ¦‚å¿µå…³ç³»æ•°æ®** (ä¼˜å…ˆçº§è®¡ç®—)
   ```python
   query_graphiti_concept_network(
       analysis_type="related_concepts",
       canvas_file="ç¬”è®°åº“/ç¦»æ•£æ•°å­¦.canvas",
       concept_ids=["node123", "node456"]
   )
   # è¿”å›: {"prerequisite_completeness": 0.8, "related_difficulty_normalized": 0.5, ...}
   ```

**CypheræŸ¥è¯¢ç¤ºä¾‹** (Neo4j):
```cypher
// æŸ¥è¯¢çŸ¥è¯†æ–­å±‚: å‰ç½®æ¦‚å¿µå·²æŒæ¡ä½†åç»­æ¦‚å¿µæœªå­¦ä¹ 
MATCH (pre:Concept)-[:PREREQUISITE_OF]->(post:Concept)
WHERE pre.mastery >= 0.8
  AND post.mastery < 0.3
  AND duration.inDays(post.last_learned, datetime()).days > 14
RETURN post.concept, pre.concept AS prerequisite, pre.mastery, post.last_learned
```

---

#### 8.2.3 Semantic Memoryæä¾›çš„æ•°æ®

**èŒè´£**: æ–‡æ¡£äº¤äº’æ•°æ®æº

**æä¾›æ•°æ®**:
1. **éšæ€§éœ€æ±‚æ£€æµ‹** (è§¦å‘ç‚¹4 - æ¡ä»¶3)
   ```python
   query_semantic_document_interactions(
       pattern="related_doc_frequent_but_concept_inactive",
       related_access_threshold=5,
       concept_inactive_days=7
   )
   # è¿”å›: [{"concept": "ç‰¹å¾å‘é‡", "related_doc_accesses": 8, "concept_inactive_days": 10, ...}]
   ```

2. **å¤ä¹ å‚ä¸åº¦ç›¸å…³æ€§** (FSRSå‚æ•°ä¼˜åŒ–)
   ```python
   query_semantic_document_interactions(
       pattern="review_engagement_correlation",
       concepts=["é€†å¦å‘½é¢˜", "å¾·æ‘©æ ¹å®šå¾‹", ...]
   )
   # è¿”å›: {"é€†å¦å‘½é¢˜": {"engagement": 0.7}, "å¾·æ‘©æ ¹å®šå¾‹": {"engagement": 0.5}, ...}
   ```

3. **è®¿é—®æŒ‡æ ‡** (ä¼˜å…ˆçº§è®¡ç®—)
   ```python
   query_semantic_document_interactions(
       pattern="access_metrics",
       canvas_file="ç¬”è®°åº“/ç¦»æ•£æ•°å­¦.canvas",
       concept_ids=["node123", "node456"]
   )
   # è¿”å›: {"recent_access_frequency": 0.3, "dwell_time_normalized": 0.4, ...}
   ```

**æ•°æ®schema (LanceDB + CUDA Metadata)**:
```python
# LanceDB Document Metadata (Pydantic Schema)
{
    "document_path": "Canvas/Math53/é€†å¦å‘½é¢˜-å£è¯­åŒ–è§£é‡Š-20250115.md",
    "concept": "é€†å¦å‘½é¢˜",
    "canvas_file": "Canvas/Math53/ç¦»æ•£æ•°å­¦.canvas",
    "node_id": "node123",
    "access_count": 12,
    "last_accessed": "2025-11-10T14:30:00Z",
    "avg_dwell_time": 180,  # ç§’
    "related_docs": ["å¾·æ‘©æ ¹å®šå¾‹-å¯¹æ¯”è¡¨.md", "å‘½é¢˜é€»è¾‘-å››å±‚æ¬¡ç­”æ¡ˆ.md"]
}
# 768ç»´å‘é‡åµŒå…¥ (sentence-transformers/all-MiniLM-L6-v2 + CUDAåŠ é€Ÿ)
```

---

### 8.3 è‰¾å®¾æµ©æ–¯ç³»ç»Ÿæ•°æ®æµå‘

```mermaid
graph TB
    subgraph "æ•°æ®é‡‡é›†å±‚"
        A1[CanvasèŠ‚ç‚¹è¯„åˆ†] -->|è¯„åˆ†â‰¥60| E[EbbinghausReviewSystem]
        A2[ç”¨æˆ·æ‰‹åŠ¨æ ‡è®°] --> E
        A3[æ‰¹é‡å¯¼å…¥å†å²] --> E
        A4[è¡Œä¸ºç›‘æ§è§¦å‘] --> E
    end

    subgraph "3å±‚è®°å¿†ç³»ç»ŸæŸ¥è¯¢"
        B1[Temporal Memory<br/>æœªè®¿é—®æ¦‚å¿µæŸ¥è¯¢] --> A4
        B2[Graphiti<br/>çŸ¥è¯†æ–­å±‚æ£€æµ‹] --> A4
        B3[Semantic Memory<br/>éšæ€§éœ€æ±‚æ£€æµ‹] --> A4
    end

    subgraph "å‚æ•°ä¼˜åŒ–"
        C1[Temporal<br/>å†å²å¤ä¹ è®°å½•] --> F[optimize_fsrs_parameters]
        C2[Graphiti<br/>æ¦‚å¿µéš¾åº¦åˆ†å¸ƒ] --> F
        C3[Semantic<br/>å¤ä¹ å‚ä¸åº¦] --> F
        F --> G[ä¼˜åŒ–åçš„17å‚æ•°]
        G --> E
    end

    subgraph "ä¼˜å…ˆçº§è®¡ç®—"
        D1[Temporal<br/>è¡Œä¸ºæ•°æ®] --> H[get_today_review_summary]
        D2[Graphiti<br/>æ¦‚å¿µå…³ç³»] --> H
        D3[Semantic<br/>æ–‡æ¡£äº¤äº’] --> H
        E --> H
        H --> I[ä¼˜å…ˆçº§æ’åºçš„Canvasåˆ—è¡¨]
    end

    style A4 fill:#ffe0b2
    style F fill:#c5e1a5
    style H fill:#b3e5fc
```

**æ•°æ®æµè¯´æ˜**:
1. **è§¦å‘ç‚¹1-3**: Canvasè¯„åˆ†ã€æ‰‹åŠ¨æ ‡è®°ã€æ‰¹é‡å¯¼å…¥ â†’ ç›´æ¥è°ƒç”¨EbbinghausReviewSystem
2. **è§¦å‘ç‚¹4**: æ¯æ—¥å‡Œæ™¨2:00å®šæ—¶ä»»åŠ¡ â†’ æŸ¥è¯¢3å±‚è®°å¿†ç³»ç»Ÿ â†’ æ‰¹é‡æ·»åŠ åˆ°EbbinghausReviewSystem
3. **å‚æ•°ä¼˜åŒ–**: ç´¯ç§¯100æ¬¡å¤ä¹ å â†’ ä»3å±‚è®°å¿†æå–æ•°æ® â†’ ä¼˜åŒ–FSRS 17å‚æ•°
4. **ä¼˜å…ˆçº§è®¡ç®—**: æ¯æ—¥æ˜¾ç¤ºå¤ä¹ é¢æ¿æ—¶ â†’ ä»3å±‚è®°å¿†èšåˆæ•°æ® â†’ å¤šç»´åº¦ä¼˜å…ˆçº§æ’åº

---

### 8.4 LangGraphé›†æˆç‚¹

#### 8.4.1 è¡Œä¸ºç›‘æ§åå°ä»»åŠ¡

**LangGraphå®ç°**: ä½¿ç”¨LangGraphçš„å‘¨æœŸæ€§ä»»åŠ¡èŠ‚ç‚¹

```python
from langgraph.graph import StateGraph
from datetime import datetime, time

class EbbinghausMonitoringState(TypedDict):
    last_check_time: datetime
    triggered_concepts: List[Dict]

def behavior_monitoring_node(state: EbbinghausMonitoringState):
    """
    æ¯æ—¥å‡Œæ™¨2:00æ‰§è¡Œçš„è¡Œä¸ºç›‘æ§èŠ‚ç‚¹
    """
    # æŸ¥è¯¢3å±‚è®°å¿†ç³»ç»Ÿ
    temporal_concepts = query_temporal_learning_behavior(
        filter_type="inactive_mastered",
        days_threshold=7,
        min_mastery=0.6
    )

    graphiti_concepts = query_graphiti_concept_network(
        analysis_type="prerequisite_gap",
        min_prerequisite_mastery=0.8,
        gap_days_threshold=14
    )

    semantic_concepts = query_semantic_document_interactions(
        pattern="related_doc_frequent_but_concept_inactive",
        related_access_threshold=5,
        concept_inactive_days=7
    )

    # åˆå¹¶å¹¶å»é‡
    all_concepts = merge_and_deduplicate(
        temporal_concepts, graphiti_concepts, semantic_concepts
    )

    # æ‰¹é‡æ·»åŠ åˆ°è‰¾å®¾æµ©æ–¯ç³»ç»Ÿ
    review_system = EbbinghausReviewSystem()
    for concept in all_concepts:
        review_system.add_concept_for_review(
            canvas_file=concept['canvas_file'],
            node_id=concept['node_id'],
            concept=concept['concept'],
            initial_mastery=concept.get('mastery', 0.6),
            trigger_source="behavior_monitoring"
        )

    return {
        "last_check_time": datetime.now(),
        "triggered_concepts": all_concepts
    }

# LangGraph StateGraphé…ç½®
monitoring_graph = StateGraph(EbbinghausMonitoringState)
monitoring_graph.add_node("behavior_monitoring", behavior_monitoring_node)
monitoring_graph.add_edge(START, "behavior_monitoring")
monitoring_graph.add_edge("behavior_monitoring", END)

# å®šæ—¶è°ƒåº¦ (ä½¿ç”¨APScheduler)
from apscheduler.schedulers.background import BackgroundScheduler

scheduler = BackgroundScheduler()
scheduler.add_job(
    func=lambda: monitoring_graph.compile().invoke({"last_check_time": None}),
    trigger="cron",
    hour=2,  # å‡Œæ™¨2ç‚¹
    minute=0
)
scheduler.start()
```

#### 8.4.2 è¯„åˆ†åè¡Œä¸ºè®°å½•

**é›†æˆç‚¹**: scoring-agentèŠ‚ç‚¹æ‰§è¡Œå

```python
@tool
def score_and_track(
    canvas_file: str,
    node_id: str,
    concept: str,
    score: int,
    config: RunnableConfig
) -> str:
    """
    è¯„åˆ†å¹¶è®°å½•åˆ°Temporal Memoryï¼ˆé›†æˆåˆ°scoring-agentï¼‰
    """
    # Step 1: æ›´æ–°CanvasèŠ‚ç‚¹é¢œè‰²
    write_to_canvas(canvas_file, {"id": node_id, "color": get_color_by_score(score)}, config)

    # Step 2: è®°å½•è¡Œä¸ºåˆ°Temporal Memory
    track_learning_behavior(
        session_id=config.configurable.get("session_id"),
        canvas_file=canvas_file,
        node_id=node_id,
        concept=concept,
        operation_type="scoring",
        behavior_data={"score": score},
        config=config
    )

    # Step 3: å¦‚æœè¯„åˆ†â‰¥60ï¼Œæ·»åŠ åˆ°è‰¾å®¾æµ©æ–¯ç³»ç»Ÿ
    if score >= 60:
        review_system = EbbinghausReviewSystem()
        review_system.add_concept_for_review(
            canvas_file=canvas_file,
            node_id=node_id,
            concept=concept,
            initial_mastery=score / 100.0,
            trigger_source="scoring"
        )

    return f"âœ… è¯„åˆ†å®Œæˆ: {score}åˆ†, å·²è®°å½•è¡Œä¸ºæ•°æ®"
```

---

### 8.5 ä¸€è‡´æ€§ä¿è¯æœºåˆ¶

#### 8.5.1 3å±‚è®°å¿†å†™å…¥å¤±è´¥å¤„ç†

**é—®é¢˜**: è¡Œä¸ºè®°å½•å†™å…¥å¤±è´¥æ—¶ï¼Œè‰¾å®¾æµ©æ–¯ç³»ç»Ÿå¯èƒ½ç¼ºå°‘æ•°æ®

**è§£å†³æ–¹æ¡ˆ**: æœ€ç»ˆä¸€è‡´æ€§ + é‡è¯•é˜Ÿåˆ—

```python
class MemoryWriteQueue:
    """3å±‚è®°å¿†ç³»ç»Ÿå†™å…¥é˜Ÿåˆ—ï¼ˆæœ€ç»ˆä¸€è‡´æ€§ä¿è¯ï¼‰"""

    def __init__(self):
        self.redis_client = redis.Redis(...)
        self.retry_max = 3

    def enqueue_temporal_write(self, event_data: Dict):
        """
        å°†Temporal Memoryå†™å…¥ä»»åŠ¡åŠ å…¥é˜Ÿåˆ—
        """
        task = {
            "type": "temporal_write",
            "data": event_data,
            "retry_count": 0,
            "created_at": datetime.now().isoformat()
        }
        self.redis_client.lpush("memory_write_queue", json.dumps(task))

    async def process_queue(self):
        """
        åå°workerå¤„ç†å†™å…¥é˜Ÿåˆ—
        """
        while True:
            task_json = self.redis_client.brpop("memory_write_queue", timeout=5)
            if not task_json:
                continue

            task = json.loads(task_json[1])

            try:
                if task["type"] == "temporal_write":
                    temporal_manager.store_learning_event(task["data"])
                elif task["type"] == "graphiti_write":
                    mcp__graphiti_memory__add_episode(content=task["data"])
                # å†™å…¥æˆåŠŸï¼Œä¸é‡æ–°å…¥é˜Ÿ
            except Exception as e:
                # å†™å…¥å¤±è´¥ï¼Œé‡è¯•
                task["retry_count"] += 1
                if task["retry_count"] < self.retry_max:
                    self.redis_client.lpush("memory_write_queue", json.dumps(task))
                else:
                    # è¶…è¿‡é‡è¯•æ¬¡æ•°ï¼Œè®°å½•åˆ°é”™è¯¯æ—¥å¿—
                    logger.error(f"Memory write failed after {self.retry_max} retries: {task}")
```

#### 8.5.2 è‰¾å®¾æµ©æ–¯ç³»ç»Ÿæ•°æ®ä¿®å¤

**é—®é¢˜**: å¦‚æœæŸäº›è¯„åˆ†äº‹ä»¶æœªæˆåŠŸè§¦å‘è‰¾å®¾æµ©æ–¯ç³»ç»Ÿï¼Œå¯¼è‡´å¤ä¹ åˆ—è¡¨ä¸å®Œæ•´

**è§£å†³æ–¹æ¡ˆ**: å®šæœŸä¸€è‡´æ€§æ‰«æ

```python
def sync_ebbinghaus_from_canvas():
    """
    ä»Canvasæ–‡ä»¶æ‰«ææ‰€æœ‰â‰¥60åˆ†èŠ‚ç‚¹ï¼Œç¡®ä¿éƒ½åœ¨è‰¾å®¾æµ©æ–¯ç³»ç»Ÿä¸­
    """
    # Step 1: æ‰«ææ‰€æœ‰Canvasæ–‡ä»¶ï¼Œæ‰¾åˆ°ç»¿è‰²+ç´«è‰²èŠ‚ç‚¹
    canvas_files = glob.glob("ç¬”è®°åº“/**/*.canvas", recursive=True)
    concepts_in_canvas = []

    for canvas_file in canvas_files:
        canvas_data = read_canvas(canvas_file)
        for node in canvas_data.get("nodes", []):
            if node.get("color") in ["2", "3"]:  # ç»¿è‰²æˆ–ç´«è‰²
                concepts_in_canvas.append({
                    "canvas_file": canvas_file,
                    "node_id": node["id"],
                    "concept": extract_concept_from_node(node)
                })

    # Step 2: æŸ¥è¯¢è‰¾å®¾æµ©æ–¯ç³»ç»Ÿç°æœ‰æ¦‚å¿µ
    review_system = EbbinghausReviewSystem()
    concepts_in_ebbinghaus = review_system.list_all_concepts()

    # Step 3: æ‰¾å‡ºç¼ºå¤±çš„æ¦‚å¿µ
    missing_concepts = [
        c for c in concepts_in_canvas
        if f"{c['canvas_file']}_{c['node_id']}" not in concepts_in_ebbinghaus
    ]

    # Step 4: æ‰¹é‡æ·»åŠ ç¼ºå¤±æ¦‚å¿µ
    for concept in missing_concepts:
        review_system.add_concept_for_review(
            canvas_file=concept['canvas_file'],
            node_id=concept['node_id'],
            concept=concept['concept'],
            initial_mastery=0.7,  # é»˜è®¤æŒæ¡åº¦
            trigger_source="consistency_sync"
        )

    logger.info(f"âœ… ä¸€è‡´æ€§æ‰«æå®Œæˆ: ä¿®å¤äº†{len(missing_concepts)}ä¸ªç¼ºå¤±æ¦‚å¿µ")

# å®šæœŸæ‰§è¡Œï¼ˆæ¯å‘¨æ—¥å‡Œæ™¨3ç‚¹ï¼‰
scheduler.add_job(
    func=sync_ebbinghaus_from_canvas,
    trigger="cron",
    day_of_week="sun",
    hour=3,
    minute=0
)
```

---

### 8.6 æ€§èƒ½ä¼˜åŒ–

#### 8.6.1 æ‰¹é‡æŸ¥è¯¢ä¼˜åŒ–

**é—®é¢˜**: ä¼˜å…ˆçº§è®¡ç®—æ—¶éœ€è¦ä¸ºæ¯ä¸ªCanvasæŸ¥è¯¢3å±‚è®°å¿†ç³»ç»Ÿï¼Œå¯èƒ½å¾ˆæ…¢

**ä¼˜åŒ–æ–¹æ¡ˆ**: æ‰¹é‡æŸ¥è¯¢ + ç¼“å­˜

```python
class MemoryBatchQuery:
    """3å±‚è®°å¿†ç³»ç»Ÿæ‰¹é‡æŸ¥è¯¢ä¼˜åŒ–"""

    def __init__(self):
        self.cache = TTLCache(maxsize=1000, ttl=300)  # 5åˆ†é’Ÿç¼“å­˜

    def batch_query_for_review_summary(self, canvas_files: List[str]) -> Dict:
        """
        æ‰¹é‡æŸ¥è¯¢å¤šä¸ªCanvasçš„3å±‚è®°å¿†æ•°æ®
        """
        # Step 1: æ£€æŸ¥ç¼“å­˜
        cached_results = {}
        uncached_files = []
        for canvas_file in canvas_files:
            cache_key = f"review_summary:{canvas_file}"
            if cache_key in self.cache:
                cached_results[canvas_file] = self.cache[cache_key]
            else:
                uncached_files.append(canvas_file)

        if not uncached_files:
            return cached_results

        # Step 2: æ‰¹é‡æŸ¥è¯¢Temporal Memoryï¼ˆå•æ¬¡æŸ¥è¯¢ï¼‰
        temporal_data = query_temporal_learning_behavior(
            filter_type="batch_recent_behavior",
            canvas_files=uncached_files,
            metrics=["review_frequency", "avg_interval", "accuracy_trend"]
        )

        # Step 3: æ‰¹é‡æŸ¥è¯¢Graphitiï¼ˆå•æ¬¡CypheræŸ¥è¯¢ï¼‰
        graphiti_data = query_graphiti_concept_network_batch(
            canvas_files=uncached_files,
            analysis=["prerequisite_mastery", "related_difficulty"]
        )

        # Step 4: æ‰¹é‡æŸ¥è¯¢Semantic Memoryï¼ˆå•æ¬¡å‘é‡æœç´¢ï¼‰
        semantic_data = query_semantic_document_interactions_batch(
            canvas_files=uncached_files,
            metrics=["access_frequency", "dwell_time"]
        )

        # Step 5: åˆå¹¶ç»“æœå¹¶ç¼“å­˜
        for canvas_file in uncached_files:
            result = {
                "temporal": temporal_data.get(canvas_file, {}),
                "graphiti": graphiti_data.get(canvas_file, {}),
                "semantic": semantic_data.get(canvas_file, {})
            }
            cache_key = f"review_summary:{canvas_file}"
            self.cache[cache_key] = result
            cached_results[canvas_file] = result

        return cached_results
```

#### 8.6.2 å¼‚æ­¥å¹¶å‘æŸ¥è¯¢

**ä½¿ç”¨LangGraphçš„å¹¶å‘èŠ‚ç‚¹**:

```python
from langgraph.graph import StateGraph

class MemoryQueryState(TypedDict):
    canvas_files: List[str]
    temporal_results: Dict
    graphiti_results: Dict
    semantic_results: Dict

async def query_temporal_parallel(state: MemoryQueryState):
    """å¹¶å‘æŸ¥è¯¢Temporal Memory"""
    results = await query_temporal_learning_behavior_async(
        filter_type="batch_recent_behavior",
        canvas_files=state["canvas_files"]
    )
    return {"temporal_results": results}

async def query_graphiti_parallel(state: MemoryQueryState):
    """å¹¶å‘æŸ¥è¯¢Graphiti"""
    results = await query_graphiti_concept_network_async_batch(
        canvas_files=state["canvas_files"]
    )
    return {"graphiti_results": results}

async def query_semantic_parallel(state: MemoryQueryState):
    """å¹¶å‘æŸ¥è¯¢Semantic Memory"""
    results = await query_semantic_document_interactions_async_batch(
        canvas_files=state["canvas_files"]
    )
    return {"semantic_results": results}

# æ„å»ºå¹¶å‘æŸ¥è¯¢å›¾
memory_query_graph = StateGraph(MemoryQueryState)
memory_query_graph.add_node("temporal", query_temporal_parallel)
memory_query_graph.add_node("graphiti", query_graphiti_parallel)
memory_query_graph.add_node("semantic", query_semantic_parallel)

# å¹¶å‘æ‰§è¡Œ3ä¸ªæŸ¥è¯¢
memory_query_graph.add_edge(START, "temporal")
memory_query_graph.add_edge(START, "graphiti")
memory_query_graph.add_edge(START, "semantic")
memory_query_graph.add_edge("temporal", END)
memory_query_graph.add_edge("graphiti", END)
memory_query_graph.add_edge("semantic", END)

# ä½¿ç”¨
compiled = memory_query_graph.compile()
result = await compiled.ainvoke({"canvas_files": ["ç¦»æ•£æ•°å­¦.canvas", "çº¿æ€§ä»£æ•°.canvas"]})
# resultåŒ…å«æ‰€æœ‰3å±‚è®°å¿†çš„æŸ¥è¯¢ç»“æœ
```

---

### 8.7 ç›‘æ§å’Œå‘Šè­¦

#### 8.7.1 å…³é”®æŒ‡æ ‡ç›‘æ§

```python
# PrometheusæŒ‡æ ‡å®šä¹‰
from prometheus_client import Counter, Histogram, Gauge

# è¡Œä¸ºç›‘æ§è§¦å‘ç»Ÿè®¡
behavior_monitoring_triggered = Counter(
    'ebbinghaus_behavior_monitoring_triggered_total',
    'Total behavior monitoring triggers',
    ['source']  # temporal/graphiti/semantic
)

# 3å±‚è®°å¿†æŸ¥è¯¢å»¶è¿Ÿ
memory_query_latency = Histogram(
    'memory_query_duration_seconds',
    'Memory query latency',
    ['memory_type']  # temporal/graphiti/semantic
)

# FSRSå‚æ•°ä¼˜åŒ–é¢‘ç‡
fsrs_optimization_count = Counter(
    'fsrs_parameter_optimization_total',
    'Total FSRS parameter optimizations'
)

# å½“å‰å¤ä¹ é˜Ÿåˆ—å¤§å°
review_queue_size = Gauge(
    'ebbinghaus_review_queue_size',
    'Current review queue size'
)
```

#### 8.7.2 å‘Šè­¦è§„åˆ™

```yaml
# AlertManagerå‘Šè­¦è§„åˆ™
groups:
  - name: ebbinghaus_system
    rules:
      - alert: MemoryQuerySlow
        expr: memory_query_duration_seconds{quantile="0.95"} > 1.0
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "3å±‚è®°å¿†ç³»ç»ŸæŸ¥è¯¢å˜æ…¢"
          description: "{{ $labels.memory_type }} æŸ¥è¯¢P95å»¶è¿Ÿ > 1ç§’"

      - alert: BehaviorMonitoringFailed
        expr: rate(behavior_monitoring_triggered_total[1h]) == 0
        for: 25h  # è¶…è¿‡1å¤©æœªè§¦å‘
        labels:
          severity: critical
        annotations:
          summary: "è¡Œä¸ºç›‘æ§æœªæ‰§è¡Œ"
          description: "è¿‡å»25å°æ—¶å†…æœªè§¦å‘è¡Œä¸ºç›‘æ§ï¼Œå¯èƒ½ç³»ç»Ÿæ•…éšœ"

      - alert: ReviewQueueTooLarge
        expr: review_queue_size > 500
        for: 1h
        labels:
          severity: warning
        annotations:
          summary: "å¤ä¹ é˜Ÿåˆ—è¿‡å¤§"
          description: "å½“å‰å¤ä¹ é˜Ÿåˆ—: {{ $value }} ä¸ªæ¦‚å¿µ"
```

---

### 8.8 æµ‹è¯•ç­–ç•¥

#### 8.8.1 é›†æˆæµ‹è¯•

```python
@pytest.mark.integration
async def test_ebbinghaus_3layer_memory_integration():
    """
    æµ‹è¯•è‰¾å®¾æµ©æ–¯ç³»ç»Ÿä¸3å±‚è®°å¿†ç³»ç»Ÿçš„å®Œæ•´é›†æˆ
    """
    # Setup: åˆ›å»ºæµ‹è¯•æ•°æ®
    canvas_file = "test_data/æµ‹è¯•-ç¦»æ•£æ•°å­¦.canvas"
    concept = "æµ‹è¯•-é€†å¦å‘½é¢˜"

    # Step 1: æ¨¡æ‹Ÿè¯„åˆ†è§¦å‘
    await score_and_track(
        canvas_file=canvas_file,
        node_id="test_node_123",
        concept=concept,
        score=85,
        config=test_config
    )

    # Step 2: éªŒè¯Temporal Memoryå·²è®°å½•
    temporal_events = query_temporal_learning_behavior(
        filter_type="all_reviews",
        min_samples=1
    )
    assert any(e["concept"] == concept for e in temporal_events)

    # Step 3: éªŒè¯è‰¾å®¾æµ©æ–¯ç³»ç»Ÿå·²æ·»åŠ 
    review_system = EbbinghausReviewSystem()
    concepts = review_system.list_all_concepts()
    assert f"{canvas_file}_test_node_123" in concepts

    # Step 4: æ¨¡æ‹Ÿ7å¤©åè¡Œä¸ºç›‘æ§è§¦å‘
    await simulate_time_advance(days=7)
    await behavior_monitoring_node({})

    # Step 5: éªŒè¯è§¦å‘äº†å¤ä¹ æ¨é€
    review_summary = get_today_review_summary()
    assert any(c["canvas_file"] == canvas_file for c in review_summary)
```

#### 8.8.2 æ€§èƒ½æµ‹è¯•

```python
@pytest.mark.performance
async def test_memory_query_performance():
    """
    æµ‹è¯•3å±‚è®°å¿†ç³»ç»ŸæŸ¥è¯¢æ€§èƒ½
    """
    canvas_files = [f"test_data/canvas_{i}.canvas" for i in range(50)]

    # æµ‹è¯•æ‰¹é‡æŸ¥è¯¢æ€§èƒ½
    start_time = time.time()
    batch_query = MemoryBatchQuery()
    results = batch_query.batch_query_for_review_summary(canvas_files)
    duration = time.time() - start_time

    # æ€§èƒ½è¦æ±‚: 50ä¸ªCanvasæ‰¹é‡æŸ¥è¯¢ < 2ç§’
    assert duration < 2.0
    assert len(results) == 50
```

---
