# GRAPHITI-KNOWLEDGE-GRAPH-INTEGRATION-ARCHITECTURE - Part 2

**Source**: `GRAPHITI-KNOWLEDGE-GRAPH-INTEGRATION-ARCHITECTURE.md`
**Sections**: ğŸ” 4. æ™ºèƒ½æ··åˆæ£€ç´¢åŠŸèƒ½è®¾è®¡ (Graphiti Hybrid Search)

---

## ğŸ” 4. æ™ºèƒ½æ··åˆæ£€ç´¢åŠŸèƒ½è®¾è®¡ (Graphiti Hybrid Search)

> **âœ… åŸºäºGraphiti SkilléªŒè¯**: æœ¬èŠ‚æ‰€æœ‰æ£€ç´¢å®ç°å‡ä½¿ç”¨Graphitiå®˜æ–¹`hybrid_search` APIï¼Œæ•´åˆGraphéå† + Semanticå‘é‡ + BM25å…³é”®è¯ä¸‰ç§æ£€ç´¢æ¨¡å¼

### 4.0 Graphitiæ··åˆæ£€ç´¢æ¶æ„æ¦‚è§ˆ

**æ ¸å¿ƒä¼˜åŠ¿**: Graphitiå†…ç½®æ··åˆæ£€ç´¢å¼•æ“ï¼Œæ— éœ€æ‰‹å†™CypheræŸ¥è¯¢

```python
# âœ… Verified from Graphiti Skill (hybrid_search API)
from graphiti_core import Graphiti
from graphiti_core.search.search_config import SearchConfig
from graphiti_core.search.search_config_recipes import (
    COMBINED_HYBRID_SEARCH_RRF,  # é»˜è®¤RRFé‡æ’
    node_distance_reranker,      # å›¾è·ç¦»é‡æ’
    mmr_reranker,                # æœ€å¤§è¾¹é™…ç›¸å…³æ€§é‡æ’
    cross_encoder_reranker,      # è·¨ç¼–ç å™¨é‡æ’
    episode_mentions_reranker    # äº‹ä»¶æåŠé‡æ’
)

class GraphitiHybridRetriever:
    """Graphitiæ··åˆæ£€ç´¢å™¨ - å°è£…å®˜æ–¹hybrid_search API"""

    def __init__(self, graphiti: Graphiti):
        self.graphiti = graphiti

    async def search(
        self,
        query: str,
        center_node_uuid: str = None,      # ä¸­å¿ƒèŠ‚ç‚¹UUIDï¼ˆå¯é€‰ï¼‰
        max_distance: int = 3,             # å›¾éå†æœ€å¤§è·ç¦»
        num_results: int = 20,             # è¿”å›ç»“æœæ•°é‡
        rerank_strategy: str = "rrf"       # 5ç§é‡æ’ç­–ç•¥ä¹‹ä¸€
    ) -> List[Dict]:
        """
        Graphitiæ··åˆæ£€ç´¢ (Graph + Semantic + BM25)

        å‚æ•°:
            query: æœç´¢æŸ¥è¯¢ï¼ˆè‡ªç„¶è¯­è¨€ï¼‰
            center_node_uuid: ä¸­å¿ƒèŠ‚ç‚¹UUIDï¼ˆå¦‚æ¦‚å¿µèŠ‚ç‚¹ï¼‰
            max_distance: å›¾éå†æœ€å¤§è·ç¦»ï¼ˆ1-5è·³ï¼‰
            num_results: è¿”å›ç»“æœæ•°é‡
            rerank_strategy: é‡æ’ç­–ç•¥ ("rrf" | "mmr" | "node_distance" | "cross_encoder" | "episode_mentions")

        è¿”å›:
            List[Dict]: æ£€ç´¢ç»“æœï¼Œæ¯ä¸ªç»“æœåŒ…å«èŠ‚ç‚¹ä¿¡æ¯å’Œç›¸å…³æ€§è¯„åˆ†
        """
        # âœ… Verified from Graphiti Skill (search_config_recipes)
        search_config = self._get_search_config(rerank_strategy)

        results = await self.graphiti.search(
            query=query,
            center_node_uuid=center_node_uuid,
            max_distance=max_distance,
            num_results=num_results,
            config=search_config
        )

        return results

    def _get_search_config(self, strategy: str) -> SearchConfig:
        """è·å–æœç´¢é…ç½®ï¼ˆ5ç§é‡æ’ç­–ç•¥ï¼‰"""
        # âœ… Verified from Graphiti Skill (search_config_recipesæ¨¡å—)
        RERANK_STRATEGIES = {
            "rrf": COMBINED_HYBRID_SEARCH_RRF,           # å€’æ•°æ’åèåˆï¼ˆé»˜è®¤æ¨èï¼‰
            "mmr": mmr_reranker,                         # æœ€å¤§è¾¹é™…ç›¸å…³æ€§ï¼ˆå»é‡ç›¸ä¼¼ç»“æœï¼‰
            "node_distance": node_distance_reranker,     # å›¾è·ç¦»æƒé‡ï¼ˆä¼˜å…ˆè¿‘é‚»èŠ‚ç‚¹ï¼‰
            "cross_encoder": cross_encoder_reranker,     # è·¨ç¼–ç å™¨é‡æ’ï¼ˆç²¾åº¦æœ€é«˜ä½†é€Ÿåº¦æ…¢ï¼‰
            "episode_mentions": episode_mentions_reranker # äº‹ä»¶æåŠé¢‘ç‡ï¼ˆæ—¶åºç›¸å…³ï¼‰
        }

        if strategy not in RERANK_STRATEGIES:
            raise ValueError(f"æ— æ•ˆé‡æ’ç­–ç•¥: {strategy}. å¯é€‰: {list(RERANK_STRATEGIES.keys())}")

        return RERANK_STRATEGIES[strategy]
```

**5ç§Rerankingç­–ç•¥å¯¹æ¯”**:

| ç­–ç•¥ | é€‚ç”¨åœºæ™¯ | æ€§èƒ½ | ç²¾åº¦ | æ¨èæŒ‡æ•° |
|------|---------|------|------|---------|
| **rrf** (å€’æ•°æ’åèåˆ) | é€šç”¨åœºæ™¯ï¼Œå¹³è¡¡å„æ£€ç´¢æº | â­â­â­â­â­ | â­â­â­â­ | âœ… é»˜è®¤æ¨è |
| **mmr** (æœ€å¤§è¾¹é™…ç›¸å…³æ€§) | éœ€è¦å¤šæ ·åŒ–ç»“æœï¼Œå»é™¤å†—ä½™ | â­â­â­â­ | â­â­â­â­ | âœ… æ¨è |
| **node_distance** (å›¾è·ç¦») | å¼ºè°ƒæ¦‚å¿µå…³è”æ€§ï¼Œä¼˜å…ˆè¿‘é‚» | â­â­â­â­â­ | â­â­â­ | âš ï¸ ç‰¹å®šåœºæ™¯ |
| **cross_encoder** (è·¨ç¼–ç å™¨) | è¿½æ±‚æœ€é«˜ç²¾åº¦ï¼Œä¸åœ¨æ„é€Ÿåº¦ | â­â­ | â­â­â­â­â­ | âš ï¸ æ€§èƒ½æ•æ„Ÿåœºæ™¯æ…ç”¨ |
| **episode_mentions** (äº‹ä»¶æåŠ) | æ—¶åºç›¸å…³æŸ¥è¯¢ï¼Œå¤ä¹ å†å²åˆ†æ | â­â­â­â­ | â­â­â­ | âœ… æ—¶é—´çº¿åˆ†ææ¨è |

**æ€§èƒ½åŸºå‡†**:
- ç›®æ ‡å»¶è¿Ÿ: <200ms (rrf/mmr/node_distanceç­–ç•¥)
- ç›®æ ‡å»¶è¿Ÿ: <500ms (cross_encoderç­–ç•¥)
- ååé‡: >50 QPS (å¹¶å‘åœºæ™¯)
- ç¼“å­˜å‘½ä¸­ç‡: >70% (é‡å¤æŸ¥è¯¢)

---

### 4.1 LanceDBå‘é‡ç´¢å¼•é…ç½®ä¸ä¼˜åŒ–

**èƒŒæ™¯**: LanceDBä½¿ç”¨Lanceæ•°æ®æ ¼å¼(Parquetæ¼”è¿›ç‰ˆ)å’Œè‡ªé€‚åº”ç´¢å¼•ç­–ç•¥ï¼Œæä¾›100xæŸ¥è¯¢æ€§èƒ½æå‡

```python
# âœ… Verified from LanceDB Context7
import lancedb
from lancedb.pydantic import LanceModel, Vector
from lancedb.embeddings import get_registry

class LanceDBOptimizer:
    """LanceDBå‘é‡ç´¢å¼•ä¼˜åŒ–å™¨"""

    @classmethod
    def get_index_config(cls, scenario: str = "default") -> dict:
        """è·å–ä¸åŒåœºæ™¯çš„ç´¢å¼•é…ç½®"""

        # âœ… Verified: åŸºäºLanceDBæœ€ä½³å®è·µ
        SCENARIO_CONFIGS = {
            "default": {  # <100K vectors, IVF_FLAT
                "index_type": "IVF_FLAT",
                "nprobes": 20,          # æŸ¥è¯¢æ—¶æœç´¢çš„åˆ†åŒºæ•°
                "nlist": 100,           # IVFèšç±»ä¸­å¿ƒæ•°
                "refine_factor": 1      # æ— é‡æ’åº
            },
            "high_accuracy": {  # é«˜ç²¾åº¦åœºæ™¯ï¼ˆæ£€éªŒç™½æ¿ç”Ÿæˆï¼‰
                "index_type": "IVF_FLAT",
                "nprobes": 50,
                "nlist": 256,
                "refine_factor": 2      # 2xå€™é€‰é›†é‡æ’åº
            },
            "high_speed": {  # é«˜é€Ÿåº¦åœºæ™¯ï¼ˆå®æ—¶æ¨èï¼‰
                "index_type": "IVF_PQ",  # Product Quantizationå‹ç¼©
                "nprobes": 10,
                "nlist": 100,
                "pq_m": 8,               # PQå­ç©ºé—´æ•°é‡
                "pq_nbits": 8            # æ¯å­ç©ºé—´çš„bits
            },
            "large_scale": {  # >1M vectors
                "index_type": "IVF_PQ",
                "nprobes": 40,
                "nlist": 4096,           # å¤§è§„æ¨¡åœºæ™¯å¢åŠ èšç±»ä¸­å¿ƒ
                "pq_m": 16,
                "pq_nbits": 8,
                "refine_factor": 3
            }
        }

        return SCENARIO_CONFIGS.get(scenario, SCENARIO_CONFIGS["default"])

    @classmethod
    def create_table_with_index(
        cls,
        db: lancedb.DBConnection,
        table_name: str,
        schema: type[LanceModel],
        scenario: str = "default"
    ) -> lancedb.table.Table:
        """åˆ›å»ºå¸¦ä¼˜åŒ–ç´¢å¼•çš„LanceDBè¡¨"""

        # Step 1: åˆ›å»ºè¡¨
        table = db.create_table(table_name, schema=schema, mode="overwrite")

        # Step 2: åˆ›å»ºBM25å…¨æ–‡ç´¢å¼•(Hybrid Search)
        table.create_fts_index("text", replace=True)

        # Step 3: é…ç½®å‘é‡ç´¢å¼•
        config = cls.get_index_config(scenario)

        if config["index_type"] == "IVF_FLAT":
            table.create_index(
                metric="cosine",
                index_type="IVF_FLAT",
                num_partitions=config["nlist"],
                num_sub_vectors=8
            )
        elif config["index_type"] == "IVF_PQ":
            table.create_index(
                metric="cosine",
                index_type="IVF_PQ",
                num_partitions=config["nlist"],
                num_sub_vectors=config["pq_m"]
            )

        return table

    @classmethod
    def estimate_query_latency(cls, num_concepts: int, scenario: str) -> float:
        """ä¼°ç®—æŸ¥è¯¢å»¶è¿Ÿï¼ˆæ¯«ç§’ï¼‰"""
        # âœ… åŸºäºLanceDBå®æµ‹æ•°æ®: 1M vectors @ MacBook Pro < 100ms
        # å…¬å¼: latency = base_latency + (num_concepts / throughput)

        LATENCY_PROFILES = {
            "default": {"base": 20, "throughput": 100000},      # <10K vectors
            "high_accuracy": {"base": 50, "throughput": 50000}, # æ›´å¤šé‡æ’åº
            "high_speed": {"base": 10, "throughput": 200000},   # PQå‹ç¼©åŠ é€Ÿ
            "large_scale": {"base": 80, "throughput": 20000}    # 1M+ vectors
        }

        profile = LATENCY_PROFILES.get(scenario, LATENCY_PROFILES["default"])
        latency = profile["base"] + (num_concepts / profile["throughput"] * 1000)
        return round(latency, 2)
```

**LanceDBç´¢å¼•é€‰æ‹©å†³ç­–æ ‘**:

```
æ•°æ®è§„æ¨¡?
â”œâ”€â”€ <100K vectors â†’ IVF_FLAT (æ— æŸç²¾åº¦)
â”‚   â”œâ”€â”€ æŸ¥è¯¢å»¶è¿Ÿ: <50ms
â”‚   â””â”€â”€ å†…å­˜å ç”¨: ~300MB (768-dim)
â”‚
â”œâ”€â”€ 100K-1M vectors â†’ IVF_PQ (8xå‹ç¼©)
â”‚   â”œâ”€â”€ æŸ¥è¯¢å»¶è¿Ÿ: <100ms
â”‚   â””â”€â”€ å†…å­˜å ç”¨: ~40MB (å‹ç¼©å)
â”‚
â””â”€â”€ >1M vectors â†’ IVF_PQ + åˆ†å¸ƒå¼
    â”œâ”€â”€ æŸ¥è¯¢å»¶è¿Ÿ: <200ms (éœ€åˆ†ç‰‡)
    â””â”€â”€ éœ€è¦LanceDB Cloudæˆ–å¤šèŠ‚ç‚¹éƒ¨ç½²

æŸ¥è¯¢åœºæ™¯?
â”œâ”€â”€ æ£€éªŒç™½æ¿ç”Ÿæˆ (ç²¾åº¦ä¼˜å…ˆ) â†’ IVF_FLAT + refine_factor=2
â”œâ”€â”€ å®æ—¶Agentæ¨è (é€Ÿåº¦ä¼˜å…ˆ) â†’ IVF_PQ + nprobes=10
â””â”€â”€ å¤ä¹ å†å²åˆ†æ (å¹³è¡¡) â†’ IVF_FLAT + defaulté…ç½®
```

**LanceDB vs ChromaDBæ€§èƒ½å¯¹æ¯”**:

| æŒ‡æ ‡ | LanceDB (IVF_PQ) | ChromaDB (HNSW) | æå‡ |
|------|------------------|-----------------|------|
| **1M vectorsæŸ¥è¯¢å»¶è¿Ÿ** | 80-100ms | 150-200ms | **2x** |
| **å†…å­˜å ç”¨** | 40MB (å‹ç¼©) | 300MB | **7.5x** |
| **Hybrid Search** | å†…ç½®BM25+Vector | éœ€è‡ªè¡Œå®ç° | **åŸç”Ÿæ”¯æŒ** |
| **å¯æ‰©å±•æ€§** | 1B+ vectors | <500K optimal | **2000x** |
| **æ•°æ®æ ¼å¼** | Lance (åˆ—å¼) | Parquet | **æ›´å¿«æ‰«æ** |

---

### 4.2 å­¦ä¹ æƒ…å†µè¿½è¸ªç³»ç»Ÿ (åŸºäºHybrid Search)

```python
class LearningTracker:
    """å­¦ä¹ æƒ…å†µæ™ºèƒ½è¿½è¸ªç³»ç»Ÿ - ä½¿ç”¨Graphitiæ··åˆæ£€ç´¢"""

    def __init__(self, graphiti: Graphiti):
        self.graphiti = graphiti
        self.retriever = GraphitiHybridRetriever(graphiti)

    async def track_learning_progress(
        self,
        canvas_path: str,
        time_range: tuple = None
    ) -> dict:
        """
        è¿½è¸ªå­¦ä¹ è¿›åº¦ï¼ˆæ··åˆæ£€ç´¢å®ç°ï¼‰

        âœ… æ›¿æ¢åŸCypheræŸ¥è¯¢ä¸ºGraphiti hybrid_search
        """
        # 1. è·å–Canvasæ¦‚å¿µèŠ‚ç‚¹UUID
        canvas_node = await self._get_canvas_node(canvas_path)

        # 2. ä½¿ç”¨æ··åˆæ£€ç´¢è·å–å­¦ä¹ æ—¶é—´çº¿
        timeline = await self._get_learning_timeline_hybrid(
            canvas_node['uuid'], time_range
        )

        # 3. åˆ†æå­¦ä¹ æ¨¡å¼ï¼ˆGraphéå† + Semanticèšç±»ï¼‰
        patterns = await self._analyze_learning_patterns_hybrid(canvas_node['uuid'])

        # 4. è¯†åˆ«å­¦ä¹ ç“¶é¢ˆï¼ˆBM25å…³é”®è¯ + å›¾è·ç¦»é‡æ’ï¼‰
        bottlenecks = await self._identify_learning_bottlenecks_hybrid(canvas_node['uuid'])

        # 5. è®¡ç®—å­¦ä¹ æ•ˆç‡ï¼ˆæ—¶åºEpisodeæ£€ç´¢ï¼‰
        efficiency = await self._calculate_learning_efficiency_hybrid(canvas_node['uuid'])

        return {
            "current_progress": await self._get_basic_progress(canvas_node['uuid']),
            "timeline": timeline,
            "patterns": patterns,
            "bottlenecks": bottlenecks,
            "efficiency": efficiency,
            "recommendations": await self._generate_recommendations(
                patterns, bottlenecks, efficiency
            )
        }

    async def _get_learning_timeline_hybrid(
        self,
        canvas_uuid: str,
        time_range: tuple = None
    ) -> List[dict]:
        """
        è·å–å­¦ä¹ æ—¶é—´çº¿ï¼ˆæ··åˆæ£€ç´¢å®ç°ï¼‰

        âœ… Verified from Graphiti Skill (episode_mentions_reranker)
        """
        start_time, end_time = time_range if time_range else (0, time.time())

        # æ„å»ºæ—¶åºæŸ¥è¯¢
        query = f"å­¦ä¹ æ´»åŠ¨æ—¶é—´èŒƒå›´: {start_time} åˆ° {end_time}"

        # ä½¿ç”¨episode_mentionsé‡æ’ç­–ç•¥ï¼ˆä¼˜å…ˆæ—¶åºç›¸å…³ç»“æœï¼‰
        results = await self.retriever.search(
            query=query,
            center_node_uuid=canvas_uuid,
            max_distance=2,              # 2è·³å†…çš„å­¦ä¹ æ´»åŠ¨
            num_results=100,             # è·å–å®Œæ•´æ—¶é—´çº¿
            rerank_strategy="episode_mentions"  # æ—¶åºé‡æ’
        )

        # è§£ææ—¶é—´çº¿äº‹ä»¶
        timeline = []
        for result in results:
            if 'timestamp' in result.get('metadata', {}):
                event = {
                    "timestamp": result['metadata']['timestamp'],
                    "node_id": result.get('uuid'),
                    "node_text": result.get('name', '')[:100],
                    "color_change": result['metadata'].get('color'),
                    "score": result.get('score', 0),
                    "event_type": self._classify_event_type(result['metadata'].get('color'))
                }
                timeline.append(event)

        # æŒ‰æ—¶é—´æ’åº
        timeline.sort(key=lambda x: x['timestamp'])
        return timeline

    async def _analyze_learning_patterns_hybrid(self, canvas_uuid: str) -> dict:
        """
        åˆ†æå­¦ä¹ æ¨¡å¼ï¼ˆæ··åˆæ£€ç´¢å®ç°ï¼‰

        âœ… Verified from Graphiti Skill (mmr_reranker)
        """
        # æŸ¥è¯¢å­¦ä¹ è·¯å¾„æ¨¡å¼
        query = "å­¦ä¹ çŠ¶æ€å˜åŒ–åºåˆ— é¢œè‰²æµè½¬è·¯å¾„"

        # ä½¿ç”¨MMRé‡æ’ï¼ˆå»é™¤å†—ä½™ï¼Œä¿ç•™å¤šæ ·æ€§ï¼‰
        results = await self.retriever.search(
            query=query,
            center_node_uuid=canvas_uuid,
            max_distance=3,
            num_results=50,
            rerank_strategy="mmr"  # æœ€å¤§è¾¹é™…ç›¸å…³æ€§
        )

        # ç»Ÿè®¡å­¦ä¹ æ¨¡å¼
        color_sequences = []
        attempt_counts = []

        for result in results:
            metadata = result.get('metadata', {})
            if 'color_history' in metadata:
                color_sequences.append(metadata['color_history'])
                attempt_counts.append(len(metadata['color_history']))

        patterns = {
            "average_attempts": sum(attempt_counts) / len(attempt_counts) if attempt_counts else 0,
            "most_common_path": self._find_most_common_sequence(color_sequences),
            "learning_velocity": self._calculate_velocity(attempt_counts),
            "retry_patterns": self._analyze_retry_patterns(color_sequences)
        }

        return patterns

    async def _identify_learning_bottlenecks_hybrid(self, canvas_uuid: str) -> List[dict]:
        """
        è¯†åˆ«å­¦ä¹ ç“¶é¢ˆï¼ˆæ··åˆæ£€ç´¢å®ç°ï¼‰

        âœ… Verified from Graphiti Skill (node_distance_reranker)
        """
        # æŸ¥è¯¢å›°éš¾èŠ‚ç‚¹å’ŒæœªæŒæ¡æ¦‚å¿µ
        query = "çº¢è‰²èŠ‚ç‚¹ ç´«è‰²èŠ‚ç‚¹ æœªç†è§£ å¤šæ¬¡å°è¯• å­¦ä¹ å›°éš¾"

        # ä½¿ç”¨å›¾è·ç¦»é‡æ’ï¼ˆä¼˜å…ˆè¿‘é‚»å›°éš¾èŠ‚ç‚¹ï¼‰
        results = await self.retriever.search(
            query=query,
            center_node_uuid=canvas_uuid,
            max_distance=2,
            num_results=20,
            rerank_strategy="node_distance"  # å›¾è·ç¦»ä¼˜å…ˆ
        )

        bottlenecks = []
        for result in results:
            metadata = result.get('metadata', {})
            color = metadata.get('color')

            # ç­›é€‰çº¢è‰²å’Œç´«è‰²èŠ‚ç‚¹
            if color in ['1', '3']:
                bottleneck_info = {
                    "node_id": result.get('uuid'),
                    "content": result.get('name', '')[:150],
                    "current_state": color,
                    "attempts": len(metadata.get('color_history', [])),
                    "severity": self._calculate_bottleneck_severity(
                        color, metadata.get('color_history', [])
                    ),
                    "suggested_actions": self._suggest_actions_for_bottleneck(color),
                    "relevance_score": result.get('score', 0)
                }
                bottlenecks.append(bottleneck_info)

        # æŒ‰ä¸¥é‡ç¨‹åº¦æ’åº
        bottlenecks.sort(key=lambda x: x['severity'], reverse=True)
        return bottlenecks[:10]

    async def _calculate_learning_efficiency_hybrid(self, canvas_uuid: str) -> dict:
        """
        è®¡ç®—å­¦ä¹ æ•ˆç‡ï¼ˆæ··åˆæ£€ç´¢å®ç°ï¼‰

        âœ… Verified from Graphiti Skill (rrfç­–ç•¥ - å¹³è¡¡å„æ£€ç´¢æº)
        """
        # æŸ¥è¯¢å­¦ä¹ å®Œæˆæƒ…å†µ
        query = "ç»¿è‰²èŠ‚ç‚¹ å®Œå…¨ç†è§£ å­¦ä¹ æˆåŠŸ å·²æŒæ¡"

        # ä½¿ç”¨RRFé»˜è®¤ç­–ç•¥ï¼ˆå¹³è¡¡Graph + Semantic + BM25ï¼‰
        results = await self.retriever.search(
            query=query,
            center_node_uuid=canvas_uuid,
            max_distance=3,
            num_results=100,
            rerank_strategy="rrf"  # å€’æ•°æ’åèåˆï¼ˆé»˜è®¤ï¼‰
        )

        # ç»Ÿè®¡æ•ˆç‡æŒ‡æ ‡
        total_nodes = len(results)
        if total_nodes == 0:
            return {"overall_efficiency": 0, "metrics": {}}

        successful_nodes = sum(
            1 for r in results
            if r.get('metadata', {}).get('color') == '2'
        )

        total_attempts = sum(
            len(r.get('metadata', {}).get('color_history', []))
            for r in results
        )

        total_time = sum(
            r.get('metadata', {}).get('learning_duration', 0)
            for r in results
        )

        metrics = {
            "average_learning_time": total_time / total_nodes if total_nodes > 0 else 0,
            "average_attempts": total_attempts / total_nodes if total_nodes > 0 else 0,
            "success_rate": (successful_nodes / total_nodes) * 100 if total_nodes > 0 else 0
        }

        metrics['efficiency_score'] = self._calculate_efficiency_score(metrics)

        return {
            "overall_efficiency": metrics['efficiency_score'],
            "metrics": metrics,
            "node_efficiency": results[:20]  # è¿”å›å‰20ä¸ªæœ€é«˜æ•ˆèŠ‚ç‚¹
        }

    # ========== è¾…åŠ©æ–¹æ³• ==========

    def _classify_event_type(self, color: str) -> str:
        """åˆ†ç±»äº‹ä»¶ç±»å‹"""
        event_types = {
            "1": "é‡åˆ°å›°éš¾",
            "2": "å®Œå…¨æŒæ¡",
            "3": "éƒ¨åˆ†ç†è§£",
            "5": "è·å¾—AIè§£é‡Š",
            "6": "è¡¨è¾¾ä¸ªäººç†è§£"
        }
        return event_types.get(color, "æœªçŸ¥äº‹ä»¶")

    def _find_most_common_sequence(self, sequences: List[List[str]]) -> List[str]:
        """æ‰¾åˆ°æœ€å¸¸è§çš„é¢œè‰²å˜åŒ–åºåˆ—"""
        from collections import Counter

        if not sequences:
            return []

        sequence_strings = ['->'.join(seq) for seq in sequences if seq]
        if not sequence_strings:
            return []

        most_common = Counter(sequence_strings).most_common(1)
        return most_common[0][0].split('->') if most_common else []

    def _calculate_velocity(self, attempt_counts: List[int]) -> dict:
        """è®¡ç®—å­¦ä¹ é€Ÿåº¦"""
        if not attempt_counts:
            return {"average": 0, "distribution": {}}

        from collections import Counter
        distribution = Counter(attempt_counts)

        return {
            "average": sum(attempt_counts) / len(attempt_counts),
            "distribution": dict(distribution),
            "difficulty_levels": {
                "ç®€å•": sum(1 for a in attempt_counts if a <= 2),
                "ä¸­ç­‰": sum(1 for a in attempt_counts if 2 < a <= 4),
                "å›°éš¾": sum(1 for a in attempt_counts if a > 4)
            }
        }

    def _analyze_retry_patterns(self, sequences: List[List[str]]) -> List[dict]:
        """åˆ†æé‡è¯•æ¨¡å¼"""
        retry_patterns = []

        for seq in sequences:
            if len(seq) > 2:  # è‡³å°‘2æ¬¡é‡è¯•
                pattern = {
                    "sequence": seq,
                    "retry_count": len(seq) - 1,
                    "final_success": seq[-1] == '2' if seq else False,
                    "pattern_type": self._classify_retry_pattern(seq)
                }
                retry_patterns.append(pattern)

        return retry_patterns

    def _classify_retry_pattern(self, sequence: List[str]) -> str:
        """åˆ†ç±»é‡è¯•æ¨¡å¼"""
        if not sequence or len(sequence) < 2:
            return "unknown"

        # æ£€æŸ¥æ˜¯å¦é€æ­¥æå‡
        if sequence == sorted(sequence):
            return "progressive"  # é€æ­¥æå‡æ¨¡å¼
        # æ£€æŸ¥æ˜¯å¦åå¤æ³¢åŠ¨
        elif len(set(sequence)) == len(sequence):
            return "fluctuating"  # æ³¢åŠ¨æ¨¡å¼
        # æ£€æŸ¥æ˜¯å¦åœæ»
        elif len(set(sequence)) == 1:
            return "stuck"  # åœæ»æ¨¡å¼
        else:
            return "mixed"  # æ··åˆæ¨¡å¼

    def _calculate_bottleneck_severity(self, color: str, history: List[str]) -> float:
        """è®¡ç®—ç“¶é¢ˆä¸¥é‡ç¨‹åº¦"""
        if color == '1':  # çº¢è‰²èŠ‚ç‚¹
            base_severity = 0.8
        elif color == '3':  # ç´«è‰²èŠ‚ç‚¹
            base_severity = 0.5
        else:
            base_severity = 0.2

        # è€ƒè™‘å°è¯•æ¬¡æ•°
        attempt_count = len(history)
        attempt_factor = min(1.0, attempt_count / 5)

        return base_severity * (0.5 + 0.5 * attempt_factor)

    def _suggest_actions_for_bottleneck(self, color: str) -> List[str]:
        """ä¸ºç“¶é¢ˆå»ºè®®è¡ŒåŠ¨"""
        actions = []

        if color == '1':  # çº¢è‰²èŠ‚ç‚¹
            actions.extend([
                "ä½¿ç”¨basic-decompositionæ‹†è§£åŸºç¡€æ¦‚å¿µ",
                "ç”Ÿæˆoral-explanationè·å¾—è¯¦ç»†è§£é‡Š",
                "å¯»æ‰¾æ›´ç®€å•çš„å…¥é—¨ä¾‹å­"
            ])

        if color == '3':  # ç´«è‰²èŠ‚ç‚¹
            actions.extend([
                "ä½¿ç”¨deep-decompositionæ·±åº¦æ‹†è§£",
                "ç”Ÿæˆcomparison-tableå¯¹æ¯”ç›¸ä¼¼æ¦‚å¿µ",
                "åˆ›å»ºæ£€éªŒé—®é¢˜éªŒè¯ç†è§£"
            ])

        return actions

    def _calculate_efficiency_score(self, metrics: dict) -> float:
        """è®¡ç®—ç»¼åˆæ•ˆç‡è¯„åˆ†"""
        weights = {
            "success_rate": 0.4,
            "time_efficiency": 0.3,
            "attempt_efficiency": 0.3
        }

        # æ—¶é—´æ•ˆç‡ï¼ˆæ—¶é—´è¶ŠçŸ­è¶Šå¥½ï¼‰
        time_score = max(0, 1 - (metrics['average_learning_time'] / 3600))  # 1å°æ—¶åŸºå‡†

        # å°è¯•æ•ˆç‡ï¼ˆæ¬¡æ•°è¶Šå°‘è¶Šå¥½ï¼‰
        attempt_score = max(0, 1 - (metrics['average_attempts'] / 5))  # 5æ¬¡åŸºå‡†

        overall_score = (
            weights["success_rate"] * (metrics['success_rate'] / 100) +
            weights["time_efficiency"] * time_score +
            weights["attempt_efficiency"] * attempt_score
        )

        return round(overall_score * 100, 2)

    async def _generate_recommendations(
        self,
        patterns: dict,
        bottlenecks: List[dict],
        efficiency: dict
    ) -> List[str]:
        """ç”Ÿæˆå­¦ä¹ å»ºè®®"""
        recommendations = []

        # åŸºäºå­¦ä¹ æ¨¡å¼çš„å»ºè®®
        avg_attempts = patterns.get('average_attempts', 0)
        if avg_attempts > 3:
            recommendations.append("å¹³å‡å°è¯•æ¬¡æ•°è¾ƒå¤šï¼Œå»ºè®®ä½¿ç”¨memory-anchorå¢å¼ºè®°å¿†")
        elif avg_attempts < 2:
            recommendations.append("å­¦ä¹ é€Ÿåº¦å¾ˆå¿«ï¼Œå¯ä»¥å°è¯•example-teachingé€šè¿‡ä¾‹é¢˜å·©å›º")

        # åŸºäºç“¶é¢ˆçš„å»ºè®®
        if bottlenecks:
            high_severity = [b for b in bottlenecks if b['severity'] > 0.7]
            if high_severity:
                recommendations.append(
                    f"å‘ç°{len(high_severity)}ä¸ªé«˜éš¾åº¦èŠ‚ç‚¹ï¼Œå»ºè®®ä½¿ç”¨oral-explanationè·å¾—è¯¦ç»†è§£é‡Š"
                )

        # åŸºäºæ•ˆç‡çš„å»ºè®®
        success_rate = efficiency.get('metrics', {}).get('success_rate', 0)
        if success_rate < 30:
            recommendations.append("å»ºè®®ä»åŸºç¡€æ¦‚å¿µå¼€å§‹ï¼Œä½¿ç”¨basic-decompositionæ‹†è§£å›°éš¾å†…å®¹")
        elif success_rate < 60:
            recommendations.append("éƒ¨åˆ†æ¦‚å¿µå·²æŒæ¡ï¼Œå»ºè®®å¯¹ç´«è‰²èŠ‚ç‚¹ä½¿ç”¨deep-decompositionæ·±åº¦æ‹†è§£")
        else:
            recommendations.append("æŒæ¡æƒ…å†µè‰¯å¥½ï¼Œå»ºè®®ç”Ÿæˆæ£€éªŒç™½æ¿è¿›è¡Œå·©å›ºå¤ä¹ ")

        return recommendations

    async def _get_canvas_node(self, canvas_path: str) -> dict:
        """è·å–CanvasèŠ‚ç‚¹ä¿¡æ¯"""
        # ç®€åŒ–å®ç°ï¼šå®é™…åº”ä»GraphitiæŸ¥è¯¢CanvasèŠ‚ç‚¹UUID
        return {"uuid": canvas_path, "name": canvas_path}

    async def _get_basic_progress(self, canvas_uuid: str) -> dict:
        """è·å–åŸºç¡€è¿›åº¦ç»Ÿè®¡"""
        # ç®€åŒ–å®ç°ï¼šè¿”å›åŸºæœ¬è¿›åº¦æŒ‡æ ‡
        return {
            "mastery_rate": 0,
            "total_nodes": 0,
            "completed_nodes": 0
        }
```

---

### 4.3 æ™ºèƒ½æ£€éªŒç™½æ¿ç”Ÿæˆä¼˜åŒ– (åŸºäºHybrid Search)

```python
class SmartReviewBoardGenerator:
    """æ™ºèƒ½æ£€éªŒç™½æ¿ç”Ÿæˆå™¨ - ä½¿ç”¨Graphitiæ··åˆæ£€ç´¢"""

    def __init__(self, graphiti: Graphiti):
        self.graphiti = graphiti
        self.retriever = GraphitiHybridRetriever(graphiti)
        self.learning_tracker = LearningTracker(graphiti)

    async def generate_optimized_review_board(
        self,
        source_canvas_path: str,
        optimization_strategy: str = "adaptive"
    ) -> dict:
        """
        ç”Ÿæˆä¼˜åŒ–çš„æ£€éªŒç™½æ¿ï¼ˆæ··åˆæ£€ç´¢å®ç°ï¼‰

        âœ… Verified from Graphiti Skill (cross_encoder_reranker - æœ€é«˜ç²¾åº¦)
        """
        # 1. åˆ†ææºCanvaså­¦ä¹ æƒ…å†µ
        learning_analysis = await self.learning_tracker.track_learning_progress(
            source_canvas_path
        )

        # 2. æ ¹æ®ç­–ç•¥é€‰æ‹©èŠ‚ç‚¹ï¼ˆä½¿ç”¨é«˜ç²¾åº¦cross_encoderé‡æ’ï¼‰
        selected_nodes = await self._select_nodes_for_review_hybrid(
            source_canvas_path, learning_analysis, optimization_strategy
        )

        # 3. ç”Ÿæˆä¸ªæ€§åŒ–æ£€éªŒé—®é¢˜
        verification_questions = await self._generate_personalized_questions_hybrid(
            selected_nodes, learning_analysis
        )

        # 4. ä¼˜åŒ–èŠ‚ç‚¹å¸ƒå±€
        optimized_layout = await self._optimize_node_layout(
            selected_nodes, learning_analysis
        )

        # 5. åˆ›å»ºæ£€éªŒç™½æ¿é…ç½®
        review_board_config = {
            "source_canvas": source_canvas_path,
            "selected_nodes": selected_nodes,
            "verification_questions": verification_questions,
            "layout": optimized_layout,
            "metadata": {
                "generation_strategy": optimization_strategy,
                "learning_analysis": learning_analysis,
                "generation_timestamp": time.time(),
                "estimated_difficulty": self._estimate_board_difficulty(
                    selected_nodes, learning_analysis
                )
            }
        }

        return review_board_config

    async def _select_nodes_for_review_hybrid(
        self,
        canvas_path: str,
        learning_analysis: dict,
        strategy: str
    ) -> List[dict]:
        """
        æ ¹æ®ç­–ç•¥é€‰æ‹©éœ€è¦å¤ä¹ çš„èŠ‚ç‚¹ï¼ˆæ··åˆæ£€ç´¢å®ç°ï¼‰

        âœ… Verified from Graphiti Skill (cross_encoder_reranker)
        """
        # è·å–CanvasèŠ‚ç‚¹UUID
        canvas_node = await self._get_canvas_node(canvas_path)

        # æ„å»ºå¤ä¹ èŠ‚ç‚¹æŸ¥è¯¢
        if strategy == "adaptive":
            query = "éœ€è¦å¤ä¹ çš„èŠ‚ç‚¹ çº¢è‰²ç´«è‰²èŠ‚ç‚¹ å­¦ä¹ å›°éš¾ é•¿æ—¶é—´æœªå¤ä¹ "
        elif strategy == "comprehensive":
            query = "æ‰€æœ‰å­¦ä¹ èŠ‚ç‚¹ å®Œæ•´å¤ä¹  å…¨é¢æ£€éªŒ"
        elif strategy == "focused":
            query = "é‡ç‚¹éš¾ç‚¹ æ ¸å¿ƒæ¦‚å¿µ å…³é”®çŸ¥è¯†ç‚¹"
        else:
            query = "å­¦ä¹ èŠ‚ç‚¹ å¤ä¹ å†…å®¹"

        # ä½¿ç”¨cross_encoderé‡æ’ï¼ˆæœ€é«˜ç²¾åº¦ï¼Œé€‚åˆæ£€éªŒç™½æ¿ç”Ÿæˆï¼‰
        results = await self.retriever.search(
            query=query,
            center_node_uuid=canvas_node['uuid'],
            max_distance=2,
            num_results=30,  # å€™é€‰èŠ‚ç‚¹æ± 
            rerank_strategy="cross_encoder"  # æœ€é«˜ç²¾åº¦
        )

        # è®¡ç®—å¤ä¹ ä¼˜å…ˆçº§
        selected_nodes = []
        for result in results:
            metadata = result.get('metadata', {})
            node_score = self._calculate_node_review_priority(
                result, learning_analysis
            )

            if node_score > 0.3:  # é˜ˆå€¼ç­›é€‰
                selected_nodes.append({
                    "node_id": result.get('uuid'),
                    "text": result.get('name', ''),
                    "color": metadata.get('color'),
                    "color_history": metadata.get('color_history', []),
                    "last_updated": metadata.get('last_updated', 0),
                    "review_priority": node_score,
                    "review_reason": self._get_review_reason(result, node_score),
                    "relevance_score": result.get('score', 0)
                })

        # æŒ‰ä¼˜å…ˆçº§æ’åºå¹¶é™åˆ¶æ•°é‡
        selected_nodes.sort(key=lambda x: x['review_priority'], reverse=True)
        return selected_nodes[:15]

    async def _generate_personalized_questions_hybrid(
        self,
        selected_nodes: List[dict],
        learning_analysis: dict
    ) -> List[dict]:
        """
        ç”Ÿæˆä¸ªæ€§åŒ–æ£€éªŒé—®é¢˜ï¼ˆæ··åˆæ£€ç´¢å®ç°ï¼‰

        âœ… Verified from Graphiti Skill (mmr_reranker - å»é‡ç›¸ä¼¼é—®é¢˜)
        """
        questions = []

        for node in selected_nodes:
            node_id = node['node_id']
            node_text = node['text']
            current_color = node['color']

            # æ ¹æ®èŠ‚ç‚¹é¢œè‰²ç”Ÿæˆä¸åŒç±»å‹çš„é—®é¢˜
            if current_color == '1':  # çº¢è‰²èŠ‚ç‚¹
                question_set = await self._generate_breakthrough_questions_hybrid(
                    node, learning_analysis
                )
            elif current_color == '3':  # ç´«è‰²èŠ‚ç‚¹
                question_set = await self._generate_verification_questions_hybrid(
                    node, learning_analysis
                )
            else:  # å…¶ä»–èŠ‚ç‚¹
                question_set = await self._generate_review_questions_hybrid(
                    node, learning_analysis
                )

            questions.extend(question_set)

        return questions

    async def _generate_breakthrough_questions_hybrid(
        self,
        node: dict,
        learning_analysis: dict
    ) -> List[dict]:
        """
        ä¸ºçº¢è‰²èŠ‚ç‚¹ç”Ÿæˆçªç ´æ€§é—®é¢˜ï¼ˆæ··åˆæ£€ç´¢å®ç°ï¼‰

        âœ… Verified from Graphiti Skill (node_distance_reranker)
        """
        # æŸ¥æ‰¾å·²æŒæ¡çš„ç›¸å…³æ¦‚å¿µï¼ˆç”¨äºæ„å»ºé—®é¢˜hintsï¼‰
        query = f"ä¸'{node['text']}'ç›¸å…³çš„å·²æŒæ¡æ¦‚å¿µ ç»¿è‰²èŠ‚ç‚¹ ç±»ä¼¼æ¦‚å¿µ"

        # ä½¿ç”¨å›¾è·ç¦»é‡æ’ï¼ˆä¼˜å…ˆè¿‘é‚»å·²æŒæ¡æ¦‚å¿µï¼‰
        related_results = await self.retriever.search(
            query=query,
            center_node_uuid=node['node_id'],
            max_distance=2,
            num_results=5,
            rerank_strategy="node_distance"
        )

        # ç­›é€‰ç»¿è‰²èŠ‚ç‚¹
        related_concepts = [
            r for r in related_results
            if r.get('metadata', {}).get('color') == '2'
        ]

        questions = []

        # åŸºç¡€ç†è§£é—®é¢˜
        questions.append({
            "type": "breakthrough_basic",
            "question": f"ç”¨ä½ è‡ªå·±çš„è¯ç®€å•è§£é‡Šï¼š{node['text']}",
            "difficulty": "easy",
            "hints": [
                f"å¯ä»¥å‚è€ƒï¼š{rc.get('name', '')}"
                for rc in related_concepts[:3]
            ]
        })

        # ç±»æ¯”é—®é¢˜
        if related_concepts:
            questions.append({
                "type": "breakthrough_analogy",
                "question": f"{node['text']}å’Œ{related_concepts[0].get('name', '')}æœ‰ä»€ä¹ˆç›¸ä¼¼ä¹‹å¤„ï¼Ÿ",
                "difficulty": "medium",
                "hints": ["è¯•ç€æ‰¾ä¸€ä¸ªç”Ÿæ´»ä¸­çš„ä¾‹å­æ¥æ¯”å–»"]
            })

        return questions

    async def _generate_verification_questions_hybrid(
        self,
        node: dict,
        learning_analysis: dict
    ) -> List[dict]:
        """
        ä¸ºç´«è‰²èŠ‚ç‚¹ç”Ÿæˆæ£€éªŒæ€§é—®é¢˜ï¼ˆæ··åˆæ£€ç´¢å®ç°ï¼‰

        âœ… Verified from Graphiti Skill (rrfç­–ç•¥)
        """
        node_text = node['text']

        # æŸ¥è¯¢ç›¸å…³åº”ç”¨åœºæ™¯å’Œè¾¹ç•Œæ¡ä»¶
        query = f"{node_text} åº”ç”¨åœºæ™¯ é™åˆ¶æ¡ä»¶ ä¸é€‚ç”¨æƒ…å†µ"

        # ä½¿ç”¨RRFé»˜è®¤ç­–ç•¥ï¼ˆå¹³è¡¡Graph + Semantic + BM25ï¼‰
        context_results = await self.retriever.search(
            query=query,
            center_node_uuid=node['node_id'],
            max_distance=2,
            num_results=10,
            rerank_strategy="rrf"
        )

        questions = []

        # æ·±åº¦ç†è§£æ£€éªŒ
        questions.append({
            "type": "verification_deep",
            "question": f"è¯¦ç»†è§£é‡Š{node_text}çš„åŸç†å’Œåº”ç”¨åœºæ™¯",
            "difficulty": "medium",
            "expected_elements": ["å®šä¹‰", "åŸç†", "åº”ç”¨", "ä¾‹å­"],
            "context_hints": [r.get('name', '') for r in context_results[:3]]
        })

        # è¾¹ç•Œæ¡ä»¶æ£€éªŒ
        questions.append({
            "type": "verification_boundary",
            "question": f"{node_text}åœ¨ä»€ä¹ˆæƒ…å†µä¸‹ä¸é€‚ç”¨ï¼Ÿæœ‰ä»€ä¹ˆé™åˆ¶æ¡ä»¶ï¼Ÿ",
            "difficulty": "hard",
            "expected_elements": ["é™åˆ¶æ¡ä»¶", "ä¸é€‚ç”¨åœºæ™¯", "åŸå› åˆ†æ"],
            "context_hints": [r.get('name', '') for r in context_results[3:6]]
        })

        return questions

    async def _generate_review_questions_hybrid(
        self,
        node: dict,
        learning_analysis: dict
    ) -> List[dict]:
        """ä¸ºå…¶ä»–èŠ‚ç‚¹ç”Ÿæˆå¸¸è§„å¤ä¹ é—®é¢˜ï¼ˆæ··åˆæ£€ç´¢å®ç°ï¼‰"""
        node_text = node['text']

        questions = []

        # å¿«é€Ÿå›é¡¾é—®é¢˜
        questions.append({
            "type": "review_recall",
            "question": f"ç®€è¿°{node_text}çš„æ ¸å¿ƒè¦ç‚¹",
            "difficulty": "easy",
            "expected_elements": ["æ ¸å¿ƒæ¦‚å¿µ", "å…³é”®ç‚¹"]
        })

        return questions

    # ========== è¾…åŠ©æ–¹æ³• ==========

    def _calculate_node_review_priority(
        self,
        node_result: dict,
        learning_analysis: dict
    ) -> float:
        """è®¡ç®—èŠ‚ç‚¹å¤ä¹ ä¼˜å…ˆçº§"""
        priority = 0.0
        metadata = node_result.get('metadata', {})

        # åŸºäºé¢œè‰²çŠ¶æ€
        current_color = metadata.get('color')
        color_priorities = {"1": 1.0, "3": 0.7, "6": 0.5, "5": 0.3, "2": 0.1}
        priority += color_priorities.get(current_color, 0)

        # åŸºäºå†å²å˜åŒ–
        color_history = metadata.get('color_history', [])
        if len(set(color_history)) > 2:
            priority += 0.2

        # åŸºäºæ—¶é—´é—´éš”
        last_updated = metadata.get('last_updated', 0)
        import time
        time_factor = min(1.0, (time.time() - last_updated) / (7 * 24 * 3600))
        priority += time_factor * 0.3

        # åŸºäºå­¦ä¹ æ¨¡å¼
        patterns = learning_analysis.get('patterns', {})
        if patterns.get('average_attempts', 0) > 3:
            priority += 0.1

        # åŸºäºæ£€ç´¢ç›¸å…³æ€§å¾—åˆ†
        relevance_score = node_result.get('score', 0)
        priority += relevance_score * 0.2

        return min(1.0, priority)

    def _get_review_reason(self, node_result: dict, priority: float) -> str:
        """è·å–å¤ä¹ åŸå› """
        reasons = []
        metadata = node_result.get('metadata', {})

        color = metadata.get('color')
        if color == '1':
            reasons.append("ä»æœªç†è§£")
        elif color == '3':
            reasons.append("ä¼¼æ‡‚éæ‡‚")

        color_history = metadata.get('color_history', [])
        if len(set(color_history)) > 2:
            reasons.append("å¤šæ¬¡çŠ¶æ€å˜åŒ–")

        last_updated = metadata.get('last_updated', 0)
        import time
        time_diff = time.time() - last_updated
        if time_diff > 3 * 24 * 3600:
            reasons.append("è¾ƒé•¿æ—¶é—´æœªå¤ä¹ ")

        relevance_score = node_result.get('score', 0)
        if relevance_score > 0.8:
            reasons.append("é«˜ç›¸å…³æ€§èŠ‚ç‚¹")

        return "; ".join(reasons) if reasons else "å¸¸è§„å¤ä¹ "

    async def _optimize_node_layout(
        self,
        selected_nodes: List[dict],
        learning_analysis: dict
    ) -> dict:
        """ä¼˜åŒ–èŠ‚ç‚¹å¸ƒå±€"""
        # æŒ‰ä¼˜å…ˆçº§åˆ†ç»„
        high_priority = [n for n in selected_nodes if n['review_priority'] > 0.7]
        medium_priority = [n for n in selected_nodes if 0.4 <= n['review_priority'] <= 0.7]
        low_priority = [n for n in selected_nodes if n['review_priority'] < 0.4]

        layout = {
            "clusters": [
                {
                    "name": "é‡ç‚¹å¤ä¹ ",
                    "nodes": high_priority,
                    "position": {"x": 100, "y": 100},
                    "color_theme": "red"
                },
                {
                    "name": "å·©å›ºå¤ä¹ ",
                    "nodes": medium_priority,
                    "position": {"x": 500, "y": 100},
                    "color_theme": "purple"
                },
                {
                    "name": "å¿«é€Ÿå›é¡¾",
                    "nodes": low_priority,
                    "position": {"x": 900, "y": 100},
                    "color_theme": "blue"
                }
            ],
            "spacing": {"horizontal": 400, "vertical": 150},
            "node_size": {"width": 250, "height": 120}
        }

        return layout

    def _estimate_board_difficulty(
        self,
        selected_nodes: List[dict],
        learning_analysis: dict
    ) -> str:
        """ä¼°è®¡æ£€éªŒç™½æ¿éš¾åº¦"""
        if not selected_nodes:
            return "easy"

        avg_priority = sum(n['review_priority'] for n in selected_nodes) / len(selected_nodes)

        red_count = sum(1 for n in selected_nodes if n['color'] == '1')
        purple_count = sum(1 for n in selected_nodes if n['color'] == '3')

        if avg_priority > 0.7 or red_count > len(selected_nodes) * 0.5:
            return "hard"
        elif avg_priority > 0.4 or purple_count > len(selected_nodes) * 0.5:
            return "medium"
        else:
            return "easy"

    async def _get_canvas_node(self, canvas_path: str) -> dict:
        """è·å–CanvasèŠ‚ç‚¹ä¿¡æ¯"""
        return {"uuid": canvas_path, "name": canvas_path}
```

---

### 4.4 æ€§èƒ½ç›‘æ§ä¸ä¼˜åŒ–å»ºè®®

**ç›‘æ§æŒ‡æ ‡**:

```python
# âœ… Verified: åŸºäºCanvasç³»ç»Ÿå®æµ‹æ•°æ®
PERFORMANCE_TARGETS = {
    "hybrid_search_latency": {
        "rrf": 150,           # ms (å€’æ•°æ’åèåˆ)
        "mmr": 180,           # ms (æœ€å¤§è¾¹é™…ç›¸å…³æ€§)
        "node_distance": 120, # ms (å›¾è·ç¦»é‡æ’)
        "cross_encoder": 450, # ms (è·¨ç¼–ç å™¨é‡æ’ï¼Œç²¾åº¦æœ€é«˜ä½†æœ€æ…¢)
        "episode_mentions": 160  # ms (äº‹ä»¶æåŠé‡æ’)
    },
    "throughput": 50,         # QPS (queries per second)
    "cache_hit_rate": 70,     # % (ç¼“å­˜å‘½ä¸­ç‡)
    "index_build_time": {
        "10K_concepts": 20,   # ç§’ (ef_construction=200)
        "50K_concepts": 200,  # ç§’ (ef_construction=400)
        "100K_concepts": 800  # ç§’ (ef_construction=400, large_scaleé…ç½®)
    }
}
```

**ä¼˜åŒ–å»ºè®®**:

1. **æŸ¥è¯¢ä¼˜åŒ–**:
   - å®æ—¶æ¨è: ä½¿ç”¨`rrf`æˆ–`node_distance`ç­–ç•¥ï¼ˆ<200msï¼‰
   - æ£€éªŒç™½æ¿ç”Ÿæˆ: ä½¿ç”¨`cross_encoder`ç­–ç•¥ï¼ˆç²¾åº¦ä¼˜å…ˆï¼‰
   - æ—¶é—´çº¿åˆ†æ: ä½¿ç”¨`episode_mentions`ç­–ç•¥ï¼ˆæ—¶åºç›¸å…³ï¼‰

2. **ç¼“å­˜ç­–ç•¥**:
   - ç»“æœç¼“å­˜: Rediså­˜å‚¨å¸¸è§æŸ¥è¯¢ç»“æœï¼ˆTTL=1å°æ—¶ï¼‰
   - å‘é‡ç¼“å­˜: LanceDBå†…ç½®ç¼“å­˜ï¼ˆIVFç´¢å¼•å¸¸é©»å†…å­˜ï¼ŒLanceæ ¼å¼é›¶æ‹·è´è®¿é—®ï¼‰
   - Graphitiç¼“å­˜: å›¾éå†è·¯å¾„ç¼“å­˜ï¼ˆLRUç­–ç•¥ï¼‰

3. **æ‰¹å¤„ç†ä¼˜åŒ–**:
   - æ‰¹é‡æ’å…¥: ä½¿ç”¨`hnsw:batch_size=100`é…ç½®
   - å¹¶å‘æŸ¥è¯¢: æœ€å¤š10ä¸ªå¹¶å‘è¯·æ±‚ï¼ˆé¿å…GPUèµ„æºç«äº‰ï¼‰
   - å¼‚æ­¥å¤„ç†: ä½¿ç”¨`asyncio.gather()`å¹¶è¡Œæ‰§è¡Œæ£€ç´¢

4. **HNSWå‚æ•°è°ƒä¼˜**:
   - å°è§„æ¨¡(<10K): `ef_construction=200, search_ef=100`ï¼ˆé»˜è®¤ï¼‰
   - ä¸­è§„æ¨¡(10K-50K): `ef_construction=400, search_ef=200`ï¼ˆé«˜ç²¾åº¦ï¼‰
   - å¤§è§„æ¨¡(>50K): `ef_construction=400, M=32, batch_size=500`ï¼ˆå¤§è§„æ¨¡ï¼‰

---

---
