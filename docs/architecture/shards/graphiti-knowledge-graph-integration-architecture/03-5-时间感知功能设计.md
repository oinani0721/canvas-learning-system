# GRAPHITI-KNOWLEDGE-GRAPH-INTEGRATION-ARCHITECTURE - Part 3

**Source**: `GRAPHITI-KNOWLEDGE-GRAPH-INTEGRATION-ARCHITECTURE.md`
**Sections**: â° 5. æ—¶é—´æ„ŸçŸ¥åŠŸèƒ½è®¾è®¡, âš¡ 6. æ€§èƒ½ä¼˜åŒ–ç­–ç•¥, ğŸ“¦ 7. æ•°æ®è¿ç§»æ–¹æ¡ˆ

---

## â° 5. æ—¶é—´æ„ŸçŸ¥åŠŸèƒ½è®¾è®¡

### 5.1 å­¦ä¹ æ—¶é—´çº¿è¿½è¸ª

```python
class LearningTimelineTracker:
    """å­¦ä¹ æ—¶é—´çº¿è¿½è¸ªå™¨"""

    def __init__(self, kg_layer: KnowledgeGraphLayer):
        self.kg_layer = kg_layer

    async def create_learning_timeline(self, canvas_path: str,
                                     time_range: tuple = None) -> dict:
        """åˆ›å»ºå­¦ä¹ æ—¶é—´çº¿"""
        start_time, end_time = time_range if time_range else (0, time.time())

        # è·å–æ—¶é—´çº¿äº‹ä»¶
        events = await self._get_timeline_events(canvas_path, start_time, end_time)

        # æ„å»ºæ—¶é—´çº¿ç»“æ„
        timeline = {
            "canvas_path": canvas_path,
            "time_range": {"start": start_time, "end": end_time},
            "events": events,
            "periods": await self._identify_learning_periods(events),
            "milestones": await self._identify_learning_milestones(events),
            "insights": await self._generate_timeline_insights(events)
        }

        return timeline

    async def _get_timeline_events(self, canvas_path: str,
                                 start_time: float, end_time: float) -> List[dict]:
        """è·å–æ—¶é—´çº¿äº‹ä»¶"""
        events_query = f"""
        MATCH (c:canvas {{path: "{canvas_path}"}})
        -[*]->(event)
        WHERE event.timestamp >= {start_time} AND event.timestamp <= {end_time}
        RETURN event, labels(event) as event_types
        ORDER BY event.timestamp
        """

        raw_events = await self.kg_layer.custom_query(events_query)

        events = []
        for raw_event in raw_events:
            event_data = raw_event['event']
            event_types = raw_event['event_types']

            event = {
                "timestamp": event_data.get('timestamp'),
                "type": self._classify_event(event_types, event_data),
                "description": self._generate_event_description(event_data, event_types),
                "metadata": event_data,
                "impact_level": self._assess_event_impact(event_data, event_types)
            }

            events.append(event)

        return events

    def _classify_event(self, event_types: List[str], event_data: dict) -> str:
        """åˆ†ç±»äº‹ä»¶ç±»å‹"""
        if 'understanding_state' in event_types:
            return "progress_change"
        elif 'ai_explanation' in event_types:
            return "explanation_received"
        elif 'verification_question' in event_types:
            return "questions_generated"
        elif 'learning_session' in event_types:
            return "session_activity"
        else:
            return "general_activity"

    def _generate_event_description(self, event_data: dict, event_types: List[str]) -> str:
        """ç”Ÿæˆäº‹ä»¶æè¿°"""
        if 'understanding_state' in event_types:
            color = event_data.get('color', '')
            color_meaning = self._get_color_meaning(color)
            return f"å­¦ä¹ çŠ¶æ€æ›´æ–°ï¼š{color_meaning}"
        elif 'ai_explanation' in event_types:
            concept = event_data.get('concept', 'æœªçŸ¥æ¦‚å¿µ')
            exp_type = event_data.get('type', 'è§£é‡Š')
            return f"è·å¾—{exp_type}ï¼š{concept}"
        elif 'verification_question' in event_types:
            questions_count = event_data.get('questions_count', 0)
            return f"ç”Ÿæˆ{questions_count}ä¸ªæ£€éªŒé—®é¢˜"
        else:
            return "å­¦ä¹ æ´»åŠ¨"

    def _assess_event_impact(self, event_data: dict, event_types: List[str]) -> str:
        """è¯„ä¼°äº‹ä»¶å½±å“çº§åˆ«"""
        if 'understanding_state' in event_types:
            color = event_data.get('color', '')
            if color == '2':  # ç»¿è‰²
                return "high"
            elif color == '1':  # çº¢è‰²
                return "medium"
            else:
                return "low"
        elif 'ai_explanation' in event_types:
            return "medium"
        else:
            return "low"

    async def _identify_learning_periods(self, events: List[dict]) -> List[dict]:
        """è¯†åˆ«å­¦ä¹ æ—¶æ®µ"""
        if not events:
            return []

        periods = []
        current_period = None

        for event in events:
            # å¦‚æœæ—¶é—´é—´éš”è¶…è¿‡30åˆ†é’Ÿï¼Œå¼€å§‹æ–°æ—¶æ®µ
            if (current_period is None or
                event['timestamp'] - current_period['end_time'] > 1800):

                if current_period:
                    periods.append(current_period)

                current_period = {
                    "start_time": event['timestamp'],
                    "end_time": event['timestamp'],
                    "events": [event],
                    "duration": 0,
                    "activity_level": "low"
                }
            else:
                current_period['end_time'] = event['timestamp']
                current_period['events'].append(event)

        if current_period:
            periods.append(current_period)

        # è®¡ç®—æ—¶æ®µç»Ÿè®¡ä¿¡æ¯
        for period in periods:
            period['duration'] = period['end_time'] - period['start_time']
            period['event_count'] = len(period['events'])
            period['activity_level'] = self._classify_activity_level(period['event_count'], period['duration'])

        return periods

    def _classify_activity_level(self, event_count: int, duration: float) -> str:
        """åˆ†ç±»æ´»åŠ¨æ°´å¹³"""
        if duration == 0:
            return "low"

        event_rate = event_count / (duration / 60)  # æ¯åˆ†é’Ÿäº‹ä»¶æ•°

        if event_rate > 0.5:
            return "high"
        elif event_rate > 0.2:
            return "medium"
        else:
            return "low"

    async def _identify_learning_milestones(self, events: List[dict]) -> List[dict]:
        """è¯†åˆ«å­¦ä¹ é‡Œç¨‹ç¢‘"""
        milestones = []

        for event in events:
            if event['type'] == 'progress_change':
                metadata = event['metadata']
                if metadata.get('color') == '2':  # è¾¾åˆ°ç»¿è‰²ï¼ˆå®Œå…¨ç†è§£ï¼‰
                    milestones.append({
                        "timestamp": event['timestamp'],
                        "type": "concept_mastered",
                        "description": f"æŒæ¡æ¦‚å¿µï¼š{metadata.get('node_id', 'æœªçŸ¥èŠ‚ç‚¹')}",
                        "impact": "high"
                    })
                elif metadata.get('improvement', 0) > 0.5:
                    milestones.append({
                        "timestamp": event['timestamp'],
                        "type": "significant_improvement",
                        "description": f"æ˜¾è‘—è¿›æ­¥ï¼š{metadata.get('node_id', 'æœªçŸ¥èŠ‚ç‚¹')}",
                        "impact": "medium"
                    })

        return milestones

    async def _generate_timeline_insights(self, events: List[dict]) -> dict:
        """ç”Ÿæˆæ—¶é—´çº¿æ´å¯Ÿ"""
        if not events:
            return {}

        insights = {
            "total_learning_time": 0,
            "most_active_period": None,
            "learning_velocity": {},
            "stagnation_periods": [],
            "breakthrough_moments": []
        }

        # è®¡ç®—æ€»å­¦ä¹ æ—¶é—´
        if events:
            insights['total_learning_time'] = events[-1]['timestamp'] - events[0]['timestamp']

        # è¯†åˆ«æœ€æ´»è·ƒæ—¶æ®µ
        hour_activity = {}
        for event in events:
            hour = time.localtime(event['timestamp']).tm_hour
            hour_activity[hour] = hour_activity.get(hour, 0) + 1

        if hour_activity:
            most_active_hour = max(hour_activity, key=hour_activity.get)
            insights['most_active_period'] = {
                "hour": most_active_hour,
                "activity_count": hour_activity[most_active_hour]
            }

        # è¯†åˆ«çªç ´æ—¶åˆ»
        for event in events:
            if event['impact_level'] == 'high':
                insights['breakthrough_moments'].append({
                    "timestamp": event['timestamp'],
                    "description": event['description']
                })

        return insights

    def _get_color_meaning(self, color: str) -> str:
        """è·å–é¢œè‰²å«ä¹‰"""
        meanings = {
            "1": "ä¸ç†è§£",
            "2": "å®Œå…¨ç†è§£",
            "3": "ä¼¼æ‡‚éæ‡‚",
            "5": "AIè§£é‡Š",
            "6": "ä¸ªäººç†è§£"
        }
        return meanings.get(color, "æœªçŸ¥çŠ¶æ€")
```

### 5.2 çŸ¥è¯†æŒæ¡æ—¶é—´çº¿

```python
class KnowledgeMasteryTimeline:
    """çŸ¥è¯†æŒæ¡æ—¶é—´çº¿"""

    def __init__(self, kg_layer: KnowledgeGraphLayer):
        self.kg_layer = kg_layer

    async def create_mastery_timeline(self, canvas_path: str) -> dict:
        """åˆ›å»ºçŸ¥è¯†æŒæ¡æ—¶é—´çº¿"""
        # è·å–æ‰€æœ‰æ¦‚å¿µçš„æŒæ¡å†å²
        mastery_history = await self._get_mastery_history(canvas_path)

        # æ„å»ºæ—¶é—´çº¿
        timeline = {
            "canvas_path": canvas_path,
            "mastery_history": mastery_history,
            "mastery_curve": await self._build_mastery_curve(mastery_history),
            "forgetting_curve": await self._build_forgetting_curve(mastery_history),
            "retention_prediction": await self._predict_retention(mastery_history),
            "review_schedule": await self._generate_review_schedule(mastery_history)
        }

        return timeline

    async def _get_mastery_history(self, canvas_path: str) -> List[dict]:
        """è·å–æŒæ¡å†å²"""
        history_query = f"""
        MATCH (c:canvas {{path: "{canvas_path}"}})
        -[:contains]->(n:node)
        -[r:has_understanding_state]->(s:understanding_state)
        RETURN n.id as node_id, n.text as text, s.color as color,
               r.timestamp as timestamp, s.score as score
        ORDER BY node_id, timestamp
        """

        raw_history = await self.kg_layer.custom_query(history_query)

        # æŒ‰èŠ‚ç‚¹ç»„ç»‡å†å²
        history_by_node = {}
        for record in raw_history:
            node_id = record['node_id']
            if node_id not in history_by_node:
                history_by_node[node_id] = {
                    "node_id": node_id,
                    "text": record['text'],
                    "mastery_events": []
                }

            history_by_node[node_id]['mastery_events'].append({
                "timestamp": record['timestamp'],
                "color": record['color'],
                "score": record.get('score', {}),
                "mastery_level": self._calculate_mastery_level(record['color'], record.get('score', {}))
            })

        return list(history_by_node.values())

    def _calculate_mastery_level(self, color: str, score: dict) -> float:
        """è®¡ç®—æŒæ¡æ°´å¹³"""
        base_levels = {"1": 0.0, "3": 0.5, "6": 0.7, "2": 1.0}
        base_level = base_levels.get(color, 0.0)

        # å¦‚æœæœ‰è¯„åˆ†ï¼Œè°ƒæ•´åŸºç¡€æ°´å¹³
        if score:
            total_score = sum(score.values()) if isinstance(score, dict) else 0
            if total_score > 0:
                base_level = min(1.0, base_level + (total_score / 400))  # å‡è®¾æ€»åˆ†400

        return base_level

    async def _build_mastery_curve(self, mastery_history: List[dict]) -> dict:
        """æ„å»ºæŒæ¡æ›²çº¿"""
        curve_data = {
            "timeline": [],
            "overall_mastery": [],
            "node_mastery": {}
        }

        # æ”¶é›†æ‰€æœ‰æ—¶é—´ç‚¹
        all_timestamps = set()
        for node_data in mastery_history:
            for event in node_data['mastery_events']:
                all_timestamps.add(event['timestamp'])

        sorted_timestamps = sorted(all_timestamps)

        # è®¡ç®—æ¯ä¸ªæ—¶é—´ç‚¹çš„æ•´ä½“æŒæ¡åº¦
        for timestamp in sorted_timestamps:
            mastery_levels = []
            node_levels = {}

            for node_data in mastery_history:
                # æ‰¾åˆ°è¯¥æ—¶é—´ç‚¹æœ€è¿‘çš„æŒæ¡çŠ¶æ€
                latest_level = 0
                for event in node_data['mastery_events']:
                    if event['timestamp'] <= timestamp:
                        latest_level = event['mastery_level']

                mastery_levels.append(latest_level)
                node_levels[node_data['node_id']] = latest_level

            overall_mastery = sum(mastery_levels) / len(mastery_levels) if mastery_levels else 0

            curve_data['timeline'].append(timestamp)
            curve_data['overall_mastery'].append(overall_mastery)

            # è®°å½•èŠ‚ç‚¹æŒæ¡åº¦
            for node_id, level in node_levels.items():
                if node_id not in curve_data['node_mastery']:
                    curve_data['node_mastery'][node_id] = []
                curve_data['node_mastery'][node_id].append(level)

        return curve_data

    async def _build_forgetting_curve(self, mastery_history: List[dict]) -> dict:
        """æ„å»ºé—å¿˜æ›²çº¿"""
        forgetting_data = {
            "retention_rates": [],
            "time_intervals": [],
            "predictions": {}
        }

        # åˆ†æä¸åŒæ—¶é—´é—´éš”çš„ä¿æŒç‡
        for node_data in mastery_history:
            events = node_data['mastery_events']
            if len(events) < 2:
                continue

            # åˆ†ææ¯æ¬¡æŒæ¡åçš„é—å¿˜æƒ…å†µ
            for i in range(len(events) - 1):
                current_event = events[i]
                next_event = events[i + 1]

                if current_event['mastery_level'] > 0.8:  # ä»æŒæ¡çŠ¶æ€å¼€å§‹
                    time_interval = next_event['timestamp'] - current_event['timestamp']
                    retention_rate = next_event['mastery_level'] / current_event['mastery_level']

                    forgetting_data['retention_rates'].append(retention_rate)
                    forgetting_data['time_intervals'].append(time_interval / 86400)  # è½¬æ¢ä¸ºå¤©

        # ç”Ÿæˆé¢„æµ‹æ¨¡å‹
        if forgetting_data['retention_rates']:
            forgetting_data['predictions'] = self._build_forgetting_model(
                forgetting_data['time_intervals'],
                forgetting_data['retention_rates']
            )

        return forgetting_data

    def _build_forgetting_model(self, time_intervals: List[float],
                              retention_rates: List[float]) -> dict:
        """æ„å»ºé—å¿˜æ¨¡å‹"""
        if not time_intervals or not retention_rates:
            return {}

        # ç®€å•çš„æŒ‡æ•°è¡°å‡æ¨¡å‹æ‹Ÿåˆ
        import numpy as np

        # å°†æ•°æ®è½¬æ¢ä¸ºnumpyæ•°ç»„
        t = np.array(time_intervals)
        r = np.array(retention_rates)

        # æ‹Ÿåˆ R(t) = a * exp(-b * t)
        # å–å¯¹æ•°ï¼šln(R) = ln(a) - b * t
        log_r = np.log(r + 0.01)  # é¿å…log(0)

        # çº¿æ€§å›å½’æ‹Ÿåˆ
        coeffs = np.polyfit(t, log_r, 1)
        b = -coeffs[0]
        a = np.exp(coeffs[1])

        return {
            "model_type": "exponential_decay",
            "parameters": {"a": a, "b": b},
            "formula": f"R(t) = {a:.2f} * exp(-{b:.2f} * t)",
            "half_life": np.log(2) / b if b > 0 else float('inf'),
            "confidence": self._calculate_model_confidence(t, r, a, b)
        }

    def _calculate_model_confidence(self, t: np.ndarray, r: np.ndarray,
                                  a: float, b: float) -> float:
        """è®¡ç®—æ¨¡å‹ç½®ä¿¡åº¦"""
        # è®¡ç®—é¢„æµ‹å€¼
        predicted_r = a * np.exp(-b * t)

        # è®¡ç®—RÂ²
        ss_res = np.sum((r - predicted_r) ** 2)
        ss_tot = np.sum((r - np.mean(r)) ** 2)
        r_squared = 1 - (ss_res / ss_tot)

        return max(0, r_squared)

    async def _predict_retention(self, mastery_history: List[dict]) -> dict:
        """é¢„æµ‹çŸ¥è¯†ä¿æŒæƒ…å†µ"""
        predictions = {
            "short_term": {},  # 1å‘¨å†…
            "medium_term": {},  # 1ä¸ªæœˆå†…
            "long_term": {}    # 3ä¸ªæœˆå†…
        }

        current_time = time.time()
        time_intervals = {
            "short_term": 7 * 86400,    # 7å¤©
            "medium_term": 30 * 86400,  # 30å¤©
            "long_term": 90 * 86400     # 90å¤©
        }

        for node_data in mastery_history:
            node_id = node_data['node_id']
            events = node_data['mastery_events']

            if not events:
                continue

            # è·å–æœ€è¿‘çš„æŒæ¡çŠ¶æ€
            latest_event = max(events, key=lambda x: x['timestamp'])
            current_mastery = latest_event['mastery_level']
            time_since_mastery = current_time - latest_event['timestamp']

            for period, interval in time_intervals.items():
                future_time = time_since_mastery + interval
                predicted_mastery = self._predict_future_mastery(
                    current_mastery, future_time
                )

                predictions[period][node_id] = {
                    "current_mastery": current_mastery,
                    "predicted_mastery": predicted_mastery,
                    "retention_rate": predicted_mastery / current_mastery if current_mastery > 0 else 0,
                    "needs_review": predicted_mastery < 0.7
                }

        return predictions

    def _predict_future_mastery(self, current_mastery: float, time_ahead: float) -> float:
        """é¢„æµ‹æœªæ¥æŒæ¡æ°´å¹³"""
        # ç®€å•çš„é—å¿˜æ›²çº¿æ¨¡å‹
        # R(t) = R(0) * exp(-Î» * t)
        # ä½¿ç”¨æ ‡å‡†é—å¿˜ç‡ Î» = 0.1/å¤©
        daily_decay_rate = 0.1
        days_ahead = time_ahead / 86400

        predicted = current_mastery * np.exp(-daily_decay_rate * days_ahead)
        return max(0, predicted)

    async def _generate_review_schedule(self, mastery_history: List[dict]) -> List[dict]:
        """ç”Ÿæˆå¤ä¹ è®¡åˆ’"""
        schedule = []
        current_time = time.time()

        for node_data in mastery_history:
            node_id = node_data['node_id']
            events = node_data['mastery_events']

            if not events:
                continue

            latest_event = max(events, key=lambda x: x['timestamp'])
            current_mastery = latest_event['mastery_level']
            last_review = latest_event['timestamp']

            # è®¡ç®—ä¸‹æ¬¡å¤ä¹ æ—¶é—´
            if current_mastery < 0.5:
                # æŒæ¡ä¸è¶³ï¼Œå°½å¿«å¤ä¹ 
                next_review = current_time + 86400  # 1å¤©å
            elif current_mastery < 0.8:
                # éƒ¨åˆ†æŒæ¡ï¼Œ3å¤©åå¤ä¹ 
                next_review = current_time + 3 * 86400
            else:
                # åŸºæœ¬æŒæ¡ï¼Œä½¿ç”¨é—´éš”é‡å¤
                days_since_review = (current_time - last_review) / 86400
                interval = self._calculate_spaced_repetition_interval(days_since_review)
                next_review = current_time + interval * 86400

            schedule.append({
                "node_id": node_id,
                "node_text": node_data['text'][:100] + "..." if len(node_data['text']) > 100 else node_data['text'],
                "current_mastery": current_mastery,
                "last_review": last_review,
                "next_review": next_review,
                "priority": self._calculate_review_priority(current_mastery, next_review - current_time),
                "review_type": self._suggest_review_type(current_mastery)
            })

        # æŒ‰å¤ä¹ æ—¶é—´æ’åº
        schedule.sort(key=lambda x: x['next_review'])

        return schedule

    def _calculate_spaced_repetition_interval(self, days_since_review: float) -> int:
        """è®¡ç®—é—´éš”é‡å¤é—´éš”ï¼ˆå¤©ï¼‰"""
        # ç®€åŒ–çš„é—´éš”é‡å¤ç®—æ³•
        if days_since_review < 1:
            return 1
        elif days_since_review < 3:
            return 3
        elif days_since_review < 7:
            return 7
        elif days_since_review < 14:
            return 14
        elif days_since_review < 30:
            return 30
        else:
            return 60

    def _calculate_review_priority(self, mastery: float, days_until_review: float) -> str:
        """è®¡ç®—å¤ä¹ ä¼˜å…ˆçº§"""
        days_until = days_until_review / 86400

        if mastery < 0.5 or days_until < 1:
            return "high"
        elif mastery < 0.8 or days_until < 3:
            return "medium"
        else:
            return "low"

    def _suggest_review_type(self, mastery: float) -> str:
        """å»ºè®®å¤ä¹ ç±»å‹"""
        if mastery < 0.5:
            return "relearn"
        elif mastery < 0.8:
            return "practice"
        else:
            return "review"
```

---


## âš¡ 6. æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

### 6.1 ç¼“å­˜ç³»ç»Ÿè®¾è®¡

```python
class KnowledgeGraphCache:
    """çŸ¥è¯†å›¾è°±ç¼“å­˜ç³»ç»Ÿ"""

    def __init__(self, cache_size: int = 1000):
        self.cache_size = cache_size
        self.cache = {}
        self.cache_timestamps = {}
        self.cache_access_count = {}
        self.cache_ttl = {
            "node_info": 3600,      # 1å°æ—¶
            "learning_progress": 300,  # 5åˆ†é’Ÿ
            "search_results": 600,   # 10åˆ†é’Ÿ
            "timeline_data": 1800,   # 30åˆ†é’Ÿ
            "mastery_history": 900   # 15åˆ†é’Ÿ
        }

    async def get_cached_result(self, cache_key: str, data_type: str) -> Optional[dict]:
        """è·å–ç¼“å­˜ç»“æœ"""
        if cache_key not in self.cache:
            return None

        # æ£€æŸ¥TTL
        timestamp = self.cache_timestamps.get(cache_key, 0)
        ttl = self.cache_ttl.get(data_type, 300)
        if time.time() - timestamp > ttl:
            self._remove_from_cache(cache_key)
            return None

        # æ›´æ–°è®¿é—®è®¡æ•°
        self.cache_access_count[cache_key] = self.cache_access_count.get(cache_key, 0) + 1
        return self.cache[cache_key]

    async def cache_result(self, cache_key: str, data: dict, data_type: str):
        """ç¼“å­˜ç»“æœ"""
        # å¦‚æœç¼“å­˜å·²æ»¡ï¼Œç§»é™¤æœ€å°‘ä½¿ç”¨çš„é¡¹
        if len(self.cache) >= self.cache_size:
            self._evict_least_used()

        self.cache[cache_key] = data
        self.cache_timestamps[cache_key] = time.time()
        self.cache_access_count[cache_key] = 1

    def _remove_from_cache(self, cache_key: str):
        """ä»ç¼“å­˜ä¸­ç§»é™¤"""
        self.cache.pop(cache_key, None)
        self.cache_timestamps.pop(cache_key, None)
        self.cache_access_count.pop(cache_key, None)

    def _evict_least_used(self):
        """ç§»é™¤æœ€å°‘ä½¿ç”¨çš„ç¼“å­˜é¡¹"""
        if not self.cache_access_count:
            return

        least_used_key = min(self.cache_access_count, key=self.cache_access_count.get)
        self._remove_from_cache(least_used_key)

    def get_cache_stats(self) -> dict:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "cache_size": len(self.cache),
            "max_cache_size": self.cache_size,
            "hit_rate": self._calculate_hit_rate(),
            "memory_usage": self._estimate_memory_usage()
        }

    def _calculate_hit_rate(self) -> float:
        """è®¡ç®—ç¼“å­˜å‘½ä¸­ç‡"""
        total_accesses = sum(self.cache_access_count.values())
        if total_accesses == 0:
            return 0.0
        return len(self.cache) / total_accesses

    def _estimate_memory_usage(self) -> int:
        """ä¼°ç®—å†…å­˜ä½¿ç”¨é‡"""
        import sys
        total_size = 0
        for key, value in self.cache.items():
            total_size += sys.getsizeof(key) + sys.getsizeof(value)
        return total_size
```

### 6.2 å¼‚æ­¥æ“ä½œä¼˜åŒ–

```python
class AsyncOptimizedKnowledgeGraph:
    """å¼‚æ­¥ä¼˜åŒ–çš„çŸ¥è¯†å›¾è°±æ“ä½œ"""

    def __init__(self, kg_layer: KnowledgeGraphLayer, cache: KnowledgeGraphCache):
        self.kg_layer = kg_layer
        self.cache = cache
        self.operation_queue = asyncio.Queue()
        self.batch_operations = {}
        self.batch_size = 50
        self.batch_timeout = 5.0  # 5ç§’
        self.background_task = None

    async def start_background_processor(self):
        """å¯åŠ¨åå°æ‰¹å¤„ç†"""
        if self.background_task is None:
            self.background_task = asyncio.create_task(self._process_batch_operations())

    async def stop_background_processor(self):
        """åœæ­¢åå°æ‰¹å¤„ç†"""
        if self.background_task:
            self.background_task.cancel()
            try:
                await self.background_task
            except asyncio.CancelledError:
                pass
            self.background_task = None

    async def add_triplet_async(self, triplet: KnowledgeGraphTriplet):
        """å¼‚æ­¥æ·»åŠ ä¸‰å…ƒç»„"""
        operation = {
            "type": "add_triplet",
            "data": triplet,
            "timestamp": time.time()
        }
        await self.operation_queue.put(operation)

    async def search_with_cache(self, query: str, search_type: str = "hybrid",
                               limit: int = 10) -> List[dict]:
        """å¸¦ç¼“å­˜çš„æœç´¢"""
        cache_key = f"search_{hash(query)}_{search_type}_{limit}"

        # å°è¯•ä»ç¼“å­˜è·å–
        cached_result = await self.cache.get_cached_result(cache_key, "search_results")
        if cached_result:
            return cached_result

        # æ‰§è¡Œæœç´¢
        try:
            results = await asyncio.wait_for(
                self.kg_layer.search_knowledge(query, limit),
                timeout=10.0  # 10ç§’è¶…æ—¶
            )

            # ç¼“å­˜ç»“æœ
            await self.cache.cache_result(cache_key, results, "search_results")
            return results

        except asyncio.TimeoutError:
            logger.warning(f"æœç´¢è¶…æ—¶: {query}")
            return []

    async def _process_batch_operations(self):
        """åå°æ‰¹å¤„ç†æ“ä½œ"""
        while True:
            try:
                # æ”¶é›†æ‰¹é‡æ“ä½œ
                batch = []
                timeout_task = asyncio.create_task(asyncio.sleep(self.batch_timeout))

                while len(batch) < self.batch_size:
                    try:
                        operation = await asyncio.wait_for(
                            self.operation_queue.get(),
                            timeout=1.0
                        )
                        batch.append(operation)
                    except asyncio.TimeoutError:
                        break

                # å¦‚æœæœ‰æ“ä½œï¼Œå¤„ç†å®ƒä»¬
                if batch:
                    await self._execute_batch(batch)

                # ç­‰å¾…è¶…æ—¶æˆ–æ–°æ“ä½œ
                try:
                    await asyncio.wait_for(timeout_task, timeout=1.0)
                except asyncio.TimeoutError:
                    continue

            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"æ‰¹å¤„ç†æ“ä½œé”™è¯¯: {e}")
                await asyncio.sleep(1.0)

    async def _execute_batch(self, batch: List[dict]):
        """æ‰§è¡Œæ‰¹é‡æ“ä½œ"""
        try:
            # æŒ‰æ“ä½œç±»å‹åˆ†ç»„
            add_triplets = []
            other_operations = []

            for operation in batch:
                if operation["type"] == "add_triplet":
                    add_triplets.append(operation["data"])
                else:
                    other_operations.append(operation)

            # æ‰¹é‡æ·»åŠ ä¸‰å…ƒç»„
            if add_triplets:
                await self.kg_layer.add_triplets_batch(add_triplets)

        except Exception as e:
            logger.error(f"æ‰§è¡Œæ‰¹é‡æ“ä½œå¤±è´¥: {e}")
```

---


## ğŸ“¦ 7. æ•°æ®è¿ç§»æ–¹æ¡ˆ

### 7.1 è¿ç§»è§„åˆ’å™¨

```python
class CanvasToKGMigrationPlanner:
    """Canvasåˆ°çŸ¥è¯†å›¾è°±è¿ç§»è§„åˆ’å™¨"""

    def __init__(self, source_dir: str, kg_layer: KnowledgeGraphLayer):
        self.source_dir = source_dir
        self.kg_layer = kg_layer

    async def plan_migration(self) -> dict:
        """è§„åˆ’è¿ç§»ç­–ç•¥"""
        # æ‰«ææ‰€æœ‰Canvasæ–‡ä»¶
        canvas_files = await self._scan_canvas_files()

        # åˆ†æCanvasæ–‡ä»¶
        canvas_analysis = await self._analyze_canvas_files(canvas_files)

        # åˆ›å»ºè¿ç§»è®¡åˆ’
        migration_plan = await self._create_migration_plan(canvas_analysis)

        return {
            "canvas_files": canvas_files,
            "analysis": canvas_analysis,
            "plan": migration_plan,
            "estimated_time": self._estimate_migration_time(canvas_analysis)
        }

    async def _scan_canvas_files(self) -> List[str]:
        """æ‰«æCanvasæ–‡ä»¶"""
        canvas_files = []
        for root, dirs, files in os.walk(self.source_dir):
            for file in files:
                if file.endswith('.canvas'):
                    canvas_files.append(os.path.join(root, file))
        return canvas_files

    async def _analyze_canvas_files(self, canvas_files: List[str]) -> dict:
        """åˆ†æCanvasæ–‡ä»¶"""
        analysis = {
            "total_files": len(canvas_files),
            "total_size": 0,
            "total_nodes": 0,
            "total_edges": 0,
            "file_details": []
        }

        for canvas_file in canvas_files:
            try:
                with open(canvas_file, 'r', encoding='utf-8') as f:
                    canvas_data = json.load(f)

                nodes = canvas_data.get('nodes', [])
                edges = canvas_data.get('edges', [])
                file_size = os.path.getsize(canvas_file)

                file_detail = {
                    "path": canvas_file,
                    "size": file_size,
                    "node_count": len(nodes),
                    "edge_count": len(edges),
                    "complexity_score": self._calculate_complexity_score(nodes, edges)
                }

                analysis["file_details"].append(file_detail)
                analysis["total_size"] += file_size
                analysis["total_nodes"] += len(nodes)
                analysis["total_edges"] += len(edges)

            except Exception as e:
                logger.error(f"åˆ†æCanvasæ–‡ä»¶å¤±è´¥ {canvas_file}: {e}")

        return analysis

    def _calculate_complexity_score(self, nodes: List[dict], edges: List[dict]) -> float:
        """è®¡ç®—Canvaså¤æ‚åº¦è¯„åˆ†"""
        node_score = len(nodes) * 1.0
        edge_score = len(edges) * 1.5
        text_complexity = sum(len(node.get('text', '')) for node in nodes) / 1000.0
        return node_score + edge_score + text_complexity

    async def _create_migration_plan(self, analysis: dict) -> dict:
        """åˆ›å»ºè¿ç§»è®¡åˆ’"""
        # æŒ‰å¤æ‚åº¦æ’åºæ–‡ä»¶
        sorted_files = sorted(analysis["file_details"],
                            key=lambda x: x["complexity_score"],
                            reverse=True)

        # åˆ†æ‰¹è§„åˆ’
        batches = []
        current_batch = []
        current_complexity = 0
        max_batch_complexity = 1000

        for file_detail in sorted_files:
            if current_complexity + file_detail["complexity_score"] > max_batch_complexity:
                if current_batch:
                    batches.append(current_batch)
                    current_batch = []
                    current_complexity = 0

            current_batch.append(file_detail)
            current_complexity += file_detail["complexity_score"]

        if current_batch:
            batches.append(current_batch)

        return {
            "batches": batches,
            "total_batches": len(batches),
            "strategy": "complexity_based_batching"
        }

    def _estimate_migration_time(self, analysis: dict) -> dict:
        """ä¼°ç®—è¿ç§»æ—¶é—´"""
        total_complexity = sum(f["complexity_score"] for f in analysis["file_details"])
        base_time_minutes = total_complexity / 100
        estimated_minutes = base_time_minutes * 1.5  # å®‰å…¨ç³»æ•°

        return {
            "estimated_minutes": int(estimated_minutes),
            "estimated_hours": estimated_minutes / 60
        }
```

### 7.2 è¿ç§»æ‰§è¡Œå™¨

```python
class CanvasToKGMigrator:
    """Canvasåˆ°çŸ¥è¯†å›¾è°±è¿ç§»æ‰§è¡Œå™¨"""

    def __init__(self, kg_layer: KnowledgeGraphLayer, cache: KnowledgeGraphCache):
        self.kg_layer = kg_layer
        self.cache = cache
        self.migration_progress = MigrationProgress()

    async def execute_migration(self, migration_plan: dict) -> dict:
        """æ‰§è¡Œè¿ç§»"""
        self.migration_progress.start_migration()

        try:
            results = []
            for batch_index, batch in enumerate(migration_plan["batches"]):
                batch_result = await self._migrate_batch(batch, batch_index)
                results.append(batch_result)

            # å®Œæˆè¿ç§»
            migration_summary = await self._complete_migration(results)
            return migration_summary

        except Exception as e:
            logger.error(f"è¿ç§»æ‰§è¡Œå¤±è´¥: {e}")
            raise

    async def _migrate_batch(self, batch: List[dict], batch_index: int) -> dict:
        """è¿ç§»å•ä¸ªæ‰¹æ¬¡"""
        batch_start_time = time.time()
        batch_results = {
            "batch_index": batch_index,
            "files": [],
            "success_count": 0,
            "error_count": 0,
            "start_time": batch_start_time
        }

        for file_detail in batch:
            try:
                file_result = await self._migrate_canvas_file(file_detail)
                batch_results["files"].append(file_result)

                if file_result["success"]:
                    batch_results["success_count"] += 1
                else:
                    batch_results["error_count"] += 1

            except Exception as e:
                logger.error(f"è¿ç§»æ–‡ä»¶å¤±è´¥ {file_detail['path']}: {e}")
                batch_results["error_count"] += 1

            await asyncio.sleep(0.01)  # è®©å‡ºæ§åˆ¶æƒ

        batch_results["end_time"] = time.time()
        batch_results["duration"] = batch_results["end_time"] - batch_results["start_time"]
        return batch_results

    async def _migrate_canvas_file(self, file_detail: dict) -> dict:
        """è¿ç§»å•ä¸ªCanvasæ–‡ä»¶"""
        canvas_path = file_detail["path"]

        try:
            # è¯»å–Canvasæ–‡ä»¶
            with open(canvas_path, 'r', encoding='utf-8') as f:
                canvas_data = json.load(f)

            # ç”ŸæˆCanvas ID
            canvas_id = f"canvas_{hash(canvas_path)}_{int(time.time())}"

            # åˆ›å»ºCanvaså®ä½“
            await self._create_canvas_entity(canvas_id, canvas_path, canvas_data)

            # è¿ç§»èŠ‚ç‚¹
            node_results = await self._migrate_nodes(canvas_id, canvas_data.get('nodes', []))

            # è¿ç§»è¾¹
            edge_results = await self._migrate_edges(canvas_id, canvas_data.get('edges', []))

            return {
                "path": canvas_path,
                "success": True,
                "canvas_id": canvas_id,
                "nodes_migrated": len(node_results["successful"]),
                "edges_migrated": len(edge_results["successful"])
            }

        except Exception as e:
            return {
                "path": canvas_path,
                "success": False,
                "error": str(e)
            }

    async def _create_canvas_entity(self, canvas_id: str, canvas_path: str, canvas_data: dict):
        """åˆ›å»ºCanvaså®ä½“"""
        canvas_triplet = KnowledgeGraphTriplet(
            subject=canvas_id,
            relation=RelationType.CREATED_AT_TIME.value,
            object=str(time.time()),
            subject_type=EntityType.CANVAS.value,
            object_type="timestamp",
            metadata={
                "path": canvas_path,
                "name": os.path.basename(canvas_path),
                "migration_timestamp": time.time()
            }
        )
        await self.kg_layer.add_triplet(canvas_triplet)

    async def _migrate_nodes(self, canvas_id: str, nodes: List[dict]) -> dict:
        """è¿ç§»èŠ‚ç‚¹"""
        node_results = {"successful": [], "failed": []}

        for node in nodes:
            try:
                node_id = f"node_{canvas_id}_{node['id']}"
                node_attrs = NodeAttributes(node)

                # åˆ›å»ºèŠ‚ç‚¹å®ä½“
                node_triplet = KnowledgeGraphTriplet(
                    subject=node_id,
                    relation=RelationType.CREATED_AT_TIME.value,
                    object=str(time.time()),
                    subject_type=EntityType.NODE.value,
                    object_type="timestamp",
                    metadata={
                        **node_attrs.__dict__,
                        "canvas_id": canvas_id,
                        "migration_timestamp": time.time()
                    }
                )
                await self.kg_layer.add_triplet(node_triplet)

                # CanvasåŒ…å«èŠ‚ç‚¹å…³ç³»
                contains_triplet = KnowledgeGraphTriplet(
                    subject=canvas_id,
                    relation=RelationType.CONTAINS.value,
                    object=node_id,
                    subject_type=EntityType.CANVAS.value,
                    object_type=EntityType.NODE.value
                )
                await self.kg_layer.add_triplet(contains_triplet)

                node_results["successful"].append({
                    "original_id": node['id'],
                    "kg_id": node_id
                })

            except Exception as e:
                node_results["failed"].append({
                    "original_id": node['id'],
                    "error": str(e)
                })

        return node_results

    async def _complete_migration(self, migration_results: List[dict]) -> dict:
        """å®Œæˆè¿ç§»"""
        self.migration_progress.complete_migration()

        total_files = sum(len(batch["files"]) for batch in migration_results)
        successful_files = sum(batch["success_count"] for batch in migration_results)

        return {
            "status": "completed",
            "migration_results": migration_results,
            "summary": {
                "total_files": total_files,
                "successful_files": successful_files,
                "success_rate": successful_files / total_files if total_files > 0 else 0,
                "total_processing_time": self.migration_progress.get_total_time()
            }
        }


class MigrationProgress:
    """è¿ç§»è¿›åº¦ç›‘æ§"""

    def __init__(self):
        self.start_time = None
        self.end_time = None

    def start_migration(self):
        """å¼€å§‹è¿ç§»"""
        self.start_time = time.time()

    def complete_migration(self):
        """å®Œæˆè¿ç§»"""
        self.end_time = time.time()

    def get_total_time(self) -> float:
        """è·å–æ€»è€—æ—¶"""
        if self.start_time and self.end_time:
            return self.end_time - self.start_time
        elif self.start_time:
            return time.time() - self.start_time
        else:
            return 0.0
```

---
