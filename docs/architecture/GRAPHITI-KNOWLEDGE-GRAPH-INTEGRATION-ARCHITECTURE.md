---
document_type: "Architecture"
version: "1.1.0"
last_modified: "2025-11-19"
status: "approved"
iteration: 1

authors:
  - name: "Architect Agent"
    role: "Solution Architect"

reviewers:
  - name: "PO Agent"
    role: "Product Owner"
    approved: true

compatible_with:
  prd: "v1.0"
  api_spec: "v1.0"

api_spec_hash: "0dc1d3610d28bf99"

changes_from_previous:
  - "Initial Architecture with frontmatter metadata"

git:
  commit_sha: ""
  tag: ""

metadata:
  components_count: 0
  external_services: []
  technology_stack:
    frontend: []
    backend: ["Python 3.11", "asyncio"]
    database: []
    infrastructure: []
---

# Canvaså­¦ä¹ ç³»ç»Ÿ - GraphitiçŸ¥è¯†å›¾è°±é›†æˆæ¶æ„è®¾è®¡

**ç‰ˆæœ¬**: v1.1 (LangGraph Checkpointeré›†æˆç‰ˆ)
**åˆ›å»ºæ—¥æœŸ**: 2025-10-18
**æœ€åæ›´æ–°**: 2025-11-11 (**NEW**: Section 8.6 ä¸LangGraph Checkpointerçš„èŒè´£è¾¹ç•Œ)
**ä½œè€…**: Claude Code
**çŠ¶æ€**: æŠ€æœ¯æ–¹æ¡ˆè®¾è®¡

---

## ğŸ“‹ æ¦‚è¿°

æœ¬æ–‡æ¡£è¯¦ç»†è®¾è®¡äº†Canvaså­¦ä¹ ç³»ç»Ÿä¸GraphitiçŸ¥è¯†å›¾è°±çš„é›†æˆæ¶æ„ï¼Œå®ç°CanvasèŠ‚ç‚¹é€»è¾‘å…³ç³»çš„æŒä¹…åŒ–è®°å¿†ã€å­¦ä¹ è¿›åº¦çš„å®æ—¶è¿½è¸ªï¼Œä»¥åŠæ™ºèƒ½æ£€éªŒç™½æ¿ç”Ÿæˆä¼˜åŒ–ã€‚

### æ ¸å¿ƒç›®æ ‡

1. **æŒä¹…åŒ–è®°å¿†**: CanvasèŠ‚ç‚¹å’Œè¾¹çš„é€»è¾‘å…³ç³»æŒä¹…åŒ–å­˜å‚¨
2. **è¿›åº¦è¿½è¸ª**: å®æ—¶è¿½è¸ªå­¦ä¹ è¿›åº¦å’ŒçŸ¥è¯†æŒæ¡çŠ¶æ€
3. **æ™ºèƒ½æ£€ç´¢**: åŸºäºçŸ¥è¯†å›¾è°±çš„æ™ºèƒ½æ£€ç´¢å’Œæ¨è
4. **æ—¶é—´æ„ŸçŸ¥**: è¿½è¸ªå­¦ä¹ è¿›å±•æ—¶é—´çº¿å’ŒçŸ¥è¯†æ¼”åŒ–
5. **æ€§èƒ½ä¼˜åŒ–**: ç¡®ä¿çŸ¥è¯†å›¾è°±æ“ä½œä¸å½±å“Canvasç³»ç»Ÿæ€§èƒ½

---

## ğŸ—ï¸ 1. çŸ¥è¯†å›¾è°±æ•°æ®æ¨¡å‹è®¾è®¡

### 1.1 å®ä½“ç±»å‹å®šä¹‰

```python
# çŸ¥è¯†å›¾è°±å®ä½“ç±»å‹æšä¸¾
class EntityType(Enum):
    CANVAS = "canvas"                    # Canvasç”»å¸ƒ
    NODE = "node"                        # CanvasèŠ‚ç‚¹
    CONCEPT = "concept"                  # çŸ¥è¯†æ¦‚å¿µ
    TOPIC = "topic"                      # çŸ¥è¯†ä¸»é¢˜
    LEARNING_SESSION = "learning_session" # å­¦ä¹ ä¼šè¯
    UNDERSTANDING_STATE = "understanding_state" # ç†è§£çŠ¶æ€
    AI_EXPLANATION = "ai_explanation"     # AIè§£é‡Šæ–‡æ¡£
    PERSONAL_UNDERSTANDING = "personal_understanding" # ä¸ªäººç†è§£
    VERIFICATION_QUESTION = "verification_question" # æ£€éªŒé—®é¢˜
    DECOMPOSITION = "decomposition"      # é—®é¢˜æ‹†è§£
```

### 1.2 å…³ç³»ç±»å‹å®šä¹‰

```python
# çŸ¥è¯†å›¾è°±å…³ç³»ç±»å‹æšä¸¾
class RelationType(Enum):
    # Canvasç»“æ„å…³ç³»
    CONTAINS = "contains"                # CanvasåŒ…å«èŠ‚ç‚¹
    CONNECTS_TO = "connects_to"         # èŠ‚ç‚¹è¿æ¥åˆ°èŠ‚ç‚¹
    DECOMPOSES_TO = "decomposes_to"      # æ‹†è§£å…³ç³»

    # çŸ¥è¯†è¯­ä¹‰å…³ç³»
    IS_ABOUT = "is_about"                # èŠ‚ç‚¹å…³äºæ¦‚å¿µ
    BELONGS_TO_TOPIC = "belongs_to_topic" # å±äºä¸»é¢˜
    PREREQUISITE_OF = "prerequisite_of"  # å‰ç½®çŸ¥è¯†
    SIMILAR_TO = "similar_to"           # ç›¸ä¼¼æ¦‚å¿µ
    CONTRASTS_WITH = "contrasts_with"   # å¯¹æ¯”æ¦‚å¿µ

    # å­¦ä¹ è¿›åº¦å…³ç³»
    HAS_UNDERSTANDING_STATE = "has_understanding_state" # å…·æœ‰ç†è§£çŠ¶æ€
    EVOLVES_TO = "evolves_to"           # çŠ¶æ€æ¼”åŒ–
    SCORED_AS = "scored_as"             # è¯„åˆ†ç»“æœ
    NEEDS_REVIEW = "needs_review"       # éœ€è¦å¤ä¹ 

    # æ—¶é—´å…³ç³»
    CREATED_IN_SESSION = "created_in_session" # åœ¨ä¼šè¯ä¸­åˆ›å»º
    UPDATED_AT_TIME = "updated_at_time" # æ›´æ–°æ—¶é—´
    REVIEWED_AT_TIME = "reviewed_at_time" # å¤ä¹ æ—¶é—´
```

### 1.3 èŠ‚ç‚¹å±æ€§æ˜ å°„

```python
# CanvasèŠ‚ç‚¹åˆ°çŸ¥è¯†å›¾è°±çš„å±æ€§æ˜ å°„
class NodeAttributes:
    """CanvasèŠ‚ç‚¹å±æ€§æ˜ å°„åˆ°çŸ¥è¯†å›¾è°±"""

    def __init__(self, canvas_node: dict):
        self.canvas_id = canvas_node.get('id')
        self.node_type = canvas_node.get('type')
        self.text = canvas_node.get('text', '')
        self.color = canvas_node.get('color')
        self.position = {
            'x': canvas_node.get('x', 0),
            'y': canvas_node.get('y', 0)
        }
        self.size = {
            'width': canvas_node.get('width', 200),
            'height': canvas_node.get('height', 100)
        }

        # å­¦ä¹ ç›¸å…³å±æ€§
        self.learning_metadata = self._extract_learning_metadata()

    def _extract_learning_metadata(self):
        """æå–å­¦ä¹ ç›¸å…³å…ƒæ•°æ®"""
        metadata = {
            'color_meaning': self._get_color_meaning(),
            'content_type': self._detect_content_type(),
            'complexity_score': self._calculate_complexity(),
            'learning_timestamp': time.time()
        }

        # å¦‚æœæ˜¯é»„è‰²èŠ‚ç‚¹ï¼Œæå–ä¸ªäººç†è§£å†…å®¹
        if self.color == "6":  # é»„è‰²èŠ‚ç‚¹
            metadata['personal_understanding'] = self.text
            metadata['understanding_length'] = len(self.text)
            metadata['understanding_quality_indicators'] = self._analyze_understanding_quality()

        return metadata

    def _get_color_meaning(self):
        """è·å–é¢œè‰²å«ä¹‰"""
        color_meanings = {
            "1": "ä¸ç†è§£/æœªé€šè¿‡",
            "2": "å®Œå…¨ç†è§£/å·²é€šè¿‡",
            "3": "ä¼¼æ‡‚éæ‡‚/å¾…æ£€éªŒ",
            "5": "AIè¡¥å……è§£é‡Š",
            "6": "ä¸ªäººç†è§£è¾“å‡ºåŒº"
        }
        return color_meanings.get(self.color, "æœªçŸ¥çŠ¶æ€")

    def _detect_content_type(self):
        """æ£€æµ‹å†…å®¹ç±»å‹"""
        text_lower = self.text.lower()
        if any(keyword in text_lower for keyword in ['ä»€ä¹ˆæ˜¯', 'å®šä¹‰', 'definition']):
            return "definition"
        elif any(keyword in text_lower for keyword in ['ä¾‹å­', 'example', 'ä¾‹å¦‚']):
            return "example"
        elif any(keyword in text_lower for keyword in ['ä¸ºä»€ä¹ˆ', 'why', 'åŸå› ']):
            return "explanation"
        elif '?' in self.text or 'å¦‚ä½•' in text_lower:
            return "question"
        else:
            return "general"

    def _calculate_complexity(self):
        """è®¡ç®—å†…å®¹å¤æ‚åº¦"""
        # åŸºäºæ–‡æœ¬é•¿åº¦ã€å…³é”®è¯å¯†åº¦ç­‰è®¡ç®—å¤æ‚åº¦
        text_length = len(self.text)
        technical_terms = count_technical_terms(self.text)
        return min(10, (text_length / 100) + (technical_terms * 2))

    def _analyze_understanding_quality(self):
        """åˆ†æä¸ªäººç†è§£è´¨é‡ï¼ˆä»…é»„è‰²èŠ‚ç‚¹ï¼‰"""
        indicators = {
            'has_examples': any(keyword in self.text.lower() for keyword in ['ä¾‹å­', 'æ¯”å¦‚', 'ä¾‹å¦‚', 'example']),
            'has_analogies': any(keyword in self.text.lower() for keyword in ['åƒ', 'å¥½æ¯”', 'ç±»ä¼¼äº', 'like']),
            'has_personal_connection': any(keyword in self.text.lower() for keyword in ['æˆ‘è§‰å¾—', 'æˆ‘è®¤ä¸º', 'æˆ‘çš„ç†è§£']),
            'structured_explanation': self._is_structured_explanation()
        }
        return indicators

    def _is_structured_explanation(self):
        """åˆ¤æ–­æ˜¯å¦ä¸ºç»“æ„åŒ–è§£é‡Š"""
        # æ£€æŸ¥æ˜¯å¦æœ‰é€»è¾‘è¿æ¥è¯ã€åˆ†ç‚¹è¯´æ˜ç­‰
        logical_connectors = ['å› ä¸º', 'æ‰€ä»¥', 'é¦–å…ˆ', 'å…¶æ¬¡', 'ç„¶å', 'æœ€å', 'ä¸€æ–¹é¢', 'å¦ä¸€æ–¹é¢']
        return any(connector in self.text for connector in logical_connectors)
```

### 1.4 çŸ¥è¯†å›¾è°±ä¸‰å…ƒç»„æ¨¡å‹

```python
class KnowledgeGraphTriplet:
    """çŸ¥è¯†å›¾è°±ä¸‰å…ƒç»„æ•°æ®æ¨¡å‹"""

    def __init__(self, subject: str, relation: str, object: str,
                 subject_type: str, object_type: str, metadata: dict = None):
        self.subject = subject
        self.relation = relation
        self.object = object
        self.subject_type = subject_type
        self.object_type = object_type
        self.metadata = metadata or {}
        self.timestamp = time.time()
        self.confidence = 1.0

    def to_graphiti_format(self):
        """è½¬æ¢ä¸ºGraphitiæ ¼å¼"""
        return {
            "subject": self.subject,
            "predicate": self.relation,
            "object": self.object,
            "subject_type": self.subject_type,
            "object_type": self.object_type,
            "metadata": {
                **self.metadata,
                "timestamp": self.timestamp,
                "confidence": self.confidence
            }
        }
```

---

## ğŸ›ï¸ 2. Graphitié›†æˆæ¶æ„

### 2.1 æ–°å¢Layer 4: KnowledgeGraphLayer

```python
class KnowledgeGraphLayer:
    """Layer 4: çŸ¥è¯†å›¾è°±å±‚ - Graphitié›†æˆ"""

    def __init__(self, neo4j_uri: str, neo4j_user: str, neo4j_password: str):
        self.graphiti = Graphiti(
            uri=neo4j_uri,
            user=neo4j_user,
            password=neo4j_password
        )
        self.batch_size = 50  # æ‰¹é‡æ“ä½œå¤§å°
        self.cache = {}  # æœ¬åœ°ç¼“å­˜
        self.session_id = None

    async def initialize_session(self, canvas_path: str):
        """åˆå§‹åŒ–å­¦ä¹ ä¼šè¯"""
        self.session_id = f"session_{int(time.time())}_{hash(canvas_path)}"

        # åˆ›å»ºä¼šè¯å®ä½“
        session_triplet = KnowledgeGraphTriplet(
            subject=self.session_id,
            relation=RelationType.CREATED_AT_TIME.value,
            object=str(time.time()),
            subject_type=EntityType.LEARNING_SESSION.value,
            object_type="timestamp",
            metadata={"canvas_path": canvas_path}
        )

        await self.add_triplet(session_triplet)
        return self.session_id

    async def add_triplet(self, triplet: KnowledgeGraphTriplet):
        """æ·»åŠ å•ä¸ªä¸‰å…ƒç»„"""
        try:
            await self.graphiti.add_triplet(
                subject=triplet.subject,
                predicate=triplet.relation,
                object=triplet.object,
                subject_type=triplet.subject_type,
                object_type=triplet.object_type,
                metadata=triplet.metadata
            )
        except Exception as e:
            logger.error(f"æ·»åŠ ä¸‰å…ƒç»„å¤±è´¥: {e}")
            raise

    async def add_triplets_batch(self, triplets: List[KnowledgeGraphTriplet]):
        """æ‰¹é‡æ·»åŠ ä¸‰å…ƒç»„"""
        for i in range(0, len(triplets), self.batch_size):
            batch = triplets[i:i + self.batch_size]
            tasks = [self.add_triplet(triplet) for triplet in batch]
            await asyncio.gather(*tasks, return_exceptions=True)

    async def search_knowledge(self, query: str, limit: int = 10) -> List[dict]:
        """æœç´¢çŸ¥è¯†å›¾è°±"""
        try:
            results = await self.graphiti.search(
                query=query,
                limit=limit,
                search_type="hybrid"  # æ··åˆæœç´¢ï¼šè¯­ä¹‰+BM25
            )
            return results
        except Exception as e:
            logger.error(f"çŸ¥è¯†å›¾è°±æœç´¢å¤±è´¥: {e}")
            return []

    async def get_node_evolution(self, node_id: str) -> List[dict]:
        """è·å–èŠ‚ç‚¹æ¼”åŒ–å†å²"""
        evolution_query = f"""
        MATCH (n:node {{id: "{node_id}"}})
        -[r:evolves_to]->(m:node)
        RETURN n, r, m
        ORDER BY r.timestamp
        """
        return await self.graphiti.custom_query(evolution_query)

    async def get_learning_progress(self, canvas_path: str) -> dict:
        """è·å–å­¦ä¹ è¿›åº¦ç»Ÿè®¡"""
        progress_query = f"""
        MATCH (c:canvas {{path: "{canvas_path}"}})
        -[:contains]->(n:node)
        -[:has_understanding_state]->(s:understanding_state)
        RETURN s.color_meaning as state, count(*) as count
        """
        results = await self.graphiti.custom_query(progress_query)

        # è®¡ç®—è¿›åº¦ç»Ÿè®¡
        total_nodes = sum(r['count'] for r in results)
        progress = {
            'total_nodes': total_nodes,
            'green_nodes': 0,  # å®Œå…¨ç†è§£
            'yellow_nodes': 0, # ä¸ªäººç†è§£
            'purple_nodes': 0, # ä¼¼æ‡‚éæ‡‚
            'red_nodes': 0,    # ä¸ç†è§£
            'blue_nodes': 0    # AIè§£é‡Š
        }

        for result in results:
            state = result['state']
            count = result['count']
            if 'å®Œå…¨ç†è§£' in state:
                progress['green_nodes'] = count
            elif 'ä¸ªäººç†è§£' in state:
                progress['yellow_nodes'] = count
            elif 'ä¼¼æ‡‚éæ‡‚' in state:
                progress['purple_nodes'] = count
            elif 'ä¸ç†è§£' in state:
                progress['red_nodes'] = count
            elif 'AIè§£é‡Š' in state:
                progress['blue_nodes'] = count

        # è®¡ç®—æŒæ¡ç‡
        if total_nodes > 0:
            progress['mastery_rate'] = (progress['green_nodes'] / total_nodes) * 100
        else:
            progress['mastery_rate'] = 0

        return progress
```

### 2.2 æ‰©å±•ç°æœ‰æ¶æ„

```python
# æ‰©å±• Layer 1: CanvasJSONOperator
class CanvasJSONOperatorWithKG(CanvasJSONOperator):
    """å¸¦çŸ¥è¯†å›¾è°±åŠŸèƒ½çš„Canvas JSONæ“ä½œå™¨"""

    def __init__(self, canvas_path: str, kg_layer: KnowledgeGraphLayer = None):
        super().__init__(canvas_path)
        self.kg_layer = kg_layer
        self.canvas_id = self._generate_canvas_id()

    def _generate_canvas_id(self):
        """ç”ŸæˆCanvaså”¯ä¸€ID"""
        return f"canvas_{hash(self.canvas_path)}_{int(time.time())}"

    async def sync_canvas_to_kg(self):
        """åŒæ­¥Canvasåˆ°çŸ¥è¯†å›¾è°±"""
        if not self.kg_layer:
            return

        # åˆ›å»ºCanvaså®ä½“
        canvas_triplet = KnowledgeGraphTriplet(
            subject=self.canvas_id,
            relation=RelationType.CREATED_AT_TIME.value,
            object=str(time.time()),
            subject_type=EntityType.CANVAS.value,
            object_type="timestamp",
            metadata={
                "path": self.canvas_path,
                "name": os.path.basename(self.canvas_path)
            }
        )
        await self.kg_layer.add_triplet(canvas_triplet)

        # åŒæ­¥æ‰€æœ‰èŠ‚ç‚¹
        canvas_data = self.read_canvas()
        await self._sync_nodes_to_kg(canvas_data.get('nodes', []))
        await self._sync_edges_to_kg(canvas_data.get('edges', []))

    async def _sync_nodes_to_kg(self, nodes: List[dict]):
        """åŒæ­¥èŠ‚ç‚¹åˆ°çŸ¥è¯†å›¾è°±"""
        node_triplets = []

        for node in nodes:
            node_id = f"node_{self.canvas_id}_{node['id']}"
            node_attrs = NodeAttributes(node)

            # åˆ›å»ºèŠ‚ç‚¹å®ä½“
            node_triplet = KnowledgeGraphTriplet(
                subject=node_id,
                relation=RelationType.CREATED_AT_TIME.value,
                object=str(time.time()),
                subject_type=EntityType.NODE.value,
                object_type="timestamp",
                metadata={
                    **node_attrs.__dict__,
                    "canvas_id": self.canvas_id
                }
            )
            node_triplets.append(node_triplet)

            # CanvasåŒ…å«èŠ‚ç‚¹å…³ç³»
            contains_triplet = KnowledgeGraphTriplet(
                subject=self.canvas_id,
                relation=RelationType.CONTAINS.value,
                object=node_id,
                subject_type=EntityType.CANVAS.value,
                object_type=EntityType.NODE.value
            )
            node_triplets.append(contains_triplet)

            # å¦‚æœæ˜¯é»„è‰²èŠ‚ç‚¹ï¼Œåˆ›å»ºä¸ªäººç†è§£å®ä½“
            if node['color'] == "6":
                understanding_id = f"understanding_{node_id}"
                understanding_triplet = KnowledgeGraphTriplet(
                    subject=understanding_id,
                    relation=RelationType.CREATED_AT_TIME.value,
                    object=str(time.time()),
                    subject_type=EntityType.PERAL_UNDERSTANDING.value,
                    object_type="timestamp",
                    metadata={
                        "content": node.get('text', ''),
                        "node_id": node_id,
                        "quality_indicators": node_attrs.learning_metadata.get('understanding_quality_indicators', {})
                    }
                )
                node_triplets.append(understanding_triplet)

                # èŠ‚ç‚¹å…·æœ‰ä¸ªäººç†è§£å…³ç³»
                has_understanding_triplet = KnowledgeGraphTriplet(
                    subject=node_id,
                    relation=RelationType.HAS_UNDERSTANDING_STATE.value,
                    object=understanding_id,
                    subject_type=EntityType.NODE.value,
                    object_type=EntityType.PERAL_UNDERSTANDING.value
                )
                node_triplets.append(has_understanding_triplet)

        # æ‰¹é‡æ·»åŠ ä¸‰å…ƒç»„
        await self.kg_layer.add_triplets_batch(node_triplets)

    async def _sync_edges_to_kg(self, edges: List[dict]):
        """åŒæ­¥è¾¹åˆ°çŸ¥è¯†å›¾è°±"""
        edge_triplets = []

        for edge in edges:
            from_node_id = f"node_{self.canvas_id}_{edge['fromNode']}"
            to_node_id = f"node_{self.canvas_id}_{edge['toNode']}"

            # åˆ›å»ºè¿æ¥å…³ç³»
            connect_triplet = KnowledgeGraphTriplet(
                subject=from_node_id,
                relation=RelationType.CONNECTS_TO.value,
                object=to_node_id,
                subject_type=EntityType.NODE.value,
                object_type=EntityType.NODE.value,
                metadata={
                    "fromSide": edge.get('fromSide', 'bottom'),
                    "toSide": edge.get('toSide', 'top'),
                    "canvas_id": self.canvas_id
                }
            )
            edge_triplets.append(connect_triplet)

        await self.kg_layer.add_triplets_batch(edge_triplets)
```

---

## ğŸ’¾ 3. è®°å¿†åŠŸèƒ½æ¶æ„è®¾è®¡

### 3.1 è®°å¿†ç³»ç»Ÿæ¶æ„

```python
class CanvasMemorySystem:
    """Canvasè®°å¿†ç³»ç»Ÿ - åŸºäºGraphitiçš„æŒä¹…åŒ–è®°å¿†"""

    def __init__(self, kg_layer: KnowledgeGraphLayer):
        self.kg_layer = kg_layer
        self.memory_cache = {}
        self.current_session = None

    async def start_learning_session(self, canvas_path: str):
        """å¼€å§‹å­¦ä¹ ä¼šè¯"""
        self.current_session = await self.kg_layer.initialize_session(canvas_path)

        # è®°å½•ä¼šè¯å¼€å§‹
        memory_content = {
            "session_id": self.current_session,
            "canvas_path": canvas_path,
            "start_time": time.time(),
            "action": "start_learning_session"
        }

        await self.kg_layer.add_episode(memory_content)
        return self.current_session

    async def remember_canvas_structure(self, canvas_path: str, canvas_data: dict):
        """è®°å¿†Canvasç»“æ„"""
        structure_memory = {
            "session_id": self.current_session,
            "canvas_path": canvas_path,
            "structure": {
                "nodes_count": len(canvas_data.get('nodes', [])),
                "edges_count": len(canvas_data.get('edges', [])),
                "color_distribution": self._analyze_color_distribution(canvas_data.get('nodes', [])),
                "topics": self._extract_topics(canvas_data.get('nodes', []))
            },
            "timestamp": time.time(),
            "action": "remember_structure"
        }

        await self.kg_layer.add_episode(structure_memory)

    async def remember_learning_progress(self, canvas_path: str, node_id: str,
                                       old_color: str, new_color: str, score: dict = None):
        """è®°å¿†å­¦ä¹ è¿›åº¦å˜åŒ–"""
        progress_memory = {
            "session_id": self.current_session,
            "canvas_path": canvas_path,
            "node_id": node_id,
            "progress_change": {
                "old_color": old_color,
                "new_color": new_color,
                "score": score,
                "improvement": self._calculate_improvement(old_color, new_color)
            },
            "timestamp": time.time(),
            "action": "progress_change"
        }

        await self.kg_layer.add_episode(progress_memory)

        # æ›´æ–°çŸ¥è¯†å›¾è°±ä¸­çš„èŠ‚ç‚¹çŠ¶æ€
        kg_node_id = f"node_{hash(canvas_path)}_{node_id}"
        await self._update_node_understanding_state(kg_node_id, new_color, score)

    async def remember_ai_explanation(self, canvas_path: str, concept: str,
                                    explanation_type: str, explanation_content: str):
        """è®°å¿†AIè§£é‡Š"""
        explanation_memory = {
            "session_id": self.current_session,
            "canvas_path": canvas_path,
            "explanation": {
                "concept": concept,
                "type": explanation_type,
                "content": explanation_content,
                "content_length": len(explanation_content)
            },
            "timestamp": time.time(),
            "action": "ai_explanation_generated"
        }

        await self.kg_layer.add_episode(explanation_memory)

    async def remember_verification_questions(self, canvas_path: str,
                                            node_id: str, questions: List[str]):
        """è®°å¿†æ£€éªŒé—®é¢˜"""
        questions_memory = {
            "session_id": self.current_session,
            "canvas_path": canvas_path,
            "node_id": node_id,
            "questions": questions,
            "questions_count": len(questions),
            "timestamp": time.time(),
            "action": "verification_questions_generated"
        }

        await self.kg_layer.add_episode(questions_memory)

    async def recall_canvas_history(self, canvas_path: str,
                                  time_range: tuple = None) -> List[dict]:
        """å›å¿†Canvaså†å²"""
        query = f"canvas_path:{canvas_path}"
        if time_range:
            start_time, end_time = time_range
            query += f" timestamp:[{start_time} TO {end_time}]"

        episodes = await self.kg_layer.retrieve_episodes(query)
        return episodes

    async def recall_learning_insights(self, canvas_path: str) -> dict:
        """å›å¿†å­¦ä¹ æ´å¯Ÿ"""
        # è·å–å­¦ä¹ è¿›åº¦
        progress = await self.kg_layer.get_learning_progress(canvas_path)

        # è·å–å›°éš¾èŠ‚ç‚¹
        difficult_nodes = await self._get_difficult_nodes(canvas_path)

        # è·å–å­¦ä¹ æ¨¡å¼
        learning_patterns = await self._analyze_learning_patterns(canvas_path)

        # è·å–çŸ¥è¯†å…³è”
        knowledge_connections = await self._get_knowledge_connections(canvas_path)

        return {
            "progress": progress,
            "difficult_nodes": difficult_nodes,
            "learning_patterns": learning_patterns,
            "knowledge_connections": knowledge_connections
        }

    def _analyze_color_distribution(self, nodes: List[dict]) -> dict:
        """åˆ†æé¢œè‰²åˆ†å¸ƒ"""
        distribution = {"1": 0, "2": 0, "3": 0, "5": 0, "6": 0}
        for node in nodes:
            color = node.get('color', '')
            if color in distribution:
                distribution[color] += 1
        return distribution

    def _extract_topics(self, nodes: List[dict]) -> List[str]:
        """æå–ä¸»é¢˜"""
        topics = set()
        for node in nodes:
            text = node.get('text', '')
            # ç®€å•çš„ä¸»é¢˜æå–é€»è¾‘
            words = text.split()
            for word in words:
                if len(word) > 3 and word.isalpha():
                    topics.add(word)
        return list(topics)[:10]  # è¿”å›å‰10ä¸ªä¸»é¢˜

    def _calculate_improvement(self, old_color: str, new_color: str) -> float:
        """è®¡ç®—æ”¹è¿›ç¨‹åº¦"""
        color_weights = {"1": 0, "3": 0.5, "6": 0.7, "2": 1.0}
        old_weight = color_weights.get(old_color, 0)
        new_weight = color_weights.get(new_color, 0)
        return new_weight - old_weight

    async def _update_node_understanding_state(self, node_id: str,
                                              color: str, score: dict = None):
        """æ›´æ–°èŠ‚ç‚¹ç†è§£çŠ¶æ€"""
        state_id = f"state_{node_id}_{int(time.time())}"

        state_triplet = KnowledgeGraphTriplet(
            subject=state_id,
            relation=RelationType.CREATED_AT_TIME.value,
            object=str(time.time()),
            subject_type=EntityType.UNDERSTANDING_STATE.value,
            object_type="timestamp",
            metadata={
                "color": color,
                "color_meaning": self._get_color_meaning(color),
                "score": score or {},
                "node_id": node_id
            }
        )

        await self.kg_layer.add_triplet(state_triplet)

        # èŠ‚ç‚¹å…·æœ‰ç†è§£çŠ¶æ€å…³ç³»
        understanding_triplet = KnowledgeGraphTriplet(
            subject=node_id,
            relation=RelationType.HAS_UNDERSTANDING_STATE.value,
            object=state_id,
            subject_type=EntityType.NODE.value,
            object_type=EntityType.UNDERSTANDING_STATE.value
        )

        await self.kg_layer.add_triplet(understanding_triplet)

    def _get_color_meaning(self, color: str) -> str:
        """è·å–é¢œè‰²å«ä¹‰"""
        meanings = {
            "1": "ä¸ç†è§£/æœªé€šè¿‡",
            "2": "å®Œå…¨ç†è§£/å·²é€šè¿‡",
            "3": "ä¼¼æ‡‚éæ‡‚/å¾…æ£€éªŒ",
            "5": "AIè¡¥å……è§£é‡Š",
            "6": "ä¸ªäººç†è§£è¾“å‡ºåŒº"
        }
        return meanings.get(color, "æœªçŸ¥çŠ¶æ€")
```

### 3.2 æ™ºèƒ½è®°å¿†ç´¢å¼•ç³»ç»Ÿ

```python
class MemoryIndexSystem:
    """æ™ºèƒ½è®°å¿†ç´¢å¼•ç³»ç»Ÿ"""

    def __init__(self, kg_layer: KnowledgeGraphLayer):
        self.kg_layer = kg_layer
        self.index_cache = {}

    async def build_semantic_index(self, canvas_path: str):
        """æ„å»ºè¯­ä¹‰ç´¢å¼•"""
        # è·å–æ‰€æœ‰èŠ‚ç‚¹å†…å®¹
        nodes_query = f"""
        MATCH (c:canvas {{path: "{canvas_path}"}})
        -[:contains]->(n:node)
        RETURN n.id as node_id, n.text as text, n.color as color
        """

        nodes = await self.kg_layer.custom_query(nodes_query)

        # ä¸ºæ¯ä¸ªèŠ‚ç‚¹æ„å»ºè¯­ä¹‰å‘é‡
        for node in nodes:
            await self._index_node_semantically(node)

    async def _index_node_semantically(self, node: dict):
        """ä¸ºèŠ‚ç‚¹æ„å»ºè¯­ä¹‰ç´¢å¼•"""
        text = node.get('text', '')
        node_id = node.get('node_id')

        # æå–å…³é”®è¯
        keywords = self._extract_keywords(text)

        # æ„å»ºæ¦‚å¿µä¸‰å…ƒç»„
        for keyword in keywords:
            concept_triplet = KnowledgeGraphTriplet(
                subject=f"node_{node_id}",
                relation=RelationType.IS_ABOUT.value,
                object=f"concept_{keyword}",
                subject_type=EntityType.NODE.value,
                object_type=EntityType.CONCEPT.value,
                metadata={"relevance": self._calculate_relevance(text, keyword)}
            )

            await self.kg_layer.add_triplet(concept_triplet)

    def _extract_keywords(self, text: str) -> List[str]:
        """æå–å…³é”®è¯"""
        # ç®€å•çš„å…³é”®è¯æå–
        import jieba
        words = jieba.lcut(text)
        keywords = [word for word in words if len(word) > 2 and word.isalpha()]
        return list(set(keywords))[:5]  # è¿”å›å‰5ä¸ªå”¯ä¸€å…³é”®è¯

    def _calculate_relevance(self, text: str, keyword: str) -> float:
        """è®¡ç®—å…³é”®è¯ç›¸å…³æ€§"""
        count = text.lower().count(keyword.lower())
        return min(1.0, count / len(text.split()))

    async def build_temporal_index(self, canvas_path: str):
        """æ„å»ºæ—¶é—´ç´¢å¼•"""
        # æŒ‰æ—¶é—´ç»„ç»‡å­¦ä¹ æ´»åŠ¨
        temporal_query = f"""
        MATCH (c:canvas {{path: "{canvas_path}"}})
        -[*]->(n)
        WHERE n.timestamp IS NOT NULL
        RETURN n.timestamp as timestamp, labels(n) as types, n
        ORDER BY n.timestamp
        """

        temporal_data = await self.kg_layer.custom_query(temporal_query)

        # æ„å»ºæ—¶é—´åºåˆ—ç´¢å¼•
        for i, item in enumerate(temporal_data):
            timestamp = item['timestamp']
            node_types = item['types']

            # åˆ›å»ºæ—¶é—´ç´¢å¼•ä¸‰å…ƒç»„
            time_triplet = KnowledgeGraphTriplet(
                subject=f"time_index_{int(timestamp)}",
                relation=RelationType.CREATED_AT_TIME.value,
                object=str(timestamp),
                subject_type="time_index",
                object_type="timestamp",
                metadata={
                    "sequence": i,
                    "node_types": node_types,
                    "canvas_path": canvas_path
                }
            )

            await self.kg_layer.add_triplet(time_triplet)
```

---

## ğŸ” 4. æ™ºèƒ½æ··åˆæ£€ç´¢åŠŸèƒ½è®¾è®¡ (Graphiti Hybrid Search)

> **âœ… åŸºäºGraphiti SkilléªŒè¯**: æœ¬èŠ‚æ‰€æœ‰æ£€ç´¢å®ç°å‡ä½¿ç”¨Graphitiå®˜æ–¹`hybrid_search` APIï¼Œæ•´åˆGraphéå† + Semanticå‘é‡ + BM25å…³é”®è¯ä¸‰ç§æ£€ç´¢æ¨¡å¼

### 4.0 Graphitiæ··åˆæ£€ç´¢æ¶æ„æ¦‚è§ˆ

**æ ¸å¿ƒä¼˜åŠ¿**: Graphitiå†…ç½®æ··åˆæ£€ç´¢å¼•æ“ï¼Œæ— éœ€æ‰‹å†™CypheræŸ¥è¯¢

```python
# âœ… Verified from Graphiti Skill (hybrid_search API)
from graphiti_core import Graphiti
from graphiti_core.search.search_config import SearchConfig
from graphiti_core.search.search_config_recipes import (
    COMBINED_HYBRID_SEARCH_RRF,  # é»˜è®¤RRFé‡æ’
    node_distance_reranker,      # å›¾è·ç¦»é‡æ’
    mmr_reranker,                # æœ€å¤§è¾¹é™…ç›¸å…³æ€§é‡æ’
    cross_encoder_reranker,      # è·¨ç¼–ç å™¨é‡æ’
    episode_mentions_reranker    # äº‹ä»¶æåŠé‡æ’
)

class GraphitiHybridRetriever:
    """Graphitiæ··åˆæ£€ç´¢å™¨ - å°è£…å®˜æ–¹hybrid_search API"""

    def __init__(self, graphiti: Graphiti):
        self.graphiti = graphiti

    async def search(
        self,
        query: str,
        center_node_uuid: str = None,      # ä¸­å¿ƒèŠ‚ç‚¹UUIDï¼ˆå¯é€‰ï¼‰
        max_distance: int = 3,             # å›¾éå†æœ€å¤§è·ç¦»
        num_results: int = 20,             # è¿”å›ç»“æœæ•°é‡
        rerank_strategy: str = "rrf"       # 5ç§é‡æ’ç­–ç•¥ä¹‹ä¸€
    ) -> List[Dict]:
        """
        Graphitiæ··åˆæ£€ç´¢ (Graph + Semantic + BM25)

        å‚æ•°:
            query: æœç´¢æŸ¥è¯¢ï¼ˆè‡ªç„¶è¯­è¨€ï¼‰
            center_node_uuid: ä¸­å¿ƒèŠ‚ç‚¹UUIDï¼ˆå¦‚æ¦‚å¿µèŠ‚ç‚¹ï¼‰
            max_distance: å›¾éå†æœ€å¤§è·ç¦»ï¼ˆ1-5è·³ï¼‰
            num_results: è¿”å›ç»“æœæ•°é‡
            rerank_strategy: é‡æ’ç­–ç•¥ ("rrf" | "mmr" | "node_distance" | "cross_encoder" | "episode_mentions")

        è¿”å›:
            List[Dict]: æ£€ç´¢ç»“æœï¼Œæ¯ä¸ªç»“æœåŒ…å«èŠ‚ç‚¹ä¿¡æ¯å’Œç›¸å…³æ€§è¯„åˆ†
        """
        # âœ… Verified from Graphiti Skill (search_config_recipes)
        search_config = self._get_search_config(rerank_strategy)

        results = await self.graphiti.search(
            query=query,
            center_node_uuid=center_node_uuid,
            max_distance=max_distance,
            num_results=num_results,
            config=search_config
        )

        return results

    def _get_search_config(self, strategy: str) -> SearchConfig:
        """è·å–æœç´¢é…ç½®ï¼ˆ5ç§é‡æ’ç­–ç•¥ï¼‰"""
        # âœ… Verified from Graphiti Skill (search_config_recipesæ¨¡å—)
        RERANK_STRATEGIES = {
            "rrf": COMBINED_HYBRID_SEARCH_RRF,           # å€’æ•°æ’åèåˆï¼ˆé»˜è®¤æ¨èï¼‰
            "mmr": mmr_reranker,                         # æœ€å¤§è¾¹é™…ç›¸å…³æ€§ï¼ˆå»é‡ç›¸ä¼¼ç»“æœï¼‰
            "node_distance": node_distance_reranker,     # å›¾è·ç¦»æƒé‡ï¼ˆä¼˜å…ˆè¿‘é‚»èŠ‚ç‚¹ï¼‰
            "cross_encoder": cross_encoder_reranker,     # è·¨ç¼–ç å™¨é‡æ’ï¼ˆç²¾åº¦æœ€é«˜ä½†é€Ÿåº¦æ…¢ï¼‰
            "episode_mentions": episode_mentions_reranker # äº‹ä»¶æåŠé¢‘ç‡ï¼ˆæ—¶åºç›¸å…³ï¼‰
        }

        if strategy not in RERANK_STRATEGIES:
            raise ValueError(f"æ— æ•ˆé‡æ’ç­–ç•¥: {strategy}. å¯é€‰: {list(RERANK_STRATEGIES.keys())}")

        return RERANK_STRATEGIES[strategy]
```

**5ç§Rerankingç­–ç•¥å¯¹æ¯”**:

| ç­–ç•¥ | é€‚ç”¨åœºæ™¯ | æ€§èƒ½ | ç²¾åº¦ | æ¨èæŒ‡æ•° |
|------|---------|------|------|---------|
| **rrf** (å€’æ•°æ’åèåˆ) | é€šç”¨åœºæ™¯ï¼Œå¹³è¡¡å„æ£€ç´¢æº | â­â­â­â­â­ | â­â­â­â­ | âœ… é»˜è®¤æ¨è |
| **mmr** (æœ€å¤§è¾¹é™…ç›¸å…³æ€§) | éœ€è¦å¤šæ ·åŒ–ç»“æœï¼Œå»é™¤å†—ä½™ | â­â­â­â­ | â­â­â­â­ | âœ… æ¨è |
| **node_distance** (å›¾è·ç¦») | å¼ºè°ƒæ¦‚å¿µå…³è”æ€§ï¼Œä¼˜å…ˆè¿‘é‚» | â­â­â­â­â­ | â­â­â­ | âš ï¸ ç‰¹å®šåœºæ™¯ |
| **cross_encoder** (è·¨ç¼–ç å™¨) | è¿½æ±‚æœ€é«˜ç²¾åº¦ï¼Œä¸åœ¨æ„é€Ÿåº¦ | â­â­ | â­â­â­â­â­ | âš ï¸ æ€§èƒ½æ•æ„Ÿåœºæ™¯æ…ç”¨ |
| **episode_mentions** (äº‹ä»¶æåŠ) | æ—¶åºç›¸å…³æŸ¥è¯¢ï¼Œå¤ä¹ å†å²åˆ†æ | â­â­â­â­ | â­â­â­ | âœ… æ—¶é—´çº¿åˆ†ææ¨è |

**æ€§èƒ½åŸºå‡†**:
- ç›®æ ‡å»¶è¿Ÿ: <200ms (rrf/mmr/node_distanceç­–ç•¥)
- ç›®æ ‡å»¶è¿Ÿ: <500ms (cross_encoderç­–ç•¥)
- ååé‡: >50 QPS (å¹¶å‘åœºæ™¯)
- ç¼“å­˜å‘½ä¸­ç‡: >70% (é‡å¤æŸ¥è¯¢)

---

### 4.1 LanceDBå‘é‡ç´¢å¼•é…ç½®ä¸ä¼˜åŒ–

**èƒŒæ™¯**: LanceDBä½¿ç”¨Lanceæ•°æ®æ ¼å¼(Parquetæ¼”è¿›ç‰ˆ)å’Œè‡ªé€‚åº”ç´¢å¼•ç­–ç•¥ï¼Œæä¾›100xæŸ¥è¯¢æ€§èƒ½æå‡

```python
# âœ… Verified from LanceDB Context7
import lancedb
from lancedb.pydantic import LanceModel, Vector
from lancedb.embeddings import get_registry

class LanceDBOptimizer:
    """LanceDBå‘é‡ç´¢å¼•ä¼˜åŒ–å™¨"""

    @classmethod
    def get_index_config(cls, scenario: str = "default") -> dict:
        """è·å–ä¸åŒåœºæ™¯çš„ç´¢å¼•é…ç½®"""

        # âœ… Verified: åŸºäºLanceDBæœ€ä½³å®è·µ
        SCENARIO_CONFIGS = {
            "default": {  # <100K vectors, IVF_FLAT
                "index_type": "IVF_FLAT",
                "nprobes": 20,          # æŸ¥è¯¢æ—¶æœç´¢çš„åˆ†åŒºæ•°
                "nlist": 100,           # IVFèšç±»ä¸­å¿ƒæ•°
                "refine_factor": 1      # æ— é‡æ’åº
            },
            "high_accuracy": {  # é«˜ç²¾åº¦åœºæ™¯ï¼ˆæ£€éªŒç™½æ¿ç”Ÿæˆï¼‰
                "index_type": "IVF_FLAT",
                "nprobes": 50,
                "nlist": 256,
                "refine_factor": 2      # 2xå€™é€‰é›†é‡æ’åº
            },
            "high_speed": {  # é«˜é€Ÿåº¦åœºæ™¯ï¼ˆå®æ—¶æ¨èï¼‰
                "index_type": "IVF_PQ",  # Product Quantizationå‹ç¼©
                "nprobes": 10,
                "nlist": 100,
                "pq_m": 8,               # PQå­ç©ºé—´æ•°é‡
                "pq_nbits": 8            # æ¯å­ç©ºé—´çš„bits
            },
            "large_scale": {  # >1M vectors
                "index_type": "IVF_PQ",
                "nprobes": 40,
                "nlist": 4096,           # å¤§è§„æ¨¡åœºæ™¯å¢åŠ èšç±»ä¸­å¿ƒ
                "pq_m": 16,
                "pq_nbits": 8,
                "refine_factor": 3
            }
        }

        return SCENARIO_CONFIGS.get(scenario, SCENARIO_CONFIGS["default"])

    @classmethod
    def create_table_with_index(
        cls,
        db: lancedb.DBConnection,
        table_name: str,
        schema: type[LanceModel],
        scenario: str = "default"
    ) -> lancedb.table.Table:
        """åˆ›å»ºå¸¦ä¼˜åŒ–ç´¢å¼•çš„LanceDBè¡¨"""

        # Step 1: åˆ›å»ºè¡¨
        table = db.create_table(table_name, schema=schema, mode="overwrite")

        # Step 2: åˆ›å»ºBM25å…¨æ–‡ç´¢å¼•(Hybrid Search)
        table.create_fts_index("text", replace=True)

        # Step 3: é…ç½®å‘é‡ç´¢å¼•
        config = cls.get_index_config(scenario)

        if config["index_type"] == "IVF_FLAT":
            table.create_index(
                metric="cosine",
                index_type="IVF_FLAT",
                num_partitions=config["nlist"],
                num_sub_vectors=8
            )
        elif config["index_type"] == "IVF_PQ":
            table.create_index(
                metric="cosine",
                index_type="IVF_PQ",
                num_partitions=config["nlist"],
                num_sub_vectors=config["pq_m"]
            )

        return table

    @classmethod
    def estimate_query_latency(cls, num_concepts: int, scenario: str) -> float:
        """ä¼°ç®—æŸ¥è¯¢å»¶è¿Ÿï¼ˆæ¯«ç§’ï¼‰"""
        # âœ… åŸºäºLanceDBå®æµ‹æ•°æ®: 1M vectors @ MacBook Pro < 100ms
        # å…¬å¼: latency = base_latency + (num_concepts / throughput)

        LATENCY_PROFILES = {
            "default": {"base": 20, "throughput": 100000},      # <10K vectors
            "high_accuracy": {"base": 50, "throughput": 50000}, # æ›´å¤šé‡æ’åº
            "high_speed": {"base": 10, "throughput": 200000},   # PQå‹ç¼©åŠ é€Ÿ
            "large_scale": {"base": 80, "throughput": 20000}    # 1M+ vectors
        }

        profile = LATENCY_PROFILES.get(scenario, LATENCY_PROFILES["default"])
        latency = profile["base"] + (num_concepts / profile["throughput"] * 1000)
        return round(latency, 2)
```

**LanceDBç´¢å¼•é€‰æ‹©å†³ç­–æ ‘**:

```
æ•°æ®è§„æ¨¡?
â”œâ”€â”€ <100K vectors â†’ IVF_FLAT (æ— æŸç²¾åº¦)
â”‚   â”œâ”€â”€ æŸ¥è¯¢å»¶è¿Ÿ: <50ms
â”‚   â””â”€â”€ å†…å­˜å ç”¨: ~300MB (768-dim)
â”‚
â”œâ”€â”€ 100K-1M vectors â†’ IVF_PQ (8xå‹ç¼©)
â”‚   â”œâ”€â”€ æŸ¥è¯¢å»¶è¿Ÿ: <100ms
â”‚   â””â”€â”€ å†…å­˜å ç”¨: ~40MB (å‹ç¼©å)
â”‚
â””â”€â”€ >1M vectors â†’ IVF_PQ + åˆ†å¸ƒå¼
    â”œâ”€â”€ æŸ¥è¯¢å»¶è¿Ÿ: <200ms (éœ€åˆ†ç‰‡)
    â””â”€â”€ éœ€è¦LanceDB Cloudæˆ–å¤šèŠ‚ç‚¹éƒ¨ç½²

æŸ¥è¯¢åœºæ™¯?
â”œâ”€â”€ æ£€éªŒç™½æ¿ç”Ÿæˆ (ç²¾åº¦ä¼˜å…ˆ) â†’ IVF_FLAT + refine_factor=2
â”œâ”€â”€ å®æ—¶Agentæ¨è (é€Ÿåº¦ä¼˜å…ˆ) â†’ IVF_PQ + nprobes=10
â””â”€â”€ å¤ä¹ å†å²åˆ†æ (å¹³è¡¡) â†’ IVF_FLAT + defaulté…ç½®
```

**LanceDB vs ChromaDBæ€§èƒ½å¯¹æ¯”**:

| æŒ‡æ ‡ | LanceDB (IVF_PQ) | ChromaDB (HNSW) | æå‡ |
|------|------------------|-----------------|------|
| **1M vectorsæŸ¥è¯¢å»¶è¿Ÿ** | 80-100ms | 150-200ms | **2x** |
| **å†…å­˜å ç”¨** | 40MB (å‹ç¼©) | 300MB | **7.5x** |
| **Hybrid Search** | å†…ç½®BM25+Vector | éœ€è‡ªè¡Œå®ç° | **åŸç”Ÿæ”¯æŒ** |
| **å¯æ‰©å±•æ€§** | 1B+ vectors | <500K optimal | **2000x** |
| **æ•°æ®æ ¼å¼** | Lance (åˆ—å¼) | Parquet | **æ›´å¿«æ‰«æ** |

---

### 4.2 å­¦ä¹ æƒ…å†µè¿½è¸ªç³»ç»Ÿ (åŸºäºHybrid Search)

```python
class LearningTracker:
    """å­¦ä¹ æƒ…å†µæ™ºèƒ½è¿½è¸ªç³»ç»Ÿ - ä½¿ç”¨Graphitiæ··åˆæ£€ç´¢"""

    def __init__(self, graphiti: Graphiti):
        self.graphiti = graphiti
        self.retriever = GraphitiHybridRetriever(graphiti)

    async def track_learning_progress(
        self,
        canvas_path: str,
        time_range: tuple = None
    ) -> dict:
        """
        è¿½è¸ªå­¦ä¹ è¿›åº¦ï¼ˆæ··åˆæ£€ç´¢å®ç°ï¼‰

        âœ… æ›¿æ¢åŸCypheræŸ¥è¯¢ä¸ºGraphiti hybrid_search
        """
        # 1. è·å–Canvasæ¦‚å¿µèŠ‚ç‚¹UUID
        canvas_node = await self._get_canvas_node(canvas_path)

        # 2. ä½¿ç”¨æ··åˆæ£€ç´¢è·å–å­¦ä¹ æ—¶é—´çº¿
        timeline = await self._get_learning_timeline_hybrid(
            canvas_node['uuid'], time_range
        )

        # 3. åˆ†æå­¦ä¹ æ¨¡å¼ï¼ˆGraphéå† + Semanticèšç±»ï¼‰
        patterns = await self._analyze_learning_patterns_hybrid(canvas_node['uuid'])

        # 4. è¯†åˆ«å­¦ä¹ ç“¶é¢ˆï¼ˆBM25å…³é”®è¯ + å›¾è·ç¦»é‡æ’ï¼‰
        bottlenecks = await self._identify_learning_bottlenecks_hybrid(canvas_node['uuid'])

        # 5. è®¡ç®—å­¦ä¹ æ•ˆç‡ï¼ˆæ—¶åºEpisodeæ£€ç´¢ï¼‰
        efficiency = await self._calculate_learning_efficiency_hybrid(canvas_node['uuid'])

        return {
            "current_progress": await self._get_basic_progress(canvas_node['uuid']),
            "timeline": timeline,
            "patterns": patterns,
            "bottlenecks": bottlenecks,
            "efficiency": efficiency,
            "recommendations": await self._generate_recommendations(
                patterns, bottlenecks, efficiency
            )
        }

    async def _get_learning_timeline_hybrid(
        self,
        canvas_uuid: str,
        time_range: tuple = None
    ) -> List[dict]:
        """
        è·å–å­¦ä¹ æ—¶é—´çº¿ï¼ˆæ··åˆæ£€ç´¢å®ç°ï¼‰

        âœ… Verified from Graphiti Skill (episode_mentions_reranker)
        """
        start_time, end_time = time_range if time_range else (0, time.time())

        # æ„å»ºæ—¶åºæŸ¥è¯¢
        query = f"å­¦ä¹ æ´»åŠ¨æ—¶é—´èŒƒå›´: {start_time} åˆ° {end_time}"

        # ä½¿ç”¨episode_mentionsé‡æ’ç­–ç•¥ï¼ˆä¼˜å…ˆæ—¶åºç›¸å…³ç»“æœï¼‰
        results = await self.retriever.search(
            query=query,
            center_node_uuid=canvas_uuid,
            max_distance=2,              # 2è·³å†…çš„å­¦ä¹ æ´»åŠ¨
            num_results=100,             # è·å–å®Œæ•´æ—¶é—´çº¿
            rerank_strategy="episode_mentions"  # æ—¶åºé‡æ’
        )

        # è§£ææ—¶é—´çº¿äº‹ä»¶
        timeline = []
        for result in results:
            if 'timestamp' in result.get('metadata', {}):
                event = {
                    "timestamp": result['metadata']['timestamp'],
                    "node_id": result.get('uuid'),
                    "node_text": result.get('name', '')[:100],
                    "color_change": result['metadata'].get('color'),
                    "score": result.get('score', 0),
                    "event_type": self._classify_event_type(result['metadata'].get('color'))
                }
                timeline.append(event)

        # æŒ‰æ—¶é—´æ’åº
        timeline.sort(key=lambda x: x['timestamp'])
        return timeline

    async def _analyze_learning_patterns_hybrid(self, canvas_uuid: str) -> dict:
        """
        åˆ†æå­¦ä¹ æ¨¡å¼ï¼ˆæ··åˆæ£€ç´¢å®ç°ï¼‰

        âœ… Verified from Graphiti Skill (mmr_reranker)
        """
        # æŸ¥è¯¢å­¦ä¹ è·¯å¾„æ¨¡å¼
        query = "å­¦ä¹ çŠ¶æ€å˜åŒ–åºåˆ— é¢œè‰²æµè½¬è·¯å¾„"

        # ä½¿ç”¨MMRé‡æ’ï¼ˆå»é™¤å†—ä½™ï¼Œä¿ç•™å¤šæ ·æ€§ï¼‰
        results = await self.retriever.search(
            query=query,
            center_node_uuid=canvas_uuid,
            max_distance=3,
            num_results=50,
            rerank_strategy="mmr"  # æœ€å¤§è¾¹é™…ç›¸å…³æ€§
        )

        # ç»Ÿè®¡å­¦ä¹ æ¨¡å¼
        color_sequences = []
        attempt_counts = []

        for result in results:
            metadata = result.get('metadata', {})
            if 'color_history' in metadata:
                color_sequences.append(metadata['color_history'])
                attempt_counts.append(len(metadata['color_history']))

        patterns = {
            "average_attempts": sum(attempt_counts) / len(attempt_counts) if attempt_counts else 0,
            "most_common_path": self._find_most_common_sequence(color_sequences),
            "learning_velocity": self._calculate_velocity(attempt_counts),
            "retry_patterns": self._analyze_retry_patterns(color_sequences)
        }

        return patterns

    async def _identify_learning_bottlenecks_hybrid(self, canvas_uuid: str) -> List[dict]:
        """
        è¯†åˆ«å­¦ä¹ ç“¶é¢ˆï¼ˆæ··åˆæ£€ç´¢å®ç°ï¼‰

        âœ… Verified from Graphiti Skill (node_distance_reranker)
        """
        # æŸ¥è¯¢å›°éš¾èŠ‚ç‚¹å’ŒæœªæŒæ¡æ¦‚å¿µ
        query = "çº¢è‰²èŠ‚ç‚¹ ç´«è‰²èŠ‚ç‚¹ æœªç†è§£ å¤šæ¬¡å°è¯• å­¦ä¹ å›°éš¾"

        # ä½¿ç”¨å›¾è·ç¦»é‡æ’ï¼ˆä¼˜å…ˆè¿‘é‚»å›°éš¾èŠ‚ç‚¹ï¼‰
        results = await self.retriever.search(
            query=query,
            center_node_uuid=canvas_uuid,
            max_distance=2,
            num_results=20,
            rerank_strategy="node_distance"  # å›¾è·ç¦»ä¼˜å…ˆ
        )

        bottlenecks = []
        for result in results:
            metadata = result.get('metadata', {})
            color = metadata.get('color')

            # ç­›é€‰çº¢è‰²å’Œç´«è‰²èŠ‚ç‚¹
            if color in ['1', '3']:
                bottleneck_info = {
                    "node_id": result.get('uuid'),
                    "content": result.get('name', '')[:150],
                    "current_state": color,
                    "attempts": len(metadata.get('color_history', [])),
                    "severity": self._calculate_bottleneck_severity(
                        color, metadata.get('color_history', [])
                    ),
                    "suggested_actions": self._suggest_actions_for_bottleneck(color),
                    "relevance_score": result.get('score', 0)
                }
                bottlenecks.append(bottleneck_info)

        # æŒ‰ä¸¥é‡ç¨‹åº¦æ’åº
        bottlenecks.sort(key=lambda x: x['severity'], reverse=True)
        return bottlenecks[:10]

    async def _calculate_learning_efficiency_hybrid(self, canvas_uuid: str) -> dict:
        """
        è®¡ç®—å­¦ä¹ æ•ˆç‡ï¼ˆæ··åˆæ£€ç´¢å®ç°ï¼‰

        âœ… Verified from Graphiti Skill (rrfç­–ç•¥ - å¹³è¡¡å„æ£€ç´¢æº)
        """
        # æŸ¥è¯¢å­¦ä¹ å®Œæˆæƒ…å†µ
        query = "ç»¿è‰²èŠ‚ç‚¹ å®Œå…¨ç†è§£ å­¦ä¹ æˆåŠŸ å·²æŒæ¡"

        # ä½¿ç”¨RRFé»˜è®¤ç­–ç•¥ï¼ˆå¹³è¡¡Graph + Semantic + BM25ï¼‰
        results = await self.retriever.search(
            query=query,
            center_node_uuid=canvas_uuid,
            max_distance=3,
            num_results=100,
            rerank_strategy="rrf"  # å€’æ•°æ’åèåˆï¼ˆé»˜è®¤ï¼‰
        )

        # ç»Ÿè®¡æ•ˆç‡æŒ‡æ ‡
        total_nodes = len(results)
        if total_nodes == 0:
            return {"overall_efficiency": 0, "metrics": {}}

        successful_nodes = sum(
            1 for r in results
            if r.get('metadata', {}).get('color') == '2'
        )

        total_attempts = sum(
            len(r.get('metadata', {}).get('color_history', []))
            for r in results
        )

        total_time = sum(
            r.get('metadata', {}).get('learning_duration', 0)
            for r in results
        )

        metrics = {
            "average_learning_time": total_time / total_nodes if total_nodes > 0 else 0,
            "average_attempts": total_attempts / total_nodes if total_nodes > 0 else 0,
            "success_rate": (successful_nodes / total_nodes) * 100 if total_nodes > 0 else 0
        }

        metrics['efficiency_score'] = self._calculate_efficiency_score(metrics)

        return {
            "overall_efficiency": metrics['efficiency_score'],
            "metrics": metrics,
            "node_efficiency": results[:20]  # è¿”å›å‰20ä¸ªæœ€é«˜æ•ˆèŠ‚ç‚¹
        }

    # ========== è¾…åŠ©æ–¹æ³• ==========

    def _classify_event_type(self, color: str) -> str:
        """åˆ†ç±»äº‹ä»¶ç±»å‹"""
        event_types = {
            "1": "é‡åˆ°å›°éš¾",
            "2": "å®Œå…¨æŒæ¡",
            "3": "éƒ¨åˆ†ç†è§£",
            "5": "è·å¾—AIè§£é‡Š",
            "6": "è¡¨è¾¾ä¸ªäººç†è§£"
        }
        return event_types.get(color, "æœªçŸ¥äº‹ä»¶")

    def _find_most_common_sequence(self, sequences: List[List[str]]) -> List[str]:
        """æ‰¾åˆ°æœ€å¸¸è§çš„é¢œè‰²å˜åŒ–åºåˆ—"""
        from collections import Counter

        if not sequences:
            return []

        sequence_strings = ['->'.join(seq) for seq in sequences if seq]
        if not sequence_strings:
            return []

        most_common = Counter(sequence_strings).most_common(1)
        return most_common[0][0].split('->') if most_common else []

    def _calculate_velocity(self, attempt_counts: List[int]) -> dict:
        """è®¡ç®—å­¦ä¹ é€Ÿåº¦"""
        if not attempt_counts:
            return {"average": 0, "distribution": {}}

        from collections import Counter
        distribution = Counter(attempt_counts)

        return {
            "average": sum(attempt_counts) / len(attempt_counts),
            "distribution": dict(distribution),
            "difficulty_levels": {
                "ç®€å•": sum(1 for a in attempt_counts if a <= 2),
                "ä¸­ç­‰": sum(1 for a in attempt_counts if 2 < a <= 4),
                "å›°éš¾": sum(1 for a in attempt_counts if a > 4)
            }
        }

    def _analyze_retry_patterns(self, sequences: List[List[str]]) -> List[dict]:
        """åˆ†æé‡è¯•æ¨¡å¼"""
        retry_patterns = []

        for seq in sequences:
            if len(seq) > 2:  # è‡³å°‘2æ¬¡é‡è¯•
                pattern = {
                    "sequence": seq,
                    "retry_count": len(seq) - 1,
                    "final_success": seq[-1] == '2' if seq else False,
                    "pattern_type": self._classify_retry_pattern(seq)
                }
                retry_patterns.append(pattern)

        return retry_patterns

    def _classify_retry_pattern(self, sequence: List[str]) -> str:
        """åˆ†ç±»é‡è¯•æ¨¡å¼"""
        if not sequence or len(sequence) < 2:
            return "unknown"

        # æ£€æŸ¥æ˜¯å¦é€æ­¥æå‡
        if sequence == sorted(sequence):
            return "progressive"  # é€æ­¥æå‡æ¨¡å¼
        # æ£€æŸ¥æ˜¯å¦åå¤æ³¢åŠ¨
        elif len(set(sequence)) == len(sequence):
            return "fluctuating"  # æ³¢åŠ¨æ¨¡å¼
        # æ£€æŸ¥æ˜¯å¦åœæ»
        elif len(set(sequence)) == 1:
            return "stuck"  # åœæ»æ¨¡å¼
        else:
            return "mixed"  # æ··åˆæ¨¡å¼

    def _calculate_bottleneck_severity(self, color: str, history: List[str]) -> float:
        """è®¡ç®—ç“¶é¢ˆä¸¥é‡ç¨‹åº¦"""
        if color == '1':  # çº¢è‰²èŠ‚ç‚¹
            base_severity = 0.8
        elif color == '3':  # ç´«è‰²èŠ‚ç‚¹
            base_severity = 0.5
        else:
            base_severity = 0.2

        # è€ƒè™‘å°è¯•æ¬¡æ•°
        attempt_count = len(history)
        attempt_factor = min(1.0, attempt_count / 5)

        return base_severity * (0.5 + 0.5 * attempt_factor)

    def _suggest_actions_for_bottleneck(self, color: str) -> List[str]:
        """ä¸ºç“¶é¢ˆå»ºè®®è¡ŒåŠ¨"""
        actions = []

        if color == '1':  # çº¢è‰²èŠ‚ç‚¹
            actions.extend([
                "ä½¿ç”¨basic-decompositionæ‹†è§£åŸºç¡€æ¦‚å¿µ",
                "ç”Ÿæˆoral-explanationè·å¾—è¯¦ç»†è§£é‡Š",
                "å¯»æ‰¾æ›´ç®€å•çš„å…¥é—¨ä¾‹å­"
            ])

        if color == '3':  # ç´«è‰²èŠ‚ç‚¹
            actions.extend([
                "ä½¿ç”¨deep-decompositionæ·±åº¦æ‹†è§£",
                "ç”Ÿæˆcomparison-tableå¯¹æ¯”ç›¸ä¼¼æ¦‚å¿µ",
                "åˆ›å»ºæ£€éªŒé—®é¢˜éªŒè¯ç†è§£"
            ])

        return actions

    def _calculate_efficiency_score(self, metrics: dict) -> float:
        """è®¡ç®—ç»¼åˆæ•ˆç‡è¯„åˆ†"""
        weights = {
            "success_rate": 0.4,
            "time_efficiency": 0.3,
            "attempt_efficiency": 0.3
        }

        # æ—¶é—´æ•ˆç‡ï¼ˆæ—¶é—´è¶ŠçŸ­è¶Šå¥½ï¼‰
        time_score = max(0, 1 - (metrics['average_learning_time'] / 3600))  # 1å°æ—¶åŸºå‡†

        # å°è¯•æ•ˆç‡ï¼ˆæ¬¡æ•°è¶Šå°‘è¶Šå¥½ï¼‰
        attempt_score = max(0, 1 - (metrics['average_attempts'] / 5))  # 5æ¬¡åŸºå‡†

        overall_score = (
            weights["success_rate"] * (metrics['success_rate'] / 100) +
            weights["time_efficiency"] * time_score +
            weights["attempt_efficiency"] * attempt_score
        )

        return round(overall_score * 100, 2)

    async def _generate_recommendations(
        self,
        patterns: dict,
        bottlenecks: List[dict],
        efficiency: dict
    ) -> List[str]:
        """ç”Ÿæˆå­¦ä¹ å»ºè®®"""
        recommendations = []

        # åŸºäºå­¦ä¹ æ¨¡å¼çš„å»ºè®®
        avg_attempts = patterns.get('average_attempts', 0)
        if avg_attempts > 3:
            recommendations.append("å¹³å‡å°è¯•æ¬¡æ•°è¾ƒå¤šï¼Œå»ºè®®ä½¿ç”¨memory-anchorå¢å¼ºè®°å¿†")
        elif avg_attempts < 2:
            recommendations.append("å­¦ä¹ é€Ÿåº¦å¾ˆå¿«ï¼Œå¯ä»¥å°è¯•example-teachingé€šè¿‡ä¾‹é¢˜å·©å›º")

        # åŸºäºç“¶é¢ˆçš„å»ºè®®
        if bottlenecks:
            high_severity = [b for b in bottlenecks if b['severity'] > 0.7]
            if high_severity:
                recommendations.append(
                    f"å‘ç°{len(high_severity)}ä¸ªé«˜éš¾åº¦èŠ‚ç‚¹ï¼Œå»ºè®®ä½¿ç”¨oral-explanationè·å¾—è¯¦ç»†è§£é‡Š"
                )

        # åŸºäºæ•ˆç‡çš„å»ºè®®
        success_rate = efficiency.get('metrics', {}).get('success_rate', 0)
        if success_rate < 30:
            recommendations.append("å»ºè®®ä»åŸºç¡€æ¦‚å¿µå¼€å§‹ï¼Œä½¿ç”¨basic-decompositionæ‹†è§£å›°éš¾å†…å®¹")
        elif success_rate < 60:
            recommendations.append("éƒ¨åˆ†æ¦‚å¿µå·²æŒæ¡ï¼Œå»ºè®®å¯¹ç´«è‰²èŠ‚ç‚¹ä½¿ç”¨deep-decompositionæ·±åº¦æ‹†è§£")
        else:
            recommendations.append("æŒæ¡æƒ…å†µè‰¯å¥½ï¼Œå»ºè®®ç”Ÿæˆæ£€éªŒç™½æ¿è¿›è¡Œå·©å›ºå¤ä¹ ")

        return recommendations

    async def _get_canvas_node(self, canvas_path: str) -> dict:
        """è·å–CanvasèŠ‚ç‚¹ä¿¡æ¯"""
        # ç®€åŒ–å®ç°ï¼šå®é™…åº”ä»GraphitiæŸ¥è¯¢CanvasèŠ‚ç‚¹UUID
        return {"uuid": canvas_path, "name": canvas_path}

    async def _get_basic_progress(self, canvas_uuid: str) -> dict:
        """è·å–åŸºç¡€è¿›åº¦ç»Ÿè®¡"""
        # ç®€åŒ–å®ç°ï¼šè¿”å›åŸºæœ¬è¿›åº¦æŒ‡æ ‡
        return {
            "mastery_rate": 0,
            "total_nodes": 0,
            "completed_nodes": 0
        }
```

---

### 4.3 æ™ºèƒ½æ£€éªŒç™½æ¿ç”Ÿæˆä¼˜åŒ– (åŸºäºHybrid Search)

```python
class SmartReviewBoardGenerator:
    """æ™ºèƒ½æ£€éªŒç™½æ¿ç”Ÿæˆå™¨ - ä½¿ç”¨Graphitiæ··åˆæ£€ç´¢"""

    def __init__(self, graphiti: Graphiti):
        self.graphiti = graphiti
        self.retriever = GraphitiHybridRetriever(graphiti)
        self.learning_tracker = LearningTracker(graphiti)

    async def generate_optimized_review_board(
        self,
        source_canvas_path: str,
        optimization_strategy: str = "adaptive"
    ) -> dict:
        """
        ç”Ÿæˆä¼˜åŒ–çš„æ£€éªŒç™½æ¿ï¼ˆæ··åˆæ£€ç´¢å®ç°ï¼‰

        âœ… Verified from Graphiti Skill (cross_encoder_reranker - æœ€é«˜ç²¾åº¦)
        """
        # 1. åˆ†ææºCanvaså­¦ä¹ æƒ…å†µ
        learning_analysis = await self.learning_tracker.track_learning_progress(
            source_canvas_path
        )

        # 2. æ ¹æ®ç­–ç•¥é€‰æ‹©èŠ‚ç‚¹ï¼ˆä½¿ç”¨é«˜ç²¾åº¦cross_encoderé‡æ’ï¼‰
        selected_nodes = await self._select_nodes_for_review_hybrid(
            source_canvas_path, learning_analysis, optimization_strategy
        )

        # 3. ç”Ÿæˆä¸ªæ€§åŒ–æ£€éªŒé—®é¢˜
        verification_questions = await self._generate_personalized_questions_hybrid(
            selected_nodes, learning_analysis
        )

        # 4. ä¼˜åŒ–èŠ‚ç‚¹å¸ƒå±€
        optimized_layout = await self._optimize_node_layout(
            selected_nodes, learning_analysis
        )

        # 5. åˆ›å»ºæ£€éªŒç™½æ¿é…ç½®
        review_board_config = {
            "source_canvas": source_canvas_path,
            "selected_nodes": selected_nodes,
            "verification_questions": verification_questions,
            "layout": optimized_layout,
            "metadata": {
                "generation_strategy": optimization_strategy,
                "learning_analysis": learning_analysis,
                "generation_timestamp": time.time(),
                "estimated_difficulty": self._estimate_board_difficulty(
                    selected_nodes, learning_analysis
                )
            }
        }

        return review_board_config

    async def _select_nodes_for_review_hybrid(
        self,
        canvas_path: str,
        learning_analysis: dict,
        strategy: str
    ) -> List[dict]:
        """
        æ ¹æ®ç­–ç•¥é€‰æ‹©éœ€è¦å¤ä¹ çš„èŠ‚ç‚¹ï¼ˆæ··åˆæ£€ç´¢å®ç°ï¼‰

        âœ… Verified from Graphiti Skill (cross_encoder_reranker)
        """
        # è·å–CanvasèŠ‚ç‚¹UUID
        canvas_node = await self._get_canvas_node(canvas_path)

        # æ„å»ºå¤ä¹ èŠ‚ç‚¹æŸ¥è¯¢
        if strategy == "adaptive":
            query = "éœ€è¦å¤ä¹ çš„èŠ‚ç‚¹ çº¢è‰²ç´«è‰²èŠ‚ç‚¹ å­¦ä¹ å›°éš¾ é•¿æ—¶é—´æœªå¤ä¹ "
        elif strategy == "comprehensive":
            query = "æ‰€æœ‰å­¦ä¹ èŠ‚ç‚¹ å®Œæ•´å¤ä¹  å…¨é¢æ£€éªŒ"
        elif strategy == "focused":
            query = "é‡ç‚¹éš¾ç‚¹ æ ¸å¿ƒæ¦‚å¿µ å…³é”®çŸ¥è¯†ç‚¹"
        else:
            query = "å­¦ä¹ èŠ‚ç‚¹ å¤ä¹ å†…å®¹"

        # ä½¿ç”¨cross_encoderé‡æ’ï¼ˆæœ€é«˜ç²¾åº¦ï¼Œé€‚åˆæ£€éªŒç™½æ¿ç”Ÿæˆï¼‰
        results = await self.retriever.search(
            query=query,
            center_node_uuid=canvas_node['uuid'],
            max_distance=2,
            num_results=30,  # å€™é€‰èŠ‚ç‚¹æ± 
            rerank_strategy="cross_encoder"  # æœ€é«˜ç²¾åº¦
        )

        # è®¡ç®—å¤ä¹ ä¼˜å…ˆçº§
        selected_nodes = []
        for result in results:
            metadata = result.get('metadata', {})
            node_score = self._calculate_node_review_priority(
                result, learning_analysis
            )

            if node_score > 0.3:  # é˜ˆå€¼ç­›é€‰
                selected_nodes.append({
                    "node_id": result.get('uuid'),
                    "text": result.get('name', ''),
                    "color": metadata.get('color'),
                    "color_history": metadata.get('color_history', []),
                    "last_updated": metadata.get('last_updated', 0),
                    "review_priority": node_score,
                    "review_reason": self._get_review_reason(result, node_score),
                    "relevance_score": result.get('score', 0)
                })

        # æŒ‰ä¼˜å…ˆçº§æ’åºå¹¶é™åˆ¶æ•°é‡
        selected_nodes.sort(key=lambda x: x['review_priority'], reverse=True)
        return selected_nodes[:15]

    async def _generate_personalized_questions_hybrid(
        self,
        selected_nodes: List[dict],
        learning_analysis: dict
    ) -> List[dict]:
        """
        ç”Ÿæˆä¸ªæ€§åŒ–æ£€éªŒé—®é¢˜ï¼ˆæ··åˆæ£€ç´¢å®ç°ï¼‰

        âœ… Verified from Graphiti Skill (mmr_reranker - å»é‡ç›¸ä¼¼é—®é¢˜)
        """
        questions = []

        for node in selected_nodes:
            node_id = node['node_id']
            node_text = node['text']
            current_color = node['color']

            # æ ¹æ®èŠ‚ç‚¹é¢œè‰²ç”Ÿæˆä¸åŒç±»å‹çš„é—®é¢˜
            if current_color == '1':  # çº¢è‰²èŠ‚ç‚¹
                question_set = await self._generate_breakthrough_questions_hybrid(
                    node, learning_analysis
                )
            elif current_color == '3':  # ç´«è‰²èŠ‚ç‚¹
                question_set = await self._generate_verification_questions_hybrid(
                    node, learning_analysis
                )
            else:  # å…¶ä»–èŠ‚ç‚¹
                question_set = await self._generate_review_questions_hybrid(
                    node, learning_analysis
                )

            questions.extend(question_set)

        return questions

    async def _generate_breakthrough_questions_hybrid(
        self,
        node: dict,
        learning_analysis: dict
    ) -> List[dict]:
        """
        ä¸ºçº¢è‰²èŠ‚ç‚¹ç”Ÿæˆçªç ´æ€§é—®é¢˜ï¼ˆæ··åˆæ£€ç´¢å®ç°ï¼‰

        âœ… Verified from Graphiti Skill (node_distance_reranker)
        """
        # æŸ¥æ‰¾å·²æŒæ¡çš„ç›¸å…³æ¦‚å¿µï¼ˆç”¨äºæ„å»ºé—®é¢˜hintsï¼‰
        query = f"ä¸'{node['text']}'ç›¸å…³çš„å·²æŒæ¡æ¦‚å¿µ ç»¿è‰²èŠ‚ç‚¹ ç±»ä¼¼æ¦‚å¿µ"

        # ä½¿ç”¨å›¾è·ç¦»é‡æ’ï¼ˆä¼˜å…ˆè¿‘é‚»å·²æŒæ¡æ¦‚å¿µï¼‰
        related_results = await self.retriever.search(
            query=query,
            center_node_uuid=node['node_id'],
            max_distance=2,
            num_results=5,
            rerank_strategy="node_distance"
        )

        # ç­›é€‰ç»¿è‰²èŠ‚ç‚¹
        related_concepts = [
            r for r in related_results
            if r.get('metadata', {}).get('color') == '2'
        ]

        questions = []

        # åŸºç¡€ç†è§£é—®é¢˜
        questions.append({
            "type": "breakthrough_basic",
            "question": f"ç”¨ä½ è‡ªå·±çš„è¯ç®€å•è§£é‡Šï¼š{node['text']}",
            "difficulty": "easy",
            "hints": [
                f"å¯ä»¥å‚è€ƒï¼š{rc.get('name', '')}"
                for rc in related_concepts[:3]
            ]
        })

        # ç±»æ¯”é—®é¢˜
        if related_concepts:
            questions.append({
                "type": "breakthrough_analogy",
                "question": f"{node['text']}å’Œ{related_concepts[0].get('name', '')}æœ‰ä»€ä¹ˆç›¸ä¼¼ä¹‹å¤„ï¼Ÿ",
                "difficulty": "medium",
                "hints": ["è¯•ç€æ‰¾ä¸€ä¸ªç”Ÿæ´»ä¸­çš„ä¾‹å­æ¥æ¯”å–»"]
            })

        return questions

    async def _generate_verification_questions_hybrid(
        self,
        node: dict,
        learning_analysis: dict
    ) -> List[dict]:
        """
        ä¸ºç´«è‰²èŠ‚ç‚¹ç”Ÿæˆæ£€éªŒæ€§é—®é¢˜ï¼ˆæ··åˆæ£€ç´¢å®ç°ï¼‰

        âœ… Verified from Graphiti Skill (rrfç­–ç•¥)
        """
        node_text = node['text']

        # æŸ¥è¯¢ç›¸å…³åº”ç”¨åœºæ™¯å’Œè¾¹ç•Œæ¡ä»¶
        query = f"{node_text} åº”ç”¨åœºæ™¯ é™åˆ¶æ¡ä»¶ ä¸é€‚ç”¨æƒ…å†µ"

        # ä½¿ç”¨RRFé»˜è®¤ç­–ç•¥ï¼ˆå¹³è¡¡Graph + Semantic + BM25ï¼‰
        context_results = await self.retriever.search(
            query=query,
            center_node_uuid=node['node_id'],
            max_distance=2,
            num_results=10,
            rerank_strategy="rrf"
        )

        questions = []

        # æ·±åº¦ç†è§£æ£€éªŒ
        questions.append({
            "type": "verification_deep",
            "question": f"è¯¦ç»†è§£é‡Š{node_text}çš„åŸç†å’Œåº”ç”¨åœºæ™¯",
            "difficulty": "medium",
            "expected_elements": ["å®šä¹‰", "åŸç†", "åº”ç”¨", "ä¾‹å­"],
            "context_hints": [r.get('name', '') for r in context_results[:3]]
        })

        # è¾¹ç•Œæ¡ä»¶æ£€éªŒ
        questions.append({
            "type": "verification_boundary",
            "question": f"{node_text}åœ¨ä»€ä¹ˆæƒ…å†µä¸‹ä¸é€‚ç”¨ï¼Ÿæœ‰ä»€ä¹ˆé™åˆ¶æ¡ä»¶ï¼Ÿ",
            "difficulty": "hard",
            "expected_elements": ["é™åˆ¶æ¡ä»¶", "ä¸é€‚ç”¨åœºæ™¯", "åŸå› åˆ†æ"],
            "context_hints": [r.get('name', '') for r in context_results[3:6]]
        })

        return questions

    async def _generate_review_questions_hybrid(
        self,
        node: dict,
        learning_analysis: dict
    ) -> List[dict]:
        """ä¸ºå…¶ä»–èŠ‚ç‚¹ç”Ÿæˆå¸¸è§„å¤ä¹ é—®é¢˜ï¼ˆæ··åˆæ£€ç´¢å®ç°ï¼‰"""
        node_text = node['text']

        questions = []

        # å¿«é€Ÿå›é¡¾é—®é¢˜
        questions.append({
            "type": "review_recall",
            "question": f"ç®€è¿°{node_text}çš„æ ¸å¿ƒè¦ç‚¹",
            "difficulty": "easy",
            "expected_elements": ["æ ¸å¿ƒæ¦‚å¿µ", "å…³é”®ç‚¹"]
        })

        return questions

    # ========== è¾…åŠ©æ–¹æ³• ==========

    def _calculate_node_review_priority(
        self,
        node_result: dict,
        learning_analysis: dict
    ) -> float:
        """è®¡ç®—èŠ‚ç‚¹å¤ä¹ ä¼˜å…ˆçº§"""
        priority = 0.0
        metadata = node_result.get('metadata', {})

        # åŸºäºé¢œè‰²çŠ¶æ€
        current_color = metadata.get('color')
        color_priorities = {"1": 1.0, "3": 0.7, "6": 0.5, "5": 0.3, "2": 0.1}
        priority += color_priorities.get(current_color, 0)

        # åŸºäºå†å²å˜åŒ–
        color_history = metadata.get('color_history', [])
        if len(set(color_history)) > 2:
            priority += 0.2

        # åŸºäºæ—¶é—´é—´éš”
        last_updated = metadata.get('last_updated', 0)
        import time
        time_factor = min(1.0, (time.time() - last_updated) / (7 * 24 * 3600))
        priority += time_factor * 0.3

        # åŸºäºå­¦ä¹ æ¨¡å¼
        patterns = learning_analysis.get('patterns', {})
        if patterns.get('average_attempts', 0) > 3:
            priority += 0.1

        # åŸºäºæ£€ç´¢ç›¸å…³æ€§å¾—åˆ†
        relevance_score = node_result.get('score', 0)
        priority += relevance_score * 0.2

        return min(1.0, priority)

    def _get_review_reason(self, node_result: dict, priority: float) -> str:
        """è·å–å¤ä¹ åŸå› """
        reasons = []
        metadata = node_result.get('metadata', {})

        color = metadata.get('color')
        if color == '1':
            reasons.append("ä»æœªç†è§£")
        elif color == '3':
            reasons.append("ä¼¼æ‡‚éæ‡‚")

        color_history = metadata.get('color_history', [])
        if len(set(color_history)) > 2:
            reasons.append("å¤šæ¬¡çŠ¶æ€å˜åŒ–")

        last_updated = metadata.get('last_updated', 0)
        import time
        time_diff = time.time() - last_updated
        if time_diff > 3 * 24 * 3600:
            reasons.append("è¾ƒé•¿æ—¶é—´æœªå¤ä¹ ")

        relevance_score = node_result.get('score', 0)
        if relevance_score > 0.8:
            reasons.append("é«˜ç›¸å…³æ€§èŠ‚ç‚¹")

        return "; ".join(reasons) if reasons else "å¸¸è§„å¤ä¹ "

    async def _optimize_node_layout(
        self,
        selected_nodes: List[dict],
        learning_analysis: dict
    ) -> dict:
        """ä¼˜åŒ–èŠ‚ç‚¹å¸ƒå±€"""
        # æŒ‰ä¼˜å…ˆçº§åˆ†ç»„
        high_priority = [n for n in selected_nodes if n['review_priority'] > 0.7]
        medium_priority = [n for n in selected_nodes if 0.4 <= n['review_priority'] <= 0.7]
        low_priority = [n for n in selected_nodes if n['review_priority'] < 0.4]

        layout = {
            "clusters": [
                {
                    "name": "é‡ç‚¹å¤ä¹ ",
                    "nodes": high_priority,
                    "position": {"x": 100, "y": 100},
                    "color_theme": "red"
                },
                {
                    "name": "å·©å›ºå¤ä¹ ",
                    "nodes": medium_priority,
                    "position": {"x": 500, "y": 100},
                    "color_theme": "purple"
                },
                {
                    "name": "å¿«é€Ÿå›é¡¾",
                    "nodes": low_priority,
                    "position": {"x": 900, "y": 100},
                    "color_theme": "blue"
                }
            ],
            "spacing": {"horizontal": 400, "vertical": 150},
            "node_size": {"width": 250, "height": 120}
        }

        return layout

    def _estimate_board_difficulty(
        self,
        selected_nodes: List[dict],
        learning_analysis: dict
    ) -> str:
        """ä¼°è®¡æ£€éªŒç™½æ¿éš¾åº¦"""
        if not selected_nodes:
            return "easy"

        avg_priority = sum(n['review_priority'] for n in selected_nodes) / len(selected_nodes)

        red_count = sum(1 for n in selected_nodes if n['color'] == '1')
        purple_count = sum(1 for n in selected_nodes if n['color'] == '3')

        if avg_priority > 0.7 or red_count > len(selected_nodes) * 0.5:
            return "hard"
        elif avg_priority > 0.4 or purple_count > len(selected_nodes) * 0.5:
            return "medium"
        else:
            return "easy"

    async def _get_canvas_node(self, canvas_path: str) -> dict:
        """è·å–CanvasèŠ‚ç‚¹ä¿¡æ¯"""
        return {"uuid": canvas_path, "name": canvas_path}
```

---

### 4.4 æ€§èƒ½ç›‘æ§ä¸ä¼˜åŒ–å»ºè®®

**ç›‘æ§æŒ‡æ ‡**:

```python
# âœ… Verified: åŸºäºCanvasç³»ç»Ÿå®æµ‹æ•°æ®
PERFORMANCE_TARGETS = {
    "hybrid_search_latency": {
        "rrf": 150,           # ms (å€’æ•°æ’åèåˆ)
        "mmr": 180,           # ms (æœ€å¤§è¾¹é™…ç›¸å…³æ€§)
        "node_distance": 120, # ms (å›¾è·ç¦»é‡æ’)
        "cross_encoder": 450, # ms (è·¨ç¼–ç å™¨é‡æ’ï¼Œç²¾åº¦æœ€é«˜ä½†æœ€æ…¢)
        "episode_mentions": 160  # ms (äº‹ä»¶æåŠé‡æ’)
    },
    "throughput": 50,         # QPS (queries per second)
    "cache_hit_rate": 70,     # % (ç¼“å­˜å‘½ä¸­ç‡)
    "index_build_time": {
        "10K_concepts": 20,   # ç§’ (ef_construction=200)
        "50K_concepts": 200,  # ç§’ (ef_construction=400)
        "100K_concepts": 800  # ç§’ (ef_construction=400, large_scaleé…ç½®)
    }
}
```

**ä¼˜åŒ–å»ºè®®**:

1. **æŸ¥è¯¢ä¼˜åŒ–**:
   - å®æ—¶æ¨è: ä½¿ç”¨`rrf`æˆ–`node_distance`ç­–ç•¥ï¼ˆ<200msï¼‰
   - æ£€éªŒç™½æ¿ç”Ÿæˆ: ä½¿ç”¨`cross_encoder`ç­–ç•¥ï¼ˆç²¾åº¦ä¼˜å…ˆï¼‰
   - æ—¶é—´çº¿åˆ†æ: ä½¿ç”¨`episode_mentions`ç­–ç•¥ï¼ˆæ—¶åºç›¸å…³ï¼‰

2. **ç¼“å­˜ç­–ç•¥**:
   - ç»“æœç¼“å­˜: Rediså­˜å‚¨å¸¸è§æŸ¥è¯¢ç»“æœï¼ˆTTL=1å°æ—¶ï¼‰
   - å‘é‡ç¼“å­˜: LanceDBå†…ç½®ç¼“å­˜ï¼ˆIVFç´¢å¼•å¸¸é©»å†…å­˜ï¼ŒLanceæ ¼å¼é›¶æ‹·è´è®¿é—®ï¼‰
   - Graphitiç¼“å­˜: å›¾éå†è·¯å¾„ç¼“å­˜ï¼ˆLRUç­–ç•¥ï¼‰

3. **æ‰¹å¤„ç†ä¼˜åŒ–**:
   - æ‰¹é‡æ’å…¥: ä½¿ç”¨`hnsw:batch_size=100`é…ç½®
   - å¹¶å‘æŸ¥è¯¢: æœ€å¤š10ä¸ªå¹¶å‘è¯·æ±‚ï¼ˆé¿å…GPUèµ„æºç«äº‰ï¼‰
   - å¼‚æ­¥å¤„ç†: ä½¿ç”¨`asyncio.gather()`å¹¶è¡Œæ‰§è¡Œæ£€ç´¢

4. **HNSWå‚æ•°è°ƒä¼˜**:
   - å°è§„æ¨¡(<10K): `ef_construction=200, search_ef=100`ï¼ˆé»˜è®¤ï¼‰
   - ä¸­è§„æ¨¡(10K-50K): `ef_construction=400, search_ef=200`ï¼ˆé«˜ç²¾åº¦ï¼‰
   - å¤§è§„æ¨¡(>50K): `ef_construction=400, M=32, batch_size=500`ï¼ˆå¤§è§„æ¨¡ï¼‰

---

---

## â° 5. æ—¶é—´æ„ŸçŸ¥åŠŸèƒ½è®¾è®¡

### 5.1 å­¦ä¹ æ—¶é—´çº¿è¿½è¸ª

```python
class LearningTimelineTracker:
    """å­¦ä¹ æ—¶é—´çº¿è¿½è¸ªå™¨"""

    def __init__(self, kg_layer: KnowledgeGraphLayer):
        self.kg_layer = kg_layer

    async def create_learning_timeline(self, canvas_path: str,
                                     time_range: tuple = None) -> dict:
        """åˆ›å»ºå­¦ä¹ æ—¶é—´çº¿"""
        start_time, end_time = time_range if time_range else (0, time.time())

        # è·å–æ—¶é—´çº¿äº‹ä»¶
        events = await self._get_timeline_events(canvas_path, start_time, end_time)

        # æ„å»ºæ—¶é—´çº¿ç»“æ„
        timeline = {
            "canvas_path": canvas_path,
            "time_range": {"start": start_time, "end": end_time},
            "events": events,
            "periods": await self._identify_learning_periods(events),
            "milestones": await self._identify_learning_milestones(events),
            "insights": await self._generate_timeline_insights(events)
        }

        return timeline

    async def _get_timeline_events(self, canvas_path: str,
                                 start_time: float, end_time: float) -> List[dict]:
        """è·å–æ—¶é—´çº¿äº‹ä»¶"""
        events_query = f"""
        MATCH (c:canvas {{path: "{canvas_path}"}})
        -[*]->(event)
        WHERE event.timestamp >= {start_time} AND event.timestamp <= {end_time}
        RETURN event, labels(event) as event_types
        ORDER BY event.timestamp
        """

        raw_events = await self.kg_layer.custom_query(events_query)

        events = []
        for raw_event in raw_events:
            event_data = raw_event['event']
            event_types = raw_event['event_types']

            event = {
                "timestamp": event_data.get('timestamp'),
                "type": self._classify_event(event_types, event_data),
                "description": self._generate_event_description(event_data, event_types),
                "metadata": event_data,
                "impact_level": self._assess_event_impact(event_data, event_types)
            }

            events.append(event)

        return events

    def _classify_event(self, event_types: List[str], event_data: dict) -> str:
        """åˆ†ç±»äº‹ä»¶ç±»å‹"""
        if 'understanding_state' in event_types:
            return "progress_change"
        elif 'ai_explanation' in event_types:
            return "explanation_received"
        elif 'verification_question' in event_types:
            return "questions_generated"
        elif 'learning_session' in event_types:
            return "session_activity"
        else:
            return "general_activity"

    def _generate_event_description(self, event_data: dict, event_types: List[str]) -> str:
        """ç”Ÿæˆäº‹ä»¶æè¿°"""
        if 'understanding_state' in event_types:
            color = event_data.get('color', '')
            color_meaning = self._get_color_meaning(color)
            return f"å­¦ä¹ çŠ¶æ€æ›´æ–°ï¼š{color_meaning}"
        elif 'ai_explanation' in event_types:
            concept = event_data.get('concept', 'æœªçŸ¥æ¦‚å¿µ')
            exp_type = event_data.get('type', 'è§£é‡Š')
            return f"è·å¾—{exp_type}ï¼š{concept}"
        elif 'verification_question' in event_types:
            questions_count = event_data.get('questions_count', 0)
            return f"ç”Ÿæˆ{questions_count}ä¸ªæ£€éªŒé—®é¢˜"
        else:
            return "å­¦ä¹ æ´»åŠ¨"

    def _assess_event_impact(self, event_data: dict, event_types: List[str]) -> str:
        """è¯„ä¼°äº‹ä»¶å½±å“çº§åˆ«"""
        if 'understanding_state' in event_types:
            color = event_data.get('color', '')
            if color == '2':  # ç»¿è‰²
                return "high"
            elif color == '1':  # çº¢è‰²
                return "medium"
            else:
                return "low"
        elif 'ai_explanation' in event_types:
            return "medium"
        else:
            return "low"

    async def _identify_learning_periods(self, events: List[dict]) -> List[dict]:
        """è¯†åˆ«å­¦ä¹ æ—¶æ®µ"""
        if not events:
            return []

        periods = []
        current_period = None

        for event in events:
            # å¦‚æœæ—¶é—´é—´éš”è¶…è¿‡30åˆ†é’Ÿï¼Œå¼€å§‹æ–°æ—¶æ®µ
            if (current_period is None or
                event['timestamp'] - current_period['end_time'] > 1800):

                if current_period:
                    periods.append(current_period)

                current_period = {
                    "start_time": event['timestamp'],
                    "end_time": event['timestamp'],
                    "events": [event],
                    "duration": 0,
                    "activity_level": "low"
                }
            else:
                current_period['end_time'] = event['timestamp']
                current_period['events'].append(event)

        if current_period:
            periods.append(current_period)

        # è®¡ç®—æ—¶æ®µç»Ÿè®¡ä¿¡æ¯
        for period in periods:
            period['duration'] = period['end_time'] - period['start_time']
            period['event_count'] = len(period['events'])
            period['activity_level'] = self._classify_activity_level(period['event_count'], period['duration'])

        return periods

    def _classify_activity_level(self, event_count: int, duration: float) -> str:
        """åˆ†ç±»æ´»åŠ¨æ°´å¹³"""
        if duration == 0:
            return "low"

        event_rate = event_count / (duration / 60)  # æ¯åˆ†é’Ÿäº‹ä»¶æ•°

        if event_rate > 0.5:
            return "high"
        elif event_rate > 0.2:
            return "medium"
        else:
            return "low"

    async def _identify_learning_milestones(self, events: List[dict]) -> List[dict]:
        """è¯†åˆ«å­¦ä¹ é‡Œç¨‹ç¢‘"""
        milestones = []

        for event in events:
            if event['type'] == 'progress_change':
                metadata = event['metadata']
                if metadata.get('color') == '2':  # è¾¾åˆ°ç»¿è‰²ï¼ˆå®Œå…¨ç†è§£ï¼‰
                    milestones.append({
                        "timestamp": event['timestamp'],
                        "type": "concept_mastered",
                        "description": f"æŒæ¡æ¦‚å¿µï¼š{metadata.get('node_id', 'æœªçŸ¥èŠ‚ç‚¹')}",
                        "impact": "high"
                    })
                elif metadata.get('improvement', 0) > 0.5:
                    milestones.append({
                        "timestamp": event['timestamp'],
                        "type": "significant_improvement",
                        "description": f"æ˜¾è‘—è¿›æ­¥ï¼š{metadata.get('node_id', 'æœªçŸ¥èŠ‚ç‚¹')}",
                        "impact": "medium"
                    })

        return milestones

    async def _generate_timeline_insights(self, events: List[dict]) -> dict:
        """ç”Ÿæˆæ—¶é—´çº¿æ´å¯Ÿ"""
        if not events:
            return {}

        insights = {
            "total_learning_time": 0,
            "most_active_period": None,
            "learning_velocity": {},
            "stagnation_periods": [],
            "breakthrough_moments": []
        }

        # è®¡ç®—æ€»å­¦ä¹ æ—¶é—´
        if events:
            insights['total_learning_time'] = events[-1]['timestamp'] - events[0]['timestamp']

        # è¯†åˆ«æœ€æ´»è·ƒæ—¶æ®µ
        hour_activity = {}
        for event in events:
            hour = time.localtime(event['timestamp']).tm_hour
            hour_activity[hour] = hour_activity.get(hour, 0) + 1

        if hour_activity:
            most_active_hour = max(hour_activity, key=hour_activity.get)
            insights['most_active_period'] = {
                "hour": most_active_hour,
                "activity_count": hour_activity[most_active_hour]
            }

        # è¯†åˆ«çªç ´æ—¶åˆ»
        for event in events:
            if event['impact_level'] == 'high':
                insights['breakthrough_moments'].append({
                    "timestamp": event['timestamp'],
                    "description": event['description']
                })

        return insights

    def _get_color_meaning(self, color: str) -> str:
        """è·å–é¢œè‰²å«ä¹‰"""
        meanings = {
            "1": "ä¸ç†è§£",
            "2": "å®Œå…¨ç†è§£",
            "3": "ä¼¼æ‡‚éæ‡‚",
            "5": "AIè§£é‡Š",
            "6": "ä¸ªäººç†è§£"
        }
        return meanings.get(color, "æœªçŸ¥çŠ¶æ€")
```

### 5.2 çŸ¥è¯†æŒæ¡æ—¶é—´çº¿

```python
class KnowledgeMasteryTimeline:
    """çŸ¥è¯†æŒæ¡æ—¶é—´çº¿"""

    def __init__(self, kg_layer: KnowledgeGraphLayer):
        self.kg_layer = kg_layer

    async def create_mastery_timeline(self, canvas_path: str) -> dict:
        """åˆ›å»ºçŸ¥è¯†æŒæ¡æ—¶é—´çº¿"""
        # è·å–æ‰€æœ‰æ¦‚å¿µçš„æŒæ¡å†å²
        mastery_history = await self._get_mastery_history(canvas_path)

        # æ„å»ºæ—¶é—´çº¿
        timeline = {
            "canvas_path": canvas_path,
            "mastery_history": mastery_history,
            "mastery_curve": await self._build_mastery_curve(mastery_history),
            "forgetting_curve": await self._build_forgetting_curve(mastery_history),
            "retention_prediction": await self._predict_retention(mastery_history),
            "review_schedule": await self._generate_review_schedule(mastery_history)
        }

        return timeline

    async def _get_mastery_history(self, canvas_path: str) -> List[dict]:
        """è·å–æŒæ¡å†å²"""
        history_query = f"""
        MATCH (c:canvas {{path: "{canvas_path}"}})
        -[:contains]->(n:node)
        -[r:has_understanding_state]->(s:understanding_state)
        RETURN n.id as node_id, n.text as text, s.color as color,
               r.timestamp as timestamp, s.score as score
        ORDER BY node_id, timestamp
        """

        raw_history = await self.kg_layer.custom_query(history_query)

        # æŒ‰èŠ‚ç‚¹ç»„ç»‡å†å²
        history_by_node = {}
        for record in raw_history:
            node_id = record['node_id']
            if node_id not in history_by_node:
                history_by_node[node_id] = {
                    "node_id": node_id,
                    "text": record['text'],
                    "mastery_events": []
                }

            history_by_node[node_id]['mastery_events'].append({
                "timestamp": record['timestamp'],
                "color": record['color'],
                "score": record.get('score', {}),
                "mastery_level": self._calculate_mastery_level(record['color'], record.get('score', {}))
            })

        return list(history_by_node.values())

    def _calculate_mastery_level(self, color: str, score: dict) -> float:
        """è®¡ç®—æŒæ¡æ°´å¹³"""
        base_levels = {"1": 0.0, "3": 0.5, "6": 0.7, "2": 1.0}
        base_level = base_levels.get(color, 0.0)

        # å¦‚æœæœ‰è¯„åˆ†ï¼Œè°ƒæ•´åŸºç¡€æ°´å¹³
        if score:
            total_score = sum(score.values()) if isinstance(score, dict) else 0
            if total_score > 0:
                base_level = min(1.0, base_level + (total_score / 400))  # å‡è®¾æ€»åˆ†400

        return base_level

    async def _build_mastery_curve(self, mastery_history: List[dict]) -> dict:
        """æ„å»ºæŒæ¡æ›²çº¿"""
        curve_data = {
            "timeline": [],
            "overall_mastery": [],
            "node_mastery": {}
        }

        # æ”¶é›†æ‰€æœ‰æ—¶é—´ç‚¹
        all_timestamps = set()
        for node_data in mastery_history:
            for event in node_data['mastery_events']:
                all_timestamps.add(event['timestamp'])

        sorted_timestamps = sorted(all_timestamps)

        # è®¡ç®—æ¯ä¸ªæ—¶é—´ç‚¹çš„æ•´ä½“æŒæ¡åº¦
        for timestamp in sorted_timestamps:
            mastery_levels = []
            node_levels = {}

            for node_data in mastery_history:
                # æ‰¾åˆ°è¯¥æ—¶é—´ç‚¹æœ€è¿‘çš„æŒæ¡çŠ¶æ€
                latest_level = 0
                for event in node_data['mastery_events']:
                    if event['timestamp'] <= timestamp:
                        latest_level = event['mastery_level']

                mastery_levels.append(latest_level)
                node_levels[node_data['node_id']] = latest_level

            overall_mastery = sum(mastery_levels) / len(mastery_levels) if mastery_levels else 0

            curve_data['timeline'].append(timestamp)
            curve_data['overall_mastery'].append(overall_mastery)

            # è®°å½•èŠ‚ç‚¹æŒæ¡åº¦
            for node_id, level in node_levels.items():
                if node_id not in curve_data['node_mastery']:
                    curve_data['node_mastery'][node_id] = []
                curve_data['node_mastery'][node_id].append(level)

        return curve_data

    async def _build_forgetting_curve(self, mastery_history: List[dict]) -> dict:
        """æ„å»ºé—å¿˜æ›²çº¿"""
        forgetting_data = {
            "retention_rates": [],
            "time_intervals": [],
            "predictions": {}
        }

        # åˆ†æä¸åŒæ—¶é—´é—´éš”çš„ä¿æŒç‡
        for node_data in mastery_history:
            events = node_data['mastery_events']
            if len(events) < 2:
                continue

            # åˆ†ææ¯æ¬¡æŒæ¡åçš„é—å¿˜æƒ…å†µ
            for i in range(len(events) - 1):
                current_event = events[i]
                next_event = events[i + 1]

                if current_event['mastery_level'] > 0.8:  # ä»æŒæ¡çŠ¶æ€å¼€å§‹
                    time_interval = next_event['timestamp'] - current_event['timestamp']
                    retention_rate = next_event['mastery_level'] / current_event['mastery_level']

                    forgetting_data['retention_rates'].append(retention_rate)
                    forgetting_data['time_intervals'].append(time_interval / 86400)  # è½¬æ¢ä¸ºå¤©

        # ç”Ÿæˆé¢„æµ‹æ¨¡å‹
        if forgetting_data['retention_rates']:
            forgetting_data['predictions'] = self._build_forgetting_model(
                forgetting_data['time_intervals'],
                forgetting_data['retention_rates']
            )

        return forgetting_data

    def _build_forgetting_model(self, time_intervals: List[float],
                              retention_rates: List[float]) -> dict:
        """æ„å»ºé—å¿˜æ¨¡å‹"""
        if not time_intervals or not retention_rates:
            return {}

        # ç®€å•çš„æŒ‡æ•°è¡°å‡æ¨¡å‹æ‹Ÿåˆ
        import numpy as np

        # å°†æ•°æ®è½¬æ¢ä¸ºnumpyæ•°ç»„
        t = np.array(time_intervals)
        r = np.array(retention_rates)

        # æ‹Ÿåˆ R(t) = a * exp(-b * t)
        # å–å¯¹æ•°ï¼šln(R) = ln(a) - b * t
        log_r = np.log(r + 0.01)  # é¿å…log(0)

        # çº¿æ€§å›å½’æ‹Ÿåˆ
        coeffs = np.polyfit(t, log_r, 1)
        b = -coeffs[0]
        a = np.exp(coeffs[1])

        return {
            "model_type": "exponential_decay",
            "parameters": {"a": a, "b": b},
            "formula": f"R(t) = {a:.2f} * exp(-{b:.2f} * t)",
            "half_life": np.log(2) / b if b > 0 else float('inf'),
            "confidence": self._calculate_model_confidence(t, r, a, b)
        }

    def _calculate_model_confidence(self, t: np.ndarray, r: np.ndarray,
                                  a: float, b: float) -> float:
        """è®¡ç®—æ¨¡å‹ç½®ä¿¡åº¦"""
        # è®¡ç®—é¢„æµ‹å€¼
        predicted_r = a * np.exp(-b * t)

        # è®¡ç®—RÂ²
        ss_res = np.sum((r - predicted_r) ** 2)
        ss_tot = np.sum((r - np.mean(r)) ** 2)
        r_squared = 1 - (ss_res / ss_tot)

        return max(0, r_squared)

    async def _predict_retention(self, mastery_history: List[dict]) -> dict:
        """é¢„æµ‹çŸ¥è¯†ä¿æŒæƒ…å†µ"""
        predictions = {
            "short_term": {},  # 1å‘¨å†…
            "medium_term": {},  # 1ä¸ªæœˆå†…
            "long_term": {}    # 3ä¸ªæœˆå†…
        }

        current_time = time.time()
        time_intervals = {
            "short_term": 7 * 86400,    # 7å¤©
            "medium_term": 30 * 86400,  # 30å¤©
            "long_term": 90 * 86400     # 90å¤©
        }

        for node_data in mastery_history:
            node_id = node_data['node_id']
            events = node_data['mastery_events']

            if not events:
                continue

            # è·å–æœ€è¿‘çš„æŒæ¡çŠ¶æ€
            latest_event = max(events, key=lambda x: x['timestamp'])
            current_mastery = latest_event['mastery_level']
            time_since_mastery = current_time - latest_event['timestamp']

            for period, interval in time_intervals.items():
                future_time = time_since_mastery + interval
                predicted_mastery = self._predict_future_mastery(
                    current_mastery, future_time
                )

                predictions[period][node_id] = {
                    "current_mastery": current_mastery,
                    "predicted_mastery": predicted_mastery,
                    "retention_rate": predicted_mastery / current_mastery if current_mastery > 0 else 0,
                    "needs_review": predicted_mastery < 0.7
                }

        return predictions

    def _predict_future_mastery(self, current_mastery: float, time_ahead: float) -> float:
        """é¢„æµ‹æœªæ¥æŒæ¡æ°´å¹³"""
        # ç®€å•çš„é—å¿˜æ›²çº¿æ¨¡å‹
        # R(t) = R(0) * exp(-Î» * t)
        # ä½¿ç”¨æ ‡å‡†é—å¿˜ç‡ Î» = 0.1/å¤©
        daily_decay_rate = 0.1
        days_ahead = time_ahead / 86400

        predicted = current_mastery * np.exp(-daily_decay_rate * days_ahead)
        return max(0, predicted)

    async def _generate_review_schedule(self, mastery_history: List[dict]) -> List[dict]:
        """ç”Ÿæˆå¤ä¹ è®¡åˆ’"""
        schedule = []
        current_time = time.time()

        for node_data in mastery_history:
            node_id = node_data['node_id']
            events = node_data['mastery_events']

            if not events:
                continue

            latest_event = max(events, key=lambda x: x['timestamp'])
            current_mastery = latest_event['mastery_level']
            last_review = latest_event['timestamp']

            # è®¡ç®—ä¸‹æ¬¡å¤ä¹ æ—¶é—´
            if current_mastery < 0.5:
                # æŒæ¡ä¸è¶³ï¼Œå°½å¿«å¤ä¹ 
                next_review = current_time + 86400  # 1å¤©å
            elif current_mastery < 0.8:
                # éƒ¨åˆ†æŒæ¡ï¼Œ3å¤©åå¤ä¹ 
                next_review = current_time + 3 * 86400
            else:
                # åŸºæœ¬æŒæ¡ï¼Œä½¿ç”¨é—´éš”é‡å¤
                days_since_review = (current_time - last_review) / 86400
                interval = self._calculate_spaced_repetition_interval(days_since_review)
                next_review = current_time + interval * 86400

            schedule.append({
                "node_id": node_id,
                "node_text": node_data['text'][:100] + "..." if len(node_data['text']) > 100 else node_data['text'],
                "current_mastery": current_mastery,
                "last_review": last_review,
                "next_review": next_review,
                "priority": self._calculate_review_priority(current_mastery, next_review - current_time),
                "review_type": self._suggest_review_type(current_mastery)
            })

        # æŒ‰å¤ä¹ æ—¶é—´æ’åº
        schedule.sort(key=lambda x: x['next_review'])

        return schedule

    def _calculate_spaced_repetition_interval(self, days_since_review: float) -> int:
        """è®¡ç®—é—´éš”é‡å¤é—´éš”ï¼ˆå¤©ï¼‰"""
        # ç®€åŒ–çš„é—´éš”é‡å¤ç®—æ³•
        if days_since_review < 1:
            return 1
        elif days_since_review < 3:
            return 3
        elif days_since_review < 7:
            return 7
        elif days_since_review < 14:
            return 14
        elif days_since_review < 30:
            return 30
        else:
            return 60

    def _calculate_review_priority(self, mastery: float, days_until_review: float) -> str:
        """è®¡ç®—å¤ä¹ ä¼˜å…ˆçº§"""
        days_until = days_until_review / 86400

        if mastery < 0.5 or days_until < 1:
            return "high"
        elif mastery < 0.8 or days_until < 3:
            return "medium"
        else:
            return "low"

    def _suggest_review_type(self, mastery: float) -> str:
        """å»ºè®®å¤ä¹ ç±»å‹"""
        if mastery < 0.5:
            return "relearn"
        elif mastery < 0.8:
            return "practice"
        else:
            return "review"
```

---

## âš¡ 6. æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

### 6.1 ç¼“å­˜ç³»ç»Ÿè®¾è®¡

```python
class KnowledgeGraphCache:
    """çŸ¥è¯†å›¾è°±ç¼“å­˜ç³»ç»Ÿ"""

    def __init__(self, cache_size: int = 1000):
        self.cache_size = cache_size
        self.cache = {}
        self.cache_timestamps = {}
        self.cache_access_count = {}
        self.cache_ttl = {
            "node_info": 3600,      # 1å°æ—¶
            "learning_progress": 300,  # 5åˆ†é’Ÿ
            "search_results": 600,   # 10åˆ†é’Ÿ
            "timeline_data": 1800,   # 30åˆ†é’Ÿ
            "mastery_history": 900   # 15åˆ†é’Ÿ
        }

    async def get_cached_result(self, cache_key: str, data_type: str) -> Optional[dict]:
        """è·å–ç¼“å­˜ç»“æœ"""
        if cache_key not in self.cache:
            return None

        # æ£€æŸ¥TTL
        timestamp = self.cache_timestamps.get(cache_key, 0)
        ttl = self.cache_ttl.get(data_type, 300)
        if time.time() - timestamp > ttl:
            self._remove_from_cache(cache_key)
            return None

        # æ›´æ–°è®¿é—®è®¡æ•°
        self.cache_access_count[cache_key] = self.cache_access_count.get(cache_key, 0) + 1
        return self.cache[cache_key]

    async def cache_result(self, cache_key: str, data: dict, data_type: str):
        """ç¼“å­˜ç»“æœ"""
        # å¦‚æœç¼“å­˜å·²æ»¡ï¼Œç§»é™¤æœ€å°‘ä½¿ç”¨çš„é¡¹
        if len(self.cache) >= self.cache_size:
            self._evict_least_used()

        self.cache[cache_key] = data
        self.cache_timestamps[cache_key] = time.time()
        self.cache_access_count[cache_key] = 1

    def _remove_from_cache(self, cache_key: str):
        """ä»ç¼“å­˜ä¸­ç§»é™¤"""
        self.cache.pop(cache_key, None)
        self.cache_timestamps.pop(cache_key, None)
        self.cache_access_count.pop(cache_key, None)

    def _evict_least_used(self):
        """ç§»é™¤æœ€å°‘ä½¿ç”¨çš„ç¼“å­˜é¡¹"""
        if not self.cache_access_count:
            return

        least_used_key = min(self.cache_access_count, key=self.cache_access_count.get)
        self._remove_from_cache(least_used_key)

    def get_cache_stats(self) -> dict:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "cache_size": len(self.cache),
            "max_cache_size": self.cache_size,
            "hit_rate": self._calculate_hit_rate(),
            "memory_usage": self._estimate_memory_usage()
        }

    def _calculate_hit_rate(self) -> float:
        """è®¡ç®—ç¼“å­˜å‘½ä¸­ç‡"""
        total_accesses = sum(self.cache_access_count.values())
        if total_accesses == 0:
            return 0.0
        return len(self.cache) / total_accesses

    def _estimate_memory_usage(self) -> int:
        """ä¼°ç®—å†…å­˜ä½¿ç”¨é‡"""
        import sys
        total_size = 0
        for key, value in self.cache.items():
            total_size += sys.getsizeof(key) + sys.getsizeof(value)
        return total_size
```

### 6.2 å¼‚æ­¥æ“ä½œä¼˜åŒ–

```python
class AsyncOptimizedKnowledgeGraph:
    """å¼‚æ­¥ä¼˜åŒ–çš„çŸ¥è¯†å›¾è°±æ“ä½œ"""

    def __init__(self, kg_layer: KnowledgeGraphLayer, cache: KnowledgeGraphCache):
        self.kg_layer = kg_layer
        self.cache = cache
        self.operation_queue = asyncio.Queue()
        self.batch_operations = {}
        self.batch_size = 50
        self.batch_timeout = 5.0  # 5ç§’
        self.background_task = None

    async def start_background_processor(self):
        """å¯åŠ¨åå°æ‰¹å¤„ç†"""
        if self.background_task is None:
            self.background_task = asyncio.create_task(self._process_batch_operations())

    async def stop_background_processor(self):
        """åœæ­¢åå°æ‰¹å¤„ç†"""
        if self.background_task:
            self.background_task.cancel()
            try:
                await self.background_task
            except asyncio.CancelledError:
                pass
            self.background_task = None

    async def add_triplet_async(self, triplet: KnowledgeGraphTriplet):
        """å¼‚æ­¥æ·»åŠ ä¸‰å…ƒç»„"""
        operation = {
            "type": "add_triplet",
            "data": triplet,
            "timestamp": time.time()
        }
        await self.operation_queue.put(operation)

    async def search_with_cache(self, query: str, search_type: str = "hybrid",
                               limit: int = 10) -> List[dict]:
        """å¸¦ç¼“å­˜çš„æœç´¢"""
        cache_key = f"search_{hash(query)}_{search_type}_{limit}"

        # å°è¯•ä»ç¼“å­˜è·å–
        cached_result = await self.cache.get_cached_result(cache_key, "search_results")
        if cached_result:
            return cached_result

        # æ‰§è¡Œæœç´¢
        try:
            results = await asyncio.wait_for(
                self.kg_layer.search_knowledge(query, limit),
                timeout=10.0  # 10ç§’è¶…æ—¶
            )

            # ç¼“å­˜ç»“æœ
            await self.cache.cache_result(cache_key, results, "search_results")
            return results

        except asyncio.TimeoutError:
            logger.warning(f"æœç´¢è¶…æ—¶: {query}")
            return []

    async def _process_batch_operations(self):
        """åå°æ‰¹å¤„ç†æ“ä½œ"""
        while True:
            try:
                # æ”¶é›†æ‰¹é‡æ“ä½œ
                batch = []
                timeout_task = asyncio.create_task(asyncio.sleep(self.batch_timeout))

                while len(batch) < self.batch_size:
                    try:
                        operation = await asyncio.wait_for(
                            self.operation_queue.get(),
                            timeout=1.0
                        )
                        batch.append(operation)
                    except asyncio.TimeoutError:
                        break

                # å¦‚æœæœ‰æ“ä½œï¼Œå¤„ç†å®ƒä»¬
                if batch:
                    await self._execute_batch(batch)

                # ç­‰å¾…è¶…æ—¶æˆ–æ–°æ“ä½œ
                try:
                    await asyncio.wait_for(timeout_task, timeout=1.0)
                except asyncio.TimeoutError:
                    continue

            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"æ‰¹å¤„ç†æ“ä½œé”™è¯¯: {e}")
                await asyncio.sleep(1.0)

    async def _execute_batch(self, batch: List[dict]):
        """æ‰§è¡Œæ‰¹é‡æ“ä½œ"""
        try:
            # æŒ‰æ“ä½œç±»å‹åˆ†ç»„
            add_triplets = []
            other_operations = []

            for operation in batch:
                if operation["type"] == "add_triplet":
                    add_triplets.append(operation["data"])
                else:
                    other_operations.append(operation)

            # æ‰¹é‡æ·»åŠ ä¸‰å…ƒç»„
            if add_triplets:
                await self.kg_layer.add_triplets_batch(add_triplets)

        except Exception as e:
            logger.error(f"æ‰§è¡Œæ‰¹é‡æ“ä½œå¤±è´¥: {e}")
```

---

## ğŸ“¦ 7. æ•°æ®è¿ç§»æ–¹æ¡ˆ

### 7.1 è¿ç§»è§„åˆ’å™¨

```python
class CanvasToKGMigrationPlanner:
    """Canvasåˆ°çŸ¥è¯†å›¾è°±è¿ç§»è§„åˆ’å™¨"""

    def __init__(self, source_dir: str, kg_layer: KnowledgeGraphLayer):
        self.source_dir = source_dir
        self.kg_layer = kg_layer

    async def plan_migration(self) -> dict:
        """è§„åˆ’è¿ç§»ç­–ç•¥"""
        # æ‰«ææ‰€æœ‰Canvasæ–‡ä»¶
        canvas_files = await self._scan_canvas_files()

        # åˆ†æCanvasæ–‡ä»¶
        canvas_analysis = await self._analyze_canvas_files(canvas_files)

        # åˆ›å»ºè¿ç§»è®¡åˆ’
        migration_plan = await self._create_migration_plan(canvas_analysis)

        return {
            "canvas_files": canvas_files,
            "analysis": canvas_analysis,
            "plan": migration_plan,
            "estimated_time": self._estimate_migration_time(canvas_analysis)
        }

    async def _scan_canvas_files(self) -> List[str]:
        """æ‰«æCanvasæ–‡ä»¶"""
        canvas_files = []
        for root, dirs, files in os.walk(self.source_dir):
            for file in files:
                if file.endswith('.canvas'):
                    canvas_files.append(os.path.join(root, file))
        return canvas_files

    async def _analyze_canvas_files(self, canvas_files: List[str]) -> dict:
        """åˆ†æCanvasæ–‡ä»¶"""
        analysis = {
            "total_files": len(canvas_files),
            "total_size": 0,
            "total_nodes": 0,
            "total_edges": 0,
            "file_details": []
        }

        for canvas_file in canvas_files:
            try:
                with open(canvas_file, 'r', encoding='utf-8') as f:
                    canvas_data = json.load(f)

                nodes = canvas_data.get('nodes', [])
                edges = canvas_data.get('edges', [])
                file_size = os.path.getsize(canvas_file)

                file_detail = {
                    "path": canvas_file,
                    "size": file_size,
                    "node_count": len(nodes),
                    "edge_count": len(edges),
                    "complexity_score": self._calculate_complexity_score(nodes, edges)
                }

                analysis["file_details"].append(file_detail)
                analysis["total_size"] += file_size
                analysis["total_nodes"] += len(nodes)
                analysis["total_edges"] += len(edges)

            except Exception as e:
                logger.error(f"åˆ†æCanvasæ–‡ä»¶å¤±è´¥ {canvas_file}: {e}")

        return analysis

    def _calculate_complexity_score(self, nodes: List[dict], edges: List[dict]) -> float:
        """è®¡ç®—Canvaså¤æ‚åº¦è¯„åˆ†"""
        node_score = len(nodes) * 1.0
        edge_score = len(edges) * 1.5
        text_complexity = sum(len(node.get('text', '')) for node in nodes) / 1000.0
        return node_score + edge_score + text_complexity

    async def _create_migration_plan(self, analysis: dict) -> dict:
        """åˆ›å»ºè¿ç§»è®¡åˆ’"""
        # æŒ‰å¤æ‚åº¦æ’åºæ–‡ä»¶
        sorted_files = sorted(analysis["file_details"],
                            key=lambda x: x["complexity_score"],
                            reverse=True)

        # åˆ†æ‰¹è§„åˆ’
        batches = []
        current_batch = []
        current_complexity = 0
        max_batch_complexity = 1000

        for file_detail in sorted_files:
            if current_complexity + file_detail["complexity_score"] > max_batch_complexity:
                if current_batch:
                    batches.append(current_batch)
                    current_batch = []
                    current_complexity = 0

            current_batch.append(file_detail)
            current_complexity += file_detail["complexity_score"]

        if current_batch:
            batches.append(current_batch)

        return {
            "batches": batches,
            "total_batches": len(batches),
            "strategy": "complexity_based_batching"
        }

    def _estimate_migration_time(self, analysis: dict) -> dict:
        """ä¼°ç®—è¿ç§»æ—¶é—´"""
        total_complexity = sum(f["complexity_score"] for f in analysis["file_details"])
        base_time_minutes = total_complexity / 100
        estimated_minutes = base_time_minutes * 1.5  # å®‰å…¨ç³»æ•°

        return {
            "estimated_minutes": int(estimated_minutes),
            "estimated_hours": estimated_minutes / 60
        }
```

### 7.2 è¿ç§»æ‰§è¡Œå™¨

```python
class CanvasToKGMigrator:
    """Canvasåˆ°çŸ¥è¯†å›¾è°±è¿ç§»æ‰§è¡Œå™¨"""

    def __init__(self, kg_layer: KnowledgeGraphLayer, cache: KnowledgeGraphCache):
        self.kg_layer = kg_layer
        self.cache = cache
        self.migration_progress = MigrationProgress()

    async def execute_migration(self, migration_plan: dict) -> dict:
        """æ‰§è¡Œè¿ç§»"""
        self.migration_progress.start_migration()

        try:
            results = []
            for batch_index, batch in enumerate(migration_plan["batches"]):
                batch_result = await self._migrate_batch(batch, batch_index)
                results.append(batch_result)

            # å®Œæˆè¿ç§»
            migration_summary = await self._complete_migration(results)
            return migration_summary

        except Exception as e:
            logger.error(f"è¿ç§»æ‰§è¡Œå¤±è´¥: {e}")
            raise

    async def _migrate_batch(self, batch: List[dict], batch_index: int) -> dict:
        """è¿ç§»å•ä¸ªæ‰¹æ¬¡"""
        batch_start_time = time.time()
        batch_results = {
            "batch_index": batch_index,
            "files": [],
            "success_count": 0,
            "error_count": 0,
            "start_time": batch_start_time
        }

        for file_detail in batch:
            try:
                file_result = await self._migrate_canvas_file(file_detail)
                batch_results["files"].append(file_result)

                if file_result["success"]:
                    batch_results["success_count"] += 1
                else:
                    batch_results["error_count"] += 1

            except Exception as e:
                logger.error(f"è¿ç§»æ–‡ä»¶å¤±è´¥ {file_detail['path']}: {e}")
                batch_results["error_count"] += 1

            await asyncio.sleep(0.01)  # è®©å‡ºæ§åˆ¶æƒ

        batch_results["end_time"] = time.time()
        batch_results["duration"] = batch_results["end_time"] - batch_results["start_time"]
        return batch_results

    async def _migrate_canvas_file(self, file_detail: dict) -> dict:
        """è¿ç§»å•ä¸ªCanvasæ–‡ä»¶"""
        canvas_path = file_detail["path"]

        try:
            # è¯»å–Canvasæ–‡ä»¶
            with open(canvas_path, 'r', encoding='utf-8') as f:
                canvas_data = json.load(f)

            # ç”ŸæˆCanvas ID
            canvas_id = f"canvas_{hash(canvas_path)}_{int(time.time())}"

            # åˆ›å»ºCanvaså®ä½“
            await self._create_canvas_entity(canvas_id, canvas_path, canvas_data)

            # è¿ç§»èŠ‚ç‚¹
            node_results = await self._migrate_nodes(canvas_id, canvas_data.get('nodes', []))

            # è¿ç§»è¾¹
            edge_results = await self._migrate_edges(canvas_id, canvas_data.get('edges', []))

            return {
                "path": canvas_path,
                "success": True,
                "canvas_id": canvas_id,
                "nodes_migrated": len(node_results["successful"]),
                "edges_migrated": len(edge_results["successful"])
            }

        except Exception as e:
            return {
                "path": canvas_path,
                "success": False,
                "error": str(e)
            }

    async def _create_canvas_entity(self, canvas_id: str, canvas_path: str, canvas_data: dict):
        """åˆ›å»ºCanvaså®ä½“"""
        canvas_triplet = KnowledgeGraphTriplet(
            subject=canvas_id,
            relation=RelationType.CREATED_AT_TIME.value,
            object=str(time.time()),
            subject_type=EntityType.CANVAS.value,
            object_type="timestamp",
            metadata={
                "path": canvas_path,
                "name": os.path.basename(canvas_path),
                "migration_timestamp": time.time()
            }
        )
        await self.kg_layer.add_triplet(canvas_triplet)

    async def _migrate_nodes(self, canvas_id: str, nodes: List[dict]) -> dict:
        """è¿ç§»èŠ‚ç‚¹"""
        node_results = {"successful": [], "failed": []}

        for node in nodes:
            try:
                node_id = f"node_{canvas_id}_{node['id']}"
                node_attrs = NodeAttributes(node)

                # åˆ›å»ºèŠ‚ç‚¹å®ä½“
                node_triplet = KnowledgeGraphTriplet(
                    subject=node_id,
                    relation=RelationType.CREATED_AT_TIME.value,
                    object=str(time.time()),
                    subject_type=EntityType.NODE.value,
                    object_type="timestamp",
                    metadata={
                        **node_attrs.__dict__,
                        "canvas_id": canvas_id,
                        "migration_timestamp": time.time()
                    }
                )
                await self.kg_layer.add_triplet(node_triplet)

                # CanvasåŒ…å«èŠ‚ç‚¹å…³ç³»
                contains_triplet = KnowledgeGraphTriplet(
                    subject=canvas_id,
                    relation=RelationType.CONTAINS.value,
                    object=node_id,
                    subject_type=EntityType.CANVAS.value,
                    object_type=EntityType.NODE.value
                )
                await self.kg_layer.add_triplet(contains_triplet)

                node_results["successful"].append({
                    "original_id": node['id'],
                    "kg_id": node_id
                })

            except Exception as e:
                node_results["failed"].append({
                    "original_id": node['id'],
                    "error": str(e)
                })

        return node_results

    async def _complete_migration(self, migration_results: List[dict]) -> dict:
        """å®Œæˆè¿ç§»"""
        self.migration_progress.complete_migration()

        total_files = sum(len(batch["files"]) for batch in migration_results)
        successful_files = sum(batch["success_count"] for batch in migration_results)

        return {
            "status": "completed",
            "migration_results": migration_results,
            "summary": {
                "total_files": total_files,
                "successful_files": successful_files,
                "success_rate": successful_files / total_files if total_files > 0 else 0,
                "total_processing_time": self.migration_progress.get_total_time()
            }
        }


class MigrationProgress:
    """è¿ç§»è¿›åº¦ç›‘æ§"""

    def __init__(self):
        self.start_time = None
        self.end_time = None

    def start_migration(self):
        """å¼€å§‹è¿ç§»"""
        self.start_time = time.time()

    def complete_migration(self):
        """å®Œæˆè¿ç§»"""
        self.end_time = time.time()

    def get_total_time(self) -> float:
        """è·å–æ€»è€—æ—¶"""
        if self.start_time and self.end_time:
            return self.end_time - self.start_time
        elif self.start_time:
            return time.time() - self.start_time
        else:
            return 0.0
```

---

## ğŸ 8. å®Œæ•´æŠ€æœ¯æ–¹æ¡ˆæ€»ç»“

### 8.1 ç³»ç»Ÿæ¶æ„æ¦‚è§ˆ

åŸºäºGraphitiçš„Canvaså­¦ä¹ ç³»ç»ŸçŸ¥è¯†å›¾è°±é›†æˆåŒ…å«ä»¥ä¸‹æ ¸å¿ƒç»„ä»¶ï¼š

**å››å±‚æ¶æ„**:
1. **Layer 1**: CanvasJSONOperator - åŸå­åŒ–Canvasæ–‡ä»¶è¯»å†™
2. **Layer 2**: CanvasBusinessLogic - ä¸šåŠ¡é€»è¾‘å’Œå¸ƒå±€ç®—æ³•
3. **Layer 3**: CanvasOrchestrator - é«˜çº§APIå’ŒSub-agentè°ƒç”¨
4. **Layer 4**: KnowledgeGraphLayer - GraphitiçŸ¥è¯†å›¾è°±é›†æˆ

**æ ¸å¿ƒåŠŸèƒ½æ¨¡å—**:
- çŸ¥è¯†å›¾è°±æ•°æ®æ¨¡å‹å’Œæ˜ å°„
- Canvasè®°å¿†ç³»ç»Ÿ
- æ™ºèƒ½æ£€ç´¢å’Œè¿½è¸ª
- æ—¶é—´æ„ŸçŸ¥å­¦ä¹ åˆ†æ
- æ€§èƒ½ä¼˜åŒ–ç­–ç•¥
- æ•°æ®è¿ç§»å·¥å…·

### 8.2 å…³é”®æŠ€æœ¯ç‰¹æ€§

**æŒä¹…åŒ–è®°å¿†**:
- CanvasèŠ‚ç‚¹å’Œè¾¹çš„é€»è¾‘å…³ç³»æ°¸ä¹…å­˜å‚¨
- å­¦ä¹ è¿›åº¦å’ŒçŠ¶æ€å˜åŒ–çš„å®Œæ•´è®°å½•
- è·¨æ—¶é—´çš„å­¦ä¹ æ¨¡å¼åˆ†æ

**æ™ºèƒ½æ£€ç´¢**:
- åŸºäºçŸ¥è¯†å›¾è°±çš„è¯­ä¹‰æœç´¢
- å­¦ä¹ ç“¶é¢ˆè‡ªåŠ¨è¯†åˆ«
- ä¸ªæ€§åŒ–æ£€éªŒç™½æ¿ç”Ÿæˆ

**æ—¶é—´æ„ŸçŸ¥**:
- å­¦ä¹ æ—¶é—´çº¿è¿½è¸ª
- çŸ¥è¯†æŒæ¡æ›²çº¿åˆ†æ
- é—å¿˜æ›²çº¿é¢„æµ‹å’Œå¤ä¹ è®¡åˆ’

**æ€§èƒ½ä¼˜åŒ–**:
- å¤šçº§ç¼“å­˜ç³»ç»Ÿ
- å¼‚æ­¥æ‰¹å¤„ç†æ“ä½œ
- è¿æ¥æ± ç®¡ç†
- å†…å­˜ä¼˜åŒ–ç­–ç•¥

### 8.3 å®æ–½ä»·å€¼

**å­¦ä¹ æ•ˆæœæå‡**:
- çŸ¥è¯†å…³è”è®°å¿†å¢å¼ºç†è§£æ·±åº¦
- ä¸ªæ€§åŒ–å¤ä¹ è®¡åˆ’æé«˜å­¦ä¹ æ•ˆç‡
- æ™ºèƒ½æ£€éªŒç™½æ¿ä¼˜åŒ–å¤ä¹ é’ˆå¯¹æ€§

**ç³»ç»Ÿæ™ºèƒ½åŒ–**:
- è‡ªåŠ¨å­¦ä¹ æ¨¡å¼è¯†åˆ«å’Œå»ºè®®
- çŸ¥è¯†æŒæ¡åº¦é¢„æµ‹å’Œé¢„è­¦
- è·¨CanvasçŸ¥è¯†å…³è”å‘ç°

**ç”¨æˆ·ä½“éªŒä¼˜åŒ–**:
- å®æ—¶å­¦ä¹ è¿›åº¦å¯è§†åŒ–
- æ™ºèƒ½å­¦ä¹ è·¯å¾„æ¨è
- æ— ç¼çš„çŸ¥è¯†æ£€ç´¢å’Œå›é¡¾

### 8.4 ä½¿ç”¨ç¤ºä¾‹

```python
# ç³»ç»Ÿåˆå§‹åŒ–
async def main():
    system = CanvasLearningSystemWithKG(
        neo4j_uri="bolt://localhost:7687",
        neo4j_user="neo4j",
        neo4j_password="password"
    )

    await system.initialize()

    try:
        # 1. åŒæ­¥Canvasåˆ°çŸ¥è¯†å›¾è°±
        sync_result = await system.process_canvas_operation(
            "./ç¬”è®°åº“/ç¦»æ•£æ•°å­¦/ç¦»æ•£æ•°å­¦.canvas", "sync_to_kg"
        )

        # 2. è¿½è¸ªå­¦ä¹ è¿›åº¦
        progress_result = await system.process_canvas_operation(
            "./ç¬”è®°åº“/ç¦»æ•£æ•°å­¦/ç¦»æ•£æ•°å­¦.canvas", "track_progress"
        )

        # 3. ç”Ÿæˆæ™ºèƒ½æ£€éªŒç™½æ¿
        review_result = await system.process_canvas_operation(
            "./ç¬”è®°åº“/ç¦»æ•£æ•°å­¦/ç¦»æ•£æ•°å­¦.canvas",
            "generate_review_board",
            strategy="adaptive"
        )

        # 4. è·å–ç³»ç»ŸçŠ¶æ€
        status = await system.get_system_status()
        print("ç³»ç»Ÿè¿è¡ŒçŠ¶æ€:", status)

    finally:
        await system.shutdown()

# è¿è¡Œç¤ºä¾‹
if __name__ == "__main__":
    asyncio.run(main())
```

### 8.5 æ€§èƒ½æŒ‡æ ‡

**é¢„æœŸæ€§èƒ½æŒ‡æ ‡**:
- çŸ¥è¯†å›¾è°±æ“ä½œå“åº”æ—¶é—´: <200ms (ç®€å•æŸ¥è¯¢), <2s (å¤æ‚åˆ†æ)
- ç¼“å­˜å‘½ä¸­ç‡: >80%
- å†…å­˜ä½¿ç”¨: <2GB (æ­£å¸¸è´Ÿè½½)
- å¹¶å‘å¤„ç†èƒ½åŠ›: æ”¯æŒ10ä¸ªå¹¶å‘Canvasæ“ä½œ
- æ•°æ®è¿ç§»é€Ÿåº¦: ~50èŠ‚ç‚¹/ç§’

**æ‰©å±•æ€§æŒ‡æ ‡**:
- æ”¯æŒCanvasèŠ‚ç‚¹æ•°é‡: 10,000+
- æ”¯æŒçŸ¥è¯†å›¾è°±ä¸‰å…ƒç»„æ•°é‡: 100,000+
- æ”¯æŒå¹¶å‘ç”¨æˆ·æ•°: 50+
- æ•°æ®å­˜å‚¨å®¹é‡: å¯æ‰©å±•è‡³TBçº§åˆ«

### 8.6 ä¸LangGraph Checkpointerçš„èŒè´£è¾¹ç•Œ

> **æ›´æ–°æ—¥æœŸ**: 2025-11-11
> **å…³è”PRD**: v1.1.3 Section 3.6

#### èƒŒæ™¯è¯´æ˜

éšç€Epic 12å¼•å…¥LangGraphæ¡†æ¶å±‚è®°å¿†ç³»ç»Ÿï¼ˆCheckpointerï¼‰ï¼ŒCanvaså­¦ä¹ ç³»ç»Ÿç°åœ¨æ‹¥æœ‰**åŒè®°å¿†æ¶æ„**ï¼š
1. **æ¡†æ¶å±‚**: LangGraph Checkpointerï¼ˆAgent StateæŒä¹…åŒ–ï¼‰
2. **ä¸šåŠ¡å±‚**: Graphiti + Temporal + Semantic Memoryï¼ˆä¸šåŠ¡çŸ¥è¯†å›¾è°±ï¼‰

æœ¬å°èŠ‚æ˜ç¡®ä¸¤ä¸ªç³»ç»Ÿçš„èŒè´£è¾¹ç•Œï¼Œé¿å…åŠŸèƒ½é‡å å’Œæ•°æ®å†²çªã€‚

---

#### èŒè´£åˆ†å·¥çŸ©é˜µ

| ç»´åº¦ | LangGraph Checkpointer | GraphitiçŸ¥è¯†å›¾è°± | å¤‡æ³¨ |
|------|----------------------|----------------|------|
| **æ•°æ®ç±»å‹** | Agent Stateï¼ˆä¼šè¯çŠ¶æ€ï¼‰ | ä¸šåŠ¡çŸ¥è¯†å…³ç³»ï¼ˆæ¦‚å¿µã€èŠ‚ç‚¹ã€æ—¶é—´çº¿ï¼‰ | ä¸åŒå±‚æ¬¡çš„æŠ½è±¡ |
| **æ—¶é—´èŒƒå›´** | å½“å‰å­¦ä¹ ä¼šè¯ï¼ˆçŸ­æœŸï¼‰ | è·¨ä¼šè¯å†å²ï¼ˆé•¿æœŸï¼‰ | Checkpointer=çŸ­æœŸï¼ŒGraphiti=é•¿æœŸ |
| **æŸ¥è¯¢åœºæ™¯** | æ¢å¤Agentæ‰§è¡Œä¸Šä¸‹æ–‡ | è·¨CanvasçŸ¥è¯†å…³è”ã€å­¦ä¹ å†å²åˆ†æ | åŠŸèƒ½äº’è¡¥ |
| **æŒä¹…åŒ–** | PostgreSQL/InMemory | Neo4j | ä¸åŒæ•°æ®åº“ |
| **æ•°æ®é‡çº§** | MBçº§ï¼ˆå•ä¼šè¯Stateï¼‰ | GBçº§ï¼ˆå…¨å±€çŸ¥è¯†å›¾è°±ï¼‰ | è§„æ¨¡å·®å¼‚ |
| **ä¸€è‡´æ€§è¦æ±‚** | å¼ºä¸€è‡´æ€§ï¼ˆä¸Canvasæ–‡ä»¶ï¼‰ | æœ€ç»ˆä¸€è‡´æ€§ | ä¸åŒSLA |
| **æ›´æ–°é¢‘ç‡** | æ¯æ¬¡Agentæ“ä½œï¼ˆé«˜é¢‘ï¼‰ | Canvasæ“ä½œåå¼‚æ­¥ï¼ˆä½é¢‘ï¼‰ | Checkpointerå®æ—¶ï¼ŒGraphitiå¼‚æ­¥ |
| **æŸ¥è¯¢æ€§èƒ½** | <50msï¼ˆStateæ¢å¤ï¼‰ | <200msï¼ˆç®€å•æŸ¥è¯¢ï¼‰ï¼Œ<2sï¼ˆå¤æ‚å›¾æŸ¥è¯¢ï¼‰ | æ€§èƒ½ç›®æ ‡ä¸åŒ |

---

#### æ•°æ®æµåä½œæœºåˆ¶

```mermaid
graph TD
    A[Canvas Operation] --> B[LangGraph Agent Node]
    B --> C{æ“ä½œæˆåŠŸ?}
    C -->|æ˜¯| D[æ›´æ–°Canvasæ–‡ä»¶]
    C -->|å¦| E[å›æ»šState]
    D --> F[LangGraph Checkpointer æŒä¹…åŒ–State]
    D --> G[å¼‚æ­¥å­˜å‚¨åˆ°Graphiti]

    F --> H[çŸ­æœŸä¼šè¯æ¢å¤]
    G --> I[é•¿æœŸçŸ¥è¯†å›¾è°±æŸ¥è¯¢]

    style F fill:#e1f5fe
    style G fill:#fff3e0
    style H fill:#e1f5fe
    style I fill:#fff3e0
```

**å…³é”®è¦ç‚¹**:
1. **åŒæ­¥è·¯å¾„**: Canvasæ“ä½œ â†’ LangGraph Stateæ›´æ–° â†’ CheckpointeræŒä¹…åŒ–ï¼ˆå¼ºä¸€è‡´æ€§ï¼‰
2. **å¼‚æ­¥è·¯å¾„**: Canvasæ“ä½œ â†’ Graphitiå­˜å‚¨ï¼ˆæœ€ç»ˆä¸€è‡´æ€§ï¼Œå…è®¸å»¶è¿Ÿï¼‰
3. **æŸ¥è¯¢åˆ†ç¦»**:
   - éœ€è¦ä¼šè¯ä¸Šä¸‹æ–‡ â†’ æŸ¥è¯¢Checkpointer
   - éœ€è¦è·¨ä¼šè¯/è·¨Canvaså…³è” â†’ æŸ¥è¯¢Graphiti

---

#### å…¸å‹ä½¿ç”¨åœºæ™¯å¯¹æ¯”

| åœºæ™¯ | ä½¿ç”¨ç³»ç»Ÿ | åŸå›  |
|------|---------|------|
| **å¤šè½®å¯¹è¯æ¢å¤** | Checkpointer | éœ€è¦æ¢å¤å½“å‰ä¼šè¯çš„Agent Stateï¼ˆdecomposition_results, scoring_resultç­‰ï¼‰ |
| **è·¨Canvasæ¦‚å¿µæŸ¥è¯¢** | Graphiti | éœ€è¦æŸ¥è¯¢æ‰€æœ‰Canvasä¸­å…³äº"çŸ©é˜µ"çš„èŠ‚ç‚¹å’Œå…³ç³» |
| **å­¦ä¹ æ—¶é—´çº¿è¿½è¸ª** | Graphiti (Temporal Memory) | éœ€è¦æŸ¥è¯¢è·¨ä¼šè¯çš„å­¦ä¹ å†å²å’Œè¿›å±• |
| **æ£€éªŒç™½æ¿ç”Ÿæˆ** | Checkpointer + Graphiti | Checkpointeræä¾›å½“å‰ä¼šè¯ä¸Šä¸‹æ–‡ï¼ŒGraphitiæä¾›å†å²æŒæ¡åº¦æ•°æ® |
| **å›æ»šæ“ä½œ** | Checkpointerï¼ˆä¼˜å…ˆï¼‰ + Canvaså¤‡ä»½ | Stateå›æ»š + æ–‡ä»¶å›æ»šï¼ŒGraphitiæ ‡è®°ä¸ºå·²æ’¤é”€ |
| **è‰¾å®¾æµ©æ–¯å¤ä¹ ** | Graphiti (Temporal Memory) | åŸºäºé•¿æœŸå­¦ä¹ å†å²è®¡ç®—å¤ä¹ è®¡åˆ’ |
| **Agentå†³ç­–ä¾æ®** | Checkpointerï¼ˆå½“å‰Stateï¼‰ + Graphitiï¼ˆå†å²æ•°æ®ï¼‰ | ç»“åˆçŸ­æœŸå’Œé•¿æœŸæ•°æ®åšæ™ºèƒ½å†³ç­– |

---

#### æ•°æ®ç¤ºä¾‹å¯¹æ¯”

**LangGraph Checkpointerå­˜å‚¨çš„æ•°æ®**:

```python
# Checkpoint Stateç¤ºä¾‹
{
    "canvas_path": "C:/Users/ROG/æ‰˜ç¦/ç¬”è®°åº“/ç¦»æ•£æ•°å­¦/ç¦»æ•£æ•°å­¦.canvas",
    "user_id": "user_12345",
    "session_id": "a1b2c3d4-e5f6-7890-abcd-ef1234567890",
    "operation": "decomposition",
    "concept": "é€†å¦å‘½é¢˜",
    "decomposition_results": [
        "ä»€ä¹ˆæ˜¯é€†å¦å‘½é¢˜çš„å®šä¹‰?",
        "é€†å¦å‘½é¢˜ä¸åŸå‘½é¢˜æœ‰ä»€ä¹ˆå…³ç³»?",
        "å¦‚ä½•åˆ¤æ–­é€†å¦å‘½é¢˜çš„çœŸå‡?"
    ],
    "scoring_result": {
        "node_id": "yellow_123",
        "accuracy": 22,
        "imagery": 18,
        "completeness": 20,
        "originality": 15,
        "total": 75,
        "color": "3"  # ç´«è‰²
    },
    "messages": [...],  # å¯¹è¯å†å²
    "last_operation": "scoring",
    "last_timestamp": "2025-11-11T14:30:00"
}
```

**GraphitiçŸ¥è¯†å›¾è°±å­˜å‚¨çš„æ•°æ®**:

```cypher
// èŠ‚ç‚¹ç¤ºä¾‹
CREATE (c:Canvas {name: "ç¦»æ•£æ•°å­¦", path: "..."})
CREATE (concept:Concept {name: "é€†å¦å‘½é¢˜", domain: "ç¦»æ•£æ•°å­¦"})
CREATE (node:Node {
    canvas_id: "red_001",
    text: "ä»€ä¹ˆæ˜¯é€†å¦å‘½é¢˜?",
    color: "1",  // çº¢è‰²
    understanding_state: "not_understood"
})
CREATE (understanding:UnderstandingState {
    node_id: "yellow_123",
    accuracy: 22,
    imagery: 18,
    completeness: 20,
    originality: 15,
    total: 75,
    timestamp: "2025-11-11T14:30:00"
})

// å…³ç³»ç¤ºä¾‹
CREATE (c)-[:CONTAINS]->(node)
CREATE (node)-[:IS_ABOUT]->(concept)
CREATE (node)-[:HAS_UNDERSTANDING_STATE]->(understanding)
CREATE (understanding)-[:EVOLVES_TO]->(next_understanding)
```

**å¯¹æ¯”æ€»ç»“**:
- **Checkpointer**: å­˜å‚¨å®Œæ•´çš„Agentæ‰§è¡Œä¸Šä¸‹æ–‡ï¼ˆç»“æ„åŒ–Stateï¼‰ï¼Œç”¨äºä¼šè¯æ¢å¤
- **Graphiti**: å­˜å‚¨çŸ¥è¯†å›¾è°±ä¸‰å…ƒç»„ï¼ˆå®ä½“+å…³ç³»ï¼‰ï¼Œç”¨äºçŸ¥è¯†å…³è”æŸ¥è¯¢

---

#### ä¸€è‡´æ€§ä¿è¯æœºåˆ¶

**å¼ºä¸€è‡´æ€§è·¯å¾„** (Canvas â†” LangGraph State):

```python
def agent_node_with_strong_consistency(state: CanvasLearningState):
    """ç¡®ä¿Canvasæ“ä½œå’ŒStateæ›´æ–°çš„å¼ºä¸€è‡´æ€§"""
    # Step 1: å¤‡ä»½Canvas
    backup = backup_canvas(state["canvas_path"])

    try:
        # Step 2: æ‰§è¡ŒCanvasæ“ä½œ
        write_to_canvas(state["canvas_path"], new_data)

        # Step 3: è¿”å›æ–°Stateï¼ˆLangGraphè‡ªåŠ¨æŒä¹…åŒ–ï¼‰
        return {
            **state,
            "last_operation": "decomposition",
            "decomposition_results": new_data
        }
    except Exception as e:
        # Step 4: å¤±è´¥æ—¶å›æ»šCanvas
        restore_canvas(state["canvas_path"], backup)
        raise  # ä¸åˆ›å»ºæ–°checkpoint
```

**æœ€ç»ˆä¸€è‡´æ€§è·¯å¾„** (Canvas â†” Graphiti):

```python
def agent_node_with_eventual_consistency(state: CanvasLearningState):
    """Canvasæ“ä½œæˆåŠŸï¼ŒGraphitiå¼‚æ­¥å­˜å‚¨ï¼ˆå…è®¸å¤±è´¥ï¼‰"""
    # Step 1: Canvasæ“ä½œï¼ˆå…³é”®è·¯å¾„ï¼‰
    write_to_canvas(state["canvas_path"], new_data)

    # Step 2: è¿”å›æ–°Stateï¼ˆå…³é”®è·¯å¾„ï¼‰
    new_state = {
        **state,
        "decomposition_results": new_data
    }

    # Step 3: å¼‚æ­¥å­˜å‚¨åˆ°Graphitiï¼ˆéå…³é”®è·¯å¾„ï¼‰
    try:
        asyncio.create_task(store_to_graphiti(state["session_id"], new_data))
    except Exception as e:
        logger.error(f"Graphiti storage failed: {e}")
        # ä¸å½±å“Canvasæ“ä½œæˆåŠŸ

    return new_state
```

---

#### å†²çªå¤„ç†ç­–ç•¥

**åœºæ™¯1: Checkpointerä¸Graphitiæ•°æ®ä¸ä¸€è‡´**

- **æ£€æµ‹**: å®šæœŸå¯¹æ¯”Checkpointerçš„Stateå¿«ç…§ä¸Graphitiçš„èŠ‚ç‚¹çŠ¶æ€
- **è§£å†³**:
  - Canvasæ–‡ä»¶ = çœŸå®æ•°æ®æº
  - Checkpointerä¼˜å…ˆçº§ > Graphitiï¼ˆå› ä¸ºCheckpointeræ˜¯å¼ºä¸€è‡´æ€§ï¼‰
  - ä¿®å¤æ–¹å¼: ä»Canvasæ–‡ä»¶é‡æ–°åŒæ­¥åˆ°Graphiti

**åœºæ™¯2: å›æ»šæ“ä½œå¯¼è‡´çš„æ•°æ®å†²çª**

```python
def handle_rollback_conflict(
    canvas_path: str,
    session_id: str,
    checkpoint_id: str
):
    """å›æ»šæ—¶ç¡®ä¿ä¸‰ä¸ªç³»ç»Ÿä¸€è‡´"""
    # Step 1: å›æ»šCanvasæ–‡ä»¶ï¼ˆä»å¤‡ä»½ï¼‰
    restore_canvas_from_backup(canvas_path, checkpoint_id)

    # Step 2: å›æ»šLangGraph Stateï¼ˆä»checkpointï¼‰
    config = create_langgraph_config(canvas_path, "user_id", session_id)
    config["configurable"]["checkpoint_id"] = checkpoint_id
    state = graph.get_state(config)

    # Step 3: æ ‡è®°Graphitiæ“ä½œä¸ºå·²æ’¤é”€ï¼ˆä¸åˆ é™¤ï¼Œä¿ç•™å†å²ï¼‰
    mark_graphiti_operations_as_reverted(
        session_id,
        after_timestamp=state.values["last_timestamp"]
    )

    # Step 4: éªŒè¯ä¸€è‡´æ€§
    assert verify_consistency(canvas_path, state, graphiti_data)
```

---

#### æ€§èƒ½ä¼˜åŒ–å»ºè®®

**1. å‡å°‘Checkpointerå†™å…¥é¢‘ç‡**

```python
# âŒ ä½æ•ˆï¼šæ¯ä¸ªå­æ“ä½œéƒ½åˆ›å»ºcheckpoint
for question in questions:
    graph.invoke({"operation": "add_question", "question": question}, config)
    # 100ä¸ªé—®é¢˜ = 100æ¬¡checkpointå†™å…¥

# âœ… é«˜æ•ˆï¼šæ‰¹é‡æ“ä½œï¼Œ1æ¬¡checkpoint
graph.invoke({"operation": "add_questions", "questions": questions}, config)
# 100ä¸ªé—®é¢˜ = 1æ¬¡checkpointå†™å…¥
```

**2. å»¶è¿ŸGraphitiå†™å…¥**

```python
# ä½¿ç”¨æ¶ˆæ¯é˜Ÿåˆ—å¼‚æ­¥æ‰¹é‡å†™å…¥
graphiti_write_queue = asyncio.Queue()

async def batch_graphiti_writer():
    """åå°ä»»åŠ¡ï¼šæ‰¹é‡å†™å…¥Graphiti"""
    while True:
        batch = []
        for _ in range(10):  # æ”’10ä¸ªæ“ä½œ
            item = await graphiti_write_queue.get()
            batch.append(item)

        await graphiti_client.batch_write(batch)
        await asyncio.sleep(1)  # æ¯ç§’æ‰§è¡Œä¸€æ¬¡

# AgentèŠ‚ç‚¹ä¸­å¼‚æ­¥å…¥é˜Ÿ
await graphiti_write_queue.put({"type": "decomposition", "data": ...})
```

**3. åˆ†å±‚ç¼“å­˜ç­–ç•¥**

- **L1 ç¼“å­˜** (LangGraph State): å½“å‰ä¼šè¯æ•°æ®ï¼ˆå†…å­˜çº§ï¼Œ<10msï¼‰
- **L2 ç¼“å­˜** (Checkpointer): å†å²ä¼šè¯Stateï¼ˆæ•°æ®åº“çº§ï¼Œ<50msï¼‰
- **L3 ç¼“å­˜** (Graphiti Redis): çƒ­é—¨çŸ¥è¯†å›¾è°±æŸ¥è¯¢ï¼ˆRedisçº§ï¼Œ<20msï¼‰
- **L4 å­˜å‚¨** (Graphiti Neo4j): å®Œæ•´çŸ¥è¯†å›¾è°±ï¼ˆNeo4jçº§ï¼Œ<200msï¼‰

---

#### è¿ç§»å’Œå…¼å®¹æ€§

**ä»çº¯Graphitiç³»ç»Ÿè¿ç§»åˆ°åŒè®°å¿†æ¶æ„**:

```python
async def migrate_to_dual_memory_architecture():
    """è¿ç§»ç°æœ‰Graphitiæ•°æ®åˆ°åŒè®°å¿†æ¶æ„"""
    # Step 1: ä¿æŒGraphitiæ•°æ®ä¸å˜ï¼ˆå‘åå…¼å®¹ï¼‰
    # Graphitiä»ç„¶å­˜å‚¨æ‰€æœ‰å†å²çŸ¥è¯†å›¾è°±æ•°æ®

    # Step 2: æ–°å¢Checkpointeré…ç½®ï¼ˆå‘å‰å…¼å®¹ï¼‰
    checkpointer = PostgresSaver.from_conn_string(DB_URI)
    graph = builder.compile(checkpointer=checkpointer)

    # Step 3: æ–°ä¼šè¯ä½¿ç”¨Checkpointerï¼Œæ—§æ•°æ®ä»åœ¨Graphiti
    # æ— ç¼è¿‡æ¸¡ï¼Œä¸å½±å“ç°æœ‰åŠŸèƒ½
```

**å…¼å®¹æ€§ä¿è¯**:
- âœ… æ—§ä»£ç ä»å¯æ­£å¸¸ä½¿ç”¨Graphitiï¼ˆé›¶ç ´åæ€§ï¼‰
- âœ… æ–°ä»£ç åŒæ—¶åˆ©ç”¨Checkpointer + Graphitiï¼ˆå¢å¼ºåŠŸèƒ½ï¼‰
- âœ… æŸ¥è¯¢æ¥å£ç»Ÿä¸€å°è£…ï¼Œè‡ªåŠ¨è·¯ç”±åˆ°æ­£ç¡®ç³»ç»Ÿ

---

#### éªŒæ”¶æ ‡å‡†

**åŠŸèƒ½éªŒæ”¶**:
- âœ… **AC 1**: Checkpointerå’ŒGraphitiå¯ç‹¬ç«‹å·¥ä½œï¼Œäº’ä¸é˜»å¡
- âœ… **AC 2**: Canvasæ“ä½œå¤±è´¥æ—¶ï¼ŒCheckpointerä¸åˆ›å»ºcheckpointï¼ŒGraphitiä¸å­˜å‚¨
- âœ… **AC 3**: Checkpointerå†™å…¥å¤±è´¥æ—¶ï¼ŒCanvasæ“ä½œå¤±è´¥å¹¶å›æ»š
- âœ… **AC 4**: Graphitiå†™å…¥å¤±è´¥æ—¶ï¼ŒCanvasæ“ä½œæˆåŠŸï¼Œä»…è®°å½•æ—¥å¿—
- âœ… **AC 5**: å›æ»šæ“ä½œåŒæ­¥æ¢å¤Canvas + Stateï¼ŒGraphitiæ ‡è®°æ’¤é”€
- âœ… **AC 6**: å¤šè½®å¯¹è¯å¯æ¢å¤Checkpointerçš„Stateï¼ŒåŒæ—¶æŸ¥è¯¢Graphitiçš„å†å²
- âœ… **AC 7**: è·¨CanvasæŸ¥è¯¢ä»…ä½¿ç”¨Graphitiï¼Œä¸è®¿é—®Checkpointer

**æ€§èƒ½éªŒæ”¶**:
- âœ… **AC 8**: Checkpointerå†™å…¥ < 100msï¼ˆPostgresSaverï¼‰
- âœ… **AC 9**: Graphitiå¼‚æ­¥å†™å…¥ä¸é˜»å¡Agentæ‰§è¡Œ
- âœ… **AC 10**: æ‰¹é‡æ“ä½œå‡å°‘90% checkpointå†™å…¥æ¬¡æ•°

**ä¸€è‡´æ€§éªŒæ”¶**:
- âœ… **AC 11**: Canvasæ–‡ä»¶ â†” Checkpointer State: å¼ºä¸€è‡´æ€§
- âœ… **AC 12**: Canvasæ–‡ä»¶ â†” Graphiti: æœ€ç»ˆä¸€è‡´æ€§ï¼ˆ<5ç§’åŒæ­¥å»¶è¿Ÿï¼‰
- âœ… **AC 13**: ä¸€è‡´æ€§æ ¡éªŒè„šæœ¬å¯æ£€æµ‹å¹¶ä¿®å¤ä¸ä¸€è‡´

---

**æ€»ç»“**: åŒè®°å¿†æ¶æ„é€šè¿‡æ¸…æ™°çš„èŒè´£åˆ†å·¥å’Œåä½œæœºåˆ¶ï¼Œå®ç°äº†çŸ­æœŸä¼šè¯æ¢å¤ï¼ˆCheckpointerï¼‰å’Œé•¿æœŸçŸ¥è¯†ç®¡ç†ï¼ˆGraphitiï¼‰çš„å®Œç¾ç»“åˆï¼Œä¸ºCanvaså­¦ä¹ ç³»ç»Ÿæä¾›äº†å¼ºå¤§çš„è®°å¿†èƒ½åŠ›ã€‚

---

## ğŸ“š 9. å®æ–½è·¯çº¿å›¾

### Phase 1: åŸºç¡€æ¶æ„ (2-3å‘¨)
- [x] çŸ¥è¯†å›¾è°±æ•°æ®æ¨¡å‹è®¾è®¡
- [x] Graphitié›†æˆæ¶æ„
- [x] åŸºç¡€è®°å¿†åŠŸèƒ½
- [ ] Neo4jç¯å¢ƒæ­å»º
- [ ] åŸºç¡€APIå®ç°

### Phase 2: æ ¸å¿ƒåŠŸèƒ½ (3-4å‘¨)
- [ ] æ™ºèƒ½æ£€ç´¢åŠŸèƒ½
- [ ] å­¦ä¹ æ—¶é—´çº¿è¿½è¸ª
- [ ] çŸ¥è¯†æŒæ¡æ—¶é—´çº¿
- [ ] æ™ºèƒ½æ£€éªŒç™½æ¿ç”Ÿæˆ

### Phase 3: ä¼˜åŒ–å’Œæ‰©å±• (2-3å‘¨)
- [ ] æ€§èƒ½ä¼˜åŒ–ç­–ç•¥
- [ ] ç¼“å­˜ç³»ç»Ÿ
- [ ] å¼‚æ­¥æ“ä½œä¼˜åŒ–
- [ ] æ•°æ®è¿ç§»å·¥å…·

### Phase 4: æµ‹è¯•å’Œéƒ¨ç½² (1-2å‘¨)
- [ ] å•å…ƒæµ‹è¯•
- [ ] é›†æˆæµ‹è¯•
- [ ] æ€§èƒ½æµ‹è¯•
- [ ] ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.1
**æœ€åæ›´æ–°**: 2025-11-11
**ä½œè€…**: Claude Code
**çŠ¶æ€**: æŠ€æœ¯æ–¹æ¡ˆè®¾è®¡å®Œæˆ
