# Story 21.5.2: AI Provideré…ç½®éªŒè¯

## Status
Ready for Development

## Story

**As a** Canvas Learning Systemå¼€å‘è€…/è¿ç»´äººå‘˜,
**I want** åœ¨åº”ç”¨å¯åŠ¨æ—¶è‡ªåŠ¨éªŒè¯å’Œæ¸…ç†AIé…ç½®,
**so that** è¿è¡Œæ—¶ä¸ä¼šå› ä¸ºé…ç½®æ ¼å¼é—®é¢˜å¯¼è‡´AgentåŠŸèƒ½å¤±è´¥ï¼Œä¸”èƒ½å¿«é€Ÿè¯Šæ–­AIè¿æ¥çŠ¶æ€ã€‚

## Acceptance Criteria

1. **AC-1**: å¯åŠ¨æ—¶æ£€æµ‹ `AI_MODEL_NAME` æ ¼å¼ï¼Œè‡ªåŠ¨æ¸…ç†å¼‚å¸¸å‰ç¼€ï¼ˆå¦‚`[K1]`ï¼‰
2. **AC-2**: æ— æ•ˆé…ç½®æ—¶ç»™å‡ºæ˜ç¡®é”™è¯¯æç¤ºï¼ŒåŒ…å«é”™è¯¯ç å’Œå»ºè®®æ“ä½œ
3. **AC-3**: æä¾› `/api/v1/health/ai` ç«¯ç‚¹æµ‹è¯•AIè¿æ¥çŠ¶æ€
4. **AC-4**: æ‰€æœ‰ä»£ç åŒ…å«æ–‡æ¡£æ¥æºæ ‡æ³¨ï¼ˆé›¶å¹»è§‰åŸåˆ™ï¼‰
5. **AC-5**: å•å…ƒæµ‹è¯•è¦†ç›–æ‰€æœ‰éªŒè¯åœºæ™¯

## Tasks / Subtasks

- [ ] Task 1: å®ç°AI_MODEL_NAMEå­—æ®µéªŒè¯å™¨ (AC: 1, 2, 4)
  - [ ] åœ¨ `backend/app/config.py` æ·»åŠ  `@field_validator('AI_MODEL_NAME')`
  - [ ] æ£€æµ‹å¹¶æ¸…ç†å¼‚å¸¸å‰ç¼€ï¼ˆå¦‚`[K1]`ã€`[K2]`ç­‰ï¼‰
  - [ ] è®°å½•è­¦å‘Šæ—¥å¿—è¯´æ˜æ¸…ç†æ“ä½œ
  - [ ] æ·»åŠ Context7æ–‡æ¡£æ¥æºæ ‡æ³¨

- [x] Task 2: AIè¿æ¥æµ‹è¯•æ–¹æ³• âœ… å·²å®ç° (AC: 2, 4)
  - [x] `test_ai_connection()` å·²å­˜åœ¨äº `agent_service.py:1845`
  - [x] è¿”å›ç»“æ„åŒ–ç»“æœï¼ˆstatus, model, provider, errorï¼‰
  - [ ] **Devéœ€éªŒè¯**: æ˜¯å¦å®Œå…¨ç¬¦åˆADR-009é”™è¯¯å¤„ç†ç­–ç•¥
  - **POéªŒè¯æ—¶é—´**: 2025-12-14

- [x] Task 3: /health/aiç«¯ç‚¹ âœ… å·²å®ç° (AC: 3, 4)
  - [x] ç«¯ç‚¹å·²å­˜åœ¨äº `health.py:339-391`
  - [x] è°ƒç”¨ `agent_service.test_ai_connection()`
  - [ ] **Devéœ€éªŒè¯**: å“åº”æ ¼å¼æ˜¯å¦åŒ¹é…Storyè§„èŒƒ
  - **POéªŒè¯æ—¶é—´**: 2025-12-14

- [ ] Task 4: ç¼–å†™å•å…ƒæµ‹è¯• (AC: 5)
  - [ ] æµ‹è¯•æ­£å¸¸æ¨¡å‹åç§°ä¸å˜
  - [ ] æµ‹è¯•å¼‚å¸¸å‰ç¼€è¢«æ­£ç¡®æ¸…ç†
  - [ ] æµ‹è¯•AIè¿æ¥æˆåŠŸåœºæ™¯ï¼ˆmockï¼‰
  - [ ] æµ‹è¯•AIè¿æ¥å¤±è´¥åœºæ™¯ï¼ˆmockï¼‰
  - [ ] æµ‹è¯• `/health/ai` ç«¯ç‚¹

## Dev Notes

### ğŸ“‹ æŠ€æœ¯éªŒè¯æŠ¥å‘Š (Step 3.6)

**éªŒè¯å®Œæˆæ—¶é—´**: 2025-12-14
**éªŒè¯æ‰§è¡Œäºº**: SM Agent (Bob)
**Quality GateçŠ¶æ€**: âœ… PASSED

#### æŠ€æœ¯æ ˆæ¸…å•

| æŠ€æœ¯æ ˆ | æŸ¥è¯¢æ–¹å¼ | éªŒè¯çŠ¶æ€ | æ–‡æ¡£ä½ç½® |
|--------|---------|---------|----------|
| Pydantic field_validator | Context7 | âœ… å·²éªŒè¯ | /websites/pydantic_dev (topic: field_validator) |
| FastAPI Depends | Context7 | âœ… å·²éªŒè¯ | /websites/fastapi_tiangolo (topic: Depends) |
| pydantic_settings BaseSettings | Context7 | âœ… å·²éªŒè¯ | /websites/fastapi_tiangolo (topic: settings) |

#### æ ¸å¿ƒAPIéªŒè¯ç»“æœ

**Pydantic field_validator**:
- âœ… Verified from Context7:/websites/pydantic_dev (topic: field_validator)
  - è£…é¥°å™¨: `@field_validator('field_name')`
  - å¿…é¡»é…åˆ `@classmethod` ä½¿ç”¨
  - å‚æ•°: `cls`, `v` (value)
  - å¯è¿”å›ä¿®æ”¹åçš„å€¼æˆ–æŠ›å‡º `ValueError`
  - modeé€‰é¡¹: `'before'`, `'after'`, `'wrap'`, `'plain'`

**ç¤ºä¾‹ä»£ç **:
```python
# âœ… Verified from Context7:/websites/pydantic_dev (topic: field_validator)
from pydantic import BaseModel, field_validator

class Model(BaseModel):
    a: str

    @field_validator('a')
    @classmethod
    def ensure_foobar(cls, v: str) -> str:
        if 'foobar' not in v:
            raise ValueError('"foobar" not found in a')
        return v
```

### SDDè§„èŒƒå‚è€ƒ (å¿…å¡«)

**ç°æœ‰APIç«¯ç‚¹è§„èŒƒ**:
- `GET /api/v1/health` - åŸºç¡€å¥åº·æ£€æŸ¥
  - [Source: specs/api/fastapi-backend-api.openapi.yml#/paths/~1api~1v1~1health]
- `GET /api/v1/health/metrics` - PrometheusæŒ‡æ ‡
  - [Source: backend/app/api/v1/endpoints/health.py:108-159]
- `GET /api/v1/health/metrics/summary` - JSONæŒ‡æ ‡æ‘˜è¦
  - [Source: backend/app/api/v1/endpoints/health.py:162-260]

**æ–°å¢ç«¯ç‚¹è§„èŒƒ**:
- `GET /api/v1/health/ai` - AIè¿æ¥çŠ¶æ€æ£€æŸ¥ (æœ¬Storyå®ç°)
  - å“åº”çŠ¶æ€: 200 (connected), 503 (disconnected)
  - å“åº”æ ¼å¼: `{status: "ok"|"error", model: string, error?: string}`

**æ•°æ®Schemaè§„èŒƒ**:
- ErrorResponse:
  - [Source: specs/data/error-response.schema.json]
  - å­—æ®µ: `code`, `message`, `details`

### ç°æœ‰å®ç°åˆ†æ (å¿…è¯»)

**config.pyç°æœ‰field_validatorç¤ºä¾‹**:
```python
# [Source: backend/app/config.py:112-138]
@field_validator("API_V1_PREFIX")
@classmethod
def validate_api_prefix(cls, v: str) -> str:
    """Fix MSYS/Git Bash path conversion issue."""
    if v.startswith("/"):
        return v
    if "/api/" in v:
        idx = v.find("/api/")
        return v[idx:]
    return "/api/v1"
```

**AIé…ç½®å­—æ®µ** (éœ€è¦æ·»åŠ éªŒè¯å™¨):
```python
# [Source: backend/app/config.py:191-209]
AI_PROVIDER: str = Field(default="google", ...)
AI_MODEL_NAME: str = Field(default="gemini-2.0-flash-exp", ...)
AI_BASE_URL: str = Field(default="", ...)
AI_API_KEY: str = Field(default="", ...)
```

**é—®é¢˜åœºæ™¯**:
- `AI_MODEL_NAME=[K1]gemini-2.5-pro` â†’ å¼‚å¸¸å‰ç¼€å¯¼è‡´APIè°ƒç”¨å¤±è´¥
- éœ€è¦è‡ªåŠ¨æ¸…ç†ä¸º `gemini-2.5-pro`

### ADRå†³ç­–å…³è” (å¿…å¡«)

| ADRç¼–å· | å†³ç­–æ ‡é¢˜ | å¯¹Storyçš„å½±å“ |
|---------|----------|---------------|
| ADR-009 | é”™è¯¯å¤„ç†ä¸é‡è¯•ç­–ç•¥ | AIè¿æ¥æµ‹è¯•éµå¾ªé”™è¯¯åˆ†ç±»ä½“ç³»ï¼Œä½¿ç”¨ErrorCode.LLM_* |

**å…³é”®çº¦æŸ** (from ADR-009):
- é”™è¯¯åˆ†ç±»: `RETRYABLE`, `NON_RETRYABLE`, `FATAL`
- LLMé”™è¯¯ç : `1001`(RATE_LIMIT), `1002`(TIMEOUT), `1003`(API_ERROR), `1004`(INVALID_RESPONSE)
- ç”¨æˆ·å‹å¥½æ¶ˆæ¯æ˜ å°„ (1003 â†’ "API Keyæ— æ•ˆæˆ–è´¦æˆ·ä½™é¢ä¸è¶³")

### ä»£ç ç¤ºä¾‹åº“

**Task 1: AI_MODEL_NAMEéªŒè¯å™¨å®ç°**:
```python
# âœ… Verified from Context7:/websites/pydantic_dev (topic: field_validator)
# [Target: backend/app/config.py]

import logging
from pydantic import field_validator

logger = logging.getLogger(__name__)

class Settings(BaseSettings):
    AI_MODEL_NAME: str = Field(...)

    @field_validator('AI_MODEL_NAME')
    @classmethod
    def validate_model_name(cls, v: str) -> str:
        """
        æ£€æµ‹å¹¶æ¸…ç†AIæ¨¡å‹åç§°ä¸­çš„å¼‚å¸¸å‰ç¼€ã€‚

        å¼‚å¸¸å‰ç¼€ç¤ºä¾‹: [K1], [K2], [TEST]
        """
        # æ£€æµ‹æ–¹æ‹¬å·å‰ç¼€æ¨¡å¼
        if v.startswith('[') and ']' in v:
            bracket_end = v.index(']') + 1
            prefix = v[:bracket_end]
            clean_name = v[bracket_end:]
            logger.warning(
                f"AI_MODEL_NAME contains abnormal prefix '{prefix}', "
                f"auto-cleaned to '{clean_name}'"
            )
            return clean_name
        return v
```

**Task 2: test_ai_connectionå®ç°**:
```python
# [Target: backend/app/services/agent_service.py]
# éµå¾ªADR-009é”™è¯¯å¤„ç†ç­–ç•¥

async def test_ai_connection(self) -> dict:
    """
    æµ‹è¯•AI APIè¿æ¥ã€‚

    Returns:
        dict: {"status": "ok"|"error", "model": str, "error"?: str}
    """
    try:
        # ä½¿ç”¨æœ€å°tokenè¯·æ±‚æµ‹è¯•è¿æ¥
        response = await self.client.chat.completions.create(
            model=self.model_name,
            messages=[{"role": "user", "content": "test"}],
            max_tokens=5
        )
        return {
            "status": "ok",
            "model": self.model_name,
            "provider": self.provider
        }
    except Exception as e:
        # [Source: ADR-009 - é”™è¯¯åˆ†ç±»ä½“ç³»]
        return {
            "status": "error",
            "model": self.model_name,
            "provider": self.provider,
            "error": str(e),
            "error_type": type(e).__name__
        }
```

**Task 3: /health/aiç«¯ç‚¹å®ç°**:
```python
# âœ… Verified from Context7:/websites/fastapi_tiangolo (topic: path operation decorators)
# [Target: backend/app/api/v1/endpoints/health.py]

from fastapi import APIRouter, Depends
from app.services.agent_service import AgentService, get_agent_service

@router.get(
    "/health/ai",
    summary="AIè¿æ¥çŠ¶æ€æ£€æŸ¥",
    description="æµ‹è¯•AI Provider APIè¿æ¥æ˜¯å¦æ­£å¸¸",
    operation_id="check_ai_health",
    responses={
        200: {"description": "AIè¿æ¥æ­£å¸¸"},
        503: {"description": "AIè¿æ¥å¤±è´¥"}
    }
)
async def check_ai_health(
    agent_service: AgentService = Depends(get_agent_service)
) -> dict:
    """
    æµ‹è¯•AI APIè¿æ¥çŠ¶æ€ã€‚

    [Source: Epic 21.5 Story 21.5.2 AC-3]
    """
    result = await agent_service.test_ai_connection()

    if result["status"] == "error":
        # è¿”å›503ä½†åŒ…å«è¯¦ç»†é”™è¯¯ä¿¡æ¯
        from fastapi.responses import JSONResponse
        return JSONResponse(
            status_code=503,
            content=result
        )

    return result
```

### é¡¹ç›®ç»“æ„å½±å“

**ä¿®æ”¹æ–‡ä»¶åˆ—è¡¨**:
```
backend/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ config.py               # æ·»åŠ AI_MODEL_NAMEéªŒè¯å™¨ â­
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â””â”€â”€ agent_service.py    # æ·»åŠ test_ai_connectionæ–¹æ³• â­
â”‚   â””â”€â”€ api/
â”‚       â””â”€â”€ v1/
â”‚           â””â”€â”€ endpoints/
â”‚               â””â”€â”€ health.py   # æ·»åŠ /health/aiç«¯ç‚¹ â­
â””â”€â”€ tests/
    â””â”€â”€ unit/
        â””â”€â”€ test_config_validation.py  # æ–°å»º â­
```

### Testing

**æµ‹è¯•æ–‡ä»¶ä½ç½®**: `backend/tests/unit/test_config_validation.py`

**æµ‹è¯•ç”¨ä¾‹æ¸…å•**:
```python
# Test Case 1: æ­£å¸¸æ¨¡å‹åç§°ä¸å˜
def test_model_name_normal():
    """AI_MODEL_NAME without prefix should remain unchanged."""
    settings = Settings(AI_MODEL_NAME="gemini-2.0-flash-exp")
    assert settings.AI_MODEL_NAME == "gemini-2.0-flash-exp"

# Test Case 2: å¼‚å¸¸å‰ç¼€è¢«æ¸…ç†
def test_model_name_with_prefix():
    """AI_MODEL_NAME with [K1] prefix should be auto-cleaned."""
    settings = Settings(AI_MODEL_NAME="[K1]gemini-2.5-pro")
    assert settings.AI_MODEL_NAME == "gemini-2.5-pro"

# Test Case 3: å¤šç§å‰ç¼€æ ¼å¼
@pytest.mark.parametrize("input_name,expected", [
    ("[K1]gemini-2.5-pro", "gemini-2.5-pro"),
    ("[K2]gpt-4o", "gpt-4o"),
    ("[TEST]claude-3", "claude-3"),
    ("normal-model", "normal-model"),  # æ— å‰ç¼€
])
def test_model_name_various_prefixes(input_name, expected):
    settings = Settings(AI_MODEL_NAME=input_name)
    assert settings.AI_MODEL_NAME == expected

# Test Case 4: /health/aiç«¯ç‚¹
async def test_health_ai_endpoint(client):
    """GET /api/v1/health/ai should return AI connection status."""
    response = await client.get("/api/v1/health/ai")
    assert response.status_code in [200, 503]
    data = response.json()
    assert "status" in data
    assert "model" in data

# Test Case 5: AIè¿æ¥æˆåŠŸåœºæ™¯ (mock)
@pytest.mark.asyncio
async def test_ai_connection_success(mocker):
    """Test AI connection returns success when API responds."""
    mock_response = mocker.MagicMock()
    mock_response.choices = [mocker.MagicMock()]

    agent_service = AgentService()
    mocker.patch.object(
        agent_service.client.chat.completions,
        'create',
        return_value=mock_response
    )

    result = await agent_service.test_ai_connection()
    assert result["status"] == "ok"
    assert "model" in result

# Test Case 6: AIè¿æ¥å¤±è´¥åœºæ™¯ (mock)
@pytest.mark.asyncio
async def test_ai_connection_failure(mocker):
    """Test AI connection returns error when API fails."""
    agent_service = AgentService()
    mocker.patch.object(
        agent_service.client.chat.completions,
        'create',
        side_effect=Exception("Invalid API key")
    )

    result = await agent_service.test_ai_connection()
    assert result["status"] == "error"
    assert "error" in result
    assert "Invalid API key" in result["error"]
```

**æµ‹è¯•æ ‡å‡†**:
- ä½¿ç”¨pytestæ¡†æ¶
- æµ‹è¯•æ–‡ä»¶å‘½å: `test_{module_name}.py`
- æµ‹è¯•å‡½æ•°å‘½å: `test_{function_name}_{scenario}`
- [Source: docs/architecture/coding-standards.md#æµ‹è¯•è§„èŒƒ]

### æŠ€æœ¯çº¦æŸå’Œæ³¨æ„äº‹é¡¹

**ç‰ˆæœ¬çº¦æŸ**:
- Pydantic: â‰¥2.5.0 (field_validator in pydantic v2)
- pydantic-settings: â‰¥2.0.0
- FastAPI: â‰¥0.104.0

**å·²çŸ¥é™åˆ¶**:
- AIè¿æ¥æµ‹è¯•ä¼šæ¶ˆè€—å°‘é‡API quota
- æµ‹è¯•è¶…æ—¶è®¾ç½®ä¸º5ç§’

**å®‰å…¨è€ƒè™‘**:
- `/health/ai` ç«¯ç‚¹ä¸è¿”å›API Key
- é”™è¯¯ä¿¡æ¯ä¸æš´éœ²æ•æ„Ÿé…ç½®

### ä¾èµ–å…³ç³»

**å‰ç½®Story**:
- Story 21.5.1 (500é”™è¯¯CORSä¿®å¤) - ç¡®ä¿é”™è¯¯å“åº”å¯è§

**åç»­Story**:
- Story 21.5.4 (Agentç«¯ç‚¹å¥åº·æ£€æŸ¥å¢å¼º) - ä½¿ç”¨æœ¬Storyçš„AIè¿æ¥æµ‹è¯•

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-12-14 | 1.0 | Initial story draft with ultrathink analysis | SM Agent (Bob) |
| 2025-12-14 | 1.1 | POéªŒè¯é€šè¿‡: æ ‡è®°Task 2/3å·²å®ç°ï¼Œæ·»åŠ mockæµ‹è¯•ç”¨ä¾‹ | PO Agent (Sarah) |

## References

- **Epic**: [docs/prd/EPIC-21.5-AGENT-RELIABILITY-FIX.md](../prd/EPIC-21.5-AGENT-RELIABILITY-FIX.md)
- **ADR-009**: [docs/architecture/decisions/ADR-009-ERROR-HANDLING-RETRY-STRATEGY.md](../architecture/decisions/ADR-009-ERROR-HANDLING-RETRY-STRATEGY.md)
- **Coding Standards**: [docs/architecture/coding-standards.md](../architecture/coding-standards.md)
- **Existing config.py**: [backend/app/config.py](../../backend/app/config.py)
- **Existing health.py**: [backend/app/api/v1/endpoints/health.py](../../backend/app/api/v1/endpoints/health.py)

---

## QA Results

### Review Date: 2025-12-14

### Reviewed By: Quinn (Test Architect)

### Code Quality Assessment

**Overall Assessment**: HIGH QUALITY with minor gaps

The implementation demonstrates excellent code quality with comprehensive documentation, proper source annotations (Context7/Story references), and adherence to coding standards. All three main tasks (Task 1-3) are fully implemented with robust error handling and logging.

**Implementation Analysis**:

| Task | Status | File | Lines | Quality |
|------|--------|------|-------|---------|
| Task 1: AI_MODEL_NAME validator | âœ… COMPLETE | `config.py` | 147-176 | Excellent |
| Task 2: test_ai_connection | âœ… COMPLETE | `agent_service.py` | 1845-1939 | Excellent |
| Task 3: /health/ai endpoint | âœ… COMPLETE | `health.py` | 345-422 | Excellent |
| Task 4: Unit tests | âš ï¸ PARTIAL | `test_health.py` | 416-467 | Missing validator tests |

**Strengths**:
1. âœ… Context7 source annotations throughout (`âœ… Verified from Context7:/websites/...`)
2. âœ… Comprehensive error classification (LLM_AUTH_FAILED, LLM_TIMEOUT, etc.)
3. âœ… Proper logging with structured messages
4. âœ… OpenAPI documentation with response examples
5. âœ… ADR-009 error handling strategy followed

### Refactoring Performed

No refactoring performed - implementation is well-structured.

### Compliance Check

- Coding Standards: âœ“ All source annotations present, docstrings complete
- Project Structure: âœ“ Files in correct locations per architecture
- Testing Strategy: âš ï¸ Endpoint tests exist but validator unit tests missing
- All ACs Met: âš ï¸ AC-5 partially met (see details below)

**Acceptance Criteria Verification**:

| AC | Requirement | Status | Evidence |
|----|-------------|--------|----------|
| AC-1 | Detect & clean AI_MODEL_NAME prefix | âœ… PASS | `config.py:167-176` - Detects `[K1]`, `[K2]`, `[TEST]` patterns |
| AC-2 | Invalid config error with code & suggestion | âœ… PASS | `agent_service.py:1923-1936` - Error codes & classification |
| AC-3 | `/api/v1/health/ai` endpoint | âœ… PASS | `health.py:345-422` - Returns 200/503 with structured response |
| AC-4 | Documentation source annotations | âœ… PASS | All files contain `[Source:]` and `âœ… Verified from Context7` |
| AC-5 | Unit tests cover all scenarios | âš ï¸ PARTIAL | `/health/ai` tests exist, validator tests **MISSING** |

### Improvements Checklist

- [x] Task 1 implementation complete with source annotations
- [x] Task 2 implementation complete with ADR-009 compliance
- [x] Task 3 implementation complete with OpenAPI docs
- [x] Endpoint integration tests exist (test_health.py:416-467)
- [ ] **Add validator unit tests to test_config.py** (Dev action required)
  - Missing: `test_model_name_normal`
  - Missing: `test_model_name_with_prefix`
  - Missing: `test_model_name_various_prefixes` (parametrized)
- [ ] Consider using ADR-009 `ErrorCode` enum instead of string literals

### Security Review

âœ… **PASS** - No security concerns found.

- `/health/ai` endpoint does NOT expose API keys
- Error messages do not leak sensitive configuration
- No injection vulnerabilities in prefix cleaning logic

### Performance Considerations

âœ… **PASS** - Performance acceptable.

- AI connection test includes latency measurement (`latency_ms`)
- No blocking operations in validator
- Endpoint response time tested (<500ms) in test_health.py:594-609

### Test Coverage Gap Analysis

**Existing Tests** (test_health.py:416-467):
- `test_health_ai_returns_200` âœ“
- `test_health_ai_response_structure` âœ“
- `test_health_ai_error_has_error_field` âœ“
- `test_health_ai_content_type` âœ“

**Missing Tests** (should be in test_config.py):
```python
# Story spec defines these but they don't exist:
def test_model_name_normal()           # AC-1
def test_model_name_with_prefix()      # AC-1
def test_model_name_various_prefixes() # AC-1 (parametrized)
def test_ai_connection_success()       # AC-2 (mock)
def test_ai_connection_failure()       # AC-2 (mock)
```

### ADR-009 Compliance Review

| Requirement | Status | Evidence |
|-------------|--------|----------|
| Error classification (RETRYABLE/NON_RETRYABLE/FATAL) | âœ… | Implemented via error_code strings |
| LLM error codes (1001-1004) | âš ï¸ | Uses string codes, not enum |
| User-friendly messages | âœ… | Implemented in agent_service |
| Logging on retry | âœ… | logger.warning in test_ai_connection |

**Note**: Implementation uses string error codes (`"LLM_AUTH_FAILED"`) instead of `ErrorCode.LLM_AUTH_FAILED` enum from ADR-009. This is a minor deviation that doesn't affect functionality.

### Files Modified During Review

None - review only, no modifications made.

### Gate Status

**Gate: CONCERNS** â†’ `docs/qa/gates/21.5.2-ai-provider-config-validation.yml`

**Reason**: Implementation is complete and high-quality, but AC-5 (unit tests) is only partially satisfied. Missing validator unit tests in test_config.py.

**Risk Profile**: MEDIUM
- Security-adjacent code (AI configuration)
- Missing P0 tests for validator logic

### Recommended Status

**âœ— Changes Required** - See unchecked items above

**Required Actions for Dev**:
1. Add validator unit tests to `backend/tests/test_config.py`:
   - Test normal model name unchanged
   - Test `[K1]` prefix cleaned
   - Test various prefix patterns (parametrized)
2. Optionally: Add mock tests for `test_ai_connection()` success/failure scenarios

**After fixes**: Re-run `*review 21.5.2` for PASS gate
