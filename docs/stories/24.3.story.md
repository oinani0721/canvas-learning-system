# Story 24.3: Intelligent Weight Algorithm for Targeted Review

## Status
- [x] Draft
- [ ] Ready for Dev
- [ ] In Progress
- [ ] QA Review
- [ ] Done

---

## Epic Context & Background

**Epic ID**: EPIC-24
**Epic Title**: Verification Canvas Redesign
**Story Priority**: P0 (Critical)
**Estimated Points**: 5

**Epic Objective**: Redesign the verification canvas (检验白板) generation system to support dual review modes (fresh/targeted), integrate with Graphiti for history tracking, and improve the overall generation workflow.

**This Story's Role**: Story 24.3 implements the **intelligent weight algorithm** that powers the targeted review mode. It calculates dynamic weights for concept selection based on historical performance metrics, ensuring weak concepts receive more focus while maintaining exposure to mastered concepts.

**Dependencies**:
- Story 24.1 (Mode Support) - Completed
- Epic 22 (Memory System) - Completed
- Epic 23 (RAG Intelligent Inference) - Completed

---

## Story Overview

**As a** Canvas Learning System user using targeted review mode,
**I want** the system to intelligently calculate and apply weights to concept selection based on my historical performance,
**So that** weak concepts are prioritized for review while mastered concepts are still occasionally included to prevent regression.

---

## Acceptance Criteria

### AC1: Weakness Score Calculation
- **Given**: A concept has review history in Graphiti
- **When**: The weight algorithm is executed
- **Then**:
  - A weakness_score (0.0-1.0) is calculated for each concept
  - Score factors in: average_rating, failure_count, days_since_last_review, trend_direction
  - Higher weakness_score = weaker concept = higher priority
- **Verification**: Unit tests confirm score calculation accuracy; scores match expected values for test data

### AC2: Configurable Weight Distribution
- **Given**: Targeted mode is selected with optional weight parameters
- **When**: Concept selection occurs
- **Then**:
  - Default weights (70% weak, 30% mastered) are applied when no custom weights provided
  - Custom weights (`weak_weight`, `mastered_weight`) override defaults when provided
  - Weights sum validation ensures weak_weight + mastered_weight = 1.0
- **Verification**: API tests confirm default and custom weights are correctly applied

### AC3: Weighted Concept Selection
- **Given**: Calculated weakness scores for all eligible concepts
- **When**: Generating verification canvas in targeted mode
- **Then**:
  - Concepts with weakness_score >= 0.6 are classified as "weak"
  - Concepts with weakness_score < 0.4 are classified as "mastered"
  - Concepts in between (0.4-0.6) are classified as "borderline"
  - Selection probability follows weight distribution
- **Verification**: Statistical analysis of generated canvases shows distribution matches configured weights (within 10% tolerance)

### AC4: New Concept Handling
- **Given**: A concept has no review history (new concept)
- **When**: Weight calculation is performed
- **Then**:
  - New concepts receive a default weakness_score of 0.5 (neutral)
  - New concepts are included in the "borderline" category
  - New concepts have equal probability in weighted selection
- **Verification**: New concepts appear in generated canvases with expected frequency

### AC5: API Response Enhancement
- **Given**: Verification canvas is generated in targeted mode
- **When**: API returns GenerateCanvasResponse
- **Then**:
  - Response includes `weak_concepts` array with:
    - `concept_name`: Name of the weak concept
    - `weakness_score`: Calculated score (0.0-1.0)
    - `failure_count`: Historical failure count
    - `avg_rating`: Average review rating
  - Response includes `weight_config` showing applied weights
- **Verification**: API contract test validates response schema; values match calculated data

### AC6: Performance Optimization
- **Given**: A canvas with 100+ eligible concepts
- **When**: Weight calculation and selection is performed
- **Then**:
  - Total calculation time < 500ms
  - Graphiti queries are batched (max 2 queries)
  - Results are cached for subsequent calls within same session
- **Verification**: Performance benchmarks confirm timing requirements met

---

## Dev Notes (CRITICAL - Self-Contained)

### Technical Context

This story implements the intelligent weight algorithm that makes targeted review mode effective. The algorithm analyzes historical review data from Graphiti to calculate per-concept weakness scores, then applies configurable weights to prioritize weak concepts in verification canvas generation.

**Current State** (backend/app/services/review_service.py:90-120):
- `ReviewService` exists with basic verification canvas generation
- Story 24.1 added mode parameter support
- No weight calculation logic implemented yet

**Target State**:
- `WeightCalculator` class for computing weakness scores
- `_apply_weighted_selection()` method with configurable weights
- Enhanced response with weak_concepts data

### API Endpoints

**Primary Endpoint**: `POST /review/generate-canvas`
- **File**: `backend/app/api/v1/endpoints/review.py`
- **OpenAPI Spec**: `specs/api/review-api.openapi.yml#L215-257`

**Request Schema** (specs/data/review-generate-request.schema.json):
```json
{
  "canvas_path": "string",
  "mode": "fresh|targeted",
  "weak_weight": 0.7,      // Story 24.1 added
  "mastered_weight": 0.3   // Story 24.1 added
}
```

**Response Enhancement** (specs/api/review-api.openapi.yml#L703-719):
```json
{
  "weak_concepts": [
    {
      "concept_name": "string",
      "weakness_score": 0.75,
      "failure_count": 3,
      "avg_rating": 2.1
    }
  ],
  "weight_config": {
    "weak_weight": 0.7,
    "mastered_weight": 0.3,
    "applied": true
  }
}
```

### Data Models

**Request Schema**: `specs/data/review-generate-request.schema.json`
- Lines 14-32 define existing properties
- Mode and weight parameters added in Story 24.1

**Response Schema**: `specs/api/review-api.openapi.yml#L684-723`
- `weak_concepts` array (Lines 703-719)
- Contains `weakness_score` (Line 713-714)

### SDD References

| Type | File | Lines | Description |
|------|------|-------|-------------|
| OpenAPI | specs/api/review-api.openapi.yml | L215-257 | generate-canvas endpoint |
| OpenAPI | specs/api/review-api.openapi.yml | L703-719 | weak_concepts response |
| Schema | specs/data/review-generate-request.schema.json | L14-32 | Request model |
| Architecture | docs/architecture/ebbinghaus-review-system-architecture.md | Full | FSRS algorithm reference |

### ADR References

| ADR | Description | Relevance |
|-----|-------------|-----------|
| docs/architecture/decisions/0003-graphiti-memory.md | Graphiti integration | Historical data queries |
| docs/architecture/decisions/0004-async-execution-engine.md | Async patterns | Performance optimization |

### Implementation Guidelines

**Step 1: Create WeightCalculator Class**
```python
# backend/app/services/weight_calculator.py
from dataclasses import dataclass
from typing import List, Dict, Optional
from datetime import datetime, timedelta

@dataclass
class ConceptWeightData:
    """Weight calculation data for a concept."""
    concept_id: str
    concept_name: str
    weakness_score: float  # 0.0-1.0
    failure_count: int
    avg_rating: float
    review_count: int
    days_since_review: int
    category: str  # "weak", "borderline", "mastered"

class WeightCalculator:
    """
    Calculates weakness scores and applies weighted selection.

    PRD Reference: v1.1.8 - calculate_targeted_review_weights tool
    """

    # Configurable thresholds
    WEAK_THRESHOLD = 0.6
    MASTERED_THRESHOLD = 0.4
    DEFAULT_NEW_SCORE = 0.5

    # Score component weights (must sum to 1.0)
    RATING_WEIGHT = 0.4      # Avg rating impact
    FAILURE_WEIGHT = 0.3     # Failure count impact
    RECENCY_WEIGHT = 0.2     # Days since review impact
    TREND_WEIGHT = 0.1       # Improvement trend impact

    async def calculate_weakness_scores(
        self,
        concepts: List[Dict],
        review_history: List[Dict]
    ) -> List[ConceptWeightData]:
        """
        Calculate weakness scores for all concepts.

        Args:
            concepts: List of concept dicts with id, name
            review_history: Review history from Graphiti

        Returns:
            List of ConceptWeightData with calculated scores
        """
        results = []
        history_map = self._build_history_map(review_history)

        for concept in concepts:
            concept_id = concept["id"]
            history = history_map.get(concept_id, [])

            if not history:
                # New concept - neutral score
                score = self.DEFAULT_NEW_SCORE
                weight_data = ConceptWeightData(
                    concept_id=concept_id,
                    concept_name=concept.get("name", ""),
                    weakness_score=score,
                    failure_count=0,
                    avg_rating=0.0,
                    review_count=0,
                    days_since_review=0,
                    category="borderline"
                )
            else:
                # Calculate score from history
                weight_data = self._calculate_from_history(concept, history)

            results.append(weight_data)

        return results

    def _calculate_from_history(
        self,
        concept: Dict,
        history: List[Dict]
    ) -> ConceptWeightData:
        """Calculate weakness score from review history."""
        # Calculate metrics
        ratings = [h["rating"] for h in history if h.get("rating")]
        avg_rating = sum(ratings) / len(ratings) if ratings else 0
        failure_count = sum(1 for h in history if h.get("rating", 0) <= 2)
        review_count = len(history)

        # Days since last review
        last_review = max(
            (h.get("timestamp") for h in history if h.get("timestamp")),
            default=None
        )
        days_since = 0
        if last_review:
            days_since = (datetime.utcnow() - last_review).days

        # Calculate component scores (all normalized to 0-1, inverted where needed)
        rating_score = 1.0 - (avg_rating - 1) / 3.0  # Rating 1-4 -> 1.0-0.0
        failure_score = min(failure_count / 5.0, 1.0)  # Cap at 5 failures
        recency_score = min(days_since / 30.0, 1.0)  # Cap at 30 days

        # Trend score (simplified: comparing recent vs older ratings)
        trend_score = self._calculate_trend_score(history)

        # Weighted combination
        weakness_score = (
            self.RATING_WEIGHT * rating_score +
            self.FAILURE_WEIGHT * failure_score +
            self.RECENCY_WEIGHT * recency_score +
            self.TREND_WEIGHT * trend_score
        )

        # Clamp to 0-1
        weakness_score = max(0.0, min(1.0, weakness_score))

        # Categorize
        if weakness_score >= self.WEAK_THRESHOLD:
            category = "weak"
        elif weakness_score < self.MASTERED_THRESHOLD:
            category = "mastered"
        else:
            category = "borderline"

        return ConceptWeightData(
            concept_id=concept["id"],
            concept_name=concept.get("name", ""),
            weakness_score=weakness_score,
            failure_count=failure_count,
            avg_rating=avg_rating,
            review_count=review_count,
            days_since_review=days_since,
            category=category
        )

    def _calculate_trend_score(self, history: List[Dict]) -> float:
        """Calculate trend score based on rating improvement."""
        if len(history) < 2:
            return 0.5  # Neutral for insufficient data

        # Sort by timestamp
        sorted_history = sorted(history, key=lambda h: h.get("timestamp", datetime.min))

        # Compare recent half vs older half
        mid = len(sorted_history) // 2
        older_ratings = [h["rating"] for h in sorted_history[:mid] if h.get("rating")]
        recent_ratings = [h["rating"] for h in sorted_history[mid:] if h.get("rating")]

        if not older_ratings or not recent_ratings:
            return 0.5

        older_avg = sum(older_ratings) / len(older_ratings)
        recent_avg = sum(recent_ratings) / len(recent_ratings)

        # Improving = lower weakness score
        # Rating improved: recent > older -> negative trend score
        trend_diff = older_avg - recent_avg  # Positive if getting worse

        # Normalize to 0-1
        return max(0.0, min(1.0, 0.5 + trend_diff / 2.0))
```

**Step 2: Add Weighted Selection Method to ReviewService**
```python
# backend/app/services/review_service.py - Add to ReviewService class

from app.services.weight_calculator import WeightCalculator, ConceptWeightData
import random

async def _apply_weighted_selection(
    self,
    weight_data: List[ConceptWeightData],
    question_count: int,
    weak_weight: float = 0.7,
    mastered_weight: float = 0.3
) -> List[ConceptWeightData]:
    """
    Apply weighted selection to concept list.

    Args:
        weight_data: Calculated weight data for all concepts
        question_count: Number of questions to select
        weak_weight: Weight for weak concepts (default 0.7)
        mastered_weight: Weight for mastered concepts (default 0.3)

    Returns:
        Selected concepts based on weight distribution
    """
    # Validate weights
    if abs(weak_weight + mastered_weight - 1.0) > 0.01:
        raise ValueError("weak_weight + mastered_weight must equal 1.0")

    # Categorize concepts
    weak = [c for c in weight_data if c.category == "weak"]
    mastered = [c for c in weight_data if c.category == "mastered"]
    borderline = [c for c in weight_data if c.category == "borderline"]

    # Calculate target counts
    weak_count = int(question_count * weak_weight)
    mastered_count = int(question_count * mastered_weight)
    borderline_count = question_count - weak_count - mastered_count

    selected = []

    # Select from each category (with fallback if insufficient)
    if weak:
        selected.extend(self._weighted_sample(weak, min(weak_count, len(weak))))
    if mastered:
        selected.extend(self._weighted_sample(mastered, min(mastered_count, len(mastered))))
    if borderline:
        selected.extend(self._weighted_sample(borderline, min(borderline_count, len(borderline))))

    # Fill remaining from any category
    remaining = question_count - len(selected)
    if remaining > 0:
        all_remaining = [c for c in weight_data if c not in selected]
        selected.extend(self._weighted_sample(all_remaining, min(remaining, len(all_remaining))))

    return selected

def _weighted_sample(
    self,
    concepts: List[ConceptWeightData],
    count: int
) -> List[ConceptWeightData]:
    """Sample concepts with weights based on weakness_score."""
    if not concepts or count <= 0:
        return []

    # Use weakness_score as probability weight
    weights = [c.weakness_score for c in concepts]
    total = sum(weights)

    if total == 0:
        # Equal probability if all scores are 0
        return random.sample(concepts, min(count, len(concepts)))

    # Normalize weights
    probabilities = [w / total for w in weights]

    # Weighted sampling without replacement
    selected = []
    remaining = list(zip(concepts, probabilities))

    for _ in range(min(count, len(concepts))):
        if not remaining:
            break

        # Random selection based on probabilities
        r = random.random()
        cumulative = 0
        for i, (concept, prob) in enumerate(remaining):
            cumulative += prob
            if r <= cumulative:
                selected.append(concept)
                remaining.pop(i)
                # Renormalize remaining probabilities
                if remaining:
                    total_prob = sum(p for _, p in remaining)
                    remaining = [(c, p / total_prob) for c, p in remaining]
                break

    return selected
```

**Step 3: Integrate Weight Calculator into generate_verification_canvas**
```python
# backend/app/services/review_service.py - Update generate_verification_canvas

async def generate_verification_canvas(
    self,
    canvas_name: str,
    mode: str = "fresh",
    weak_weight: float = 0.7,
    mastered_weight: float = 0.3,
    include_colors: List[str] = ["3", "4"],
    question_count: Optional[int] = None
) -> Dict[str, Any]:
    """
    Generate verification canvas with mode support and weight algorithm.

    Args:
        canvas_name: Source canvas name
        mode: "fresh" or "targeted"
        weak_weight: Weight for weak concepts (targeted mode)
        mastered_weight: Weight for mastered concepts (targeted mode)
        include_colors: Node colors to include
        question_count: Maximum questions (None = all eligible)

    Returns:
        Generation result with weak_concepts data for targeted mode
    """
    # Get eligible concepts from canvas
    eligible_concepts = await self._get_eligible_concepts(canvas_name, include_colors)

    if not eligible_concepts:
        raise ValueError(f"No eligible concepts found in {canvas_name}")

    weak_concepts_data = []
    weight_config = {"weak_weight": weak_weight, "mastered_weight": mastered_weight, "applied": False}

    if mode == "targeted":
        # Query Graphiti for review history
        review_history = await self._query_review_history_from_graphiti(canvas_name)

        # Calculate weakness scores
        calculator = WeightCalculator()
        weight_data = await calculator.calculate_weakness_scores(
            eligible_concepts,
            review_history
        )

        # Apply weighted selection
        target_count = question_count or len(eligible_concepts)
        selected_concepts = await self._apply_weighted_selection(
            weight_data,
            target_count,
            weak_weight,
            mastered_weight
        )

        # Prepare weak_concepts for response
        weak_concepts_data = [
            {
                "concept_name": c.concept_name,
                "weakness_score": c.weakness_score,
                "failure_count": c.failure_count,
                "avg_rating": c.avg_rating
            }
            for c in weight_data if c.category == "weak"
        ]

        weight_config["applied"] = True
        concepts_to_use = selected_concepts
    else:
        # Fresh mode: equal probability selection
        concepts_to_use = eligible_concepts
        if question_count and len(concepts_to_use) > question_count:
            concepts_to_use = random.sample(concepts_to_use, question_count)

    # Generate verification canvas (existing logic)
    result = await self._create_verification_canvas(canvas_name, concepts_to_use)

    # Store relationship in Graphiti
    await self._store_review_relationship(canvas_name, result["review_canvas_name"], mode)

    # Enhance response
    result["weak_concepts"] = weak_concepts_data
    result["weight_config"] = weight_config
    result["mode_used"] = mode

    return result
```

**Step 4: Add Graphiti Query for Review History**
```python
# backend/app/services/review_service.py - Add method

async def _query_review_history_from_graphiti(
    self,
    canvas_name: str
) -> List[Dict]:
    """
    Query review history from Graphiti knowledge graph.

    PRD Reference: v1.1.8 - query_review_history_from_graphiti tool

    Returns:
        List of review records with: concept_id, rating, timestamp, etc.
    """
    # Cypher query for review history
    query = """
    MATCH (c:Canvas {name: $canvas_name})<-[:BELONGS_TO]-(concept:Concept)
    OPTIONAL MATCH (concept)-[r:REVIEWED]->(review:ReviewResult)
    RETURN concept.id AS concept_id,
           concept.name AS concept_name,
           collect({
               rating: r.rating,
               timestamp: r.timestamp,
               score: r.score
           }) AS reviews
    ORDER BY concept.id
    """

    # Execute via Graphiti client
    try:
        results = await self.graphiti_client.execute_query(
            query,
            {"canvas_name": canvas_name}
        )

        # Flatten results for WeightCalculator
        history = []
        for row in results:
            for review in row.get("reviews", []):
                if review.get("rating") is not None:
                    history.append({
                        "concept_id": row["concept_id"],
                        "concept_name": row["concept_name"],
                        "rating": review["rating"],
                        "timestamp": review.get("timestamp"),
                        "score": review.get("score")
                    })

        return history

    except Exception as e:
        logger.warning(f"Graphiti query failed, using empty history: {e}")
        return []
```

### Key Files to Modify

| File Path | Change Type | Description |
|-----------|-------------|-------------|
| `backend/app/services/weight_calculator.py` | Create | New WeightCalculator class |
| `backend/app/services/review_service.py` | Modify | Add weighted selection, integrate calculator |
| `specs/data/review-generate-response.schema.json` | Modify | Add weight_config to response schema |
| `src/tests/test_weight_calculator.py` | Create | Unit tests for weight calculation |
| `src/tests/test_weighted_selection.py` | Create | Tests for weighted concept selection |

### Existing Code References

| Location | Line Range | Purpose |
|----------|------------|---------|
| backend/app/services/review_service.py | 90-120 | ReviewService class definition |
| backend/app/services/review_service.py | 148-150 | generate_review_canvas method start |
| specs/api/review-api.openapi.yml | 703-719 | weak_concepts response definition |
| src/agentic_rag/clients/graphiti_client.py | Full | Graphiti client for queries |

### Performance Considerations

1. **Batch Graphiti Queries**: Single query retrieves all review history for canvas
2. **Cache Weight Data**: Cache calculated scores for session duration
3. **Lazy Calculation**: Only calculate weights in targeted mode
4. **Efficient Sampling**: Use weighted reservoir sampling for large concept sets

---

## Testing Requirements

### Unit Tests

**Test File**: `src/tests/test_weight_calculator.py`

```python
import pytest
from datetime import datetime, timedelta
from backend.app.services.weight_calculator import WeightCalculator, ConceptWeightData

class TestWeightCalculator:

    @pytest.fixture
    def calculator(self):
        return WeightCalculator()

    def test_new_concept_gets_neutral_score(self, calculator):
        """AC4: New concepts receive default 0.5 score."""
        concepts = [{"id": "c1", "name": "Test Concept"}]
        history = []

        result = await calculator.calculate_weakness_scores(concepts, history)

        assert len(result) == 1
        assert result[0].weakness_score == 0.5
        assert result[0].category == "borderline"

    def test_weak_concept_high_score(self, calculator):
        """AC1: Concept with low ratings gets high weakness score."""
        concepts = [{"id": "c1", "name": "Weak Concept"}]
        history = [
            {"concept_id": "c1", "rating": 1, "timestamp": datetime.utcnow()},
            {"concept_id": "c1", "rating": 2, "timestamp": datetime.utcnow()},
            {"concept_id": "c1", "rating": 1, "timestamp": datetime.utcnow()}
        ]

        result = await calculator.calculate_weakness_scores(concepts, history)

        assert result[0].weakness_score >= 0.6
        assert result[0].category == "weak"
        assert result[0].failure_count == 3

    def test_mastered_concept_low_score(self, calculator):
        """AC1: Concept with high ratings gets low weakness score."""
        concepts = [{"id": "c1", "name": "Mastered Concept"}]
        history = [
            {"concept_id": "c1", "rating": 4, "timestamp": datetime.utcnow()},
            {"concept_id": "c1", "rating": 4, "timestamp": datetime.utcnow()},
            {"concept_id": "c1", "rating": 3, "timestamp": datetime.utcnow()}
        ]

        result = await calculator.calculate_weakness_scores(concepts, history)

        assert result[0].weakness_score < 0.4
        assert result[0].category == "mastered"

    def test_category_thresholds(self, calculator):
        """AC3: Verify category classification thresholds."""
        # Manually set scores and verify categories
        assert calculator.WEAK_THRESHOLD == 0.6
        assert calculator.MASTERED_THRESHOLD == 0.4
```

**Test File**: `src/tests/test_weighted_selection.py`

```python
import pytest
from unittest.mock import AsyncMock, MagicMock
from backend.app.services.review_service import ReviewService
from backend.app.services.weight_calculator import ConceptWeightData

class TestWeightedSelection:

    @pytest.fixture
    def review_service(self):
        canvas_service = MagicMock()
        task_manager = MagicMock()
        return ReviewService(canvas_service, task_manager)

    @pytest.mark.asyncio
    async def test_default_weights_70_30(self, review_service):
        """AC2: Default weights are 70% weak, 30% mastered."""
        weight_data = [
            ConceptWeightData("c1", "Weak1", 0.8, 3, 1.5, 5, 10, "weak"),
            ConceptWeightData("c2", "Weak2", 0.7, 2, 2.0, 4, 8, "weak"),
            ConceptWeightData("c3", "Mastered1", 0.2, 0, 3.8, 6, 5, "mastered"),
            ConceptWeightData("c4", "Mastered2", 0.3, 0, 3.5, 5, 7, "mastered"),
        ]

        # Run selection multiple times to verify distribution
        weak_count = 0
        mastered_count = 0
        iterations = 100

        for _ in range(iterations):
            selected = await review_service._apply_weighted_selection(
                weight_data,
                question_count=2
            )
            for c in selected:
                if c.category == "weak":
                    weak_count += 1
                elif c.category == "mastered":
                    mastered_count += 1

        # Verify distribution within 10% tolerance
        total = weak_count + mastered_count
        weak_ratio = weak_count / total
        assert 0.6 <= weak_ratio <= 0.8  # 70% ± 10%

    @pytest.mark.asyncio
    async def test_custom_weights_applied(self, review_service):
        """AC2: Custom weights override defaults."""
        weight_data = [
            ConceptWeightData("c1", "Weak1", 0.8, 3, 1.5, 5, 10, "weak"),
            ConceptWeightData("c2", "Mastered1", 0.2, 0, 3.8, 6, 5, "mastered"),
        ]

        selected = await review_service._apply_weighted_selection(
            weight_data,
            question_count=2,
            weak_weight=0.5,
            mastered_weight=0.5
        )

        # With 50/50 split, should get 1 of each
        assert len(selected) == 2

    @pytest.mark.asyncio
    async def test_weight_sum_validation(self, review_service):
        """AC2: Weights must sum to 1.0."""
        with pytest.raises(ValueError, match="must equal 1.0"):
            await review_service._apply_weighted_selection(
                [],
                question_count=2,
                weak_weight=0.6,
                mastered_weight=0.6
            )
```

### Integration Tests

**Test File**: `src/tests/integration/test_targeted_mode_integration.py`

```python
@pytest.mark.integration
async def test_targeted_mode_full_flow():
    """E2E test for targeted mode with weight algorithm."""
    # Setup: Create canvas with concepts that have review history
    # Execute: POST /review/generate-canvas with mode=targeted
    # Verify:
    #   1. Response includes weak_concepts array
    #   2. weakness_score values are calculated correctly
    #   3. weight_config shows applied weights
    #   4. Selected concepts follow weight distribution
    pass
```

### API Contract Tests

**Schemathesis**:
```bash
schemathesis run specs/api/review-api.openapi.yml \
  --endpoint "/review/generate-canvas" \
  --hypothesis-seed=42 \
  --checks all
```

### Performance Tests

```python
@pytest.mark.performance
async def test_weight_calculation_under_500ms():
    """AC6: Weight calculation for 100+ concepts < 500ms."""
    concepts = [{"id": f"c{i}", "name": f"Concept {i}"} for i in range(150)]
    history = [
        {"concept_id": f"c{i}", "rating": random.randint(1, 4), "timestamp": datetime.utcnow()}
        for i in range(150) for _ in range(5)
    ]

    calculator = WeightCalculator()

    start = time.time()
    await calculator.calculate_weakness_scores(concepts, history)
    elapsed = time.time() - start

    assert elapsed < 0.5  # 500ms
```

---

## Dependencies

### Story Dependencies
- Story 24.1 (Mode Support) - Must be completed first
- Story 24.1 provides mode parameter and basic Graphiti integration

### Technical Dependencies
- Neo4j running for Graphiti queries
- Graphiti client available (`src/agentic_rag/clients/graphiti_client.py`)
- ReviewService with mode support (Story 24.1)

### External Dependencies
- `neo4j>=5.0.0` - Graph database
- `graphiti-core>=0.6.0` - Knowledge graph SDK

---

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-12-13 | 1.0 | Initial draft - SM Agent automated batch mode | Bob (SM Agent) |

---

## Dev Agent Record

### Agent Model Used
*To be filled by Dev Agent*

### Debug Log References
*To be filled by Dev Agent*

### Completion Notes List
*To be filled by Dev Agent*

### File List
*To be filled by Dev Agent*

---

## QA Results

*Pending QA Agent review*

---

## Related Documentation

**Epic Documentation**:
- Epic 24: Verification Canvas Redesign
- Story 24.1: Mode Support (prerequisite)

**PRD References**:
- docs/prd/v118-检验白板历史关联与可选复习模式-2025-11-14-必读.md - Core requirements
- PRD FR3.7: 检验历史关联功能 (calculate_targeted_review_weights tool)

**Architecture Documentation**:
- docs/architecture/decisions/0003-graphiti-memory.md - Graphiti ADR
- docs/architecture/ebbinghaus-review-system-architecture.md - FSRS and review system design
- docs/architecture/EPIC-11-BACKEND-ARCHITECTURE.md - Backend layer structure

**API Specifications**:
- specs/api/review-api.openapi.yml - Review API OpenAPI spec (L215-257, L703-719)
- specs/data/review-generate-request.schema.json - Request schema

**Skills References**:
- .claude/skills/graphiti/SKILL.md - Graphiti documentation
