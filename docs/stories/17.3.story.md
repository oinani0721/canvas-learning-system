# Story 17.3: 告警系统和Dashboard实现

## Status
Completed (QA-PASS 2025-12-03, 39/39 tests)

## Story

**As a** Canvas Learning System运维工程师和开发者,
**I want** 拥有完整的告警系统和监控Dashboard,
**so that** 我可以在性能问题发生时及时收到通知、快速定位问题、并通过可视化界面实时了解系统健康状态。

## Acceptance Criteria

1. 告警规则配置：实现5种核心告警规则（HighAPILatency, HighErrorRate, AgentExecutionSlow, MemorySystemDown, HighConcurrentTasks）
2. 告警级别分类：告警分为Critical（红）、Warning（橙）、Info（蓝）三级，响应时间分别为立即、15分钟、1小时
3. 告警触发机制：告警在条件满足并持续指定时间（for duration）后触发，支持告警恢复
4. 告警API端点：`/metrics/alerts` 返回当前活跃告警列表，符合OpenAPI规范定义的Alert Schema
5. 通知渠道：支持控制台日志、文件日志、Obsidian插件通知三种渠道
6. Dashboard数据API：`/metrics/summary` 返回完整的MetricsSummary，支持Dashboard展示
7. AlertManager集成：实现告警评估循环（每30秒），支持告警状态管理（firing, resolved）
8. 所有代码包含文档来源标注(Context7/Skill/ADR验证)

## Tasks / Subtasks

- [ ] Task 1: 告警规则引擎实现 (AC: 1, 2, 3)
  - [ ] 创建 `backend/app/services/alert_manager.py`
  - [ ] 定义 `AlertRule` 数据类 (name, expr, for_duration, severity, annotations)
  - [ ] 定义 `Alert` 数据类 (id, name, severity, message, triggered_at, value, threshold, labels)
  - [ ] 定义 `AlertSeverity` 枚举 (critical, warning, info)
  - [ ] 实现 `AlertManager` 类 (evaluate, fire_alert, resolve_alert, get_active_alerts)
  - [ ] 实现 `PromQLEvaluator` 简化版 (支持比较表达式和rate计算)

- [ ] Task 2: 核心告警规则配置 (AC: 1)
  - [ ] 创建 `config/alerts.yaml` 告警规则配置文件
  - [ ] 实现 HighAPILatency 规则: P95延迟>1s持续5分钟
  - [ ] 实现 HighErrorRate 规则: 5分钟错误率>5%持续2分钟
  - [ ] 实现 AgentExecutionSlow 规则: Agent P95执行>10s持续5分钟
  - [ ] 实现 MemorySystemDown 规则: 记忆系统连接失败持续1分钟
  - [ ] 实现 HighConcurrentTasks 规则: 并发任务>100持续2分钟

- [ ] Task 3: 告警评估循环 (AC: 3, 7)
  - [ ] 实现后台告警评估任务 (asyncio.create_task)
  - [ ] 实现告警状态转换 (pending → firing → resolved)
  - [ ] 实现告警去重 (fingerprint based)
  - [ ] 实现告警恢复检测
  - [ ] 配置评估间隔 (30秒)

- [ ] Task 4: 告警API端点 (AC: 4)
  - [ ] 创建 `backend/app/api/v1/endpoints/monitoring.py`
  - [ ] 实现 `GET /metrics/alerts` 端点
  - [ ] 返回格式: `{ alerts: Alert[], total: integer }`
  - [ ] 支持severity过滤参数 (可选)
  - [ ] 支持limit/offset分页 (可选)

- [ ] Task 5: 通知渠道实现 (AC: 5)
  - [ ] 创建 `backend/app/services/notification_channels.py`
  - [ ] 实现 `NotificationChannel` 抽象基类
  - [ ] 实现 `ConsoleNotificationChannel` (structlog日志)
  - [ ] 实现 `FileNotificationChannel` (logs/alerts.log)
  - [ ] 实现 `ObsidianNotificationChannel` (SSE推送，依赖ADR-006)
  - [ ] 实现 `NotificationDispatcher` (路由告警到各渠道)

- [ ] Task 6: Dashboard数据提供 (AC: 6)
  - [ ] 扩展 `backend/app/services/metrics_collector.py`
  - [ ] 实现 `get_dashboard_data()` 方法
  - [ ] 返回字段: api, agents, memory_system, resources, alerts
  - [ ] 计算统计数据: requests_per_second, avg_latency_ms, p95_latency_ms, error_rate

- [ ] Task 7: 测试和验证 (AC: 8)
  - [ ] 创建 `tests/unit/test_alert_manager.py`
  - [ ] 创建 `tests/unit/test_notification_channels.py`
  - [ ] 创建 `tests/integration/test_alerts_api.py`
  - [ ] 测试告警触发和恢复逻辑
  - [ ] 测试通知渠道分发
  - [ ] 确认所有代码有文档来源标注

## Dev Notes

### 技术验证报告 (Step 3.6)

**验证完成时间**: 2025-12-03
**验证执行人**: SM Agent (Bob)
**Quality Gate状态**: Pending

#### 技术栈清单

| 技术栈 | 查询方式 | 验证状态 | 文档位置 |
|--------|---------|---------|----------|
| prometheus_client | Context7 | 待验证 | /prometheus/client_python |
| structlog | Local ADR | 已验证 | ADR-010-LOGGING-AGGREGATION-STRUCTLOG.md |
| FastAPI BackgroundTasks | Context7 | 待验证 | /fastapi/fastapi |
| asyncio | Context7 | 待验证 | /python/cpython (asyncio) |
| pydantic | Context7 | 待验证 | /pydantic/pydantic |
| SSE (Server-Sent Events) | Local ADR | 已验证 | ADR-006-REALTIME-COMMUNICATION-SSE-HTTP.md |
| YAML配置 | Context7 | 待验证 | PyYAML |

#### 核心API验证待办

**开发前必须验证**:
- [ ] prometheus_client REGISTRY.get_sample_value() → Context7: /prometheus/client_python
- [ ] prometheus_client Histogram quantile calculation → Context7: /prometheus/client_python
- [ ] asyncio.create_task() for background evaluation loop → Context7: /python/cpython
- [ ] pydantic BaseModel for Alert/AlertRule schemas → Context7: /pydantic/pydantic
- [ ] FastAPI BackgroundTasks for async notification → Context7: /fastapi/fastapi
- [ ] structlog get_logger() and bind() → ADR-010:77-100
- [ ] SSE EventSourceResponse for Obsidian notification → ADR-006

### SDD规范参考 (必填)

**API端点规范** (Epic 17 Monitoring):
- `GET /metrics/alerts` - 获取当前活跃告警
  - [Source: specs/api/canvas-api.openapi.yml:644-662]
  - 响应格式:
    ```json
    {
      "alerts": [Alert],
      "total": integer
    }
    ```
  - 请求参数 (可选):
    - `severity`: 过滤告警级别 (critical, warning, info)
    - `limit`: 返回数量限制
    - `offset`: 分页偏移

- `GET /metrics/summary` - 获取指标摘要 (Dashboard数据源)
  - [Source: specs/api/canvas-api.openapi.yml:630-642]
  - 响应Schema: `MetricsSummary`

**Alert Schema** (本Story实现重点):
- [Source: specs/api/canvas-api.openapi.yml:1062-1091]
- 字段:
  - `id`: string (必填) - 告警唯一标识
  - `name`: string (必填) - 告警名称 (e.g., "HighAPILatency")
  - `severity`: enum (必填) - 告警级别 ["critical", "warning", "info"]
  - `message`: string (必填) - 告警消息
  - `triggered_at`: string/date-time (必填) - 触发时间
  - `value`: number (可选) - 当前触发值
  - `threshold`: number (可选) - 告警阈值
  - `labels`: object (可选) - 附加标签 (e.g., {"agent_type": "scoring-agent"})

**MetricsSummary Schema** (Dashboard数据):
- [Source: specs/api/canvas-api.openapi.yml:987-1060]
- 字段:
  - `timestamp`: ISO 8601时间戳
  - `api`: API性能统计
    - `requests_total`, `requests_per_second`, `avg_latency_ms`, `p95_latency_ms`, `error_rate`
  - `agents`: Agent执行统计
    - `invocations_total`, `avg_execution_time_s`, `by_type`
  - `memory_system`: 记忆系统统计
    - `graphiti`, `temporal`, `semantic` (各含 queries_total, avg_latency_ms)
  - `resources`: 资源使用
    - `cpu_usage_percent`, `memory_usage_percent`, `disk_usage_percent`

### ADR决策关联 (必填)

| ADR编号 | 决策标题 | 对Story的影响 |
|---------|----------|---------------|
| ADR-0006 | Realtime Communication (SSE+HTTP) | Obsidian通知渠道使用SSE推送告警 |
| ADR-0009 | Error Handling & Retry | 告警与ErrorCode体系关联 (1xxx-5xxx错误码触发告警) |
| ADR-0010 | Logging Aggregation | 告警日志使用structlog格式记录 |

**关键约束**:
- 告警消息必须通过structlog记录 (ADR-010)
- Obsidian通知使用SSE推送 (ADR-006:127-158)
- 告警与错误码映射: LLM错误(1xxx)→Critical, 数据库错误(2xxx)→Warning, Agent错误(5xxx)→Warning

### 架构设计参考

**架构文档引用**:
- 告警策略: [Source: docs/architecture/performance-monitoring-architecture.md:269-333]
- 告警级别定义: [Source: docs/architecture/performance-monitoring-architecture.md:271-278]
- 告警规则配置: [Source: docs/architecture/performance-monitoring-architecture.md:279-323]
- Dashboard布局: [Source: docs/architecture/performance-monitoring-architecture.md:401-440]
- 监控API端点: [Source: docs/architecture/performance-monitoring-architecture.md:443-484]
- 实施计划Phase 3: [Source: docs/architecture/performance-monitoring-architecture.md:502-507]

**告警级别定义** (Source: docs/architecture/performance-monitoring-architecture.md:271-278):
| 级别 | 颜色 | 触发条件 | 响应时间 |
|------|------|---------|---------|
| Critical | 红色 | 服务不可用、数据丢失风险 | 立即 |
| Warning | 橙色 | 性能下降、资源紧张 | 15分钟 |
| Info | 蓝色 | 异常但不影响服务 | 1小时 |

**告警规则详情** (Source: docs/architecture/performance-monitoring-architecture.md:281-323):
| 告警名称 | 表达式 | 持续时间 | 级别 |
|----------|--------|---------|------|
| HighAPILatency | `canvas_api_request_latency_seconds{quantile="0.95"} > 1.0` | 5m | warning |
| HighErrorRate | `rate(canvas_api_requests_total{status=~"5.."}[5m]) / rate(canvas_api_requests_total[5m]) > 0.05` | 2m | critical |
| AgentExecutionSlow | `canvas_agent_execution_seconds{quantile="0.95"} > 10` | 5m | warning |
| MemorySystemDown | `up{job="memory_system"} == 0` | 1m | critical |
| HighConcurrentTasks | `canvas_api_concurrent_requests > 100` | 2m | warning |

**通知渠道** (Source: docs/architecture/performance-monitoring-architecture.md:325-333):
| 渠道 | 用途 | 配置 |
|------|------|------|
| 控制台日志 | 开发调试 | 默认启用 |
| 文件日志 | 历史记录 | `logs/alerts.log` |
| Obsidian通知 | 用户提醒 | Plugin内置SSE |
| Webhook | 自定义集成 | 可选 (P2) |

**Dashboard面板** (Source: docs/architecture/performance-monitoring-architecture.md:433-440):
1. **概览面板**: 请求量、延迟、错误率
2. **API性能**: 各端点响应时间分布
3. **Agent性能**: 各Agent执行时间对比
4. **记忆系统**: 各层查询延迟
5. **资源监控**: CPU、内存、磁盘IO

### 代码示例库

**AlertManager核心实现** (Source: docs/architecture/performance-monitoring-architecture.md:281-323):
```python
# backend/app/services/alert_manager.py
# ✅ Verified from Architecture Doc (performance-monitoring-architecture.md:281-323)
# ⚠️ prometheus_client REGISTRY API需Context7验证
import asyncio
from datetime import datetime, timedelta
from typing import Dict, List, Optional
from enum import Enum
from dataclasses import dataclass, field
from prometheus_client import REGISTRY
import structlog
import hashlib

logger = structlog.get_logger(__name__)

class AlertSeverity(Enum):
    """告警级别"""
    CRITICAL = "critical"
    WARNING = "warning"
    INFO = "info"

class AlertState(Enum):
    """告警状态"""
    PENDING = "pending"    # 条件满足，等待持续时间
    FIRING = "firing"      # 告警触发中
    RESOLVED = "resolved"  # 告警已恢复

@dataclass
class AlertRule:
    """告警规则"""
    name: str
    expression: str           # 简化的PromQL表达式
    for_duration: int         # 持续时间(秒)
    severity: AlertSeverity
    summary: str
    description: str
    labels: Dict[str, str] = field(default_factory=dict)

@dataclass
class Alert:
    """告警实例"""
    id: str
    name: str
    severity: AlertSeverity
    message: str
    triggered_at: datetime
    value: Optional[float] = None
    threshold: Optional[float] = None
    labels: Dict[str, str] = field(default_factory=dict)
    state: AlertState = AlertState.PENDING
    pending_since: Optional[datetime] = None

    def to_dict(self) -> dict:
        """转换为API响应格式"""
        return {
            "id": self.id,
            "name": self.name,
            "severity": self.severity.value,
            "message": self.message,
            "triggered_at": self.triggered_at.isoformat(),
            "value": self.value,
            "threshold": self.threshold,
            "labels": self.labels,
        }

class AlertManager:
    """告警管理器

    负责告警规则评估、告警状态管理、通知分发。
    评估间隔: 30秒
    """

    def __init__(
        self,
        rules: List[AlertRule],
        notification_dispatcher: "NotificationDispatcher",
        evaluation_interval: int = 30,
    ):
        self.rules = rules
        self.notification_dispatcher = notification_dispatcher
        self.evaluation_interval = evaluation_interval
        self._active_alerts: Dict[str, Alert] = {}
        self._running = False
        self._task: Optional[asyncio.Task] = None

    async def start(self):
        """启动告警评估循环"""
        self._running = True
        self._task = asyncio.create_task(self._evaluation_loop())
        logger.info("alert_manager.started", interval=self.evaluation_interval)

    async def stop(self):
        """停止告警评估"""
        self._running = False
        if self._task:
            self._task.cancel()
            try:
                await self._task
            except asyncio.CancelledError:
                pass
        logger.info("alert_manager.stopped")

    async def _evaluation_loop(self):
        """告警评估循环"""
        while self._running:
            try:
                await self._evaluate_all_rules()
            except Exception as e:
                logger.error("alert_manager.evaluation_error", error=str(e))

            await asyncio.sleep(self.evaluation_interval)

    async def _evaluate_all_rules(self):
        """评估所有告警规则"""
        now = datetime.now()

        for rule in self.rules:
            fingerprint = self._generate_fingerprint(rule)

            try:
                is_firing, value = self._evaluate_expression(rule.expression)
            except Exception as e:
                logger.warning(
                    "alert_manager.rule_evaluation_failed",
                    rule=rule.name,
                    error=str(e),
                )
                continue

            existing_alert = self._active_alerts.get(fingerprint)

            if is_firing:
                if existing_alert is None:
                    # 新告警，进入pending状态
                    alert = Alert(
                        id=fingerprint,
                        name=rule.name,
                        severity=rule.severity,
                        message=rule.description.format(value=value),
                        triggered_at=now,
                        value=value,
                        threshold=self._extract_threshold(rule.expression),
                        labels=rule.labels,
                        state=AlertState.PENDING,
                        pending_since=now,
                    )
                    self._active_alerts[fingerprint] = alert
                    logger.debug("alert_manager.alert_pending", alert=rule.name)

                elif existing_alert.state == AlertState.PENDING:
                    # 检查是否满足持续时间
                    if (now - existing_alert.pending_since).seconds >= rule.for_duration:
                        existing_alert.state = AlertState.FIRING
                        existing_alert.triggered_at = now
                        await self._fire_alert(existing_alert)

            else:
                if existing_alert and existing_alert.state == AlertState.FIRING:
                    # 告警恢复
                    await self._resolve_alert(existing_alert)
                    del self._active_alerts[fingerprint]
                elif existing_alert:
                    # 未触发的pending告警，移除
                    del self._active_alerts[fingerprint]

    def _evaluate_expression(self, expression: str) -> tuple[bool, float]:
        """评估简化的PromQL表达式

        支持格式:
        - metric_name > threshold
        - metric_name{label="value"} > threshold
        - rate(metric_name[5m]) > threshold

        Returns:
            (is_firing, current_value)
        """
        # 简化实现: 解析metric_name和threshold
        # 实际生产应使用完整PromQL解析器

        if ">" in expression:
            parts = expression.split(">")
            metric_part = parts[0].strip()
            threshold = float(parts[1].strip())

            # 获取metric值
            value = self._get_metric_value(metric_part)
            return value > threshold, value

        elif "<" in expression:
            parts = expression.split("<")
            metric_part = parts[0].strip()
            threshold = float(parts[1].strip())

            value = self._get_metric_value(metric_part)
            return value < threshold, value

        elif "==" in expression:
            parts = expression.split("==")
            metric_part = parts[0].strip()
            threshold = float(parts[1].strip())

            value = self._get_metric_value(metric_part)
            return value == threshold, value

        return False, 0.0

    def _get_metric_value(self, metric_expr: str) -> float:
        """从Prometheus Registry获取指标值

        ⚠️ 需要Context7验证: prometheus_client REGISTRY API
        """
        # 解析metric名称和labels
        # 简化实现: 直接从REGISTRY获取

        # 处理rate()函数
        if metric_expr.startswith("rate("):
            # rate计算需要时间序列，简化为获取Counter差值
            return 0.0  # TODO: 实现rate计算

        # 处理quantile
        if "{quantile=" in metric_expr:
            # Histogram quantile
            return 0.0  # TODO: 实现quantile计算

        # 直接metric
        metric_name = metric_expr.split("{")[0].strip()
        labels = {}

        if "{" in metric_expr:
            label_part = metric_expr.split("{")[1].rstrip("}")
            for pair in label_part.split(","):
                key, value = pair.split("=")
                labels[key.strip()] = value.strip().strip('"')

        value = REGISTRY.get_sample_value(metric_name, labels) or 0.0
        return value

    def _extract_threshold(self, expression: str) -> Optional[float]:
        """从表达式提取阈值"""
        for op in [">", "<", "=="]:
            if op in expression:
                return float(expression.split(op)[1].strip())
        return None

    def _generate_fingerprint(self, rule: AlertRule) -> str:
        """生成告警指纹用于去重"""
        content = f"{rule.name}:{rule.expression}:{rule.labels}"
        return hashlib.md5(content.encode()).hexdigest()[:12]

    async def _fire_alert(self, alert: Alert):
        """触发告警通知"""
        logger.warning(
            "alert_manager.alert_fired",
            alert_id=alert.id,
            name=alert.name,
            severity=alert.severity.value,
            value=alert.value,
            threshold=alert.threshold,
        )
        await self.notification_dispatcher.dispatch(alert, "fired")

    async def _resolve_alert(self, alert: Alert):
        """告警恢复通知"""
        logger.info(
            "alert_manager.alert_resolved",
            alert_id=alert.id,
            name=alert.name,
        )
        alert.state = AlertState.RESOLVED
        await self.notification_dispatcher.dispatch(alert, "resolved")

    def get_active_alerts(
        self,
        severity: Optional[AlertSeverity] = None,
    ) -> List[Alert]:
        """获取活跃告警列表"""
        alerts = [
            a for a in self._active_alerts.values()
            if a.state == AlertState.FIRING
        ]

        if severity:
            alerts = [a for a in alerts if a.severity == severity]

        return sorted(alerts, key=lambda a: a.triggered_at, reverse=True)
```

**通知渠道实现** (Source: docs/architecture/performance-monitoring-architecture.md:325-333):
```python
# backend/app/services/notification_channels.py
# ✅ Verified from Architecture Doc (performance-monitoring-architecture.md:325-333)
# ✅ Verified from ADR-010 (structlog logging)
# ⚠️ SSE实现需参考ADR-006
from abc import ABC, abstractmethod
from typing import List
import structlog
from pathlib import Path

logger = structlog.get_logger(__name__)

class NotificationChannel(ABC):
    """通知渠道抽象基类"""

    @abstractmethod
    async def send(self, alert: "Alert", event_type: str) -> bool:
        """发送通知

        Args:
            alert: 告警实例
            event_type: "fired" 或 "resolved"

        Returns:
            是否发送成功
        """
        pass

class ConsoleNotificationChannel(NotificationChannel):
    """控制台日志通知渠道

    使用structlog记录告警到控制台/日志文件。
    Source: ADR-010-LOGGING-AGGREGATION-STRUCTLOG.md:77-100
    """

    async def send(self, alert: "Alert", event_type: str) -> bool:
        log_method = logger.warning if event_type == "fired" else logger.info
        log_method(
            f"alert.{event_type}",
            alert_id=alert.id,
            name=alert.name,
            severity=alert.severity.value,
            message=alert.message,
            value=alert.value,
            threshold=alert.threshold,
        )
        return True

class FileNotificationChannel(NotificationChannel):
    """文件日志通知渠道

    告警记录到专用文件: logs/alerts.log
    """

    def __init__(self, log_path: str = "logs/alerts.log"):
        self.log_path = Path(log_path)
        self.log_path.parent.mkdir(parents=True, exist_ok=True)

    async def send(self, alert: "Alert", event_type: str) -> bool:
        try:
            with open(self.log_path, "a", encoding="utf-8") as f:
                f.write(
                    f"{alert.triggered_at.isoformat()} | "
                    f"{event_type.upper()} | "
                    f"{alert.severity.value.upper()} | "
                    f"{alert.name} | "
                    f"{alert.message}\n"
                )
            return True
        except Exception as e:
            logger.error("file_notification.failed", error=str(e))
            return False

class ObsidianNotificationChannel(NotificationChannel):
    """Obsidian插件SSE通知渠道

    通过SSE推送告警到Obsidian插件。
    Source: ADR-006-REALTIME-COMMUNICATION-SSE-HTTP.md:127-158

    ⚠️ 依赖SSE连接管理器 (由Story 15.x实现)
    """

    def __init__(self, sse_manager: "SSEConnectionManager"):
        self.sse_manager = sse_manager

    async def send(self, alert: "Alert", event_type: str) -> bool:
        try:
            await self.sse_manager.broadcast({
                "type": f"alert.{event_type}",
                "data": alert.to_dict(),
            })
            return True
        except Exception as e:
            logger.error("obsidian_notification.failed", error=str(e))
            return False

class NotificationDispatcher:
    """通知分发器

    将告警路由到配置的通知渠道。
    """

    def __init__(self, channels: List[NotificationChannel]):
        self.channels = channels

    async def dispatch(self, alert: "Alert", event_type: str):
        """分发告警到所有渠道"""
        for channel in self.channels:
            try:
                await channel.send(alert, event_type)
            except Exception as e:
                logger.error(
                    "notification_dispatch.failed",
                    channel=type(channel).__name__,
                    error=str(e),
                )
```

**告警规则配置文件** (Source: docs/architecture/performance-monitoring-architecture.md:281-323):
```yaml
# config/alerts.yaml
# ✅ Verified from Architecture Doc (performance-monitoring-architecture.md:281-323)
alerts:
  - name: HighAPILatency
    expression: 'canvas_api_request_latency_seconds{quantile="0.95"} > 1.0'
    for: 300  # 5分钟
    severity: warning
    summary: "API响应时间过高"
    description: "95分位API响应时间超过1秒，当前值: {value}s"

  - name: HighErrorRate
    expression: 'rate(canvas_api_requests_total{status=~"5.."}[5m]) / rate(canvas_api_requests_total[5m]) > 0.05'
    for: 120  # 2分钟
    severity: critical
    summary: "错误率过高"
    description: "5分钟内错误率超过5%，当前值: {value:.2%}"

  - name: AgentExecutionSlow
    expression: 'canvas_agent_execution_seconds{quantile="0.95"} > 10'
    for: 300  # 5分钟
    severity: warning
    summary: "Agent执行过慢"
    description: "Agent 95分位执行时间超过10秒，当前值: {value}s"
    labels:
      component: agent

  - name: MemorySystemDown
    expression: 'up{job="memory_system"} == 0'
    for: 60  # 1分钟
    severity: critical
    summary: "记忆系统不可用"
    description: "记忆系统连接失败，请检查Neo4j/LanceDB服务"
    labels:
      component: memory

  - name: HighConcurrentTasks
    expression: 'canvas_api_concurrent_requests > 100'
    for: 120  # 2分钟
    severity: warning
    summary: "并发任务过多"
    description: "当前并发任务数: {value}，可能导致性能下降"
```

**告警API端点** (Source: specs/api/canvas-api.openapi.yml:644-662):
```python
# backend/app/api/v1/endpoints/monitoring.py
# ✅ Verified from OpenAPI Spec (canvas-api.openapi.yml:644-662)
from fastapi import APIRouter, Query, Depends
from typing import Optional, List
from pydantic import BaseModel
from datetime import datetime

router = APIRouter(prefix="/metrics", tags=["Monitoring"])

class AlertResponse(BaseModel):
    """告警响应模型 (OpenAPI: Alert schema)"""
    id: str
    name: str
    severity: str  # critical, warning, info
    message: str
    triggered_at: datetime
    value: Optional[float] = None
    threshold: Optional[float] = None
    labels: dict = {}

class AlertListResponse(BaseModel):
    """告警列表响应"""
    alerts: List[AlertResponse]
    total: int

@router.get("/alerts", response_model=AlertListResponse)
async def get_active_alerts(
    severity: Optional[str] = Query(None, description="过滤告警级别"),
    limit: int = Query(100, ge=1, le=1000),
    offset: int = Query(0, ge=0),
    alert_manager: "AlertManager" = Depends(get_alert_manager),
):
    """获取当前活跃告警

    Source: specs/api/canvas-api.openapi.yml:644-662

    Returns:
        AlertListResponse: 告警列表和总数
    """
    severity_enum = None
    if severity:
        severity_enum = AlertSeverity(severity)

    alerts = alert_manager.get_active_alerts(severity=severity_enum)

    # 分页
    paginated = alerts[offset:offset + limit]

    return AlertListResponse(
        alerts=[AlertResponse(**a.to_dict()) for a in paginated],
        total=len(alerts),
    )
```

**Dashboard数据收集** (Source: specs/api/canvas-api.openapi.yml:987-1060):
```python
# backend/app/services/metrics_collector.py (扩展)
# ✅ Verified from OpenAPI Spec (canvas-api.openapi.yml:987-1060)
from prometheus_client import REGISTRY
from datetime import datetime
from typing import Dict, Any

def get_dashboard_data(alert_manager: "AlertManager") -> Dict[str, Any]:
    """获取Dashboard展示数据

    Returns:
        MetricsSummary格式数据
    """
    now = datetime.now()

    return {
        "timestamp": now.isoformat(),
        "api": get_api_metrics_summary(),
        "agents": get_agent_metrics_summary(),
        "memory_system": get_memory_metrics_summary(),
        "resources": get_resource_metrics_summary(),
        "alerts": {
            "active_count": len(alert_manager.get_active_alerts()),
            "critical_count": len(alert_manager.get_active_alerts(AlertSeverity.CRITICAL)),
            "warning_count": len(alert_manager.get_active_alerts(AlertSeverity.WARNING)),
        },
    }

def get_api_metrics_summary() -> Dict[str, Any]:
    """获取API指标摘要"""
    # 从Prometheus REGISTRY读取
    requests_total = REGISTRY.get_sample_value(
        'canvas_api_requests_total'
    ) or 0

    return {
        "requests_total": int(requests_total),
        "requests_per_second": 0,  # 需要计算rate
        "avg_latency_ms": 0,       # 需要从Histogram计算
        "p95_latency_ms": 0,       # 需要从Histogram计算
        "error_rate": 0,           # 需要计算
    }
```

### 文件路径参考

**现有监控代码** (Story 17.1/17.2产出):
- 监控中间件: `backend/app/middleware/metrics.py`
- Agent指标: `backend/app/middleware/agent_metrics.py`
- 记忆指标: `backend/app/middleware/memory_metrics.py`
- 资源监控: `backend/app/services/resource_monitor.py`
- 指标端点: `backend/app/api/v1/endpoints/health.py`

**需要新增的文件**:
- 新增: `backend/app/services/alert_manager.py`
- 新增: `backend/app/services/notification_channels.py`
- 新增: `backend/app/api/v1/endpoints/monitoring.py`
- 新增: `config/alerts.yaml`
- 修改: `backend/app/services/metrics_collector.py` (扩展Dashboard数据)
- 修改: `backend/app/main.py` (启动AlertManager)
- 新增: `tests/unit/test_alert_manager.py`
- 新增: `tests/unit/test_notification_channels.py`
- 新增: `tests/integration/test_alerts_api.py`

**现有依赖代码**:
- SSE连接管理: `backend/app/services/sse_manager.py` (ADR-006产出)
- structlog配置: `src/logging_config.py` (ADR-010产出)

### 依赖关系

**前置Story**:
- Story 17.1: 基础监控 (提供metrics_middleware, Prometheus指标端点)
- Story 17.2: 深度监控 (提供Agent/Memory/Resource指标)

**后续影响**:
- Story 17.4: 性能优化策略实施 (使用告警数据识别性能瓶颈)
- Story 17.5: E2E测试 (测试告警触发准确性)

### Definition of Done

- [ ] 所有AC通过验收测试
- [ ] 5种核心告警规则全部实现并可配置
- [ ] AlertManager后台评估循环稳定运行 (30秒间隔)
- [ ] `/metrics/alerts` 端点返回正确的Alert格式
- [ ] 3种通知渠道 (Console/File/Obsidian) 全部可用
- [ ] Dashboard数据API返回完整MetricsSummary
- [ ] 告警触发/恢复逻辑正确
- [ ] 单元测试覆盖率≥90%
- [ ] 集成测试通过
- [ ] 代码Review通过
- [ ] 文档更新完成

### 风险和缓解

| 风险 | 概率 | 影响 | 缓解措施 |
|------|------|------|---------|
| PromQL表达式解析复杂 | 高 | 中 | 使用简化解析器，仅支持核心表达式 |
| 告警评估影响性能 | 低 | 中 | 异步评估，30秒间隔，避免阻塞 |
| SSE连接不稳定 | 中 | 低 | Obsidian通知作为可选渠道，Console/File为主 |
| 告警误报 | 中 | 中 | 设置合理的for duration，测试验证阈值 |
| 告警风暴 | 低 | 高 | 实现告警去重 (fingerprint)，限制通知频率 |

### 估算

- **Story Points**: 5
- **预估工时**: 2-3天
- **复杂度**: 中 (主要是告警规则引擎和通知分发)

---

**文档版本**: v1.0.0
**创建时间**: 2025-12-03
**创建者**: SM Agent (Bob)
**Epic**: Epic 17 - 性能优化和监控
